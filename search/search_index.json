{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LatteReview \ud83e\udd16\u2615","text":"<p>A framework for multi-agent review workflows using large language models.</p>"},{"location":"#overview","title":"Overview","text":"<p>LatteReview is a Python framework that enables you to create and manage multi-agent review workflows using various large language models. It provides a flexible and extensible architecture for implementing different types of review processes, from simple single-agent reviews to complex multi-stage workflows with multiple agents.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Support for multiple LLM providers (OpenAI, Ollama, and most models provided by LiteLLM)</li> <li>Flexible agent-based architecture</li> <li>Configurable review workflows</li> <li>Asynchronous processing with concurrent requests</li> <li>Cost tracking and monitoring</li> <li>Comprehensive error handling and validation</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> <li>GitHub Repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#author","title":"Author","text":"<p>Pouria Rouzrokh, MD, MPH, MHPE Medical Practitioner and Machine Learning Engineer Incoming Radiology Resident @Yale University Former Data Scientist @Mayo Clinic AI Lab</p> <p>Find my work:  </p>"},{"location":"#support-lattereview","title":"Support LatteReview","text":"<p>If you find LatteReview helpful in your research or work, consider supporting its continued development. Since we're already sharing a virtual coffee break while reviewing papers, maybe you'd like to treat me to a real one? \u2615 \ud83d\ude0a</p>"},{"location":"#ways-to-support","title":"Ways to Support:","text":"<ul> <li>Treat me to a coffee on Ko-fi \u2615</li> <li>Star the repository to help others discover the project</li> <li>Submit bug reports, feature requests, or contribute code</li> <li>Share your experience using LatteReview in your research</li> </ul>"},{"location":"about/","title":"\ud83d\udca1About","text":""},{"location":"about/#author","title":"\ud83d\udc68\u200d\ud83d\udcbb Author","text":"<p>Pouria Rouzrokh, MD, MPH, MHPE Medical Practitioner and Machine Learning Engineer Incoming Radiology Resident @Yale University Former Data Scientist @Mayo Clinic AI Lab</p> <p>Find my work:  </p>"},{"location":"about/#support-lattereview","title":"\u2764\ufe0f Support LatteReview","text":"<p>If you find LatteReview helpful in your research or work, consider supporting its continued development. Since we're already sharing a virtual coffee break while reviewing papers, maybe you'd like to treat me to a real one? \u2615 \ud83d\ude0a</p>"},{"location":"about/#ways-to-support","title":"Ways to Support:","text":"<ul> <li>Treat me to a coffee on Ko-fi \u2615</li> <li>Star the repository to help others discover the project</li> <li>Submit bug reports, feature requests, or contribute code</li> <li>Share your experience using LatteReview in your research</li> </ul>"},{"location":"about/#license","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"about/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please feel free to submit a Pull Request.</p>"},{"location":"about/#citation","title":"\ud83d\udcda Citation","text":"<p>If you use LatteReview in your research, please cite our paper:</p> <pre><code># Preprint citation to be added\n</code></pre>"},{"location":"installation/","title":"\ud83d\udee0\ufe0fInstallation","text":"<p>There are several ways to install LatteReview:</p>"},{"location":"installation/#1-install-from-pypi-recommended","title":"1. Install from PyPI (Recommended)","text":"<pre><code>pip install lattereview\n</code></pre> <p>You can also install additional features using these extras:</p> <pre><code># Development tools\npip install \"lattereview[dev]\"\n\n# Documentation tools\npip install \"lattereview[docs]\"\n\n# All extras\npip install \"lattereview[all]\"\n</code></pre>"},{"location":"installation/#2-install-from-source-code","title":"2. Install from Source Code","text":""},{"location":"installation/#option-a-using-git","title":"Option A: Using Git","text":"<pre><code># Clone the repository\ngit clone https://github.com/PouriaRouzrokh/LatteReview.git\ncd LatteReview\n</code></pre>"},{"location":"installation/#option-b-using-zip-download","title":"Option B: Using ZIP Download","text":"<ol> <li>Go to https://github.com/PouriaRouzrokh/LatteReview</li> <li>Click the green \"Code\" button</li> <li>Select \"Download ZIP\"</li> <li>Extract the ZIP file and navigate to the directory:</li> </ol> <pre><code>cd path/to/LatteReview-main\n</code></pre> <p>After obtaining the source code through either option, you can install it using one of these methods:</p> <pre><code># Basic installation\npip install .\n\n# Install from specific versions of dependencies mentioned in requirements.txt\npip install -r requirements.txt\n\n# Development installation (all optional dependencies)\npip install -e \".[all]\"\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import lattereview\nprint(lattereview.__version__)\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or later</li> <li>Core dependencies (automatically installed):</li> <li>litellm (&gt;=1.55.2)</li> <li>openai (&gt;=1.57.4)</li> <li>pandas (&gt;=2.2.3)</li> <li>pydantic (&gt;=2.10.3)</li> <li>And others as specified in <code>setup.py</code></li> </ul>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter installation issues:</p> <pre><code># Check Python version\npython --version  # Should be 3.9 or later\n\n# Update pip\npip install --upgrade pip\n\n# Install build dependencies\npip install build wheel setuptools\n</code></pre>"},{"location":"quickstart/","title":"\ud83d\ude80 Quick Start","text":"<p>LatteReview enables you to create custom literature review workflows with multiple AI reviewers. Each reviewer can use different models and providers based on your needs. Please follow the below steps to perform a review task with LatteReview.</p>"},{"location":"quickstart/#step-1-set-up-api-keys","title":"Step 1: Set Up API Keys","text":"<p>To use LatteReview with different LLM engines (OpenAI, Anthropic, Google, etc.), you'll need to set up the API keys for the specific providers you plan to use. For example, if you're only using OpenAI models, you only need the OpenAI API key. Here are three ways to set up your API keys:</p> <ol> <li>Using python-dotenv (Recommended):</li> <li>Create a <code>.env</code> file in your project directory and add only the keys you need:</li> </ol> <pre><code># .env file - Example keys (add only what you need)\nOPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre> <ul> <li>Load it in your code:</li> </ul> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Load before importing any providers\n</code></pre> <ol> <li>Setting environment variables directly:</li> </ol> <pre><code># Example: Set only the keys for providers you're using\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre> <ol> <li>Passing API keys directly to providers (supported by some providers):</li> </ol> <pre><code>from lattereview.providers import OpenAIProvider\nprovider = OpenAIProvider(api_key=\"your-openai-key\")  # Optional, will use environment variable if not provided\n</code></pre> <p>Note: No API keys are needed if you're exclusively using local models through Ollama.</p>"},{"location":"quickstart/#step-2-prepare-your-data","title":"Step 2: Prepare Your Data","text":"<p>Your input data should be in a CSV or Excel file with columns for the content you want to review. The column names should match the <code>inputs</code> specified in your workflow:</p> <pre><code># Load your data\ndata = pd.read_excel(\"articles.xlsx\")\n\n# Example data structure:\ndata = pd.DataFrame({\n    'title': ['Paper 1 Title', 'Paper 2 Title'],\n    'abstract': ['Paper 1 Abstract', 'Paper 2 Abstract']\n})\n</code></pre>"},{"location":"quickstart/#step-3-create-reviewers","title":"Step 3: Create Reviewers","text":"<p>Create reviewer agents by configuring <code>ScoringReviewer</code> objects. Each reviewer needs:</p> <ul> <li>A provider (e.g., LiteLLMProvider, OpenAIProvider, OllamaProvider)</li> <li>A unique name</li> <li>A scoring task and rules</li> <li>Optional configuration like temperature and model parameters</li> </ul> <pre><code># Example of creating a scoring reviewer\nreviewer1 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o-mini\"),  # Choose your model provider\n    name=\"Alice\",                                    # Unique name for the reviewer\n    scoring_task=\"Your review task description\",     # What to evaluate\n    scoring_set=[1, 2, 3, 4, 5],                      # Possible scores\n    scoring_rules=\"Rules for assigning scores\",      # How to score\n    model_args={\"temperature\": 0.1}                  # Model configuration\n)\n</code></pre>"},{"location":"quickstart/#step-4-create-review-workflow","title":"Step 4: Create Review Workflow","text":"<p>Define your workflow by specifying review rounds, reviewers, and input columns. The workflow automatically creates output columns for each reviewer based on their name and review round. For each reviewer, three columns are created:</p> <ul> <li><code>round-{ROUND}_{REVIEWER_NAME}_output</code>: Full output dictionary</li> <li><code>round-{ROUND}_{REVIEWER_NAME}_score</code>: Extracted score</li> <li><code>round-{ROUND}_{REVIEWER_NAME}_reasoning</code>: Extracted reasoning</li> </ul> <p>These automatically generated columns can be used as inputs in subsequent rounds, allowing later reviewers to access and evaluate the outputs of previous reviewers:</p> <pre><code>workflow = ReviewWorkflow(\n    workflow_schema=[\n        {\n            \"round\": 'A',               # First round\n            \"reviewers\": [reviewer1, reviewer2],\n            \"inputs\": [\"title\", \"abstract\"]  # Original data columns\n        },\n        {\n            \"round\": 'B',               # Second round\n            \"reviewers\": [expert],\n            # Access both original columns and previous reviewers' outputs\n            \"inputs\": [\"title\", \"abstract\", \"round-A_reviewer1_output\", \"round-A_reviewer2_score\"],\n            # Optional filter to review only certain cases\n            \"filter\": lambda row: row[\"round-A_reviewer1_score\"] != row[\"round-A_reviewer2_score\"]\n        }\n    ]\n)\n</code></pre> <p>In this example, the expert reviewer in round B can access both the original data columns and the outputs from round A's reviewers. The filter ensures the expert only reviews cases where the first two reviewers disagreed.</p>"},{"location":"quickstart/#step-5-run-the-workflow","title":"Step 5: Run the Workflow","text":"<p>Execute the workflow and get results:</p> <pre><code># Run workflow\nresults = asyncio.run(workflow(data))  # Returns DataFrame with all results\n\n# Results include original columns plus new columns for each reviewer:\n# - round-{ROUND}_{REVIEWER_NAME}_output: Full output dictionary\n# - round-{ROUND}_{REVIEWER_NAME}_score: Extracted score\n# - round-{ROUND}_{REVIEWER_NAME}_reasoning: Extracted reasoning\n</code></pre>"},{"location":"quickstart/#complete-working-example","title":"Complete Working Example","text":"<pre><code>from lattereview.providers import LiteLLMProvider\nfrom lattereview.agents import ScoringReviewer\nfrom lattereview.workflows import ReviewWorkflow\nimport pandas as pd\nimport asyncio\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# First Reviewer: Conservative approach\nreviewer1 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o-mini\"),\n    name=\"Alice\",\n    backstory=\"a radiologist with expertise in systematic reviews\",\n    scoring_task=\"Evaluate how relevant the article is to artificial intelligence applications in radiology\",\n    scoring_set=[1, 2, 3, 4, 5],\n    scoring_rules=\"Rate the relevance on a scale of 1 to 5, where 1 means not relevant to AI in radiology, and 5 means directly focused on AI in radiology\",\n    model_args={\"temperature\": 0.1}\n)\n\n# Second Reviewer: More exploratory approach\nreviewer2 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gemini/gemini-1.5-flash\"),\n    name=\"Bob\",\n    backstory=\"a computer scientist specializing in medical AI\",\n    scoring_task=\"Evaluate how relevant the article is to artificial intelligence applications in radiology\",\n    scoring_set=[1, 2, 3, 4, 5],\n    scoring_rules=\"Rate the relevance on a scale of 1 to 5, where 1 means not relevant to AI in radiology, and 5 means directly focused on AI in radiology\",\n    model_args={\"temperature\": 0.8}\n)\n\n# Expert Reviewer: Resolves disagreements\nexpert = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o\"),\n    name=\"Carol\",\n    backstory=\"a professor of AI in medical imaging\",\n    scoring_task=\"Review Alice and Bob's relevance assessments of this article to AI in radiology\",\n    scoring_set=[1, 2],\n    scoring_rules='Score 1 if you agree with Alice\\'s assessment, 2 if you agree with Bob\\'s assessment',\n    model_args={\"temperature\": 0.1}\n)\n\n# Define workflow\nworkflow = ReviewWorkflow(\n    workflow_schema=[\n        {\n            \"round\": 'A',  # First round: Initial review by both reviewers\n            \"reviewers\": [reviewer1, reviewer2],\n            \"inputs\": [\"title\", \"abstract\"]\n        },\n        {\n            \"round\": 'B',  # Second round: Expert reviews only disagreements\n            \"reviewers\": [expert],\n            \"inputs\": [\"title\", \"abstract\", \"round-A_Alice_output\", \"round-A_Bob_output\"],\n            \"filter\": lambda row: row[\"round-A_Alice_score\"] != row[\"round-A_Bob_score\"]\n        }\n    ]\n)\n\n# Load and process your data\ndata = pd.read_excel(\"articles.xlsx\")  # Must have 'title' and 'abstract' columns\nresults = asyncio.run(workflow(data))  # Returns a pandas DataFrame with all original and output columns\n\n# Save results\nresults.to_csv(\"review_results.csv\", index=False)\n</code></pre>"},{"location":"api/agents/","title":"Agents Module Documentation","text":"<p>This module provides the core agent functionality for the LatteReview package, implementing agent-based review workflows.</p>"},{"location":"api/agents/#overview","title":"Overview","text":"<p>The agents module consists of two main classes:</p> <ul> <li><code>BaseAgent</code>: An abstract base class that provides core agent functionality</li> <li><code>ScoringReviewer</code>: A concrete implementation for reviewing and scoring items</li> </ul>"},{"location":"api/agents/#baseagent","title":"BaseAgent","text":""},{"location":"api/agents/#description","title":"Description","text":"<p>The <code>BaseAgent</code> class serves as an abstract base class for all agents in the system. It provides core functionality for handling prompts, managing agent state, and processing examples.</p>"},{"location":"api/agents/#class-definition","title":"Class Definition","text":"<pre><code>class BaseAgent(BaseModel):\n    response_format: Dict[str, Any]\n    provider: Optional[Any] = None\n    model_args: Dict[str, Any] = Field(default_factory=dict)\n    max_concurrent_requests: int = DEFAULT_CONCURRENT_REQUESTS\n    name: str = \"BaseAgent\"\n    backstory: str = \"a generic base agent\"\n    input_description: str = \"\"\n    examples: Union[str, List[Union[str, Dict[str, Any]]]] = None\n    reasoning: ReasoningType = ReasoningType.BRIEF\n    system_prompt: Optional[str] = None\n    item_prompt: Optional[str] = None\n    cost_so_far: float = 0\n    memory: List[Dict[str, Any]] = []\n    identity: Dict[str, Any] = {}\n</code></pre>"},{"location":"api/agents/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>response_format</code>: Defines the expected format of agent responses, which now includes a mandatory <code>certainty</code> field.</li> <li><code>provider</code>: The LLM provider instance used by the agent</li> <li><code>model_args</code>: Arguments passed to the language model</li> <li><code>max_concurrent_requests</code>: Maximum number of concurrent requests (default: 20)</li> <li><code>name</code>: Agent's name</li> <li><code>backstory</code>: Agent's background story</li> <li><code>reasoning</code>: Type of reasoning (NONE, BRIEF, COT)</li> <li><code>memory</code>: List storing agent's interactions</li> <li><code>identity</code>: Dictionary containing agent's configuration</li> </ul>"},{"location":"api/agents/#methods","title":"Methods","text":""},{"location":"api/agents/#setup","title":"<code>setup()</code>","text":"<p>Abstract method that must be implemented by subclasses to initialize the agent.</p>"},{"location":"api/agents/#build_system_prompt","title":"<code>build_system_prompt()</code>","text":"<p>Constructs the system prompt based on agent's configuration.</p> <pre><code>def build_system_prompt(self) -&gt; str:\n    # Returns formatted system prompt containing agent's name, backstory,\n    # task description, and expected output format\n</code></pre>"},{"location":"api/agents/#build_item_prompt","title":"<code>build_item_prompt()</code>","text":"<p>Builds a prompt for a specific item with variable substitution.</p> <pre><code>def build_item_prompt(self, base_prompt: str, item_dict: Dict[str, Any]) -&gt; str:\n    # Substitutes variables in base_prompt with values from item_dict\n</code></pre>"},{"location":"api/agents/#process_reasoning","title":"<code>process_reasoning()</code>","text":"<p>Processes the reasoning type into a prompt string.</p> <pre><code>def process_reasoning(self, reasoning: Union[str, ReasoningType]) -&gt; str:\n    # Converts reasoning type to corresponding prompt text\n</code></pre>"},{"location":"api/agents/#process_examples","title":"<code>process_examples()</code>","text":"<p>Formats examples into a string suitable for prompting.</p> <pre><code>def process_examples(self, examples: Union[str, Dict[str, Any],\n                    List[Union[str, Dict[str, Any]]]]) -&gt; str:\n    # Formats examples into a consistent string format\n</code></pre>"},{"location":"api/agents/#scoringreviewer","title":"ScoringReviewer","text":""},{"location":"api/agents/#description_1","title":"Description","text":"<p>The <code>ScoringReviewer</code> class implements a reviewer agent that can score items based on defined criteria. It extends <code>BaseAgent</code> with scoring-specific functionality.</p>"},{"location":"api/agents/#class-definition_1","title":"Class Definition","text":"<pre><code>class ScoringReviewer(BaseAgent):\n    response_format: Dict[str, Any] = {\n        \"reasoning\": str,\n        \"score\": int,\n        \"certainty\": int\n    }\n    scoring_task: Optional[str] = None\n    scoring_set: List[int] = [1, 2]\n    scoring_rules: str = \"Your scores should follow the defined schema.\"\n    generic_item_prompt: Optional[str] = Field(default=None)\n    input_description: str = \"article title/abstract\"\n    reasoning: ReasoningType = ReasoningType.BRIEF\n    max_retries: int = DEFAULT_MAX_RETRIES\n</code></pre>"},{"location":"api/agents/#key-attributes_1","title":"Key Attributes","text":"<ul> <li><code>generic_item_prompt</code>: The path to the template but dynamic prompt used for the <code>ScoringReviewer</code>.</li> <li><code>response_format</code>: The expected response format from the ScoringReviewer agent with three necessary keys: <code>reasoning</code>, <code>score</code>, and <code>certainty</code>.</li> <li><code>scoring_task</code>: Description of the scoring task.</li> <li><code>scoring_set</code>: List of valid scores (now supports 0 as a valid score)</li> <li><code>scoring_rules</code>: Rules for scoring items</li> <li><code>max_retries</code>: Maximum number of retry attempts for failed reviews</li> </ul>"},{"location":"api/agents/#methods_1","title":"Methods","text":""},{"location":"api/agents/#review_items","title":"<code>review_items()</code>","text":"<p>Reviews multiple items concurrently with progress tracking.</p> <pre><code>async def review_items(self, items: List[str],\n                      tqdm_keywords: dict = None) -&gt; List[Dict[str, Any]]:\n    # Reviews multiple items concurrently\n    # Returns list of review results and associated costs\n</code></pre>"},{"location":"api/agents/#review_item","title":"<code>review_item()</code>","text":"<p>Reviews a single item with retry logic.</p> <pre><code>async def review_item(self, item: str) -&gt; tuple[Dict[str, Any], Dict[str, float]]:\n    # Reviews single item with retry mechanism\n    # Returns review result and associated cost\n</code></pre>"},{"location":"api/agents/#usage-examples","title":"Usage Examples","text":""},{"location":"api/agents/#creating-a-basic-scoring-reviewer","title":"Creating a Basic Scoring Reviewer","text":"<pre><code>from lattereview.agents import ScoringReviewer\nfrom lattereview.providers import OpenAIProvider\n\nreviewer = ScoringReviewer(\n    provider=OpenAIProvider(model=\"gpt-4\"),\n    name=\"ReviewerOne\",\n    backstory=\"an expert reviewer in scientific literature\",\n    scoring_task=\"Evaluate papers for methodology quality\",\n    scoring_set=[1, 2, 3, 4, 5],\n    scoring_rules=\"Score 1-5 where 1 is poor and 5 is excellent\",\n    reasoning=ReasoningType.COT\n)\n</code></pre>"},{"location":"api/agents/#running-reviews","title":"Running Reviews","text":"<pre><code># Single item review\nresult, cost = await reviewer.review_item(\"Paper abstract text...\")\n\n# Multiple items review\nitems = [\"Abstract 1...\", \"Abstract 2...\", \"Abstract 3...\"]\nresults, total_cost = await reviewer.review_items(items)\n</code></pre>"},{"location":"api/agents/#customizing-review-behavior","title":"Customizing Review Behavior","text":"<pre><code>reviewer = ScoringReviewer(\n    provider=OpenAIProvider(model=\"gpt-4\"),\n    max_concurrent_requests=10,  # Limit concurrent requests\n    max_retries=5,              # Increase retry attempts\n    model_args={                # Customize model behavior\n        \"temperature\": 0.7,\n        \"max_tokens\": 500\n    }\n)\n</code></pre>"},{"location":"api/agents/#error-handling","title":"Error Handling","text":"<p>Common error scenarios:</p> <ul> <li>Provider initialization failures</li> <li>Prompt building errors</li> <li>Review process failures</li> <li>Response format validation errors</li> </ul> <p>All operations are wrapped in try-except blocks with detailed error messages.</p>"},{"location":"api/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Always initialize agents with appropriate providers</li> <li>Set reasonable concurrency limits based on API constraints</li> <li>Use appropriate reasoning types for your use case</li> <li>Monitor costs through the <code>cost_so_far</code> attribute</li> <li>Implement retry logic for robust production systems</li> <li>Keep prompts and scoring rules clear and specific</li> <li>Regularly clear agent memory for long-running processes</li> </ol>"},{"location":"api/agents/#limitations","title":"Limitations","text":"<ul> <li>Requires async/await syntax for operation</li> <li>Depends on external LLM providers</li> <li>Memory grows with usage (clear regularly)</li> <li>Rate limits depend on provider capabilities</li> <li>For now, agents cannot converse with each other.</li> </ul>"},{"location":"api/providers/","title":"Providers Module Documentation","text":"<p>This module provides different language model provider implementations for the LatteReview package. It handles interactions with various LLM APIs in a consistent and type-safe manner.</p>"},{"location":"api/providers/#overview","title":"Overview","text":"<p>The providers module includes:</p> <ul> <li><code>BaseProvider</code>: Abstract base class defining the provider interface</li> <li><code>OpenAIProvider</code>: Implementation for OpenAI API (including GPT models)</li> <li><code>OllamaProvider</code>: Implementation for local Ollama models</li> <li><code>LiteLLMProvider</code>: Implementation using LiteLLM for unified API access</li> </ul>"},{"location":"api/providers/#baseprovider","title":"BaseProvider","text":""},{"location":"api/providers/#description","title":"Description","text":"<p>The <code>BaseProvider</code> class serves as the foundation for all LLM provider implementations. It provides a consistent interface and error handling for interacting with language models.</p>"},{"location":"api/providers/#class-definition","title":"Class Definition","text":"<pre><code>class BaseProvider(pydantic.BaseModel):\n    provider: str = \"DefaultProvider\"\n    client: Optional[Any] = None\n    api_key: Optional[str] = None\n    model: str = \"default-model\"\n    system_prompt: str = \"You are a helpful assistant.\"\n    response_format: Optional[Dict[str, Any]] = None\n    last_response: Optional[Any] = None\n</code></pre>"},{"location":"api/providers/#error-types","title":"Error Types","text":"<pre><code>class ProviderError(Exception): pass\nclass ClientCreationError(ProviderError): pass\nclass ResponseError(ProviderError): pass\nclass InvalidResponseFormatError(ProviderError): pass\nclass ClientNotInitializedError(ProviderError): pass\n</code></pre>"},{"location":"api/providers/#core-methods","title":"Core Methods","text":""},{"location":"api/providers/#create_client","title":"<code>create_client()</code>","text":"<p>Abstract method for initializing the provider's client.</p> <pre><code>def create_client(self) -&gt; Any:\n    \"\"\"Create and initialize the client for the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#get_response","title":"<code>get_response()</code>","text":"<p>Get a text response from the model.</p> <pre><code>async def get_response(\n    self,\n    messages: Union[str, List[str]],\n    message_list: Optional[List[Dict[str, str]]] = None,\n    system_message: Optional[str] = None,\n) -&gt; tuple[Any, Dict[str, float]]:\n    \"\"\"Get a response from the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#get_json_response","title":"<code>get_json_response()</code>","text":"<p>Get a JSON-formatted response from the model.</p> <pre><code>async def get_json_response(\n    self,\n    messages: Union[str, List[str]],\n    message_list: Optional[List[Dict[str, str]]] = None,\n    system_message: Optional[str] = None,\n) -&gt; tuple[Any, Dict[str, float]]:\n    \"\"\"Get a JSON-formatted response from the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#openaiprovider","title":"OpenAIProvider","text":""},{"location":"api/providers/#description_1","title":"Description","text":"<p>Implementation for OpenAI's API, supporting both OpenAI models and Gemini models through a compatible endpoint.</p>"},{"location":"api/providers/#class-definition_1","title":"Class Definition","text":"<pre><code>class OpenAIProvider(BaseProvider):\n    provider: str = \"OpenAI\"\n    api_key: str = None\n    client: Optional[openai.AsyncOpenAI] = None\n    model: str = \"gpt-4o-mini\"\n    response_format_class: Optional[BaseModel] = None\n</code></pre>"},{"location":"api/providers/#key-features","title":"Key Features","text":"<ul> <li>Automatic API key handling from environment variables</li> <li>Support for OpenAI and Gemini models</li> <li>JSON response validation</li> <li>Comprehensive error handling</li> </ul>"},{"location":"api/providers/#usage-example","title":"Usage Example","text":"<pre><code>from lattereview.providers import OpenAIProvider\n\n# Initialize with OpenAI model\nprovider = OpenAIProvider(model=\"gpt-4\")\n\n# Initialize with Gemini model\nprovider = OpenAIProvider(model=\"gemini/gemini-1.5-flash\")\n\n# Get a response\nresponse, cost = await provider.get_response(\"What is the capital of France?\")\n\n# Get JSON response\nprovider.set_response_format({\"key\": str, \"value\": int})\nresponse, cost = await provider.get_json_response(\"Format this as JSON\")\n</code></pre>"},{"location":"api/providers/#ollamaprovider","title":"OllamaProvider","text":""},{"location":"api/providers/#description_2","title":"Description","text":"<p>Implementation for local Ollama models, supporting both chat and streaming responses.</p>"},{"location":"api/providers/#class-definition_2","title":"Class Definition","text":"<pre><code>class OllamaProvider(BaseProvider):\n    provider: str = \"Ollama\"\n    client: Optional[AsyncClient] = None\n    model: str = \"llama3.2-vision:latest\"\n    response_format_class: Optional[BaseModel] = None\n    invalid_keywords: List[str] = [\"temperature\", \"max_tokens\"]\n    host: str = \"http://localhost:11434\"\n</code></pre>"},{"location":"api/providers/#key-features_1","title":"Key Features","text":"<ul> <li>Local model support</li> <li>Streaming capability</li> <li>Free cost tracking (local models)</li> <li>Connection management</li> </ul>"},{"location":"api/providers/#usage-example_1","title":"Usage Example","text":"<pre><code>from lattereview.providers import OllamaProvider\n\n# Initialize provider\nprovider = OllamaProvider(\n    model=\"llama3.2-vision:latest\",\n    host=\"http://localhost:11434\"\n)\n\n# Get normal response\nresponse, cost = await provider.get_response(\"What is the capital of France?\")\n\n# Stream response\nasync for chunk in provider.get_response(\"Tell me a story\", stream=True):\n    print(chunk, end=\"\")\n\n# Clean up\nawait provider.close()\n</code></pre>"},{"location":"api/providers/#litellmprovider","title":"LiteLLMProvider","text":""},{"location":"api/providers/#description_3","title":"Description","text":"<p>A unified provider implementation using LiteLLM, enabling access to multiple LLM providers through a single interface.</p>"},{"location":"api/providers/#class-definition_3","title":"Class Definition","text":"<pre><code>class LiteLLMProvider(BaseProvider):\n    provider: str = \"LiteLLM\"\n    model: str = \"gpt-4o-mini\"\n    custom_llm_provider: Optional[str] = None\n    response_format_class: Optional[Union[Dict[str, Any], Type[BaseModel]]] = None\n</code></pre>"},{"location":"api/providers/#key-features_2","title":"Key Features","text":"<ul> <li>Support for multiple model providers</li> <li>JSON schema validation</li> <li>Cost tracking integration</li> <li>Tool calls handling</li> </ul>"},{"location":"api/providers/#usage-example_2","title":"Usage Example","text":"<pre><code>from lattereview.providers import LiteLLMProvider\n\n# Initialize with different models\nprovider = LiteLLMProvider(model=\"gpt-4o-mini\")\n# provider = LiteLLMProvider(model=\"claude-3-5-sonnet-20241022\")\n# provider = LiteLLMProvider(model=\"gemini/gemini-1.5-flash\")\n\n# Get response\nresponse, cost = await provider.get_response(\"What is the capital of France?\")\n\n# Get JSON response with schema\nprovider.set_response_format({\"answer\": str, \"confidence\": float})\nresponse, cost = await provider.get_json_response(\"What is the capital of France?\")\n</code></pre>"},{"location":"api/providers/#error-handling","title":"Error Handling","text":"<p>Common error scenarios:</p> <ul> <li>API key errors (missing or invalid keys)</li> <li>Unsupported model configurations</li> <li>Models not supporting structured outputs or JSON responses</li> </ul>"},{"location":"api/providers/#best-practices","title":"Best Practices","text":"<ol> <li>For all online APIs, prefer using LiteLLMProvider class as it provides unified access and error handling</li> <li>For local APIs, use OllamaProvider directly (rather than through LiteLLMProvider) for better performance and control</li> <li>Set API keys at the environment level using the python-dotenv package and .env files for better security</li> </ol>"},{"location":"api/providers/#limitations","title":"Limitations","text":"<ul> <li>Requires async/await syntax for all operations</li> <li>Depends on external LLM providers' availability and stability</li> <li>Rate limits and quotas depend on provider capabilities</li> </ul>"},{"location":"api/workflows/","title":"Workflows Module Documentation","text":"<p>This module provides workflow management functionality for the LatteReview package, implementing multi-agent review orchestration.</p>"},{"location":"api/workflows/#overview","title":"Overview","text":"<p>The workflows module consists of one main class:</p> <ul> <li><code>ReviewWorkflow</code>: A class that orchestrates multi-agent review processes with support for sequential rounds and filtering</li> </ul>"},{"location":"api/workflows/#reviewworkflow","title":"ReviewWorkflow","text":""},{"location":"api/workflows/#description","title":"Description","text":"<p>The <code>ReviewWorkflow</code> class manages review workflows where multiple agents can review items across different rounds. It handles the orchestration of reviews, manages outputs, and provides content validation and cost tracking.</p>"},{"location":"api/workflows/#class-definition","title":"Class Definition","text":"<pre><code>class ReviewWorkflow(pydantic.BaseModel):\n    workflow_schema: List[Dict[str, Any]]\n    memory: List[Dict] = list()\n    reviewer_costs: Dict = dict()\n    total_cost: float = 0.0\n    verbose: bool = True\n</code></pre>"},{"location":"api/workflows/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>workflow_schema</code>: List of dictionaries defining the review process structure</li> <li><code>memory</code>: List storing the workflow's execution history</li> <li><code>reviewer_costs</code>: Dictionary tracking costs per reviewer and round</li> <li><code>total_cost</code>: Total accumulated cost of all reviews</li> <li><code>verbose</code>: Flag to enable/disable logging output</li> <li><code>reviewer_names</code>: Automatically generated list of reviewer identifiers</li> </ul>"},{"location":"api/workflows/#methods","title":"Methods","text":""},{"location":"api/workflows/#__call__","title":"<code>__call__()</code>","text":"<p>Execute the workflow on provided data.</p> <pre><code>async def __call__(self, data: Union[pd.DataFrame, Dict[str, Any]]) -&gt; pd.DataFrame:\n    # Execute workflow on DataFrame or dictionary input\n    # Returns DataFrame with review results\n</code></pre>"},{"location":"api/workflows/#run","title":"<code>run()</code>","text":"<p>Core method to execute the review workflow.</p> <pre><code>async def run(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    # Execute main review workflow\n    # Process each round sequentially\n    # Returns updated DataFrame with review results\n</code></pre>"},{"location":"api/workflows/#get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Get total cost of workflow execution.</p> <pre><code>def get_total_cost(self) -&gt; float:\n    # Return sum of all review costs\n</code></pre>"},{"location":"api/workflows/#internal-methods","title":"Internal Methods","text":"<ul> <li><code>_create_content_hash()</code>: Generate hash for content tracking</li> <li><code>_format_input_text()</code>: Format input for reviewers</li> <li><code>_log()</code>: Handle logging based on verbose setting</li> </ul>"},{"location":"api/workflows/#usage-examples","title":"Usage Examples","text":""},{"location":"api/workflows/#creating-a-basic-review-workflow","title":"Creating a Basic Review Workflow","text":"<pre><code>from lattereview.workflows import ReviewWorkflow\nfrom lattereview.agents import ScoringReviewer\nfrom lattereview.providers import LiteLLMProvider\n\n# Create reviewers\nreviewer1 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4\"),\n    name=\"Initial\",\n    scoring_task=\"Initial paper screening\"\n)\n\nreviewer2 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4\"),\n    name=\"Expert\",\n    scoring_task=\"Detailed technical review\"\n)\n\n# Define workflow schema\nworkflow_schema = [\n    {\n        \"round\": 'A',\n        \"reviewers\": reviewer1,\n        \"inputs\": [\"title\", \"abstract\"]\n    },\n    {\n        \"round\": 'B',\n        \"reviewers\": reviewer2,\n        \"inputs\": [\"title\", \"abstract\", \"round-A_Initial_output\"],\n        \"filter\": lambda row: row[\"round-A_Initial_score\"] &gt; 3\n    }\n]\n\n# Create and run workflow\nworkflow = ReviewWorkflow(workflow_schema=workflow_schema)\nresults = await workflow(input_data)\n</code></pre>"},{"location":"api/workflows/#understanding-workflow-construction","title":"Understanding Workflow Construction","text":"<p>A workflow is defined by a list of dictionaries where each dictionary represents a review round. The rounds are executed sequentially in the order they appear in the list. Each round dictionary must contain:</p> <p>Required Arguments:</p> <ul> <li><code>round</code>: A string identifier for the round (e.g., 'A', 'B', '1', 'initial')</li> <li><code>reviewers</code>: A single ScoringReviewer or list of ScoringReviewers</li> <li><code>inputs</code>: A string or list of strings representing DataFrame column names to use</li> </ul> <p>Optional Arguments:</p> <ul> <li> <p><code>filter</code>: A lambda function that determines which rows to review in this round. This function:</p> </li> <li> <p>Receives each row of the DataFrame as a pandas Series object</p> </li> <li>Is applied to every row at the start of each round</li> <li>Must return a boolean value (True/False)</li> <li>Has access to all columns in the DataFrame, including outputs from previous rounds</li> <li>Determines whether the row should be included in the current round's review</li> </ul> <p>The workflow automatically generates output columns for each key in the reviewer outputs, e.g.:</p> <ul> <li><code>round-{round_id}_{reviewer.name}_output</code>: Raw reviewer output</li> <li><code>round-{round_id}_{reviewer.name}_score</code>: Numerical score</li> <li><code>round-{round_id}_{reviewer.name}_reasoning</code>: Reasoning text</li> <li><code>round-{round_id}_{reviewer.name}_certainty</code>: certainty score</li> <li>...</li> </ul> <p>These output columns can be referenced in subsequent rounds as inputs or in filter conditions.</p>"},{"location":"api/workflows/#running-complex-reviews","title":"Running Complex Reviews","text":"<p>Here are examples of increasingly complex workflow patterns:</p>"},{"location":"api/workflows/#1-basic-multi-reviewer-round","title":"1. Basic Multi-Reviewer Round","text":"<p>Multiple reviewers examining the same content:</p> <pre><code>workflow_schema = [\n    {\n        \"round\": 'A',\n        \"reviewers\": [reviewer1, reviewer2],\n        \"inputs\": [\"title\", \"abstract\"]\n    }\n]\n</code></pre>"},{"location":"api/workflows/#2-sequential-review-with-dependencies","title":"2. Sequential Review with Dependencies","text":"<p>One reviewer examining previous reviews:</p> <pre><code>workflow_schema = [\n    {\n        \"round\": 'A',\n        \"reviewers\": initial_reviewer,\n        \"inputs\": [\"title\", \"abstract\"]\n    },\n    {\n        \"round\": 'B',\n        \"reviewers\": expert_reviewer,\n        \"inputs\": [\n            \"title\",\n            \"abstract\",\n            \"round-A_initial_reviewer_score\",\n            \"round-A_initial_reviewer_reasoning\"\n        ]\n    }\n]\n</code></pre>"},{"location":"api/workflows/#3-disagreement-resolution-pattern","title":"3. Disagreement Resolution Pattern","text":"<p>A third reviewer resolving disagreements:</p> <pre><code>workflow_schema = [\n    {\n        \"round\": 'A',\n        \"reviewers\": [reviewer1, reviewer2],\n        \"inputs\": [\"title\", \"abstract\"]\n    },\n    {\n        \"round\": 'B',\n        \"reviewers\": arbitrator,\n        \"inputs\": [\n            \"title\",\n            \"abstract\",\n            \"round-A_reviewer1_score\",\n            \"round-A_reviewer2_score\",\n            \"round-A_reviewer1_reasoning\",\n            \"round-A_reviewer2_reasoning\"\n        ],\n        \"filter\": lambda row: abs(\n            row[\"round-A_reviewer1_score\"] -\n            row[\"round-A_reviewer2_score\"]\n        ) &gt; 2\n    }\n]\n</code></pre>"},{"location":"api/workflows/#4-multi-stage-review-with-filtering","title":"4. Multi-Stage Review with Filtering","text":"<p>Progressive review stages with filtering:</p> <pre><code>workflow_schema = [\n    # Initial screening\n    {\n        \"round\": 'screening',\n        \"reviewers\": screening_reviewer,\n        \"inputs\": [\"title\", \"abstract\"]\n    },\n    # Detailed review of passing papers\n    {\n        \"round\": 'detailed',\n        \"reviewers\": [expert1, expert2],\n        \"inputs\": [\"title\", \"abstract\", \"full_text\"],\n        \"filter\": lambda row: row[\"round-screening_screening_reviewer_score\"] &gt;= 4\n    },\n    # Final decision for contested papers\n    {\n        \"round\": 'final',\n        \"reviewers\": senior_reviewer,\n        \"inputs\": [\n            \"title\",\n            \"abstract\",\n            \"round-detailed_expert1_score\",\n            \"round-detailed_expert2_score\",\n            \"round-detailed_expert1_reasoning\",\n            \"round-detailed_expert2_reasoning\"\n        ],\n        \"filter\": lambda row:\n            row[\"round-detailed_expert1_score\"] != row[\"round-detailed_expert2_score\"]\n    }\n]\n</code></pre>"},{"location":"api/workflows/#5-hierarchical-review-with-validation","title":"5. Hierarchical Review with Validation","text":"<p>Multiple review levels with validation checks:</p> <pre><code>workflow_schema = [\n    # Initial automated screening\n    {\n        \"round\": 'auto',\n        \"reviewers\": auto_reviewer,\n        \"inputs\": [\"title\", \"abstract\", \"metrics\"]\n    },\n    # Human validation of uncertain cases\n    {\n        \"round\": 'validation',\n        \"reviewers\": human_validator,\n        \"inputs\": [\n            \"title\",\n            \"abstract\",\n            \"round-auto_auto_reviewer_score\",\n            \"round-auto_auto_reviewer_reasoning\"\n        ],\n        \"filter\": lambda row:\n            2 &lt;= row[\"round-auto_auto_reviewer_score\"] &lt;= 4\n    },\n    # Expert review of validated papers\n    {\n        \"round\": 'expert',\n        \"reviewers\": domain_expert,\n        \"inputs\": [\"title\", \"abstract\", \"full_text\"],\n        \"filter\": lambda row:\n            row[\"round-validation_human_validator_score\"] &gt;= 3 or\n            row[\"round-auto_auto_reviewer_score\"] &gt;= 4\n    }\n]\n</code></pre>"},{"location":"api/workflows/#handling-results","title":"Handling Results","text":"<pre><code># Execute workflow\nworkflow = ReviewWorkflow(workflow_schema=workflow_schema)\ntry:\n    results_df = await workflow(input_df)\n\n    # Access costs\n    total_cost = workflow.get_total_cost()\n    per_reviewer = workflow.reviewer_costs\n\n    # Access results\n    round_a_scores = results_df[\"round-A_Initial_score\"]\n    round_b_reasoning = results_df[\"round-B_Expert_reasoning\"]\n\nexcept ReviewWorkflowError as e:\n    print(f\"Workflow failed: {e}\")\n</code></pre>"},{"location":"api/workflows/#error-handling","title":"Error Handling","text":"<p>The module uses <code>ReviewWorkflowError</code> for all workflow-related errors:</p> <pre><code>class ReviewWorkflowError(Exception):\n    \"\"\"Base exception for workflow-related errors.\"\"\"\n    pass\n</code></pre> <p>Common error scenarios:</p> <ul> <li>Invalid workflow schema</li> <li>Missing input columns</li> <li>Reviewer initialization failures</li> <li>Content validation errors</li> <li>Output processing failures</li> </ul>"},{"location":"api/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Schema Design</p> </li> <li> <p>Use meaningful round identifiers</p> </li> <li>Design filter functions carefully</li> <li> <p>Consider round dependencies</p> </li> <li> <p>Data Management</p> </li> <li> <p>Validate input data structure</p> </li> <li>Handle missing values appropriately</li> <li> <p>Use appropriate column names</p> </li> <li> <p>Cost Control</p> </li> <li> <p>Monitor per-round costs</p> </li> <li>Set appropriate concurrent request limits</li> <li> <p>Track total workflow costs</p> </li> <li> <p>Error Handling</p> </li> <li>Implement proper exception handling</li> <li>Validate workflow schema</li> <li>Monitor review outputs</li> </ol>"},{"location":"api/workflows/#limitations","title":"Limitations","text":"<ul> <li>Sequential round execution only</li> <li>Memory grows with number of reviews</li> <li>No direct reviewer communication</li> <li>Limited to DataFrame-based workflows</li> <li>Requires async/await syntax</li> <li>Filter functions must be serializable</li> </ul>"}]}