{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LatteReview \ud83e\udd16\u2615","text":"<p>A framework for multi-agent review workflows using large language models.</p>"},{"location":"#overview","title":"Overview","text":"<p>LatteReview is a Python framework that enables you to create and manage multi-agent review workflows using various large language models. It provides a flexible and extensible architecture for implementing different types of review processes, from simple single-agent reviews to complex multi-stage workflows with multiple agents.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-agent review system with customizable roles and expertise levels for each reviewer</li> <li>Support for multiple review rounds with hierarchical decision-making workflows</li> <li>Review diverse content types including article titles, abstracts, custom texts, and images using LLM-powered reviewer agents</li> <li>Define reviewer agents with specialized backgrounds and distinct evaluation capabilities</li> <li>Create flexible review workflows where multiple agents operate in parallel or sequential arrangements</li> <li>Enable reviewer agents to analyze peer feedback, cast votes, and propose corrections to other reviewers' assessments</li> <li>Enhance reviews with item-specific context integration, supporting use cases like Retrieval Augmented Generation (RAG)</li> <li>Broad compatibility with LLM providers through LiteLLM, including OpenAI and Ollama</li> <li>Model-agnostic integration supporting OpenAI, Gemini, Claude, Groq, and local models via Ollama</li> <li>High-performance asynchronous processing for efficient batch reviews</li> <li>Standardized output format featuring detailed scoring metrics and reasoning transparency</li> <li>Robust cost tracking and memory management systems</li> <li>Extensible architecture supporting custom review workflow implementation</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Tutorial notebooks</li> <li>API Reference</li> <li>GitHub Repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#authors","title":"\ud83d\udc68\u200d\ud83d\udcbb Authors","text":"Pouria Rouzrokh, MD, MPH, MHPE Medical Practitioner and Machine Learning Engineer Incoming Radiology Resident @Yale University Former Data Scientist @Mayo Clinic AI Lab Moein Shariatnia, MD Medical Practitioner and Machine Learning Engineer"},{"location":"#support-lattereview","title":"Support LatteReview","text":"<p>If you find LatteReview helpful in your research or work, consider supporting its continued development. Since we're already sharing a virtual coffee break while reviewing papers, maybe you'd like to treat me to a real one? \u2615 \ud83d\ude0a</p>"},{"location":"#ways-to-support","title":"Ways to Support:","text":"<ul> <li>Become my sponsor on GitHub</li> <li>Treat me to a cup of coffee on Ko-fi \u2615</li> <li>Star the repository to help others discover the project</li> <li>Submit bug reports, feature requests, or contribute code</li> <li>Share your experience using LatteReview in your research</li> </ul>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>I would like to express my heartfelt gratitude to Moein Shariatnia for his invaluable support and contributions to this project.</p>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>If you use LatteReview in your research, please cite our paper:</p> <pre><code>@misc{rouzrokh2025lattereview,\n    title={LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models},\n    author={Pouria Rouzrokh and Moein Shariatnia},\n    year={2025},\n    eprint={2501.05468},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"about/","title":"\ud83d\udca1About","text":""},{"location":"about/#author","title":"\ud83d\udc68\u200d\ud83d\udcbb Author","text":"<p>Pouria Rouzrokh, MD, MPH, MHPE Medical Practitioner and Machine Learning Engineer Incoming Radiology Resident @Yale University Former Data Scientist @Mayo Clinic AI Lab</p> <p>Find my work:  </p>"},{"location":"about/#support-lattereview","title":"\u2764\ufe0f Support LatteReview","text":"<p>If you find LatteReview helpful in your research or work, consider supporting its continued development. Since we're already sharing a virtual coffee break while reviewing papers, maybe you'd like to treat me to a real one? \u2615 \ud83d\ude0a</p>"},{"location":"about/#ways-to-support","title":"Ways to Support:","text":"<ul> <li>Treat me to a coffee on Ko-fi \u2615</li> <li>Star the repository to help others discover the project</li> <li>Submit bug reports, feature requests, or contribute code</li> <li>Share your experience using LatteReview in your research</li> </ul>"},{"location":"about/#license","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"about/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please feel free to submit a Pull Request.</p>"},{"location":"about/#citation","title":"\ud83d\udcda Citation","text":"<p>If you use LatteReview in your research, please cite our paper:</p> <pre><code># Preprint citation to be added\n</code></pre>"},{"location":"installation/","title":"\ud83d\udee0\ufe0fInstallation","text":"<p>There are several ways to install LatteReview:</p>"},{"location":"installation/#1-install-from-pypi-recommended","title":"1. Install from PyPI (Recommended)","text":"<pre><code>pip install lattereview\n</code></pre> <p>You can also install additional features using these extras:</p> <pre><code># Development tools\npip install \"lattereview[dev]\"\n\n# Documentation tools\npip install \"lattereview[docs]\"\n\n# All extras\npip install \"lattereview[all]\"\n</code></pre>"},{"location":"installation/#2-install-from-source-code","title":"2. Install from Source Code","text":""},{"location":"installation/#option-a-using-git","title":"Option A: Using Git","text":"<pre><code># Clone the repository\ngit clone https://github.com/PouriaRouzrokh/LatteReview.git\ncd LatteReview\n</code></pre>"},{"location":"installation/#option-b-using-zip-download","title":"Option B: Using ZIP Download","text":"<ol> <li>Go to https://github.com/PouriaRouzrokh/LatteReview</li> <li>Click the green \"Code\" button</li> <li>Select \"Download ZIP\"</li> <li>Extract the ZIP file and navigate to the directory:</li> </ol> <pre><code>cd path/to/LatteReview-main\n</code></pre> <p>After obtaining the source code through either option, you can install it using one of these methods:</p> <pre><code># Basic installation\npip install .\n\n# Install from specific versions of dependencies mentioned in requirements.txt\npip install -r requirements.txt\n\n# Development installation (all optional dependencies)\npip install -e \".[all]\"\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import lattereview\nprint(lattereview.__version__)\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or later</li> <li>Core dependencies (automatically installed):</li> <li>litellm (&gt;=1.55.2)</li> <li>openai (&gt;=1.57.4)</li> <li>pandas (&gt;=2.2.3)</li> <li>pydantic (&gt;=2.10.3)</li> <li>And others as specified in <code>setup.py</code></li> </ul>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter installation issues:</p> <pre><code># Check Python version\npython --version  # Should be 3.9 or later\n\n# Update pip\npip install --upgrade pip\n\n# Install build dependencies\npip install build wheel setuptools\n</code></pre>"},{"location":"quickstart/","title":"\ud83d\ude80 Quick Start","text":"<p>LatteReview enables you to create custom literature review workflows with multiple AI reviewers. Each reviewer can use different models and providers based on your needs. Please follow the below steps to perform a review task with LatteReview.</p> <p>\ud83d\udca1Also, please check our tutorial notebooks that provide complete code examples for all main functionalities of the LatteReview package.</p>"},{"location":"quickstart/#step-1-set-up-api-keys","title":"Step 1: Set Up API Keys","text":"<p>To use LatteReview with different LLM engines (OpenAI, Anthropic, Google, etc.), you'll need to set up the API keys for the specific providers you plan to use. For example, if you're only using OpenAI models, you only need the OpenAI API key. Here are three ways to set up your API keys:</p> <ol> <li>Using python-dotenv (Recommended):</li> <li>Create a <code>.env</code> file in your project directory and add only the keys you need:</li> </ol> <pre><code># .env file - Example keys (add only what you need)\nOPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre> <ul> <li>Load it in your code:</li> </ul> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Load before importing any providers\n</code></pre> <ol> <li>Setting environment variables directly:</li> </ol> <pre><code># Example: Set only the keys for providers you're using\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre> <ol> <li>Passing API keys directly to providers (supported by some providers):</li> </ol> <pre><code>from lattereview.providers import OpenAIProvider\nprovider = OpenAIProvider(api_key=\"your-openai-key\")  # Optional, will use environment variable if not provided\n</code></pre> <p>Note: No API keys are needed if you're exclusively using local models through Ollama.</p>"},{"location":"quickstart/#step-2-prepare-your-data","title":"Step 2: Prepare Your Data","text":"<p>Your input data should be in a CSV or Excel file with columns for the content you want to review. The column names should match the <code>inputs</code> specified in your workflow:</p> <pre><code># Load your data\ndata = pd.read_excel(\"articles.xlsx\")\n\n# Example data structure:\ndata = pd.DataFrame({\n    'title': ['Paper 1 Title', 'Paper 2 Title'],\n    'abstract': ['Paper 1 Abstract', 'Paper 2 Abstract']\n})\n</code></pre>"},{"location":"quickstart/#step-3-create-reviewers","title":"Step 3: Create Reviewers","text":"<p>Create reviewer agents by configuring <code>TitleAbstractReviewer</code> objects. Each reviewer needs:</p> <ul> <li>A provider (e.g., LiteLLMProvider, OpenAIProvider, OllamaProvider)</li> <li>A unique name</li> <li>Inclusion and exclusion criteria</li> <li>A reasoning argument that defaults to <code>brief</code> but can also be set to <code>cot</code> for detailed step-by-step reasoning. This cannot be <code>None</code>.</li> <li>Optional configuration like temperature and model parameters</li> </ul> <pre><code># Example of creating a TitleAbstractReviewer\nreviewer1 = TitleAbstractReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o-mini\"),  # Choose your model provider\n    name=\"Alice\",                                    # Unique name for the reviewer\n    inclusion_criteria=\"Must be relevant to AI in medical imaging.\",\n    exclusion_criteria=\"Exclude papers focusing only on basic sciences.\",\n    reasoning=\"brief\",                               # Reasoning explanation\n    model_args={\"temperature\": 0.1}                 # Model configuration\n)\n</code></pre>"},{"location":"quickstart/#step-4-create-review-workflow","title":"Step 4: Create Review Workflow","text":"<p>Define your workflow by specifying review rounds, reviewers, and input columns. The workflow automatically creates output columns for each reviewer based on their name and review round. For each reviewer, two columns are created:</p> <ul> <li><code>round-{ROUND}_{REVIEWER_NAME}_output</code>: Full output dictionary</li> <li><code>round-{ROUND}_{REVIEWER_NAME}_evaluation</code>: Extracted evaluation score</li> <li><code>round-{ROUND}_{REVIEWER_NAME}_reasoning</code>: Extracted reasoning explanation, which follows the reasoning style (<code>brief</code> or <code>cot</code>) defined for the agent.</li> </ul> <p>These automatically generated columns can be used as inputs in subsequent rounds, allowing later reviewers to access and evaluate the outputs of previous reviewers:</p> <pre><code>workflow = ReviewWorkflow(\n    workflow_schema=[\n        {\n            \"round\": 'A',               # First round\n            \"reviewers\": [reviewer1, reviewer2],\n            \"text_inputs\": [\"title\", \"abstract\"]  # Original data columns\n        },\n        {\n            \"round\": 'B',               # Second round\n            \"reviewers\": [expert],\n            # Access both original columns and previous reviewers' outputs\n            \"text_inputs\": [\"title\", \"abstract\", \"round-A_reviewer1_output\", \"round-A_reviewer2_output\"],\n            # Optional filter to review only certain cases\n            \"filter\": lambda row: row[\"round-A_reviewer1_evaluation\"] != row[\"round-A_reviewer2_evaluation\"]\n        }\n    ]\n)\n</code></pre> <p>In this example, the expert reviewer in round B can access both the original data columns and the outputs from round A's reviewers. The filter ensures the expert only reviews cases where the first two reviewers disagreed.</p>"},{"location":"quickstart/#step-5-run-the-workflow","title":"Step 5: Run the Workflow","text":"<p>Execute the workflow and get results:</p> <pre><code># Run workflow\nresults = asyncio.run(workflow(data))  # Returns DataFrame with all results\n\n# Results include original columns plus new columns for each reviewer:\n# - round-{ROUND}_{REVIEWER_NAME}_output: Full output dictionary\n# - round-{ROUND}_{REVIEWER_NAME}_evaluation: Extracted evaluation score\n# - round-{ROUND}_{REVIEWER_NAME}_reasoning: Reasoning explanation based on the defined style (`brief` or `cot`)\n</code></pre>"},{"location":"quickstart/#complete-working-example","title":"Complete Working Example","text":"<pre><code>from lattereview.providers import LiteLLMProvider\nfrom lattereview.agents import TitleAbstractReviewer\nfrom lattereview.workflows import ReviewWorkflow\nimport pandas as pd\nimport asyncio\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# First Reviewer: Conservative approach\nreviewer1 = TitleAbstractReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o-mini\"),\n    name=\"Alice\",\n    backstory=\"a radiologist with expertise in systematic reviews\",\n    inclusion_criteria=\"Must be relevant to artificial intelligence in radiology.\",\n    exclusion_criteria=\"Exclude studies that are not peer-reviewed.\",\n    reasoning=\"brief\",\n    model_args={\"temperature\": 0.1}\n)\n\n# Second Reviewer: More exploratory approach\nreviewer2 = TitleAbstractReviewer(\n    provider=LiteLLMProvider(model=\"gemini/gemini-1.5-flash\"),\n    name=\"Bob\",\n    backstory=\"a computer scientist specializing in medical AI\",\n    inclusion_criteria=\"Relevant to artificial intelligence in radiology.\",\n    exclusion_criteria=\"Exclude studies focused solely on hardware.\",\n    reasoning=\"cot\",\n    model_args={\"temperature\": 0.8}\n)\n\n# Expert Reviewer: Resolves disagreements\nexpert = TitleAbstractReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o\"),\n    name=\"Carol\",\n    backstory=\"a professor of AI in medical imaging\",\n    inclusion_criteria=\"Must align with at least one of Alice or Bob's recommendations.\",\n    exclusion_criteria=\"Exclude only if both Alice and Bob disagreed.\",\n    reasoning=\"brief\",\n    model_args={\"temperature\": 0.1}\n)\n\n# Define workflow\nworkflow = ReviewWorkflow(\n    workflow_schema=[\n        {\n            \"round\": 'A',  # First round: Initial review by both reviewers\n            \"reviewers\": [reviewer1, reviewer2],\n            \"text_inputs\": [\"title\", \"abstract\"]\n        },\n        {\n            \"round\": 'B',  # Second round: Expert reviews only disagreements\n            \"reviewers\": [expert],\n            \"text_inputs\": [\"title\", \"abstract\", \"round-A_Alice_output\", \"round-A_Bob_output\"],\n            \"filter\": lambda row: row[\"round-A_Alice_evaluation\"] != row[\"round-A_Bob_evaluation\"]\n        }\n    ]\n)\n\n# Load and process your data\ndata = pd.read_excel(\"articles.xlsx\")  # Must have 'title' and 'abstract' columns\nresults = asyncio.run(workflow(data))  # Returns a pandas DataFrame with all original and output columns\n\n# Save results\nresults.to_csv(\"review_results.csv\", index=False)\n</code></pre>"},{"location":"api/agents/","title":"Agents Module Documentation for LatteReview","text":"<p>This document provides a comprehensive explanation of the agents module within the LatteReview package.</p>"},{"location":"api/agents/#overview","title":"Overview","text":"<p>The agents module is a central part of LatteReview, implementing agent-based workflows for reviewing and processing inputs like text and images. The module includes the following primary classes:</p> <ul> <li><code>BasicReviewer</code>: The abstract base class that serves as the foundation for all agent types.</li> <li><code>ScoringReviewer</code>: A concrete implementation designed to review and score inputs based on defined criteria.</li> <li><code>AbstractionReviewer</code>: A specialized agent for structured data extraction tasks.</li> <li><code>TitleAbstractReviewer</code>: A specialized agent for reviewing and evaluating article titles and abstracts against defined criteria.</li> </ul>"},{"location":"api/agents/#basicreviewer-class","title":"BasicReviewer Class","text":""},{"location":"api/agents/#overview_1","title":"Overview","text":"<p>The <code>BasicReviewer</code> class provides essential functionality for all agent types. It manages prompts, agent states, example processing, and integration with language model providers.</p>"},{"location":"api/agents/#class-definition","title":"Class Definition","text":"<pre><code>class BasicReviewer(BaseModel):\n    generic_prompt: Optional[str] = None\n    prompt_path: Optional[Union[str, Path]] = None\n    response_format: Dict[str, Any] = None\n    provider: Optional[Any] = None\n    model_args: Dict[str, Any] = Field(default_factory=dict)\n    max_concurrent_requests: int = DEFAULT_CONCURRENT_REQUESTS\n    name: str = \"BasicReviewer\"\n    backstory: str = \"a generic base agent\"\n    input_description: str = \"\"\n    examples: Union[str, List[Union[str, Dict[str, Any]]]] = None\n    reasoning: str = None\n    system_prompt: Optional[str] = None\n    formatted_prompt: Optional[str] = None\n    cost_so_far: float = 0\n    memory: List[Dict[str, Any]] = []\n    identity: Dict[str, Any] = {}\n    additional_context: Optional[Union[Callable, str]] = \"\"\n    verbose: bool = True\n</code></pre>"},{"location":"api/agents/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>generic_prompt</code>: Template string for creating prompts.</li> <li><code>prompt_path</code>: Path to the template file for constructing prompts.</li> <li><code>response_format</code>: Dictionary defining the structure of expected responses.</li> <li><code>provider</code>: Language model provider instance (e.g., OpenAI, Ollama).</li> <li><code>model_args</code>: Arguments passed to the language model.</li> <li><code>max_concurrent_requests</code>: Limit for concurrent processing tasks.</li> <li><code>name</code>: Identifier for the agent.</li> <li><code>backstory</code>: Description of the agent's role.</li> <li><code>input_description</code>: Description of the input format.</li> <li><code>examples</code>: Examples for the model\u2019s guidance.</li> <li><code>reasoning</code>: Type of reasoning employed (e.g., <code>brief</code>).</li> <li><code>system_prompt</code>: Generated system-level prompt for the model.</li> <li><code>formatted_prompt</code>: Fully constructed input prompt.</li> <li><code>cost_so_far</code>: Tracks cumulative API costs.</li> <li><code>memory</code>: Log of agent interactions and responses.</li> <li><code>identity</code>: Metadata defining the agent\u2019s setup.</li> <li><code>additional_context</code>: Additional data provided as a callable or string.</li> <li><code>verbose</code>: Controls logging verbosity.</li> </ul>"},{"location":"api/agents/#key-methods","title":"Key Methods","text":""},{"location":"api/agents/#initialization-and-setup","title":"Initialization and Setup","text":"<ul> <li><code>setup(self)</code>: Initializes the agent by preparing prompts and configuring the provider.</li> <li><code>model_post_init(self, __context: Any)</code>: Post-initialization setup after creating the agent instance.</li> <li><code>reset_memory(self)</code>: Clears memory and cost tracking.</li> </ul>"},{"location":"api/agents/#prompt-management","title":"Prompt Management","text":"<ul> <li><code>_build_system_prompt(self)</code>: Constructs the system prompt incorporating agent identity and task details.</li> <li><code>_process_prompt(self, base_prompt: str, item_dict: Dict[str, Any])</code>: Substitutes variables in a base prompt.</li> <li><code>_extract_prompt_keywords(self, prompt: str)</code>: Extracts keywords for dynamic variable replacement from the prompt.</li> </ul>"},{"location":"api/agents/#reasoning-and-examples","title":"Reasoning and Examples","text":"<ul> <li><code>_process_reasoning(self, reasoning: str)</code>: Maps reasoning types to corresponding text.</li> <li><code>_process_examples(self, examples: Union[str, Dict[str, Any], List[Union[str, Dict[str, Any]]]])</code>: Formats examples consistently for prompt use.</li> </ul>"},{"location":"api/agents/#additional-utilities","title":"Additional Utilities","text":"<ul> <li><code>_clean_text(self, text: str)</code>: Removes extra spaces and blank lines from the text.</li> <li><code>_log(self, message: str)</code>: Logs messages if verbose mode is enabled.</li> </ul>"},{"location":"api/agents/#review-operations","title":"Review Operations","text":"<ul> <li><code>review_items(self, text_input_strings: List[str], image_path_lists: List[List[str]] = None, tqdm_keywords: dict = None)</code>: Reviews multiple items asynchronously with concurrency control and a progress bar.</li> <li><code>review_item(self, text_input_string: str, image_path_list: List[str] = [])</code>: Reviews a single item asynchronously with error handling.</li> </ul>"},{"location":"api/agents/#scoringreviewer-class","title":"ScoringReviewer Class","text":""},{"location":"api/agents/#overview_2","title":"Overview","text":"<p>The <code>ScoringReviewer</code> extends <code>BasicReviewer</code> to provide scoring functionalities for input data. It supports structured scoring tasks, reasoning explanations, and customizable scoring rules.</p>"},{"location":"api/agents/#class-definition_1","title":"Class Definition","text":"<pre><code>class ScoringReviewer(BasicReviewer):\n    response_format: Dict[str, Any] = {\n        \"reasoning\": str,\n        \"score\": int,\n        \"certainty\": int\n    }\n    scoring_task: Optional[str] = None\n    scoring_set: List[int] = [1, 2]\n    scoring_rules: str = \"Your scores should follow the defined schema.\"\n    reasoning: str = \"brief\"\n    max_retries: int = DEFAULT_MAX_RETRIES\n</code></pre>"},{"location":"api/agents/#key-attributes_1","title":"Key Attributes","text":"<ul> <li><code>response_format</code>: Structure of the scoring output, including reasoning, score, and certainty.</li> <li><code>scoring_task</code>: Description of the scoring task.</li> <li><code>scoring_set</code>: Valid scoring values.</li> <li><code>scoring_rules</code>: Rules to guide scoring decisions.</li> <li><code>reasoning</code>: Type of reasoning employed.</li> <li><code>max_retries</code>: Maximum retry attempts.</li> </ul>"},{"location":"api/agents/#key-methods_1","title":"Key Methods","text":"<ul> <li><code>review_items(self, text_input_strings: List[str], image_path_lists: List[List[str]] = None)</code>: Processes multiple items concurrently.</li> <li><code>review_item(self, text_input_string: str, image_path_list: List[str] = [])</code>: Processes a single item.</li> </ul>"},{"location":"api/agents/#abstractionreviewer-class","title":"AbstractionReviewer Class","text":""},{"location":"api/agents/#overview_3","title":"Overview","text":"<p>The <code>AbstractionReviewer</code> is a specialized agent for extracting structured information from inputs. It leverages predefined keys and detailed instructions for consistent extraction tasks.</p>"},{"location":"api/agents/#class-definition_2","title":"Class Definition","text":"<pre><code>class AbstractionReviewer(BasicReviewer):\n    generic_prompt: Optional[str] = generic_prompt\n    input_description: str = \"article title/abstract\"\n    abstraction_keys: Dict\n    key_descriptions: Dict\n    max_retries: int = DEFAULT_MAX_RETRIES\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Initialize after Pydantic model initialization.\"\"\"\n        try:\n            assert self.reasoning == None, \"Reasoning type should be None for AbstractionReviewer\"\n            self.response_format = self.abstraction_keys\n            self.setup()\n        except Exception as e:\n            raise AgentError(f\"Error initializing agent: {str(e)}\")\n</code></pre>"},{"location":"api/agents/#key-attributes_2","title":"Key Attributes","text":"<ul> <li><code>generic_prompt</code>: Template string for creating prompts.</li> <li><code>input_description</code>: Description of the expected input.</li> <li><code>abstraction_keys</code>: Specifies the keys to extract from the input.</li> <li><code>key_descriptions</code>: Provides detailed descriptions or guidelines for each key.</li> <li><code>max_retries</code>: Retry limit for failed tasks.</li> </ul>"},{"location":"api/agents/#key-methods_2","title":"Key Methods","text":"<ul> <li><code>model_post_init(self, __context: Any)</code>: Ensures the initialization matches the agent\u2019s abstraction-specific requirements.</li> <li><code>review_items(self, text_input_strings: List[str], image_path_lists: List[List[str]] = None)</code>: Handles multiple inputs.</li> <li><code>review_item(self, text_input_string: str, image_path_list: List[str] = [])</code>: Processes a single input.</li> </ul>"},{"location":"api/agents/#titleabstractreviewer-class","title":"TitleAbstractReviewer Class","text":""},{"location":"api/agents/#overview_4","title":"Overview","text":"<p>The <code>TitleAbstractReviewer</code> is a specialized agent for reviewing and evaluating article titles and abstracts. It supports the evaluation of inclusion and exclusion criteria and provides a reasoning-backed evaluation score.</p>"},{"location":"api/agents/#class-definition_3","title":"Class Definition","text":"<pre><code>class TitleAbstractReviewer(BasicReviewer):\n    generic_prompt: Optional[str] = generic_prompt\n    inclusion_criteria: str = \"\"\n    exclusion_criteria: str = \"\"\n    response_format: Dict[str, Any] = {\"reasoning\": str, \"evaluation\": int}\n    input_description: str = \"article title/abstract\"\n    reasoning: str = \"brief\"\n    max_retries: int = DEFAULT_MAX_RETRIES\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Initialize after Pydantic model initialization.\"\"\"\n        try:\n            assert self.reasoning != None, \"Reasoning type cannot be None for TitleAbstractReviewer\"\n            self.setup()\n        except Exception as e:\n            raise AgentError(f\"Error initializing agent: {str(e)}\")\n</code></pre>"},{"location":"api/agents/#key-attributes_3","title":"Key Attributes","text":"<ul> <li><code>generic_prompt</code>: Template string for creating prompts.</li> <li><code>inclusion_criteria</code>: Criteria for including an article.</li> <li><code>exclusion_criteria</code>: Criteria for excluding an article.</li> <li><code>response_format</code>: Structure of the evaluation output, including reasoning and evaluation.</li> <li><code>input_description</code>: Description of the expected input.</li> <li><code>reasoning</code>: Type of reasoning employed.</li> <li><code>max_retries</code>: Retry limit for failed tasks.</li> </ul>"},{"location":"api/agents/#key-methods_3","title":"Key Methods","text":"<ul> <li><code>model_post_init(self, __context: Any)</code>: Ensures the initialization matches the agent\u2019s requirements.</li> <li><code>review_items(self, text_input_strings: List[str], image_path_lists: List[List[str]] = None)</code>: Handles multiple inputs.</li> <li><code>review_item(self, text_input_string: str, image_path_list: List[str] = [])</code>: Processes a single input.</li> </ul>"},{"location":"api/agents/#error-handling","title":"Error Handling","text":"<p>The module implements robust error handling:</p> <ul> <li>Custom Exceptions: Classes like <code>AgentError</code> and <code>ReviewWorkflowError</code> handle specific errors.</li> <li>Retry Mechanisms: Configurable retry limits for failed tasks.</li> <li>Validation: Uses Pydantic models for input and output validation.</li> </ul>"},{"location":"api/agents/#usage-examples","title":"Usage Examples","text":""},{"location":"api/agents/#scoringreviewer-example","title":"ScoringReviewer Example","text":"<pre><code>from lattereview.agents import ScoringReviewer\nfrom lattereview.providers import OpenAIProvider\n\n# Create a ScoringReviewer instance\nreviewer = ScoringReviewer(\n    provider=OpenAIProvider(model=\"gpt-4o\"),\n    name=\"ContentQualityReviewer\",\n    scoring_task=\"Assess the quality of given content\",\n    scoring_set=[1, 2, 3, 4, 5],\n    reasoning=\"brief\"\n)\n\n# Review content\ntext_items = [\"Content piece A\", \"Content piece B\"]\nresults, costs = await reviewer.review_items(text_items)\n</code></pre>"},{"location":"api/agents/#titleabstractreviewer-example","title":"TitleAbstractReviewer Example","text":"<pre><code>from lattereview.agents import TitleAbstractReviewer\nfrom lattereview.providers import OpenAIProvider\n\n# Create a TitleAbstractReviewer instance\nreviewer = TitleAbstractReviewer(\n    provider=OpenAIProvider(model=\"gpt-4o\"),\n    inclusion_criteria=\"Must be a peer-reviewed study\",\n    exclusion_criteria=\"Does not focus on AI in healthcare\",\n    reasoning=\"brief\"\n)\n\n# Review titles and abstracts\ntext_items = [\n    \"Title: Advances in AI\\nAbstract: A review of recent progress...\",\n    \"Title: Basic Chemistry\\nAbstract: Discussion of periodic elements...\"\n]\nresults, costs = await reviewer.review_items(text_items)\n</code></pre>"},{"location":"api/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling:</li> <li>Implement retry mechanisms.</li> <li>Log errors for debugging.</li> <li>Performance Optimization:</li> <li>Set appropriate concurrency limits.</li> <li>Process tasks in batches.</li> <li>Prompt Engineering:</li> <li>Define clear and concise prompts.</li> <li>Include examples for better task understanding.</li> </ol>"},{"location":"api/agents/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Enhanced support for collaborative workflows.</li> <li>Advanced memory and cost management.</li> <li>New agent types for diverse tasks.</li> </ul>"},{"location":"api/providers/","title":"Providers Module Documentation","text":"<p>This module provides different language model provider implementations for the LatteReview package. It handles interactions with various LLM APIs in a consistent, type-safe manner and now supports image-based inputs in addition to text inputs.</p>"},{"location":"api/providers/#overview","title":"Overview","text":"<p>The providers module includes:</p> <ul> <li><code>BaseProvider</code>: Abstract base class defining the provider interface</li> <li><code>OpenAIProvider</code>: Implementation for OpenAI API (including GPT models)</li> <li><code>OllamaProvider</code>: Implementation for local Ollama models</li> <li><code>LiteLLMProvider</code>: Implementation using LiteLLM for unified API access</li> </ul> <p>** You can use any of the models offered by the providers above as far as they support structured outputs. **</p>"},{"location":"api/providers/#baseprovider","title":"BaseProvider","text":""},{"location":"api/providers/#description","title":"Description","text":"<p>The <code>BaseProvider</code> class serves as the foundation for all LLM provider implementations. It provides a consistent interface and error handling for interacting with language models.</p>"},{"location":"api/providers/#class-definition","title":"Class Definition","text":"<pre><code>class BaseProvider(pydantic.BaseModel):\n    provider: str = \"DefaultProvider\"\n    client: Optional[Any] = None\n    api_key: Optional[str] = None\n    model: str = \"default-model\"\n    system_prompt: str = \"You are a helpful assistant.\"\n    response_format: Optional[Any] = None\n    last_response: Optional[Any] = None\n</code></pre>"},{"location":"api/providers/#error-types","title":"Error Types","text":"<pre><code>class ProviderError(Exception): pass\nclass ClientCreationError(ProviderError): pass\nclass ResponseError(ProviderError): pass\nclass InvalidResponseFormatError(ProviderError): pass\nclass ClientNotInitializedError(ProviderError): pass\n</code></pre>"},{"location":"api/providers/#core-methods","title":"Core Methods","text":""},{"location":"api/providers/#create_client","title":"<code>create_client()</code>","text":"<p>Abstract method for initializing the provider's client.</p> <pre><code>def create_client(self) -&gt; Any:\n    \"\"\"Create and initialize the client for the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#get_response","title":"<code>get_response()</code>","text":"<p>Get a text or image response from the model.</p> <pre><code>async def get_response(\n    self,\n    input_prompt: str,\n    image_path_list: List[str],\n    message_list: Optional[List[Dict[str, str]]] = None,\n    system_message: Optional[str] = None,\n) -&gt; tuple[Any, Dict[str, float]]:\n    \"\"\"Get a response from the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#get_json_response","title":"<code>get_json_response()</code>","text":"<p>Get a JSON-formatted response from the model.</p> <pre><code>async def get_json_response(\n    self,\n    input_prompt: str,\n    image_path_list: List[str],\n    message_list: Optional[List[Dict[str, str]]] = None,\n    system_message: Optional[str] = None,\n) -&gt; tuple[Any, Dict[str, float]]:\n    \"\"\"Get a JSON-formatted response from the provider.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/providers/#openaiprovider","title":"OpenAIProvider","text":""},{"location":"api/providers/#description_1","title":"Description","text":"<p>Implementation for OpenAI's API, supporting both OpenAI models and Gemini models through a compatible endpoint. The updated version supports processing text and image inputs.</p>"},{"location":"api/providers/#class-definition_1","title":"Class Definition","text":"<pre><code>class OpenAIProvider(BaseProvider):\n    provider: str = \"OpenAI\"\n    api_key: str = None\n    client: Optional[openai.AsyncOpenAI] = None\n    model: str = \"gpt-4o-mini\"\n    response_format_class: Optional[Any] = None\n</code></pre>"},{"location":"api/providers/#key-features","title":"Key Features","text":"<ul> <li>Automatic API key handling from environment variables</li> <li>Support for OpenAI and Gemini models</li> <li>JSON response validation</li> <li>Image input handling</li> <li>Comprehensive error handling</li> </ul>"},{"location":"api/providers/#usage-example","title":"Usage Example","text":"<pre><code>from lattereview.providers import OpenAIProvider\n\n# Initialize with OpenAI model\nprovider = OpenAIProvider(model=\"gpt-4o\")\n\n# Initialize with Gemini model\nprovider = OpenAIProvider(model=\"gemini/gemini-1.5-flash\")\n\n# Get a response\nresponse, cost = await provider.get_response(\"What is the capital of the country shown in this map?\", [\"path/to/image.jpg\"])\n\n# Get JSON response\nprovider.set_response_format({\"key\": str, \"value\": int})\nresponse, cost = await provider.get_json_response(\"Format this as JSON\", [])\n</code></pre>"},{"location":"api/providers/#ollamaprovider","title":"OllamaProvider","text":""},{"location":"api/providers/#description_2","title":"Description","text":"<p>Implementation for local Ollama models, supporting both chat and streaming responses. The updated version handles image inputs for advanced tasks.</p>"},{"location":"api/providers/#class-definition_2","title":"Class Definition","text":"<pre><code>class OllamaProvider(BaseProvider):\n    provider: str = \"Ollama\"\n    client: Optional[AsyncClient] = None\n    model: str = \"llama3.2-vision:latest\"\n    response_format_class: Optional[Any] = None\n    invalid_keywords: List[str] = [\"temperature\", \"max_tokens\"]\n    host: str = \"http://localhost:11434\"\n</code></pre>"},{"location":"api/providers/#key-features_1","title":"Key Features","text":"<ul> <li>Local model support</li> <li>Streaming capability</li> <li>Free cost tracking (local models)</li> <li>Image input processing</li> <li>Connection management</li> </ul>"},{"location":"api/providers/#usage-example_1","title":"Usage Example","text":"<pre><code>from lattereview.providers import OllamaProvider\n\n# Initialize provider\nprovider = OllamaProvider(\n    model=\"llama3.2-vision:latest\",\n    host=\"http://localhost:11434\"\n)\n\n# Get response\nresponse, cost = await provider.get_response(\"What is the capital of the country shown in this map?\", [\"path/to/image1.png\"])\n\n# Get JSON response with schema\nprovider.set_response_format({\"answer\": str, \"confidence\": float})\nresponse, cost = await provider.get_json_response(\"What is the capital of France?\", [])\n\n# Stream response\nasync for chunk in provider.get_response(\"Tell me a story\", [], stream=True):\n    print(chunk, end=\"\")\n\n# Clean up\nawait provider.close()\n</code></pre>"},{"location":"api/providers/#litellmprovider","title":"LiteLLMProvider","text":""},{"location":"api/providers/#description_3","title":"Description","text":"<p>A unified provider implementation using LiteLLM, enabling access to multiple LLM providers through a single interface. It now supports text and image-based interactions.</p>"},{"location":"api/providers/#class-definition_3","title":"Class Definition","text":"<pre><code>class LiteLLMProvider(BaseProvider):\n    provider: str = \"LiteLLM\"\n    model: str = \"gpt-4o-mini\"\n    custom_llm_provider: Optional[str] = None\n    response_format_class: Optional[Any] = None\n</code></pre>"},{"location":"api/providers/#key-features_2","title":"Key Features","text":"<ul> <li>Support for multiple model providers</li> <li>JSON schema validation</li> <li>Image input handling</li> <li>Cost tracking integration</li> <li>Tool calls handling</li> </ul>"},{"location":"api/providers/#usage-example_2","title":"Usage Example","text":"<pre><code>from lattereview.providers import LiteLLMProvider\n\n# Initialize with different models\nprovider = LiteLLMProvider(model=\"gpt-4o-mini\")\n\n# Get response\nresponse, cost = await provider.get_response(\"What is the capital of the country shown in this map?\", [\"path/to/image1.png\"])\n\n# Get JSON response with schema\nprovider.set_response_format({\"answer\": str, \"confidence\": float})\nresponse, cost = await provider.get_json_response(\"What is the capital of France?\", [])\n</code></pre>"},{"location":"api/providers/#error-handling","title":"Error Handling","text":"<p>Common error scenarios:</p> <ul> <li>API key errors (missing or invalid keys)</li> <li>Unsupported model configurations</li> <li>Models not supporting structured outputs or JSON responses</li> <li>Invalid image file paths</li> </ul>"},{"location":"api/providers/#best-practices","title":"Best Practices","text":"<ol> <li>For all online APIs, prefer using LiteLLMProvider class as it provides unified access and error handling.</li> <li>For local APIs, use OllamaProvider directly (rather than through LiteLLMProvider) for better performance and control.</li> <li>Set API keys at the environment level using the python-dotenv package and .env files for better security.</li> </ol>"},{"location":"api/providers/#limitations","title":"Limitations","text":"<ul> <li>Requires async/await syntax for all operations</li> <li>Depends on external LLM providers' availability and stability</li> <li>Rate limits and quotas depend on provider capabilities</li> </ul>"},{"location":"api/workflows/","title":"Workflows Module Documentation","text":"<p>This module provides workflow management functionality for the LatteReview package, implementing multi-agent review orchestration. The updated version includes support for image-based inputs alongside text inputs in the review workflow.</p>"},{"location":"api/workflows/#overview","title":"Overview","text":"<p>The workflows module consists of one main class:</p> <ul> <li><code>ReviewWorkflow</code>: A class that orchestrates multi-agent review processes with support for sequential rounds, filtering, and handling both text and image inputs.</li> </ul>"},{"location":"api/workflows/#reviewworkflow","title":"ReviewWorkflow","text":""},{"location":"api/workflows/#description","title":"Description","text":"<p>The <code>ReviewWorkflow</code> class manages review workflows where multiple agents can review items across different rounds. It handles the orchestration of reviews, manages outputs, and provides content validation and cost tracking.</p>"},{"location":"api/workflows/#class-definition","title":"Class Definition","text":"<pre><code>class ReviewWorkflow(pydantic.BaseModel):\n    workflow_schema: List[Dict[str, Any]]\n    memory: List[Dict] = list()\n    reviewer_costs: Dict = dict()\n    total_cost: float = 0.0\n    verbose: bool = True\n</code></pre>"},{"location":"api/workflows/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>workflow_schema</code>: List of dictionaries defining the review process structure</li> <li><code>memory</code>: List storing the workflow's execution history</li> <li><code>reviewer_costs</code>: Dictionary tracking costs per reviewer and round</li> <li><code>total_cost</code>: Total accumulated cost of all reviews</li> <li><code>verbose</code>: Flag to enable/disable logging output</li> </ul>"},{"location":"api/workflows/#methods","title":"Methods","text":""},{"location":"api/workflows/#__call__","title":"<code>__call__()</code>","text":"<p>Execute the workflow on provided data.</p> <pre><code>async def __call__(self, data: Union[pd.DataFrame, Dict[str, Any]]) -&gt; pd.DataFrame:\n    \"\"\"Execute workflow on DataFrame or dictionary input.\"\"\"\n    # Returns DataFrame with review results\n</code></pre>"},{"location":"api/workflows/#run","title":"<code>run()</code>","text":"<p>Core method to execute the review workflow.</p> <pre><code>async def run(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Execute main review workflow.\"\"\"\n    # Process each round sequentially\n    # Returns updated DataFrame with review results\n</code></pre>"},{"location":"api/workflows/#get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Get total cost of workflow execution.</p> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"Return sum of all review costs.\"\"\"\n</code></pre>"},{"location":"api/workflows/#internal-methods","title":"Internal Methods","text":"<ul> <li><code>_format_text_input()</code>: Format input for reviewers</li> <li><code>_format_image_input()</code>: Validate and format image paths</li> <li><code>_log()</code>: Handle logging based on verbose setting</li> </ul>"},{"location":"api/workflows/#image-input-handling","title":"Image Input Handling","text":"<p>The updated workflow supports image inputs in addition to text inputs. Images are validated for file existence and format before being passed to reviewers.</p>"},{"location":"api/workflows/#usage-examples","title":"Usage Examples","text":""},{"location":"api/workflows/#creating-a-basic-review-workflow","title":"Creating a Basic Review Workflow","text":"<pre><code>from lattereview.workflows import ReviewWorkflow\nfrom lattereview.agents import ScoringReviewer\nfrom lattereview.providers import LiteLLMProvider\n\n# Create reviewers\nreviewer1 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o\"),\n    name=\"Initial\",\n    scoring_task=\"Initial paper screening\"\n)\n\nreviewer2 = ScoringReviewer(\n    provider=LiteLLMProvider(model=\"gpt-4o\"),\n    name=\"Expert\",\n    scoring_task=\"Detailed technical review\"\n)\n\n# Define workflow schema\nworkflow_schema = [\n    {\n        \"round\": 'A',\n        \"reviewers\": reviewer1,\n        \"text_inputs\": [\"title\", \"abstract\"],\n        \"image_inputs\": [\"figure_path\"]\n    },\n    {\n        \"round\": 'B',\n        \"reviewers\": reviewer2,\n        \"text_inputs\": [\"title\", \"abstract\", \"round-A_Initial_output\"],\n        \"filter\": lambda row: row[\"round-A_Initial_score\"] &gt; 3\n    }\n]\n\n# Create and run workflow\nworkflow = ReviewWorkflow(workflow_schema=workflow_schema)\nresults = await workflow(input_data)\n</code></pre>"},{"location":"api/workflows/#understanding-workflow-construction","title":"Understanding Workflow Construction","text":"<p>A workflow is defined by a list of dictionaries where each dictionary represents a review round. The rounds are executed sequentially in the order they appear in the list. Each round dictionary must contain:</p> <p>Required Arguments:</p> <ul> <li><code>round</code>: A string identifier for the round (e.g., 'A', 'B', '1', 'initial')</li> <li><code>reviewers</code>: A single ScoringReviewer or list of ScoringReviewers</li> <li><code>text_inputs</code>: A string or list of strings representing DataFrame column names to use</li> </ul> <p>Optional Arguments:</p> <ul> <li><code>image_inputs</code>: A list of DataFrame column names containing paths to image files</li> <li><code>filter</code>: A lambda function that determines which rows to review in this round</li> </ul>"},{"location":"api/workflows/#handling-results","title":"Handling Results","text":"<pre><code># Execute workflow\nworkflow = ReviewWorkflow(workflow_schema=workflow_schema)\ntry:\n    results_df = await workflow(input_df)\n\n    # Access costs\n    total_cost = workflow.get_total_cost()\n    per_reviewer = workflow.reviewer_costs\n\n    # Access results\n    round_a_scores = results_df[\"round-A_Initial_score\"]\n    round_b_reasoning = results_df[\"round-B_Expert_reasoning\"]\n\nexcept ReviewWorkflowError as e:\n    print(f\"Workflow failed: {e}\")\n</code></pre>"},{"location":"api/workflows/#error-handling","title":"Error Handling","text":"<p>The module uses <code>ReviewWorkflowError</code> for all workflow-related errors:</p> <pre><code>class ReviewWorkflowError(Exception):\n    \"\"\"Base exception for workflow-related errors.\"\"\"\n    pass\n</code></pre> <p>Common error scenarios:</p> <ul> <li>Invalid workflow schema</li> <li>Missing input columns</li> <li>Reviewer initialization failures</li> <li>Content validation errors</li> <li>Output processing failures</li> <li>Invalid image file paths</li> </ul>"},{"location":"api/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Schema Design</p> </li> <li> <p>Use meaningful round identifiers</p> </li> <li>Design filter functions carefully</li> <li> <p>Consider round dependencies</p> </li> <li> <p>Data Management</p> </li> <li> <p>Validate input data structure</p> </li> <li>Handle missing values appropriately</li> <li> <p>Use appropriate column names</p> </li> <li> <p>Cost Control</p> </li> <li> <p>Monitor per-round costs</p> </li> <li>Set appropriate concurrent request limits</li> <li> <p>Track total workflow costs</p> </li> <li> <p>Error Handling</p> </li> <li>Implement proper exception handling</li> <li>Validate workflow schema</li> <li>Monitor review outputs</li> </ol>"},{"location":"api/workflows/#limitations","title":"Limitations","text":"<ul> <li>Sequential round execution only</li> <li>Memory grows with number of reviews</li> <li>No direct reviewer communication</li> <li>Limited to DataFrame-based workflows</li> <li>Requires async/await syntax</li> <li>Filter functions must be serializable</li> </ul>"}]}