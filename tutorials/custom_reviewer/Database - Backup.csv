Manuscript Title,Manuscript Abstract,Year of publication,DOI,Study type,Clinical Application,Target modalities,Organ(s) of Interest
(18)F-FDG PET/CT Uptake Classification in Lymphoma and Lung Cancer by Using Deep Convolutional Neural Networks,"Background Fluorine 18 ((18)F)-fluorodeoxyglucose (FDG) PET/CT is a routine tool for staging patients with lymphoma and lung cancer. Purpose To evaluate configurations of deep convolutional neural networks (CNNs) to localize and classify uptake patterns of whole-body (18)F-FDG PET/CT images in patients with lung cancer and lymphoma. Materials and Methods This was a retrospective analysis of consecutive patients with lung cancer or lymphoma referred to a single center from August 2011 to August 2013. Two nuclear medicine experts manually delineated foci with increased (18)F-FDG uptake, specified the anatomic location, and classified these findings as suspicious for tumor or metastasis or nonsuspicious. By using these expert readings as the reference standard, a CNN was developed to detect foci positive for (18)F-FDG uptake, predict the anatomic location, and determine the expert classification. Examinations were divided into independent training (60%), validation (20%), and test (20%) subsets. Results This study included 629 patients (mean age, 52.2 years ± 20.4 [standard deviation]; 394 men). There were 302 patients with lung cancer and 327 patients with lymphoma. For the test set (123 patients; 10 782 foci), the CNN areas under the receiver operating characteristic curve (AUCs) for determining hypermetabolic (18)F-FDG PET/CT foci that were suspicious for cancer versus nonsuspicious by using the five input features were as follows: CT alone, 0.78 (95% confidence interval [CI]: 0.72, 0.83); (18)F-FDG PET alone, 0.97 (95% CI: 0.97, 0.98); (18)F-FDG PET/CT, 0.98 (95% CI: 0.97, 0.99); (18)F-FDG PET/CT maximum intensity projection (MIP), 0.98 (95% CI: 0.98, 0.99); and (18)F-FDG PET/CT MIP atlas, 0.99 (95% CI: 0.98, 1.00). The combination of (18)F-FDG PET and CT information improved overall classification accuracy (AUC, 0.975 vs 0.981, respectively; P < .001). Anatomic localization accuracy of the CNN was 2543 of 2639 (96.4%; 95% CI: 95.5%, 97.1%) for body part, 2292 of 2639 (86.9%; 95% CI: 85.3%, 88.5%) for region (ie, organ), and 2149 of 2639 (81.4%; 95% CI: 79.3%-83.5%) for subregion. Conclusion The fully automated anatomic localization and classification of fluorine 18-fluorodeoxyglucose PET uptake patterns in foci suspicious and nonsuspicious for cancer in patients with lung cancer and lymphoma by using a convolutional neural network is feasible and achieves high diagnostic performance when both CT and PET images are used. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Froelich and Salavati in this issue.",2020,10.1148/radiol.2019191114,cross-sectional,diagnosis,"(CT, PET, and PET/CT)",(Lung) and whole body
(18)F-FDG-PET/CT Whole-Body Imaging Lung Tumor Diagnostic Model: An Ensemble E-ResNet-NRC with Divided Sample Space,"Under the background of (18)F-FDG-PET/CT multimodal whole-body imaging for lung tumor diagnosis, for the problems of network degradation and high dimension features during convolutional neural network (CNN) training, beginning with the perspective of dividing sample space, an E-ResNet-NRC (ensemble ResNet nonnegative representation classifier) model is proposed in this paper. The model includes the following steps: (1) Parameters of a pretrained ResNet model are initialized using transfer learning. (2) Samples are divided into three different sample spaces (CT, PET, and PET/CT) based on the differences in multimodal medical images PET/CT, and ROI of the lesion was extracted. (3) The ResNet neural network was used to extract ROI features and obtain feature vectors. (4) Individual classifier ResNet-NRC was constructed with nonnegative representation NRC at a fully connected layer. (5) Ensemble classifier E-ResNet-NRC was constructed using the ""relative majority voting method."" Finally, two network models, AlexNet and ResNet-50, and three classification algorithms, nearest neighbor classification algorithm (NNC), softmax, and nonnegative representation classification algorithm (NRC), were combined to compare with the E-ResNet-NRC model in this paper. The experimental results show that the overall classification performance of the Ensemble E-ResNet-NRC model is better than the individual ResNet-NRC, and specificity and sensitivity are more higher; the E-ResNet-NRC has better robustness and generalization ability.",2021,10.1155/2021/8865237,cross-sectional,diagnosis,"(CT, PET, and PET/CT)",Lung
[(123)I]Metaiodobenzylguanidine (MIBG) Cardiac Scintigraphy and Automated Classification Techniques in Parkinsonian Disorders,"PURPOSE: To provide reliable and reproducible heart/mediastinum (H/M) ratio cut-off values for parkinsonian disorders using two machine learning techniques, Support Vector Machines (SVM) and Random Forest (RF) classifier, applied to [(123)I]MIBG cardiac scintigraphy. PROCEDURES: We studied 85 subjects, 50 with idiopathic Parkinson's disease, 26 with atypical Parkinsonian syndromes (P), and 9 with essential tremor (ET). All patients underwent planar early and delayed cardiac scintigraphy after [(123)I]MIBG (111 MBq) intravenous injection. Images were evaluated both qualitatively and quantitatively; the latter by the early and delayed H/M ratio obtained from regions of interest (ROIt(1) and ROIt(2)) drawn on planar images. SVM and RF classifiers were finally used to obtain the correct cut-off value. RESULTS: SVM and RF produced excellent classification performances: SVM classifier achieved perfect classification and RF also attained very good accuracy. The better cut-off for H/M value was 1.55 since it remains the same for both ROIt(1) and ROIt(2.) This value allowed to correctly classify PD from P and ET: patients with H/M ratio less than 1.55 were classified as PD while those with values higher than 1.55 were considered as affected by parkinsonism and/or ET. No difference was found when early or late H/M ratio were considered separately thus suggesting that a single early evaluation could be sufficient to obtain the final diagnosis. CONCLUSIONS: Our results evidenced that the use of SVM and CT permitted to define the better cut-off value for H/M ratios both in early and in delayed phase thus underlining the role of [(123)I]MIBG cardiac scintigraphy and the effectiveness of H/M ratio in differentiating PD from other parkinsonism or ET. Moreover, early scans alone could be used for a reliable diagnosis since no difference was found between early and late. Definitely, a larger series of cases is needed to confirm this data.",2020,10.1007/s11307-019-01406-6,cross-sectional,diagnosis,cardiac scintigraphy,Heart?
[Formula: see text]: deep learning-based radiomics for the time-to-event outcome prediction in lung cancer,"Hand-crafted radiomics has been used for developing models in order to predict time-to-event clinical outcomes in patients with lung cancer. Hand-crafted features, however, are pre-defined and extracted without taking the desired target into account. Furthermore, accurate segmentation of the tumor is required for development of a reliable predictive model, which may be objective and a time-consuming task. To address these drawbacks, we propose a deep learning-based radiomics model for the time-to-event outcome prediction, referred to as DRTOP that takes raw images as inputs, and calculates the image-based risk of death or recurrence, for each patient. Our experiments on an in-house dataset of 132 lung cancer patients show that the obtained image-based risks are significant predictors of the time-to-event outcomes. Computed Tomography (CT)-based features are predictors of the overall survival (OS), with the hazard ratio (HR) of 1.35, distant control (DC), with HR of 1.06, and local control (LC), with HR of 2.66. The Positron Emission Tomography (PET)-based features are predictors of OS and recurrence free survival (RFS), with hazard ratios of 1.67 and 1.18, respectively. The concordance indices of [Formula: see text], [Formula: see text], and [Formula: see text] for predicting the OS, DC, and RFS show that the deep learning-based radiomics model is as accurate or better in predicting predefined clinical outcomes compared to hand-crafted radiomics, with concordance indices of [Formula: see text], [Formula: see text], and [Formula: see text], for predicting the OS, DC, and RFS, respectively. Deep learning-based radiomics has the potential to offer complimentary predictive information in the personalized management of lung cancer patients.",2020,10.1038/s41598-020-69106-8,cross-sectional,prognosis,"(CT, PET)",Lung
3-D Convolutional Neural Networks for Automatic Detection of Pulmonary Nodules in Chest CT,"Deep two-dimensional (2-D) convolutional neural networks (CNNs) have been remarkably successful in producing record-breaking results in a variety of computer vision tasks. It is possible to extend CNNs to three dimensions using 3-D kernels to make them suitable for volumetric medical imaging data such as CT or MRI, but this increases the processing time as well as the required number of training samples (due to the higher number of parameters that need to be learned). In this paper, we address both of these issues for a 3-D CNN implementation through the development of a two-stage computer-aided detection system for automatic detection of pulmonary nodules. The first stage consists of a 3-D fully convolutional network for fast screening and generation of candidate suspicious regions. The second stage consists of an ensemble of 3-D CNNs trained using extensive transformations applied to both the positive and negative patches to augment the training set. To enable the second stage classifiers to learn differently, they are trained on false positive patches obtained from the screening model using different thresholds on their associated scores as well as different augmentation types. The networks in the second stage are averaged together to produce the final classification score for each candidate patch. Using this procedure, our overall nodule detection system called DeepMed is fast and can achieve 91% sensitivity at 2 false positives per scan on cases from the LIDC dataset.",2019,10.1109/jbhi.2018.2879449,cross-sectional,diagnosis,CT,Lung
3-D Neural denoising for low-dose Coronary CT Angiography (CCTA),"CCTA has become an important tool for coronary arteries assessment in low and medium risk patients. However, it exposes the patient to significant radiation doses, resulting from high image quality requirements and acquisitions at multiple cardiac phases. For widespread use of CCTA for coronary assessment, significant reduction of radiation exposure with minimal image quality loss is still needed. A neural denoising scheme, relying on a fully convolutional neural network (FCNN) architecture, is developed and applied to noisy CCTA. In contrast to previously published methods, the proposed FCNN is trained directly on 3-D CT data patches (blocks), implementing 3-D convolutions. Considering that anatomy is inherently tridimensional, the proposed 3-D approach may better capture and enforce inter-slice continuity of tiny structures. While training is performed on individual blocks, whole input scans can be fed and denoised in one piece, thus leveraging the fully convolutional architecture to maximize processing speed. The proposed method is compared to state-of-the-art denoising algorithms on a dataset of 45 CCTA scans. Low-dose scans are simulated by synthetic Poisson noise applied to the sinogram corresponding to a 90% reduction in radiation dose. The average feature similarity score (0.864) and the peak signal-to-noise ratio (41.47) obtained for the proposed algorithm outperformed the compared methods while requiring significantly shorter processing time. A set of 2-D FCNNs, structurally similar to the proposed 3-D network, are also implemented to demonstrate contribution of the additional dimension to the improved denoising. For further validation of the method coronary reconstruction using the Intellispace cardiac tool (Philips, Holland) is performed both on a real noisy CCTA scan and on the denoised scan using the proposed method. It is shown that the cardiac tool succeeds in reconstructing more coronaries using the scan denoised by the proposed method. The obtained results suggest the proposed method provides an efficient and powerful approach to low-dose CCTA denoising.",2018,10.1016/j.compmedimag.2018.07.004,cross-sectional,others (Denoising tool),CT angiogram,Vessels
3-D Quantification of Filopodia in Motile Cancer Cells,"We present a 3D bioimage analysis workflow to quantitatively analyze single, actin-stained cells with filopodial protrusions of diverse structural and temporal attributes, such as number, length, thickness, level of branching, and lifetime, in time-lapse confocal microscopy image data. Our workflow makes use of convolutional neural networks trained using real as well as synthetic image data, to segment the cell volumes with highly heterogeneous fluorescence intensity levels and to detect individual filopodial protrusions, followed by a constrained nearest-neighbor tracking algorithm to obtain valuable information about the spatio-temporal evolution of individual filopodia. We validated the workflow using real and synthetic 3-D time-lapse sequences of lung adenocarcinoma cells of three morphologically distinct filopodial phenotypes and show that it achieves reliable segmentation and tracking performance, providing a robust, reproducible and less time-consuming alternative to manual analysis of the 3D+t image data.",2019,10.1109/tmi.2018.2873842,,,,
3D CNN with Visual Insights for Early Detection of Lung Cancer Using Gradient-Weighted Class Activation,"The 3D convolutional neural network is able to make use of the full nonlinear 3D context information of lung nodule detection from the DICOM (Digital Imaging and Communications in Medicine) images, and the Gradient Class Activation has shown to be useful for tailoring classification tasks and localization interpretation for fine-grained features and visual explanation for the internal working. Gradient-weighted class activation plays a crucial role for clinicians and radiologists in terms of trusting and adopting the model. Practitioners not only rely on a model that can provide high precision but also really want to gain the respect of radiologists. So, in this paper, we explored the lung nodule classification using the improvised 3D AlexNet with lightweight architecture. Our network employed the full nature of the multiview network strategy. We have conducted the binary classification (benign and malignant) on computed tomography (CT) images from the LUNA 16 database conglomerate and database image resource initiative. The results obtained are through the 10-fold cross-validation. Experimental results have shown that the proposed lightweight architecture achieved a superior classification accuracy of 97.17% on LUNA 16 dataset when compared with existing classification algorithms and low-dose CT scan images as well.",2021,10.1155/2021/6695518,cross-sectional,diagnosis,CT,Lung
3D deep learning based classification of pulmonary ground glass opacity nodules with automatic segmentation,"Classifying ground-glass lung nodules (GGNs) into atypical adenomatous hyperplasia (AAH), adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC) on diagnostic CT images is important to evaluate the therapy options for lung cancer patients. In this paper, we propose a joint deep learning model where the segmentation can better facilitate the classification of pulmonary GGNs. Based on our observation that masking the nodule to train the model results in better lesion classification, we propose to build a cascade architecture with both segmentation and classification networks. The segmentation model works as a trainable preprocessing module to provide the classification-guided 'attention' weight map to the raw CT data to achieve better diagnosis performance. We evaluate our proposed model and compare with other baseline models for 4 clinically significant nodule classification tasks, defined by a combination of pathology types, using 4 classification metrics: Accuracy, Average F1 Score, Matthews Correlation Coefficient (MCC), and Area Under the Receiver Operating Characteristic Curve (AUC). Experimental results show that the proposed method outperforms other baseline models on all the diagnostic classification tasks.",2021,10.1016/j.compmedimag.2020.101814,cross-sectional,diagnosis,CT,Lung
3D deep learning for detecting pulmonary nodules in CT scans,"OBJECTIVE: To demonstrate and test the validity of a novel deep-learning-based system for the automated detection of pulmonary nodules. MATERIALS AND METHODS: The proposed system uses 2 3D deep learning models, 1 for each of the essential tasks of computer-aided nodule detection: candidate generation and false positive reduction. A total of 888 scans from the LIDC-IDRI dataset were used for training and evaluation. RESULTS: Results for candidate generation on the test data indicated a detection rate of 94.77% with 30.39 false positives per scan, while the test results for false positive reduction exhibited a sensitivity of 94.21% with 1.789 false positives per scan. The overall system detection rate on the test data was 89.29% with 1.789 false positives per scan. DISCUSSION: An extensive and rigorous validation was conducted to assess the performance of the proposed system. The system demonstrated a novel combination of 3D deep neural network architectures and demonstrates the use of deep learning for both candidate generation and false positive reduction to be evaluated with a substantial test dataset. The results strongly support the ability of deep learning pulmonary nodule detection systems to generalize to unseen data. The source code and trained model weights have been made available. CONCLUSION: A novel deep-neural-network-based pulmonary nodule detection system is demonstrated and validated. The results provide comparison of the proposed deep-learning-based system over other similar systems based on performance.",2018,10.1093/jamia/ocy098,cross-sectional,diagnosis,CT,Lung
3D Deep Learning from CT Scans Predicts Tumor Invasiveness of Subcentimeter Pulmonary Adenocarcinomas,,2018,"10.1158/0008-5472.Can-18-0696: Identification of early-stage pulmonary adenocarcinomas before surgery, especially in cases of subcentimeter cancers, would be clinically important and could provide guidance to clinical decision making. In this study, we developed a deep learning system based on 3D convolutional neural networks and multitask learning, which automatically predicts tumor invasiveness, together with 3D nodule segmentation masks. The system processes a 3D nodule-centered patch of preprocessed CT and learns a deep representation of a given nodule without the need for any additional information. A dataset of 651 nodules with manually segmented voxel-wise masks and pathological labels of atypical adenomatous hyperplasia (AAH), adenocarcinomas in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive pulmonary adenocarcinoma (IA) was used in this study. We trained and validated our deep learning system on 523 nodules and tested its performance on 128 nodules. An observer study with 2 groups of radiologists, 2 senior and 2 junior, was also investigated. We merged AAH and AIS into one single category AAH-AIS, comprising a 3-category classification in our study. The proposed deep learning system achieved better classification performance than the radiologists; in terms of 3-class weighted average F1 score, the model achieved 63.3% while the radiologists achieved 55.6%, 56.6%, 54.3%, and 51.0%, respectively. These results suggest that deep learning methods improve the yield of discriminative results and hold promise in the CADx application domain, which could help doctors work efficiently and facilitate the application of precision medicine. SIGNIFICANCE: Machine learning tools are beginning to be implemented for clinical applications. This study represents an important milestone for this emerging technology, which could improve therapy selection for patients with lung cancer.",cross-sectional,diagnosis,CT,Lung
3D gray density coding feature for benign-malignant pulmonary nodule classification on chest CT,"PURPOSE: Early detection is significant to reduce lung cancer-related death. Computer-aided detection system (CADs) can help radiologists to make an early diagnosis. In this paper, we propose a novel 3D gray density coding feature (3D GDC) and fuse it with extracted geometric features. The fusion feature and random forest are used for benign-malignant pulmonary nodule classification on Chest CT. METHODS: First, a dictionary model is created to acquire codebook. It is used to obtain feature descriptors and includes 3D block database (BD) and distance matrix clustering centers. 3D BD is balanced and randomly selecting from benign and malignant pulmonary nodules of training data. Clustering centers is got by clustering the distance matrix, which is the distance between every two blocks in 3D BD. Then, feature descriptor is obtained by coding the pulmonary nodule with codebook, and 3D GDC feature is the result of histogram statistics on feature descriptor. Second, geometric features are extracted for fusion feature. Finally, random forest is performed for benign-malignant pulmonary nodule classification with fusion feature of the 3D gray density coding feature and the geometric features. RESULTS: We verify the effectiveness of our method on the public LIDC-IDRI dataset and the private ZSHD dataset. For LIDC-IDRI dataset, compared with other state-of-the-art methods, we achieve more satisfactory results with 93.17 ± 1.94% for accuracy and 97.53 ± 1.62% for AUC. As for private ZSHD dataset, it contains a total of 238 lung nodules from 203 patients. The accuracy and AUC achieved by our method are 90.0% and 93.15%. CONCLUSIONS: The results show that our method can provide doctors with more accurate results of benign-malignant pulmonary nodule classification for auxiliary diagnosis, and our method is more interpretable than 3D CNN methods, which can provide doctors with more auxiliary information.",2021,10.1002/mp.15298,,,,
3D Kinect Camera Scheme with Time-Series Deep-Learning Algorithms for Classification and Prediction of Lung Tumor Motility,"This paper proposes a time-series deep-learning 3D Kinect camera scheme to classify the respiratory phases with a lung tumor and predict the lung tumor displacement. Specifically, the proposed scheme is driven by two time-series deep-learning algorithmic models: the respiratory-phase classification model and the regression-based prediction model. To assess the performance of the proposed scheme, the classification and prediction models were tested with four categories of datasets: patient-based datasets with regular and irregular breathing patterns; and pseudopatient-based datasets with regular and irregular breathing patterns. In this study, 'pseudopatients' refer to a dynamic thorax phantom with a lung tumor programmed with varying breathing patterns and breaths per minute. The total accuracy of the respiratory-phase classification model was 100%, 100%, 100%, and 92.44% for the four dataset categories, with a corresponding mean squared error (MSE), mean absolute error (MAE), and coefficient of determination (R(2)) of 1.2-1.6%, 0.65-0.8%, and 0.97-0.98, respectively. The results demonstrate that the time-series deep-learning classification and regression-based prediction models can classify the respiratory phases and predict the lung tumor displacement with high accuracy. Essentially, the novelty of this research lies in the use of a low-cost 3D Kinect camera with time-series deep-learning algorithms in the medical field to efficiently classify the respiratory phase and predict the lung tumor displacement.",2022,10.3390/s22082918,cross-sectional,prognosis,CT,Lung
3D multi-scale deep convolutional neural networks for pulmonary nodule detection,"With the rapid development of big data and artificial intelligence technology, computer-aided pulmonary nodule detection based on deep learning has achieved some successes. However, the sizes of pulmonary nodules vary greatly, and the pulmonary nodules have visual similarity with structures such as blood vessels and shadows around pulmonary nodules, which make the quick and accurate detection of pulmonary nodules in CT image still a challenging task. In this paper, we propose two kinds of 3D multi-scale deep convolution neural networks for nodule candidate detection and false positive reduction respectively. Among them, the nodule candidate detection network consists of two parts: 1) the backbone network part Res2SENet, which is used to extract multi-scale feature information of pulmonary nodules, it is composed of the multi-scale Res2Net modules of multiple available receptive fields at a granular level and the squeeze-and-excitation units; 2) the detection part, which uses a region proposal network structure to determine region candidates, and introduces context enhancement module and spatial attention module to improve detection performance. The false positive reduction network, also composed of the multi-scale Res2Net modules and the squeeze-and-excitation units, can further classify the nodule candidates generated by the nodule candidate detection network and screen out the ground truth positive nodules. Finally, the prediction probability generated by the nodule candidate detection network is weighted average with the prediction probability generated by the false positive reduction network to obtain the final results. The experimental results on the publicly available LUNA16 dataset showed that the proposed method has a superior ability to detect pulmonary nodules in CT images.",2021,10.1371/journal.pone.0244406,cross-sectional,diagnosis,CT,Lung
"3D multi-scale, multi-task, and multi-label deep learning for prediction of lymph node metastasis in T1 lung adenocarcinoma patients' CT images","The diagnosis of preoperative lymph node (LN) metastasis is crucial to evaluate possible therapy options for T1 lung adenocarcinoma patients. Radiologists preoperatively diagnose LN metastasis by evaluating signs related to LN metastasis, like spiculation or lobulation of pulmonary nodules in CT images. However, this type of evaluation is subjective and time-consuming, which may result in poor consistency and low efficiency of diagnoses. In this study, a 3D Multi-scale, Multi-task, and Multi-label classification network (3M-CN) was proposed to predict LN metastasis, as well as evaluate multiple related signs of pulmonary nodules in order to improve the accuracy of LN metastasis prediction. The following key approaches were adapted for this method. First, a multi-scale feature fusion module was proposed to aggregate the features from different levels for which different labels be best modeled at different levels; second, an auxiliary segmentation task was applied to force the model to focus more on the nodule region and less on surrounding unrelated structures; and third, a cross-modal integration module called the refine layer was designed to integrate the related risk factors into the model to further improve its confidence level. The 3M-CN was trained using data from 401 cases and then validated on both internal and external datasets, which consisted of 100 cases and 53 cases, respectively. The proposed 3M-CN model was then compared with existing state-of-the-art methods for prediction of LN metastasis. The proposed model outperformed other methods, achieving the best performance with AUCs of 0.945 and 0.948 in the internal and external test datasets, respectively. The proposed model not only obtain strong generalization, but greatly enhance the interpretability of the deep learning model, increase doctors' confidence in the model results, conform to doctors' diagnostic process, and may also be transferable to the diagnosis of other diseases.",2021,10.1016/j.compmedimag.2021.101987,cross-sectional,prognosis,CT,Thorax
3D multi-view convolutional neural networks for lung nodule classification,"The 3D convolutional neural network (CNN) is able to make full use of the spatial 3D context information of lung nodules, and the multi-view strategy has been shown to be useful for improving the performance of 2D CNN in classifying lung nodules. In this paper, we explore the classification of lung nodules using the 3D multi-view convolutional neural networks (MV-CNN) with both chain architecture and directed acyclic graph architecture, including 3D Inception and 3D Inception-ResNet. All networks employ the multi-view-one-network strategy. We conduct a binary classification (benign and malignant) and a ternary classification (benign, primary malignant and metastatic malignant) on Computed Tomography (CT) images from Lung Image Database Consortium and Image Database Resource Initiative database (LIDC-IDRI). All results are obtained via 10-fold cross validation. As regards the MV-CNN with chain architecture, results show that the performance of 3D MV-CNN surpasses that of 2D MV-CNN by a significant margin. Finally, a 3D Inception network achieved an error rate of 4.59% for the binary classification and 7.70% for the ternary classification, both of which represent superior results for the corresponding task. We compare the multi-view-one-network strategy with the one-view-one-network strategy. The results reveal that the multi-view-one-network strategy can achieve a lower error rate than the one-view-one-network strategy.",2017,10.1371/journal.pone.0188290,cross-sectional,diagnosis,CT,Lung
3D virtual histopathology of cardiac tissue from Covid-19 patients based on phase-contrast X-ray tomography,"For the first time, we have used phase-contrast X-ray tomography to characterize the three-dimensional (3d) structure of cardiac tissue from patients who succumbed to Covid-19. By extending conventional histopathological examination by a third dimension, the delicate pathological changes of the vascular system of severe Covid-19 progressions can be analyzed, fully quantified and compared to other types of viral myocarditis and controls. To this end, cardiac samples with a cross-section of 3.5mm were scanned at a laboratory setup as well as at a parallel beam setup at a synchrotron radiation facility the synchrotron in a parallel beam configuration. The vascular network was segmented by a deep learning architecture suitable for 3d datasets (V-net), trained by sparse manual annotations. Pathological alterations of vessels, concerning the variation of diameters and the amount of small holes, were observed, indicative of elevated occurrence of intussusceptive angiogenesis, also confirmed by high-resolution cone beam X-ray tomography and scanning electron microscopy. Furthermore, we implemented a fully automated analysis of the tissue structure in the form of shape measures based on the structure tensor. The corresponding distributions show that the histopathology of Covid-19 differs from both influenza and typical coxsackie virus myocarditis.",2021,10.7554/eLife.71359,cross-sectional,diagnosis,X-Ray Tumography,Heart
3D-MCN: A 3D Multi-scale Capsule Network for Lung Nodule Malignancy Prediction,"Despite the advances in automatic lung cancer malignancy prediction, achieving high accuracy remains challenging. Existing solutions are mostly based on Convolutional Neural Networks (CNNs), which require a large amount of training data. Most of the developed CNN models are based only on the main nodule region, without considering the surrounding tissues. Obtaining high sensitivity is challenging with lung nodule malignancy prediction. Moreover, the interpretability of the proposed techniques should be a consideration when the end goal is to utilize the model in a clinical setting. Capsule networks (CapsNets) are new and revolutionary machine learning architectures proposed to overcome shortcomings of CNNs. Capitalizing on the success of CapsNet in biomedical domains, we propose a novel model for lung tumor malignancy prediction. The proposed framework, referred to as the 3D Multi-scale Capsule Network (3D-MCN), is uniquely designed to benefit from: (i) 3D inputs, providing information about the nodule in 3D; (ii) Multi-scale input, capturing the nodule's local features, as well as the characteristics of the surrounding tissues, and; (iii) CapsNet-based design, being capable of dealing with a small number of training samples. The proposed 3D-MCN architecture predicted lung nodule malignancy with a high accuracy of 93.12%, sensitivity of 94.94%, area under the curve (AUC) of 0.9641, and specificity of 90% when tested on the LIDC-IDRI dataset. When classifying patients as having a malignant condition (i.e., at least one malignant nodule is detected) or not, the proposed model achieved an accuracy of 83%, and a sensitivity and specificity of 84% and 81% respectively.",2020,10.1038/s41598-020-64824-5,cross-sectional,diagnosis,CT,Lung
3D-PulCNN: Pulmonary cancer classification from hyperspectral images using convolution combination unit based CNN,"Pulmonary cancer is one of the most common malignancies worldwide. Accurate classification of its subtypes is required in differential diagnosis. However, existing algorithms are mostly based on color images, and the improvement of accuracy is quite challenging. In this study, we propose a convolution combination unit (CCU)-based three-dimensional convolutional neural network (3D-PulCNN) for classifying pulmonary cancer presented in microscopic hyperspectral image with both spatial and spectral information. CCU is designed to fuse the features acquired by different convolution scales. Compared with VGGNet, only two fully connected layers are used in this model, reducing the network parameters and model complexity. Experimental results show that 3D-PulCNN achieves overall average (OA) of 0.962 and Precision, Recall, and Kappa of more than 0.920, superior to 2D-VGGNet. Then, 3D-UNet is leveraged to segment cancer cells, and their morphological characteristics are calculated to supply quantitative virtual analysis data for classification results explanation and prognosis assessment.",2021,10.1002/jbio.202100142,,,,
4D radiomics: impact of 4D-CBCT image quality on radiomic analysis,"PURPOSE: To investigate the impact of 4D-CBCT image quality on radiomic analysis and the efficacy of using deep learning based image enhancement to improve the accuracy of radiomic features of 4D-CBCT. MATERIAL AND METHODS: In this study, 4D-CT data from 16 lung cancer patients were obtained. Digitally reconstructed radiographs (DRRs) were simulated from the 4D-CT, and then used to reconstruct 4D CBCT using the conventional FDK (Feldkamp et al 1984 J. Opt. Soc. Am. A 1 612-9) algorithm. Different projection numbers (i.e. 72, 120, 144, 180) and projection angle distributions (i.e. evenly distributed and unevenly distributed using angles from real 4D-CBCT scans) were simulated to generate the corresponding 4D-CBCT. A deep learning model (TecoGAN) was trained on 10 patients and validated on 3 patients to enhance the 4D-CBCT image quality to match with the corresponding ground-truth 4D-CT. The remaining 3 patients with different tumor sizes were used for testing. The radiomic features in 6 different categories, including histogram, GLCM, GLRLM, GLSZM, NGTDM, and wavelet, were extracted from the gross tumor volumes of each phase of original 4D-CBCT, enhanced 4D-CBCT, and 4D-CT. The radiomic features in 4D-CT were used as the ground-truth to evaluate the errors of the radiomic features in the original 4D-CBCT and enhanced 4D-CBCT. Errors in the original 4D-CBCT demonstrated the impact of image quality on radiomic features. Comparison between errors in the original 4D-CBCT and enhanced 4D-CBCT demonstrated the efficacy of using deep learning to improve the radiomic feature accuracy. RESULTS: 4D-CBCT image quality can substantially affect the accuracy of the radiomic features, and the degree of impact is feature-dependent. The deep learning model was able to enhance the anatomical details and edge information in the 4D-CBCT as well as removing other image artifacts. This enhancement of image quality resulted in reduced errors for most radiomic features. The average reduction of radiomics errors for 3 patients are 20.0%, 31.4%, 36.7%, 50.0%, 33.6% and 11.3% for histogram, GLCM, GLRLM, GLSZM, NGTDM and Wavelet features. And the error reduction was more significant for patients with larger tumors. The findings were consistent across different respiratory phases, projection numbers, and angle distributions. CONCLUSIONS: The study demonstrated that 4D-CBCT image quality has a significant impact on the radiomic analysis. The deep learning-based augmentation technique proved to be an effective approach to enhance 4D-CBCT image quality to improve the accuracy of radiomic analysis.",2021,10.1088/1361-6560/abd668,cross-sectional,diagnosis,CT,Lung
4S-DT: Self-Supervised Super Sample Decomposition for Transfer Learning With Application to COVID-19 Detection,"Due to the high availability of large-scale annotated image datasets, knowledge transfer from pretrained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this article, we propose a novel deep convolutional neural network, which we called self-supervised super sample decomposition for transfer learning (4S-DT) model. The 4S-DT encourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabeled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition (CD) layer to simplify the local structure of the data. The 4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream CD mechanism. We used 50000 unlabeled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. The 4S-DT has achieved a high accuracy of 99.8% on the larger of the two datasets used in the experimental study and an accuracy of 97.54% on the smaller dataset, which was enriched by augmented images, out of which all real COVID-19 cases were detected.",2021,10.1109/tnnls.2021.3082015,cross-sectional,diagnosis,Xray,Lung
A 2D-3D hybrid convolutional neural network for lung lobe auto-segmentation on standard slice thickness computed tomography of patients receiving radiotherapy,"BACKGROUND: Accurate segmentation of lung lobe on routine computed tomography (CT) images of locally advanced stage lung cancer patients undergoing radiotherapy can help radiation oncologists to implement lobar-level treatment planning, dose assessment and efficacy prediction. We aim to establish a novel 2D-3D hybrid convolutional neural network (CNN) to provide reliable lung lobe auto-segmentation results in the clinical setting. METHODS: We retrospectively collected and evaluated thorax CT scans of 105 locally advanced non-small-cell lung cancer (NSCLC) patients treated at our institution from June 2019 to August 2020. The CT images were acquired with 5 mm slice thickness. Two CNNs were used for lung lobe segmentation, a 3D CNN for extracting 3D contextual information and a 2D CNN for extracting texture information. Contouring quality was evaluated using six quantitative metrics and visual evaluation was performed to assess the clinical acceptability. RESULTS: For the 35 cases in the test group, Dice Similarity Coefficient (DSC) of all lung lobes contours exceeded 0.75, which met the pass criteria of the segmentation result. Our model achieved high performances with DSC as high as 0.9579, 0.9479, 0.9507, 0.9484, and 0.9003 for left upper lobe (LUL), left lower lobe (LLL), right upper lobe (RUL), right lower lobe (RLL), and right middle lobe (RML), respectively. The proposed model resulted in accuracy, sensitivity, and specificity of 99.57, 98.23, 99.65 for LUL; 99.6, 96.14, 99.76 for LLL; 99.67, 96.13, 99.81 for RUL; 99.72, 92.38, 99.83 for RML; 99.58, 96.03, 99.78 for RLL, respectively. Clinician's visual assessment showed that 164/175 lobe contours met the requirements for clinical use, only 11 contours need manual correction. CONCLUSIONS: Our 2D-3D hybrid CNN model achieved accurate automatic segmentation of lung lobes on conventional slice-thickness CT of locally advanced lung cancer patients, and has good clinical practicability.",2021,10.1186/s12938-021-00932-1,cross-sectional,diagnosis,CT,Lung
A 3-D Riesz-Covariance Texture Model for Prediction of Nodule Recurrence in Lung CT,"This paper proposes a novel imaging biomarker of lung cancer relapse from 3-D texture analysis of CT images. Three-dimensional morphological nodular tissue properties are described in terms of 3-D Riesz-wavelets. The responses of the latter are aggregated within nodular regions by means of feature covariances, which leverage rich intra- and inter-variations of the feature space dimensions. When compared to the classical use of the average for feature aggregation, feature covariances preserve spatial co-variations between features. The obtained Riesz-covariance descriptors lie on a manifold governed by Riemannian geometry allowing geodesic measurements and differentiations. The latter property is incorporated both into a kernel for support vector machines (SVM) and a manifold-aware sparse regularized classifier. The effectiveness of the presented models is evaluated on a dataset of 110 patients with non-small cell lung carcinoma (NSCLC) and cancer recurrence information. Disease recurrence within a timeframe of 12 months could be predicted with an accuracy of 81.3-82.7%. The anatomical location of recurrence could be discriminated between local, regional and distant failure with an accuracy of 78.3-93.3%. The obtained results open novel research perspectives by revealing the importance of the nodular regions used to build the predictive models.",2016,10.1109/tmi.2016.2591921,retrospective cohort,prognosis,CT,Lung
A 3D image segmentation for lung cancer using V.Net architecture based deep convolutional networks,"Lung segmentation of chest CT scan is utilised to identify lung cancer and this step is also critical in other diagnostic pathways. Therefore, powerful algorithms to accomplish this accurate segmentation task are highly needed in the medical imaging domain, where the tumours are required to be segmented with the lung parenchyma. Also, the lung parenchyma needs to be detached from the tumour regions that are often confused with the lung tissue. Recently, lung semantic segmentation is more suitable to allocate each pixel in the image to a predefined class based on fully convolutional networks (FCNs). In this paper, CT cancer scans from the Task06_Lung database were applied to FCN that was inspired by V.Net architecture for efficiently selecting a region of interest (ROI) using the 3D segmentation. This lung database is segregated into 64 training images and 32 testing images. The proposed system is generalised by three steps including data preprocessing, data augmentation and neural network based on the V-Net model. Then, it was evaluated by dice score coefficient (DSC) to calculate the ratio of the segmented image and the ground truth image. This proposed system outperformed other previous schemes for 3D lung segmentation with an average DCS of 80% for ROI and 98% for surrounding lung tissues. Moreover, this system demonstrated that 3D views of lung tumours in CT images precisely carried tumour estimation and robust lung segmentation.",2021,10.1080/03091902.2021.1905895,cross-sectional,diagnosis,CT,Lung
A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans,"We introduce a new computer aided detection and diagnosis system for lung cancer screening with low-dose CT scans that produces meaningful probability assessments. Our system is based entirely on 3D convolutional neural networks and achieves state-of-the-art performance for both lung nodule detection and malignancy classification tasks on the publicly available LUNA16 and Kaggle Data Science Bowl challenges. While nodule detection systems are typically designed and optimized on their own, we find that it is important to consider the coupling between detection and diagnosis components. Exploiting this coupling allows us to develop an end-to-end system that has higher and more robust performance and eliminates the need for a nodule detection false positive reduction stage. Furthermore, we characterize model uncertainty in our deep learning systems, a first for lung CT analysis, and show that we can use this to provide well-calibrated classification probabilities for both nodule detection and patient malignancy diagnosis. These calibrated probabilities informed by model uncertainty can be used for subsequent risk-based decision making towards diagnostic interventions or disease treatments, as we demonstrate using a probability-based patient referral strategy to further improve our results.",2020,10.1109/tmi.2019.2947595,cross-sectional,diagnosis,CT,Lung
A 3D-CNN model with CT-based parametric response mapping for classifying COPD subjects,"Chronic obstructive pulmonary disease (COPD) is a respiratory disorder involving abnormalities of lung parenchymal morphology with different severities. COPD is assessed by pulmonary-function tests and computed tomography-based approaches. We introduce a new classification method for COPD grouping based on deep learning and a parametric-response mapping (PRM) method. We extracted parenchymal functional variables of functional small airway disease percentage (fSAD%) and emphysema percentage (Emph%) with an image registration technique, being provided as input parameters of 3D convolutional neural network (CNN). The integrated 3D-CNN and PRM (3D-cPRM) achieved a classification accuracy of 89.3% and a sensitivity of 88.3% in five-fold cross-validation. The prediction accuracy of the proposed 3D-cPRM exceeded those of the 2D model and traditional 3D CNNs with the same neural network, and was comparable to that of 2D pretrained PRM models. We then applied a gradient-weighted class activation mapping (Grad-CAM) that highlights the key features in the CNN learning process. Most of the class-discriminative regions appeared in the upper and middle lobes of the lung, consistent with the regions of elevated fSAD% and Emph% in COPD subjects. The 3D-cPRM successfully represented the parenchymal abnormalities in COPD and matched the CT-based diagnosis of COPD.",2021,10.1038/s41598-020-79336-5,cross-sectional,diagnosis,CT,Lung
A Bayesian approach to tissue-fraction estimation for oncological PET segmentation,"Tumor segmentation in oncological PET is challenging, a major reason being the partial-volume effects (PVEs) that arise due to low system resolution and finite voxel size. The latter results in tissue-fraction effects (TFEs), i.e. voxels contain a mixture of tissue classes. Conventional segmentation methods are typically designed to assign each image voxel as belonging to a certain tissue class. Thus, these methods are inherently limited in modeling TFEs. To address the challenge of accounting for PVEs, and in particular, TFEs, we propose a Bayesian approach to tissue-fraction estimation for oncological PET segmentation. Specifically, this Bayesian approach estimates the posterior mean of the fractional volume that the tumor occupies within each image voxel. The proposed method, implemented using a deep-learning-based technique, was first evaluated using clinically realistic 2D simulation studies with known ground truth, in the context of segmenting the primary tumor in PET images of patients with lung cancer. The evaluation studies demonstrated that the method accurately estimated the tumor-fraction areas and significantly outperformed widely used conventional PET segmentation methods, including a U-net-based method, on the task of segmenting the tumor. In addition, the proposed method was relatively insensitive to PVEs and yielded reliable tumor segmentation for different clinical-scanner configurations. The method was then evaluated using clinical images of patients with stage IIB/III non-small cell lung cancer from ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed that the proposed method significantly outperformed all other considered methods and yielded accurate tumor segmentation on patient images with Dice similarity coefficient (DSC) of 0.82 (95% CI: 0.78, 0.86). In particular, the method accurately segmented relatively small tumors, yielding a high DSC of 0.77 for the smallest segmented cross-section of 1.30 cm(2). Overall, this study demonstrates the efficacy of the proposed method to accurately segment tumors in PET images.",2021,10.1088/1361-6560/ac01f4,cross-sectional,diagnosis,PET,Lung
A Bayesian hidden Potts mixture model for analyzing lung cancer pathology images,"Digital pathology imaging of tumor tissues, which captures histological details in high resolution, is fast becoming a routine clinical procedure. Recent developments in deep-learning methods have enabled the identification, characterization, and classification of individual cells from pathology images analysis at a large scale. This creates new opportunities to study the spatial patterns of and interactions among different types of cells. Reliable statistical approaches to modeling such spatial patterns and interactions can provide insight into tumor progression and shed light on the biological mechanisms of cancer. In this article, we consider the problem of modeling a pathology image with irregular locations of three different types of cells: lymphocyte, stromal, and tumor cells. We propose a novel Bayesian hierarchical model, which incorporates a hidden Potts model to project the irregularly distributed cells to a square lattice and a Markov random field prior model to identify regions in a heterogeneous pathology image. The model allows us to quantify the interactions between different types of cells, some of which are clinically meaningful. We use Markov chain Monte Carlo sampling techniques, combined with a double Metropolis-Hastings algorithm, in order to simulate samples approximately from a distribution with an intractable normalizing constant. The proposed model was applied to the pathology images of $205$ lung cancer patients from the National Lung Screening trial, and the results show that the interaction strength between tumor and stromal cells predicts patient prognosis (P = $0.005$). This statistical methodology provides a new perspective for understanding the role of cell-cell interactions in cancer progression.",2019,10.1093/biostatistics/kxy019,,,,
A bilinear convolutional neural network for lung nodules classification on CT images,"PURPOSE: Lung cancer is the most frequent cancer worldwide and is the leading cause of cancer-related deaths. Its early detection and treatment at the stage of a lung nodule improve the prognosis. In this study was proposed a new classification approach named bilinear convolutional neural network (BCNN) for the classification of lung nodules on CT images. METHODS: Convolutional neural network (CNN) is considered as the leading model in deep learning and is highly recommended for the design of computer-aided diagnosis systems thanks to its promising results on medical image analysis. The proposed BCNN scheme consists of two-stream CNNs (VGG16 and VGG19) as feature extractors followed by a support vector machine (SVM) classifier for false positive reduction. Series of experiments are performed by introducing the bilinear vector features extracted from three BCNN combinations into various types of SVMs that we adopted instead of the original softmax to determine the most suitable classifier for our study. RESULTS: The method performance was evaluated on 3186 images from the public LUNA16 database. We found that the BCNN [VGG16, VGG19] combination with and without SVM surpassed the [VGG16]2 and [VGG19]2 architectures, achieved an accuracy rate of 91.99% against 91.84% and 90.58%, respectively, and an area under the curve (AUC) rate of 95.9% against 94.8% and 94%, respectively. CONCLUSION: The proposed method improved the outcomes of conventional CNN-based architectures and showed promising and satisfying results, compared to other works, with an affordable complexity. We believe that the proposed BCNN can be used as an assessment tool for radiologists to make a precise analysis of lung nodules and an early diagnosis of lung cancers.",2021,10.1007/s11548-020-02283-z,cross-sectional,diagnosis,CT,Lung
A biomarker basing on radiomics for the prediction of overall survival in non-small cell lung cancer patients,"BACKGROUND: This study aimed at predicting the survival status on non-small cell lung cancer patients with the phenotypic radiomics features obtained from the CT images. METHODS: A total of 186 patients' CT images were used for feature extraction via Pyradiomics. The minority group was balanced via SMOTE method. The final dataset was randomized into training set (n = 223) and validation set (n = 75) with the ratio of 3:1. Multiple random forest models were trained applying hyperparameters grid search with 10-fold cross-validation using precision or recall as evaluation standard. Then a decision threshold was searched on the selected model. The final model was evaluated through ROC curve and prediction accuracy. RESULTS: From those segmented images of 186 patients, 1218 features were obtained via feature extraction. The preferred model was selected with recall as evaluation standard and the optimal decision threshold was set 0.56. The model had a prediction accuracy of 89.33% and the AUC score was 0.9296. CONCLUSION: A hyperparameters tuning random forest classifier had greater performance in predicting the survival status of non-small cell lung cancer patients, which could be taken for an automated classifier promising to stratify patients.",2018,10.1186/s12931-018-0887-8,cross-sectional,diagnosis,CT,Lung
A CAD system for pulmonary nodule prediction based on deep three-dimensional convolutional neural networks and ensemble learning,"BACKGROUND: Detection of pulmonary nodules is an important aspect of an automatic detection system. Incomputer-aided diagnosis (CAD) systems, the ability to detect pulmonary nodules is highly important, which plays an important role in the diagnosis and early treatment of lung cancer. Currently, the detection of pulmonary nodules depends mainly on doctor experience, which varies. This paper aims to address the challenge of pulmonary nodule detection more effectively. METHODS: A method for detecting pulmonary nodules based on an improved neural network is presented in this paper. Nodules are clusters of tissue with a diameter of 3 mm to 30 mm in the pulmonary parenchyma. Because pulmonary nodules are similar to other lung structures and have a low density, false positive nodules often occur. Thus, our team proposed an improved convolutional neural network (CNN) framework to detect nodules. First, a nonsharpening mask is used to enhance the nodules in computed tomography (CT) images; then, CT images of 512×512 pixels are segmented into smaller images of 96×96 pixels. Second, in the 96×96 pixel images which contain or exclude pulmonary nodules, the plaques corresponding to positive and negative samples are segmented. Third, CT images segmented into 96×96 pixels are down-sampled to 64×64 and 32×32 size respectively. Fourth, an improved fusion neural network structure is constructed that consists of three three-dimensional convolutional neural networks, designated as CNN-1, CNN-2, and CNN-3, to detect false positive pulmonary nodules. The networks' input sizes are 32×32×32, 64×64×64, and 96×96×96 and include 5, 7, and 9 layers, respectively. Finally, we use the AdaBoost classifier to fuse the results of CNN-1, CNN-2, and CNN-3. We call this new neural network framework the Amalgamated-Convolutional Neural Network (A-CNN) and use it to detect pulmonary nodules. FINDINGS: Our team trained A-CNN using the LUNA16 and Ali Tianchi datasets and evaluated its performance using the LUNA16 dataset. We discarded nodules less than 5mm in diameter. When the average number of false positives per scan was 0.125 and 0.25, the sensitivity of A-CNN reached as high as 81.7% and 85.1%, respectively.",2019,10.1371/journal.pone.0219369,cross-sectional,diagnosis,CT,Lung
A cascade and heterogeneous neural network for CT pulmonary nodule detection and its evaluation on both phantom and patient data,"Screening of pulmonary nodules in computed tomography (CT) is crucial for early diagnosis and treatment of lung cancer. Although computer-aided diagnosis (CAD) systems have been designed to assist radiologists to detect nodules, fully automated detection is still challenging due to variations in nodule size, shape, and density. In this paper, we first propose a fully automated nodule detection method using a cascade and heterogeneous neural network trained on chest CT images of 12155 patients, then evaluate the performance by using phantom (828 CT images) and clinical datasets (2640 CT images) scanned with different imaging parameters. The nodule detection network employs two feature pyramid networks (FPNs) and a classification network (BasicNet). The first FPN is trained to achieve high sensitivity for nodule detection, and the second FPN refines the candidates for false positive reduction (FPR). Then, a BasicNet is combined with the second FPR to classify the candidates into either nodules or non-nodules for the final refinement. This study investigates the performance of nodule detection of solid and ground-glass nodules in phantom and patient data scanned with different imaging parameters. The results show that the detection of the solid nodules is robust to imaging parameters, and for GGO detection, reconstruction methods ""iDose4-YA"" and ""STD-YA"" achieve better performance. For thin-slice images, higher performance is achieved across different nodule sizes with reconstruction method ""iDose4-STD"". For 5 mm slice thickness, the best choice is the reconstruction method ""iDose4-YA"" for larger nodules (>5 mm). Overall, the reconstruction method ""iDose4-YA"" is suggested to achieve the best balanced results for both solid and GGO nodules.",2021,10.1016/j.compmedimag.2021.101889,cross-sectional,diagnosis,CT,Lung
A Cascade-SEME network for COVID-19 detection in chest x-ray images,"PURPOSE: The worldwide spread of the SARS-CoV-2 virus poses unprecedented challenges to medical resources and infection prevention and control measures around the world. In this case, a rapid and effective detection method for COVID-19 can not only relieve the pressure of the medical system but find and isolate patients in time, to a certain extent, slow down the development of the epidemic. In this paper, we propose a method that can quickly and accurately diagnose whether pneumonia is viral pneumonia, and classify viral pneumonia in a fine-grained way to diagnose COVID-19. METHODS: We proposed a Cascade Squeeze-Excitation and Moment Exchange (Cascade-SEME) framework that can effectively detect COVID-19 cases by evaluating the chest x-ray images, where SE is the structure we designed in the network which has attention mechanism, and ME is a method for image enhancement from feature dimension. The framework integrates a model for a coarse level detection of virus cases among other forms of lung infection, and a model for fine-grained categorisation of pneumonia types identifying COVID-19 cases. In addition, a Regional Learning approach is proposed to mitigate the impact of non-lesion features on network training. The network output is also visualised, highlighting the likely areas of lesion, to assist experts' assessment and diagnosis of COVID-19. RESULTS: Three datasets were used: a set of Chest x-ray Images for Classification with bacterial pneumonia, viral pneumonia and normal chest x-rays, a COVID chest x-ray dataset with COVID-19, and a Lung Segmentation dataset containing 1000 chest x-rays with masks in the lung region. We evaluated all the models on the test set. The results shows the proposed SEME structure significantly improves the performance of the models: in the task of pneumonia infection type diagnosis, the sensitivity, specificity, accuracy and F1 score of ResNet50 with SEME structure are significantly improved in each category, and the accuracy and AUC of the whole test set are also enhanced; in the detection task of COVID-19, the evaluation results shows that when SEME structure was added to the task, the sensitivities, specificities, accuracy and F1 scores of ResNet50 and DenseNet169 are improved. Although the sensitivities and specificities are not significantly promoted, SEME well balanced these two significant indicators. Regional learning also plays an important role. Experiments show that Regional Learning can effectively correct the impact of non-lesion features on the network, which can be seen in the Grad-CAM method. CONCLUSIONS: Experiments show that after the application of SEME structure in the network, the performance of SEME-ResNet50 and SEME-DenseNet169 in both two datasets show a clear enhancement. And the proposed regional learning method effectively directs the network's attention to focus on relevant pathological regions in the lung radiograph, ensuring the performance of the proposed framework even when a small training set is used. The visual interpretation step using Grad-CAM finds that the region of attention on radiographs of different types of pneumonia are located in different regions of the lungs.",2021,10.1002/mp.14711,cross-sectional,diagnosis,X-Ray,Lung
A cascaded dual-pathway residual network for lung nodule segmentation in CT images,"It is difficult to obtain an accurate segmentation due to the variety of lung nodules in computed tomography (CT) images. In this study, we propose a data-driven model, called the Cascaded Dual-Pathway Residual Network (CDP-ResNet) to improve the segmentation of lung nodules in the CT images. Our approach incorporates the multi-view and multi-scale features of different nodules from CT images. The proposed residual block based dual-path network extracts local features and rich contextual information of lung nodules. In addition, we designed an improved weighted sampling strategy to select training samples based on the edge. The proposed method was extensively evaluated on an LIDC dataset, which contains 986 nodules. Experimental results show that the CDP-ResNet achieves superior segmentation performance with an average DICE score (standard deviation) of 81.58% (11.05) on the LIDC dataset. Moreover, we compared our results with those of four radiologists on the same dataset. The comparison shows that the CDP-ResNet is slightly better than human experts in terms of segmentation accuracy. Meanwhile, the proposed segmentation method outperforms existing methods.",2019,10.1016/j.ejmp.2019.06.003,cross-sectional,diagnosis,CT,Lung
A clinical evaluation study of cardiothoracic ratio measurement using artificial intelligence,"BACKGROUND: Artificial intelligence, particularly the deep learning (DL) model, can provide reliable results for automated cardiothoracic ratio (CTR) measurement on chest X-ray (CXR) images. In everyday clinical use, however, this technology is usually implemented in a non-automated (AI-assisted) capacity because it still requires approval from radiologists. We investigated the performance and efficiency of our recently proposed models for the AI-assisted method intended for clinical practice. METHODS: We validated four proposed DL models (AlbuNet, SegNet, VGG-11, and VGG-16) to find the best model for clinical implementation using a dataset of 7517 CXR images from manual operations. These models were investigated in single-model and combined-model modes to find the model with the highest percentage of results where the user could accept the results without further interaction (excellent grade), and with measurement variation within ± 1.8% of the human-operating range. The best model from the validation study was then tested on an evaluation dataset of 9386 CXR images using the AI-assisted method with two radiologists to measure the yield of excellent grade results, observer variation, and operating time. A Bland-Altman plot with coefficient of variation (CV) was employed to evaluate agreement between measurements. RESULTS: The VGG-16 gave the highest excellent grade result (68.9%) of any single-model mode with a CV comparable to manual operation (2.12% vs 2.13%). No DL model produced a failure-grade result. The combined-model mode of AlbuNet + VGG-11 model yielded excellent grades in 82.7% of images and a CV of 1.36%. Using the evaluation dataset, the AlbuNet + VGG-11 model produced excellent grade results in 77.8% of images, a CV of 1.55%, and reduced CTR measurement time by almost ten-fold (1.07 ± 2.62 s vs 10.6 ± 1.5 s) compared with manual operation. CONCLUSION: Due to its excellent accuracy and speed, the AlbuNet + VGG-11 model could be clinically implemented to assist radiologists with CTR measurement.",2022,10.1186/s12880-022-00767-9,cross-sectional,others,X-Ray,Heart
"A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists' search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI).",2019,10.1016/j.media.2018.10.010,,,,
A combined microfluidic deep learning approach for lung cancer cell high throughput screening toward automatic cancer screening applications,"Lung cancer is a leading cause of cancer death in both men and women worldwide. The high mortality rate in lung cancer is in part due to late-stage diagnostics as well as spread of cancer-cells to organs and tissues by metastasis. Automated lung cancer detection and its sub-types classification from cell's images play a crucial role toward an early-stage cancer prognosis and more individualized therapy. The rapid development of machine learning techniques, especially deep learning algorithms, has attracted much interest in its application to medical image problems. In this study, to develop a reliable Computer-Aided Diagnosis (CAD) system for accurately distinguishing between cancer and healthy cells, we grew popular Non-Small Lung Cancer lines in a microfluidic chip followed by staining with Phalloidin and images were obtained by using an IX-81 inverted Olympus fluorescence microscope. We designed and tested a deep learning image analysis workflow for classification of lung cancer cell-line images into six classes, including five different cancer cell-lines (P-C9, SK-LU-1, H-1975, A-427, and A-549) and normal cell-line (16-HBE). Our results demonstrate that ResNet18, a residual learning convolutional neural network, is an efficient and promising method for lung cancer cell-lines categorization with a classification accuracy of 98.37% and F1-score of 97.29%. Our proposed workflow is also able to successfully distinguish normal versus cancerous cell-lines with a remarkable average accuracy of 99.77% and F1-score of 99.87%. The proposed CAD system completely eliminates the need for extensive user intervention, enabling the processing of large amounts of image data with robust and highly accurate results.",2021,10.1038/s41598-021-89352-8,,,,
A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions,"The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.",2021,10.1016/j.compbiomed.2021.104887,cross-sectional,diagnosis,CT,Lung
A comparative study of auto-contouring softwares in delineation of organs at risk in lung cancer and rectal cancer,"Radiotherapy requires the target area and the organs at risk to be contoured on the CT image of the patient. During the process of organs-at-Risk (OAR) of the chest and abdomen, the doctor needs to contour at each CT image. The delineations of large and varied shapes are time-consuming and laborious. This study aims to evaluate the results of two automatic contouring softwares on OARs definition of CT images of lung cancer and rectal cancer patients. The CT images of 15 patients with rectal cancer and 15 patients with lung cancer were selected separately, and the organs at risk were manually contoured by experienced physicians as reference structures. And then the same datasets were automatically contoured based on AiContour (version 3.1.8.0, Manufactured by Linking MED, Beijing, China) and Raystation (version 4.7.5.4, Manufactured by Raysearch, Stockholm, Sweden) respectively. Deep learning auto-segmentations and Atlas were respectively performed with AiContour and Raystation. Overlap index (OI), Dice similarity index (DSC) and Volume difference (D(v)) were evaluated based on the auto-contours, and independent-sample t-test analysis is applied to the results. The results of deep learning auto-segmentations on OI and DSC were better than that of Atlas with statistical difference. There was no significant difference in D(v) between the results of two software. With deep learning auto-segmentations, auto-contouring results of most organs in the chest and abdomen are good, and with slight modification, it can meet the clinical requirements for planning. With Atlas, auto-contouring results in most OAR is not as good as deep learning auto-segmentations, and only the auto-contouring results of some organs can be used clinically after modification.",2021,10.1038/s41598-021-02330-y,,,,
A Comparative Texture Analysis Based on NECT and CECT Images to Differentiate Lung Adenocarcinoma from Squamous Cell Carcinoma,"The purpose of the study was to compare the texture based discriminative performances between non-contrast enhanced computed tomography (NECT) and contrast-enhanced computed tomography (CECT) images in differentiating lung adenocarcinoma (ADC) from squamous cell carcinoma (SCC) patients. Eighty-seven lung cancer subjects were enrolled in the study, including pathologically proved 47 ADC patients and 40 SCC patients, and 261 texture features were extracted from the manually delineated region of interests on CECT and NECT images respectively. Fisher score was then used to select the effective discriminative texture features between groups, and the selected texture features were adopted to differentiate ADC from SCC using Support Vector Machine and Leave-one-out cross-validation. Both NECT and CECT images could achieve the same best classification accuracy of 95.4%, and most of the informative features were from the gray-level co-occurrence matrix. In addition, CECT images were found with enhanced texture features compared with NECT images, and combining texture features of CECT and NECT images together could further improve the prediction accuracy. Besides the texture feature, the tumor location information also contributed to the differential diagnosis between ADC and SCC.",2019,10.1007/s10916-019-1175-y,,,,
A comparison between deep learning convolutional neural networks and radiologists in the differentiation of benign and malignant thyroid nodules on CT images,"INTRODUCTION: We designed 5 convolutional neural network (CNN) models and ensemble models to differentiate malignant and benign thyroid nodules on CT, and compared the diagnostic performance of CNN models with that of radiologists. MATERIAL AND METHODS: We retrospectively included CT images of 880 patients with 986 thyroid nodules confirmed by surgical pathology between July 2017 and December 2019. Two radiologists retrospectively diagnosed benign and malignant thyroid nodules on CT images in a test set. Five CNNs (ResNet50, DenseNet121, DenseNet169, SE-ResNeXt50, and Xception) were trained-validated and tested using 788 and 198 thyroid nodule CT images, respectively. Then, we selected the 3 models with the best diagnostic performance on the test set for the model ensemble. We then compared the diagnostic performance of 2 radiologists with 5 CNN models and the integrated model. RESULTS: Of the 986 thyroid nodules, 541 were malignant, and 445 were benign. The area under the curves (AUCs) for diagnosing thyroid malignancy was 0.587-0.754 for 2 radiologists. The AUCs for diagnosing thyroid malignancy for the 5 CNN models and ensemble model was 0.901-0.947. There were significant differences in AUC between the radiologists' models and the CNN models (p < 0.05). The ensemble model had the highest AUC value. CONCLUSIONS: Five CNN models and an ensemble model performed better than radiologists in distinguishing malignant thyroid nodules from benign nodules on CT. The diagnostic performance of the ensemble model improved and showed good potential.",2021,10.5603/EP.a2021.0015,,,,
A comparison between manual and artificial intelligence-based automatic positioning in CT imaging for COVID-19 patients,"OBJECTIVE: To analyze and compare the imaging workflow, radiation dose, and image quality for COVID-19 patients examined using either the conventional manual positioning (MP) method or an AI-based automatic positioning (AP) method. MATERIALS AND METHODS: One hundred twenty-seven adult COVID-19 patients underwent chest CT scans on a CT scanner using the same scan protocol except with the manual positioning (MP group) for the initial scan and an AI-based automatic positioning method (AP group) for the follow-up scan. Radiation dose, patient positioning time, and off-center distance of the two groups were recorded and compared. Image noise and signal-to-noise ratio (SNR) were assessed by three experienced radiologists and were compared between the two groups. RESULTS: The AP operation was successful for all patients in the AP group and reduced the total positioning time by 28% compared with the MP group. Compared with the MP group, the AP group had significantly less patient off-center distance (AP 1.56 cm ± 0.83 vs. MP 4.05 cm ± 2.40, p < 0.001) and higher proportion of positioning accuracy (AP 99% vs. MP 92%), resulting in 16% radiation dose reduction (AP 6.1 mSv ± 1.3 vs. MP 7.3 mSv ± 1.2, p < 0.001) and 9% image noise reduction in erector spinae and lower noise and higher SNR for lesions in the pulmonary peripheral areas. CONCLUSION: The AI-based automatic positioning and centering in CT imaging is a promising new technique for reducing radiation dose and optimizing imaging workflow and image quality in imaging the chest. KEY POINTS: • The AI-based automatic positioning (AP) operation was successful for all patients in our study. • AP method reduced the total positioning time by 28% compared with the manual positioning (MP). • AP method had less patient off-center distance and higher proportion of positioning accuracy than MP method, resulting in 16% radiation dose reduction and 9% image noise reduction in erector spinae.",2021,10.1007/s00330-020-07629-4,cross-sectional,diagnosis,CT,Lung
A comparison of machine learning methods for predicting recurrence and death after curative-intent radiotherapy for non-small cell lung cancer: Development and validation of multivariable clinical prediction models,"BACKGROUND: Surveillance is universally recommended for non-small cell lung cancer (NSCLC) patients treated with curative-intent radiotherapy. High-quality evidence to inform optimal surveillance strategies is lacking. Machine learning demonstrates promise in accurate outcome prediction for a variety of health conditions. The purpose of this study was to utilise readily available patient, tumour, and treatment data to develop, validate and externally test machine learning models for predicting recurrence, recurrence-free survival (RFS) and overall survival (OS) at 2 years from treatment. METHODS: A retrospective, multicentre study of patients receiving curative-intent radiotherapy for NSCLC was undertaken. A total of 657 patients from 5 hospitals were eligible for inclusion. Data pre-processing derived 34 features for predictive modelling. Combinations of 8 feature reduction methods and 10 machine learning classification algorithms were compared, producing risk-stratification models for predicting recurrence, RFS and OS. Models were compared with 10-fold cross validation and an external test set and benchmarked against TNM-stage and performance status. Youden Index was derived from validation set ROC curves to distinguish high and low risk groups and Kaplan-Meier analyses performed. FINDINGS: Median follow-up time was 852 days. Parameters were well matched across training-validation and external test sets: Mean age was 73 and 71 respectively, and recurrence, RFS and OS rates at 2 years were 43% vs 34%, 54% vs 47% and 54% vs 47% respectively. The respective validation and test set AUCs were as follows: 1) RFS: 0·682 (0·575-0·788) and 0·681 (0·597-0·766), 2) Recurrence: 0·687 (0·582-0·793) and 0·722 (0·635-0·81), and 3) OS: 0·759 (0·663-0·855) and 0·717 (0·634-0·8). Our models were superior to TNM stage and performance status in predicting recurrence and OS. INTERPRETATION: This robust and ready to use machine learning method, validated and externally tested, sets the stage for future clinical trials entailing quantitative personalised risk-stratification and surveillance following curative-intent radiotherapy for NSCLC. FUNDING: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section.",2022,10.1016/j.ebiom.2022.103911,retrospective cohort,prognosis,CT,
A comparison of the fusion model of deep learning neural networks with human observation for lung nodule detection and classification,"OBJECTIVES: To compare the diagnostic performance of a newly developed artificial intelligence (AI) algorithm derived from the fusion of convolution neural networks (CNN) versus human observers in the estimation of malignancy risk in pulmonary nodules. METHODS: The study population consists of 158 nodules from 158 patients. All nodules (81 benign and 77 malignant) were determined to be malignant or benign by a radiologist based on pathologic assessment and/or follow-up imaging. Two radiologists and an AI platform analyzed the nodules based on the Lung-RADS classification. The two observers also noted the size, location, and morphologic features of the nodules. An intraclass correlation coefficient was calculated for both observers and the AI; ROC curve analysis was performed to determine diagnostic performances. RESULTS: Nodule size, presence of spiculation, and presence of fat were significantly different between the malignant and benign nodules (p < 0.001, for all three). Eighteen (11.3%) nodules were not detected and analyzed by the AI. Observer 1, observer 2, and the AI had an AUC of 0.917 ± 0.023, 0.870 ± 0.033, and 0.790 ± 0.037 in the ROC analysis of malignity probability, respectively. The observers were in almost perfect agreement for localization, nodule size, and lung-RADS classification [κ (95% CI)=0.984 (0.961-1.000), 0.978 (0.970-0.984), and 0.924 (0.878-0.970), respectively]. CONCLUSION: The performance of the fusion AI algorithm in estimating the risk of malignancy was slightly lower than the performance of the observers. Fusion AI algorithms might be applied in an assisting role, especially for inexperienced radiologists. ADVANCES IN KNOWLEDGE: In this study, we proposed a fusion model using four state-of-art object detectors for lung nodule detection and discrimination. The use of fusion of deep learning neural networks might be used in a supportive role for radiologists when interpreting lung nodule discrimination.",2021,10.1259/bjr.20210222,cross-sectional,diagnosis,CT,Lung
A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks,"The use of imaging data has been reported to be useful for rapid diagnosis of COVID-19. Although computed tomography (CT) scans show a variety of signs caused by the viral infection, given a large amount of images, these visual features are difficult and can take a long time to be recognized by radiologists. Artificial intelligence methods for automated classification of COVID-19 on CT scans have been found to be very promising. However, current investigation of pretrained convolutional neural networks (CNNs) for COVID-19 diagnosis using CT data is limited. This study presents an investigation on 16 pretrained CNNs for classification of COVID-19 using a large public database of CT scans collected from COVID-19 patients and non-COVID-19 subjects. The results show that, using only 6 epochs for training, the CNNs achieved very high performance on the classification task. Among the 16 CNNs, DenseNet-201, which is the deepest net, is the best in terms of accuracy, balance between sensitivity and specificity, [Formula: see text] score, and area under curve. Furthermore, the implementation of transfer learning with the direct input of whole image slices and without the use of data augmentation provided better classification rates than the use of data augmentation. Such a finding alleviates the task of data augmentation and manual extraction of regions of interest on CT images, which are adopted by current implementation of deep-learning models for COVID-19 classification.",2020,10.1038/s41598-020-74164-z,cross-sectional,diagnosis,CT,Lung
A computer-aided diagnosis approach for emphysema recognition in chest radiography,"The purpose of this work is twofold: (i) to develop a CAD system for the assessment of emphysema by digital chest radiography and (ii) to test it against CT imaging. The system is based on the analysis of the shape of lung silhouette as imaged in standard chest examination. Postero-anterior and lateral views are processed to extract the contours of the lung fields automatically. Subsequently, the shape of lung silhouettes is described by polyline approximation and the computed feature-set processed by a neural network to estimate the probability of emphysema. Images of radiographic studies from 225 patients were collected and properly annotated to build an experimental dataset named EMPH. Each patient had undergone a standard two-views chest radiography and CT for diagnostic purposes. In addition, the images (247) from JSRT dataset were used to evaluate lung segmentation in postero-anterior view. System performances were assessed by: (i) analyzing the quality of the automatic segmentation of the lung silhouette against manual tracing and (ii) measuring the capabilities of emphysema recognition. As to step i, on JSRT dataset, we obtained overlap percentage (Ω) 92.7±3.3%, Dice Similarity Coefficient (DSC) 95.5±3.7% and average contour distance (ACD) 1.73±0.87 mm. On EMPH dataset we had Ω=93.1±2.9%, DSC=96.1±3.5% and ACD=1.62±0.92 mm, for the postero-anterior view, while we had Ω=94.5±4.6%, DSC=91.0±6.3% and ACD=2.22±0.86 mm, for the lateral view. As to step ii, accuracy of emphysema recognition was 95.4%, with sensitivity and specificity 94.5% and 96.1% respectively. According to experimental results our system allows reliable and inexpensive recognition of emphysema on digital chest radiography.",2013,10.1016/j.medengphy.2012.03.011,cross-sectional,diagnosis,X-ray and CT,Lung
A computer-aided diagnosis system for the classification of COVID-19 and non-COVID-19 pneumonia on chest X-ray images by integrating CNN with sparse autoencoder and feed forward neural network,"Several infectious diseases have affected the lives of many people and have caused great dilemmas all over the world. COVID-19 was declared a pandemic caused by a newly discovered virus named Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) by the World Health Organisation in 2019. RT-PCR is considered the golden standard for COVID-19 detection. Due to the limited RT-PCR resources, early diagnosis of the disease has become a challenge. Radiographic images such as Ultrasound, CT scans, X-rays can be used for the detection of the deathly disease. Developing deep learning models using radiographic images for detecting COVID-19 can assist in countering the outbreak of the virus. This paper presents a computer-aided detection model utilizing chest X-ray images for combating the pandemic. Several pre-trained networks and their combinations have been used for developing the model. The method uses features extracted from pre-trained networks along with Sparse autoencoder for dimensionality reduction and a Feed Forward Neural Network (FFNN) for the detection of COVID-19. Two publicly available chest X-ray image datasets, consisting of 504 COVID-19 images and 542 non-COVID-19 images, have been combined to train the model. The method was able to achieve an accuracy of 0.9578 and an AUC of 0.9821, using the combination of InceptionResnetV2 and Xception. Experiments have proved that the accuracy of the model improves with the usage of sparse autoencoder as the dimensionality reduction technique.",2022,10.1016/j.compbiomed.2021.105134,cross-sectional,diagnosis,X-ray ,Lung
A Computer-Aided Pipeline for Automatic Lung Cancer Classification on Computed Tomography Scans,"Lung cancer is one of the most common cancer types. For the survival of the patient, early detection of lung cancer with the best treatment method is crucial. In this study, we propose a novel computer-aided pipeline on computed tomography (CT) scans for early diagnosis of lung cancer thanks to the classification of benign and malignant nodules. The proposed pipeline is composed of four stages. In preprocessing steps, CT images are enhanced, and lung volumes are extracted from the image with the help of a novel method called lung volume extraction method (LUVEM). The significance of the proposed pipeline is using LUVEM for extracting lung region. In nodule detection stage, candidate nodules are determined according to the circular Hough transform- (CHT-) based method. Then, lung nodules are segmented with self-organizing maps (SOM). In feature computation stage, intensity, shape, texture, energy, and combined features are used for feature extraction, and principal component analysis (PCA) is used for feature reduction step. In the final stage, probabilistic neural network (PNN) classifies benign and malign nodules. According to the experiments performed on our dataset, the proposed pipeline system can classify benign and malign nodules with 95.91% accuracy, 97.42% sensitivity, and 94.24% specificity. Even in cases of small-sized nodules (3-10 mm), the proposed system can determine the nodule type with 94.68% accuracy.",2018,10.1155/2018/9409267,cross-sectional,,CT,Lung
A conventional-to-spectral CT image translation augmentation workflow for robust contrast injection-independent organ segmentation,"PURPOSE: In computed tomography (CT) cardiovascular imaging, the numerous contrast injection protocols used to enhance structures make it difficult to gather training datasets for deep learning applications supporting diverse protocols. Moreover, creating annotations on noncontrast scans is extremely tedious. Recently, spectral CT's virtual-noncontrast images (VNC) have been used as data augmentation to train segmentation networks performing on enhanced and true-noncontrast (TNC) scans alike, while improving results on protocols absent of their training dataset. However, spectral data are not widely available, making it difficult to gather specific datasets for each task. As a solution, we present a data augmentation workflow based on a trained image translation network, to bring spectral-like augmentation to any conventional CT dataset. METHOD: The conventional CT-to-spectral image translation network (HUSpectNet) was first trained to generate VNC from conventional housnfied units images (HU), using an unannotated spectral dataset of 1830 patients. It was then tested on a second dataset of 300 spectral CT scans by comparing VNC generated through deep learning (VNC(DL) ) to their true counterparts. To illustrate and compare our workflow's efficiency with true spectral augmentation, HUSpectNet was applied to a third dataset of 112 spectral scans to generate VNC(DL) along HU and VNC images. Three different three-dimensional (3D) networks (U-Net, X-Net, and U-Net++) were trained for multilabel heart segmentation, following four augmentation strategies. As baselines, trainings were performed on contrasted images without (HUonly) and with conventional gray-values augmentation (HUaug). Then, the same networks were trained using a proportion of contrasted and VNC/VNC(DL) images (TrueSpec/GenSpec). Each training strategy applied to each architecture was evaluated using Dice coefficients on a fourth multicentric multivendor single-energy CT dataset of 121 patients, including different contrast injection protocols and unenhanced scans. The U-Net++ results were further explored with distance metrics on every label. RESULTS: Tested on 300 full scans, our HUSpectNet translation network shows a mean absolute error of 6.70 ± 2.83 HU between VNC(DL) and VNC, while peak signal-to-noise ratio reaches 43.89 dB. GenSpec and TrueSpec show very close results regardless of the protocol and used architecture: mean Dice coefficients (DSC(mean) ) are equal with a margin of 0.006, ranging from 0.879 to 0.938. Their performances significantly increase on TNC scans (p-values < 0.017 for all architectures) compared to HUonly and HUaug, with DSC(mean) of 0.448/0.770/0.879/0.885 for HUonly/HUaug/TrueSpec/GenSpec using the U-Net++ architecture. Significant improvements are also noted for all architectures on chest-abdominal-pelvic scans (p-values < 0.007) compared to HUonly and for pulmonary embolism scans (p-values < 0.039) compared to HUaug. Using U-Net++, DSC(mean) reaches 0.892/0.901/0.903 for HUonly/TrueSpec/GenSpec on pulmonary embolism scans and 0.872/0.896/0.896 for HUonly/TrueSpec/GenSpec on chest-abdominal-pelvic scans. CONCLUSION: Using the proposed workflow, we trained versatile heart segmentation networks on a dataset of conventional enhanced CT scans, providing robust predictions on both enhanced scans with different contrast injection protocols and TNC scans. The performances obtained were not significantly inferior to training the model on a genuine spectral CT dataset, regardless of the architecture implemented. Using a general-purpose conventional-to-spectral CT translation network as data augmentation could therefore contribute to reducing data collection and annotation requirements for machine learning-based CT studies, while extending their range of application.",2022,10.1002/mp.15310,,,,
A convolutional neural network for ultra-low-dose CT denoising and emphysema screening,"PURPOSE: Reducing dose level to achieve ALARA is an important task in diagnostic and therapeutic applications of computed tomography (CT) imaging. Effective image quality enhancement strategies are crucial to compensate for the degradation caused by dose reduction. In the past few years, deep learning approaches have demonstrated promising denoising performance on natural/synthetic images. This study tailors a neural network model for (ultra-)low-dose CT denoising, and assesses its performance in enhancing CT image quality and emphysema quantification. METHODS: The noise statistics in low-dose CT images has its unique characteristics and differs from that used in general denoising models. In this study, we first simulate the paired ultra-low-dose and targeted high-quality image of reference, with a well-validated pipeline. These paired images are used to train a denoising convolutional neural network (DnCNN) with residual mapping. The performance of the DnCNN tailored to CT denoising (DnCNN-CT) is assessed over various dose reduction levels, with respect to both image quality and emphysema scoring quantification. The possible over-smoothing behavior of DnCNN and its impact on different subcohort of patients are also investigated. RESULTS: Performance evaluation results showed that DnCNN-CT provided significant image quality enhancement, especially for very-low-dose level. With DnCNN-CT denoising on 3%-dose cases, the peak signal-to-noise ratio improved by 8 dB and the structure similarity index increased by 0.15. This outperformed the original DnCNN and the state-of-the-art nonlocal-mean-type denoising scheme. Emphysema mask was also investigated, where lung voxels of abnormally low attenuation coefficient were marked as potential emphysema. Emphysema mask generated after DnCNN-CT denoising on 3%-dose image was demonstrated to agree well with that from the full-dose reference. Despite over-smoothing in DnCNN denoising, which contributed to slight underestimation of emphysema score compared to the reference, such minor overcorrection did not affect clinical conclusions. The proposed method provided effective detection for cases with appreciable emphysema while serving as a reasonable correction for normal cases without emphysema. CONCLUSIONS: This work provides a tailored DnCNN for (ultra-)low-dose CT denoising, and demonstrates significant improvement on both the image quality and the clinical emphysema quantification accuracy over various dose levels. The clinical conclusion of emphysema obtained from the denoised low-dose images agrees well with that from the full-dose ones.",2019,10.1002/mp.13666,cross-sectional,others,CT,Lung
A cross-modal 3D deep learning for accurate lymph node metastasis prediction in clinical stage T1 lung adenocarcinoma,"OBJECTIVES: The evaluation of lymph node (LN) status by radiologists based on preoperative computed tomography (CT) lacks high precision for early lung cancer patients; erroneous evaluations result in inappropriate therapeutic plans and increase the risk of complications. This study aims to develop a cross-modal 3D neural network based on CT images and prior clinical knowledge for accurate prediction of LN metastasis in clinical stage T1 lung adenocarcinoma. PATIENTS AND METHODS: Five hundred one lung adenocarcinoma patients with clinical stage T1 were enrolled. Data including: corresponding 3D nodule-centered patches of CT; prior clinical features; and pathological labels of LN status were obtained. We proposed a cross-modal deep learning system, which can successfully incorporate prior clinical knowledge and CT images into a 3D neural network to predict LN metastasis. We trained and validated our system with 401 cases and tested its performance with 100 cases. The result was compared with that of the logistic regression integration model, the single deep learning model without prior clinical knowledge integration, radiomics method, and manual evaluation by radiologists. RESULTS: The model proposed DensePriNet achieved an AUC of 0.926, which is significantly higher than the logistic regression integration model (0.904) single deep learning model (0.880), and radiomics method (0.891). The Matthews Correlation Coefficient (MCC) of DensePriNet (0.705) was significantly higher than manual classification by one senior radiologist (0.534) and one junior radiologist (0.416), respectively. CONCLUSION: The performance of the single deep learning method is significantly higher than the radiomics method and the radiologists, and integration of prior clinical knowledge into the deep learning model enhance the diagnostic precision of LN status and facilitate the application of precision medicine.",2020,10.1016/j.lungcan.2020.04.014,cross-sectional,diagnosis,CT,Lung
A CT-based deep learning model for subsolid pulmonary nodules to distinguish minimally invasive adenocarcinoma and invasive adenocarcinoma,"OBJECTIVE: To develop and validate a deep learning nomogram (DLN) model constructed from non-contrast computed tomography (CT) images for discriminating minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC) in patients with subsolid pulmonary nodules (SSPNs). MATERIALS AND METHODS: In total, 365 consecutive patients who presented with SSPNs and were pathologically diagnosed with MIA or IAC after surgery, were recruited from two medical institutions from 2016 to 2019. Deep learning features were selected from preoperative CT images using convolutional neural network. Deep learning signature (DLS) was developed via the least absolute shrinkage and selection operator (LASSO). New DLN integrating clinical variables, subjective CT findings, and DLS was constructed. The diagnostic efficiency and discriminative capability were analyzed using the receiver operating characteristic method and decision curve analysis (DCA). RESULTS: In total, 18 deep learning features with non-zero coefficients were enrolled to develop the DLS, which was statistically different between the MIA and IAC groups. Independent predictors of DLS and lobulated sharp were used to build the DLN. The areas under the curves of the DLN were 0.889 (95% confidence interval (CI): 0.824-0.936), 0.915 (95% CI: 0.846-0.959), and 0.914 (95% CI: 0.848-0.958) in the training, internal validation, and external validation cohorts, respectively. After stratification analysis and DCA, the DLN showed potential generalization ability. CONCLUSION: The DLN incorporating the DLS and subjective CT findings have strong potential to distinguish MIA from IAC in patients with SSPNs, and will facilitate the suitable treatment method selection for the management of SSPNs.",2021,10.1016/j.ejrad.2021.110041,cross-sectional,diagnosis,CT,Lung
A deep 3D residual CNN for false-positive reduction in pulmonary nodule detection,"PURPOSE: The automatic detection of pulmonary nodules using CT scans improves the efficiency of lung cancer diagnosis, and false-positive reduction plays a significant role in the detection. In this paper, we focus on the false-positive reduction task and propose an effective method for this task. METHODS: We construct a deep 3D residual CNN (convolution neural network) to reduce false-positive nodules from candidate nodules. The proposed network is much deeper than the traditional 3D CNNs used in medical image processing. Specifically, in the network, we design a spatial pooling and cropping (SPC) layer to extract multilevel contextual information of CT data. Moreover, we employ an online hard sample selection strategy in the training process to make the network better fit hard samples (e.g., nodules with irregular shapes). RESULTS: Our method is evaluated on 888 CT scans from the dataset of the LUNA16 Challenge. The free-response receiver operating characteristic (FROC) curve shows that the proposed method achieves a high detection performance. CONCLUSIONS: Our experiments confirm that our method is robust and that the SPC layer helps increase the prediction accuracy. Additionally, the proposed method can easily be extended to other 3D object detection tasks in medical image processing.",2018,10.1002/mp.12846,cross-sectional,diagnosis,CT,Lung
A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19),"OBJECTIVE: The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control. METHODS: We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation. RESULTS: The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%. CONCLUSION: These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis. KEY POINTS: • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season. • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets. • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.",2021,10.1007/s00330-021-07715-1,cross-sectional,diagnosis,CT,Lung
A deep learning approach to characterize 2019 coronavirus disease (COVID-19) pneumonia in chest CT images,"OBJECTIVES: To utilize a deep learning model for automatic detection of abnormalities in chest CT images from COVID-19 patients and compare its quantitative determination performance with radiological residents. METHODS: A deep learning algorithm consisted of lesion detection, segmentation, and location was trained and validated in 14,435 participants with chest CT images and definite pathogen diagnosis. The algorithm was tested in a non-overlapping dataset of 96 confirmed COVID-19 patients in three hospitals across China during the outbreak. Quantitative detection performance of the model was compared with three radiological residents with two experienced radiologists' reading reports as reference standard by assessing the accuracy, sensitivity, specificity, and F1 score. RESULTS: Of 96 patients, 88 had pneumonia lesions on CT images and 8 had no abnormities on CT images. For per-patient basis, the algorithm showed superior sensitivity of 1.00 (95% confidence interval (CI) 0.95, 1.00) and F1 score of 0.97 in detecting lesions from CT images of COVID-19 pneumonia patients. While for per-lung lobe basis, the algorithm achieved a sensitivity of 0.96 (95% CI 0.94, 0.98) and a slightly inferior F1 score of 0.86. The median volume of lesions calculated by algorithm was 40.10 cm(3). An average running speed of 20.3 s ± 5.8 per case demonstrated the algorithm was much faster than the residents in assessing CT images (all p < 0.017). The deep learning algorithm can also assist radiologists make quicker diagnosis (all p < 0.0001) with superior diagnostic performance. CONCLUSIONS: The algorithm showed excellent performance in detecting COVID-19 pneumonia on chest CT images compared with resident radiologists. KEY POINTS: • The higher sensitivity of deep learning model in detecting COVID-19 pneumonia were found compared with radiological residents on a per-lobe and per-patient basis. • The deep learning model improves diagnosis efficiency by shortening processing time. • The deep learning model can automatically calculate the volume of the lesions and whole lung.",2020,10.1007/s00330-020-07044-9,cross-sectional,diagnosis,CT,Lung
A deep learning approach using effective preprocessing techniques to detect COVID-19 from chest CT-scan and X-ray images,"Coronavirus disease-19 (COVID-19) is a severe respiratory viral disease first reported in late 2019 that has spread worldwide. Although some wealthy countries have made significant progress in detecting and containing this disease, most underdeveloped countries are still struggling to identify COVID-19 cases in large populations. With the rising number of COVID-19 cases, there are often insufficient COVID-19 diagnostic kits and related resources in such countries. However, other basic diagnostic resources often do exist, which motivated us to develop Deep Learning models to assist clinicians and radiologists to provide prompt diagnostic support to the patients. In this study, we have developed a deep learning-based COVID-19 case detection model trained with a dataset consisting of chest CT scans and X-ray images. A modified ResNet50V2 architecture was employed as deep learning architecture in the proposed model. The dataset utilized to train the model was collected from various publicly available sources and included four class labels: confirmed COVID-19, normal controls and confirmed viral and bacterial pneumonia cases. The aggregated dataset was preprocessed through a sharpening filter before feeding the dataset into the proposed model. This model attained an accuracy of 96.452% for four-class cases (COVID-19/Normal/Bacterial pneumonia/Viral pneumonia), 97.242% for three-class cases (COVID-19/Normal/Bacterial pneumonia) and 98.954% for two-class cases (COVID-19/Viral pneumonia) using chest X-ray images. The model acquired a comprehensive accuracy of 99.012% for three-class cases (COVID-19/Normal/Community-acquired pneumonia) and 99.99% for two-class cases (Normal/COVID-19) using CT-scan images of the chest. This high accuracy presents a new and potentially important resource to enable radiologists to identify and rapidly diagnose COVID-19 cases with only basic but widely available equipment.",2021,10.1016/j.compbiomed.2021.105014,cross-sectional,diagnosis,Xray and CT,Lung
A deep learning integrated radiomics model for identification of coronavirus disease 2019 using computed tomography,"Since its first outbreak, Coronavirus Disease 2019 (COVID-19) has been rapidly spreading worldwide and caused a global pandemic. Rapid and early detection is essential to contain COVID-19. Here, we first developed a deep learning (DL) integrated radiomics model for end-to-end identification of COVID-19 using CT scans and then validated its clinical feasibility. We retrospectively collected CT images of 386 patients (129 with COVID-19 and 257 with other community-acquired pneumonia) from three medical centers to train and externally validate the developed models. A pre-trained DL algorithm was utilized to automatically segment infected lesions (ROIs) on CT images which were used for feature extraction. Five feature selection methods and four machine learning algorithms were utilized to develop radiomics models. Trained with features selected by L1 regularized logistic regression, classifier multi-layer perceptron (MLP) demonstrated the optimal performance with AUC of 0.922 (95% CI 0.856-0.988) and 0.959 (95% CI 0.910-1.000), the same sensitivity of 0.879, and specificity of 0.900 and 0.887 on internal and external testing datasets, which was equivalent to the senior radiologist in a reader study. Additionally, diagnostic time of DL-MLP was more efficient than radiologists (38 s vs 5.15 min). With an adequate performance for identifying COVID-19, DL-MLP may help in screening of suspected cases.",2021,10.1038/s41598-021-83237-6,cross-sectional,diagnosis,CT,Lung
A Deep Learning Prognosis Model Help Alert for COVID-19 Patients at High-Risk of Death: A Multi-Center Study,"Since its outbreak in December 2019, the persistent coronavirus disease (COVID-19) became a global health emergency. It is imperative to develop a prognostic tool to identify high-risk patients and assist in the formulation of treatment plans. We retrospectively collected 366 severe or critical COVID-19 patients from four centers, including 70 patients who died within 14 days (labeled as high-risk patients) since their initial CT scan and 296 who survived more than 14 days or were cured (labeled as low-risk patients). We developed a 3D densely connected convolutional neural network (termed De-COVID19-Net) to predict the probability of COVID-19 patients belonging to the high-risk or low-risk group, combining CT and clinical information. The area under the curve (AUC) and other evaluation techniques were used to assess our model. The De-COVID19-Net yielded an AUC of 0.952 (95% confidence interval, 0.928-0.977) on the training set and 0.943 (0.904-0.981) on the test set. The stratified analyses indicated that our model's performance is independent of age, sex, and with/without chronic diseases. The Kaplan-Meier analysis revealed that our model could significantly categorize patients into high-risk and low-risk groups (p < 0.001). In conclusion, De-COVID19-Net can non-invasively predict whether a patient will die shortly based on the patient's initial CT scan with an impressive performance, which indicated that it could be used as a potential prognosis tool to alert high-risk patients and intervene in advance.",2020,10.1109/jbhi.2020.3034296,cross-sectional,prognosis,CT,Lung
A Deep Learning Radiomics Model to Identify Poor Outcome in COVID-19 Patients With Underlying Health Conditions: A Multicenter Study,"OBJECTIVE: Coronavirus disease 2019 (COVID-19) has caused considerable morbidity and mortality, especially in patients with underlying health conditions. A precise prognostic tool to identify poor outcomes among such cases is desperately needed. METHODS: Total 400 COVID-19 patients with underlying health conditions were retrospectively recruited from 4 centers, including 54 dead cases (labeled as poor outcomes) and 346 patients discharged or hospitalized for at least 7 days since initial CT scan. Patients were allocated to a training set (n = 271), a test set (n = 68), and an external test set (n = 61). We proposed an initial CT-derived hybrid model by combining a 3D-ResNet10 based deep learning model and a quantitative 3D radiomics model to predict the probability of COVID-19 patients reaching poor outcome. The model performance was assessed by area under the receiver operating characteristic curve (AUC), survival analysis, and subgroup analysis. RESULTS: The hybrid model achieved AUCs of 0.876 (95% confidence interval: 0.752-0.999) and 0.864 (0.766-0.962) in test and external test sets, outperforming other models. The survival analysis verified the hybrid model as a significant risk factor for mortality (hazard ratio, 2.049 [1.462-2.871], P < 0.001) that could well stratify patients into high-risk and low-risk of reaching poor outcomes (P < 0.001). CONCLUSION: The hybrid model that combined deep learning and radiomics could accurately identify poor outcomes in COVID-19 patients with underlying health conditions from initial CT scans. The great risk stratification ability could help alert risk of death and allow for timely surveillance plans.",2021,10.1109/jbhi.2021.3076086,cross-sectional,prognosis,CT,Lung
A deep learning- and CT image-based prognostic model for the prediction of survival in non-small cell lung cancer,"OBJECTIVE: To assist clinicians in arranging personalized treatment, planning follow-up programs and extending survival times for non-small cell lung cancer (NSCLC) patients, a method of deep learning combined with computed tomography (CT) imaging for survival prediction was designed. METHODS: Data were collected from 484 patients from four research centers. The data from 344 patients were utilized to build the A_CNN survival prognosis model to classify 2-year overall survival time ranges (730 days cut-off). Data from 140 patients, including independent internal and external test sets, were utilized for model testing. First, a series of preprocessing techniques were used to process the original CT images and generate training and test data sets from the axial, coronal, and sagittal planes. Second, the structure of the A_CNN model was designed based on asymmetric convolution, bottleneck blocks, the uniform cross-entropy (UC) loss function, and other advanced techniques. After that, the A_CNN model was trained, and numerous comparative experiments were designed to obtain the best prognostic survival model. Last, the model performance was evaluated, and the predicted survival curves were analyzed. RESULTS: The A_CNN survival prognosis model yielded a high patient-level accuracy of 88.8%, a patch-level accuracy of 82.9%, and an area under the receiver operating characteristic (ROC) curve (AUC) of 0.932. When tested on an external data set, the maximum patient-level accuracy was 80.0%. CONCLUSIONS: The results suggest that using a deep learning method can improve prognosis in patients with NSCLC and has important application value in establishing individualized prognostic models.",2021,10.1002/mp.15302,cross-sectional,prognosis,CT,Lung
A deep learning-based dual-omics prediction model for radiation pneumonitis,"PURPOSE: Radiation pneumonitis (RP) is the main source of toxicity in thoracic radiotherapy. This study proposed a deep learning-based dual-omics model, which aims to improve the RP prediction performance by integrating more data points and exploring the data in greater depth. MATERIALS AND METHODS: The bimodality data were the original dose (OD) distribution and the ventilation image (VI) derived from four-dimensional computed tomography (4DCT). The functional dose (FD) distribution was obtained by weighting OD with VI. A pre-trained three-dimensional convolution (C3D) network was used to extract the features from FD, VI, and OD. The extracted features were then filtered and selected using entropy-based methods. The prediction models were constructed with four most commonly used binary classifiers. Cross-validation, bootstrap, and nested sampling methods were adopted in the process of training and hyper-tuning. RESULTS: Data from 217 thoracic cancer patients treated with radiotherapy were used to train and validate the prediction model. The 4DCT-based VI showed the inhomogeneous pulmonary function of the lungs. More than half of the extracted features were singular (of none-zero value for few patients), which were eliminated to improve the stability of the model. The area under curve (AUC) of the dual-omics model was 0.874 (95% confidence interval: 0.871-0.877), and the AUC of the single-omics model was 0.780 (0.775-0.785, VI) and 0.810 (0.804-0.811, OD), respectively. CONCLUSIONS: The dual-omics outperformed single-omics for RP prediction, which can be contributed to: (1) using more data points; (2) exploring the data in greater depth; and (3) incorporating of the bimodality data.",2021,10.1002/mp.15079,cross-sectional,diagnosis,CT,Lung
A deep residual learning network for predicting lung adenocarcinoma manifesting as ground-glass nodule on CT images,"OBJECTIVE: To develop a deep learning-based artificial intelligence (AI) scheme for predicting the likelihood of the ground-glass nodule (GGN) detected on CT images being invasive adenocarcinoma (IA) and also compare the accuracy of this AI scheme with that of two radiologists. METHODS: First, we retrospectively collected 828 histopathologically confirmed GGNs of 644 patients from two centers. Among them, 209 GGNs are confirmed IA and 619 are non-IA, including 409 adenocarcinomas in situ and 210 minimally invasive adenocarcinomas. Second, we applied a series of pre-preprocessing techniques, such as image resampling, rescaling and cropping, and data augmentation, to process original CT images and generate new training and testing images. Third, we built an AI scheme based on a deep convolutional neural network by using a residual learning architecture and batch normalization technique. Finally, we conducted an observer study and compared the prediction performance of the AI scheme with that of two radiologists using an independent dataset with 102 GGNs. RESULTS: The new AI scheme yielded an area under the receiver operating characteristic curve (AUC) of 0.92 ± 0.03 in classifying between IA and non-IA GGNs, which is equivalent to the senior radiologist's performance (AUC 0.92 ± 0.03) and higher than the score of the junior radiologist (AUC 0.90 ± 0.03). The Kappa value of two sets of subjective prediction scores generated by two radiologists is 0.6. CONCLUSIONS: The study result demonstrates using an AI scheme to improve the performance in predicting IA, which can help improve the development of a more effective personalized cancer treatment paradigm. KEY POINTS: • The feasibility of using a deep learning method to predict the likelihood of the ground-glass nodule being invasive adenocarcinoma. • Residual learning-based CNN model improves the performance in classifying between IA and non-IA nodules. • Artificial intelligence (AI) scheme yields higher performance than radiologists in predicting invasive adenocarcinoma.",2020,10.1007/s00330-019-06533-w,cross-sectional,diagnosis,CT,Lung
"A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest X-ray images","Common lung diseases are first diagnosed using chest X-rays. Here, we show that a fully automated deep-learning pipeline for the standardization of chest X-ray images, for the visualization of lesions and for disease diagnosis can identify viral pneumonia caused by coronavirus disease 2019 (COVID-19) and assess its severity, and can also discriminate between viral pneumonia caused by COVID-19 and other types of pneumonia. The deep-learning system was developed using a heterogeneous multicentre dataset of 145,202 images, and tested retrospectively and prospectively with thousands of additional images across four patient cohorts and multiple countries. The system generalized across settings, discriminating between viral pneumonia, other types of pneumonia and the absence of disease with areas under the receiver operating characteristic curve (AUCs) of 0.94-0.98; between severe and non-severe COVID-19 with an AUC of 0.87; and between COVID-19 pneumonia and other viral or non-viral pneumonia with AUCs of 0.87-0.97. In an independent set of 440 chest X-rays, the system performed comparably to senior radiologists and improved the performance of junior radiologists. Automated deep-learning systems for the assessment of pneumonia could facilitate early intervention and provide support for clinical decision-making.",2021,10.1038/s41551-021-00704-1,cross-sectional,diagnosis,X-ray,Lung
A depthwise separable dense convolutional network with convolution block attention module for COVID-19 diagnosis on CT scans,"Coronavirus disease 2019 (COVID-19) has caused more than 3 million deaths and infected more than 170 million individuals all over the world. Rapid identification of patients with COVID-19 is the key to control transmission and prevent depletion of hospitals. Several networks have been proposed to assist radiologists in diagnosing COVID-19 based on CT scans. However, CTs used in these studies are unavailable for other researchers to do deeper extensions due to privacy concerns. Furthermore, these networks are too heavy-weighted to satisfy the general trend applying on a computationally limited platform. In this paper, we aim to solve these two problems. Firstly, we establish an available dataset COVID-CTx, which contains 828 CT scans positive for COVID-19 across 324 patient cases from three open access data repositories. To our knowledge, it has the largest number of publicly available COVID-19 positive cases compared to other public datasets. Secondly, we propose a light-weighted hybrid neural network: Depthwise Separable Dense Convolutional Network with Convolution Block Attention Module (AM-SdenseNet). AM-SdenseNet synergistically integrates Convolutional Block Attention Module with depthwise separable convolutions to learn powerful feature representations while reducing the parameters to overcome the overfitting problem. Through experiments, we demonstrate the superior performance of our proposed AM-SdenseNet compared with several state-of-the-art baselines. The excellent performance of AM-SdenseNet can improve the speed and accuracy of COVID-19 diagnosis, which is extremely useful to control the spreading of infection.",2021,10.1016/j.compbiomed.2021.104837,cross-sectional,diagnosis,CT,Lung
A doubly robust approach for cost-effectiveness estimation from observational data,"Estimation of common cost-effectiveness measures, including the incremental cost-effectiveness ratio and the net monetary benefit, is complicated by the need to account for informative censoring and inherent skewness of the data. In addition, since the two components of these measures, medical costs and survival are often collected from observational claims data, one must account for potential confounders. We propose a novel doubly robust, unbiased estimator for cost-effectiveness based on propensity scores that allow the incorporation of cost history and time-varying covariates. Further, we use an ensemble machine learning approach to obtain improved predictions from parametric and non-parametric cost and propensity score models. Our simulation studies demonstrate that the proposed doubly robust approach performs well even under mis-specification of either the propensity score model or the outcome model. We apply our approach to a cost-effectiveness analysis of two competing lung cancer surveillance procedures, CT vs. chest X-ray, using SEER-Medicare data.",2018,10.1177/0962280217693262,,,,
A fast neural network approach to predict lung tumor motion during respiration for radiation therapy applications,"During radiotherapy treatment for thoracic and abdomen cancers, for example, lung cancers, respiratory motion moves the target tumor and thus badly affects the accuracy of radiation dose delivery into the target. A real-time image-guided technique can be used to monitor such lung tumor motion for accurate dose delivery, but the system latency up to several hundred milliseconds for repositioning the radiation beam also affects the accuracy. In order to compensate the latency, neural network prediction technique with real-time retraining can be used. We have investigated real-time prediction of 3D time series of lung tumor motion on a classical linear model, perceptron model, and on a class of higher-order neural network model that has more attractive attributes regarding its optimization convergence and computational efficiency. The implemented static feed-forward neural architectures are compared when using gradient descent adaptation and primarily the Levenberg-Marquardt batch algorithm as the ones of the most common and most comprehensible learning algorithms. The proposed technique resulted in fast real-time retraining, so the total computational time on a PC platform was equal to or even less than the real treatment time. For one-second prediction horizon, the proposed techniques achieved accuracy less than one millimeter of 3D mean absolute error in one hundred seconds of total treatment time.",2015,10.1155/2015/489679,cross-sectional,treatment,CT,Lung
A fully automated noncontrast CT 3-D reconstruction algorithm enabled accurate anatomical demonstration for lung segmentectomy,"BACKGROUND: Three-dimensional reconstruction of chest computerized tomography (CT) excels in intuitively demonstrating anatomical patterns for pulmonary segmentectomy. However, current methods are labor-intensive and rely on contrast CT. We hereby present a novel fully automated reconstruction algorithm based on noncontrast CT and assess its performance both independently and in combination with surgeons. METHODS: A retrospective pilot study was performed. Patients between May 2020 to August 2020 who underwent segmentectomy in our single institution were enrolled. Noncontrast CTs were used for reconstruction. In the first part of the study, the accuracy of the demonstration of anatomical variants by either automated or manual reconstruction algorithm were compared to surgical observation, respectively. In the second part of the study, we tested the accuracy of the identification of anatomical variants by four independent attendees who reviewed 3-D reconstruction in combination with CT scans. RESULTS: A total of 20 cases were enrolled in this study. All segments were represented in this study with two left S1-3, two left S4 + 5, one left S6, five left basal segmentectomies, one right S1, three right S2, 1 right S2b + 3a, one right S3, two right S6 and two right basal segmentectomies. The median time consumption for the automated reconstruction was 280 (205-324) s. Accurate vessel and bronchial detection were achieved in 85% by the AI approach and 80% by Mimics, p = 1.00. The accuracy of vessel classification was 80 and 95% by AI and manual approaches, respectively, p = 0.34. In real-world application, the accuracy of the identification of anatomical variant by thoracic surgeons was 85% by AI+CT, and the median time consumption was 2 (1-3) min. CONCLUSIONS: The AI reconstruction algorithm overcame defects of traditional methods and is valuable in surgical planning for segmentectomy. With the AI reconstruction, surgeons may achieve high identification accuracy of anatomical patterns in a short time frame.",2022,10.1111/1759-7714.14322,cross-sectional,treatment,CT,Lung
A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis,"Coronavirus disease 2019 (COVID-19) has spread globally, and medical resources become insufficient in many regions. Fast diagnosis of COVID-19 and finding high-risk patients with worse prognosis for early prevention and medical resource optimisation is important. Here, we proposed a fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis by routinely used computed tomography.We retrospectively collected 5372 patients with computed tomography images from seven cities or provinces. Firstly, 4106 patients with computed tomography images were used to pre-train the deep learning system, making it learn lung features. Following this, 1266 patients (924 with COVID-19 (471 had follow-up for >5 days) and 342 with other pneumonia) from six cities or provinces were enrolled to train and externally validate the performance of the deep learning system.In the four external validation sets, the deep learning system achieved good performance in identifying COVID-19 from other pneumonia (AUC 0.87 and 0.88, respectively) and viral pneumonia (AUC 0.86). Moreover, the deep learning system succeeded to stratify patients into high- and low-risk groups whose hospital-stay time had significant difference (p=0.013 and p=0.014, respectively). Without human assistance, the deep learning system automatically focused on abnormal areas that showed consistent characteristics with reported radiological findings.Deep learning provides a convenient tool for fast screening of COVID-19 and identifying potential high-risk patients, which may be helpful for medical resource optimisation and early prevention before patients show severe symptoms.",2020,10.1183/13993003.00775-2020,cross-sectional,diagnosis,CT,Lung
A Generalized Deep Learning-Based Diagnostic System for Early Diagnosis of Various Types of Pulmonary Nodules,"A novel framework for the classification of lung nodules using computed tomography scans is proposed in this article. To get an accurate diagnosis of the detected lung nodules, the proposed framework integrates the following 2 groups of features: (1) appearance features modeled using the higher order Markov Gibbs random field model that has the ability to describe the spatial inhomogeneities inside the lung nodule and (2) geometric features that describe the shape geometry of the lung nodules. The novelty of this article is to accurately model the appearance of the detected lung nodules using a new developed seventh-order Markov Gibbs random field model that has the ability to model the existing spatial inhomogeneities for both small and large detected lung nodules, in addition to the integration with the extracted geometric features. Finally, a deep autoencoder classifier is fed by the above 2 feature groups to distinguish between the malignant and benign nodules. To evaluate the proposed framework, we used the publicly available data from the Lung Image Database Consortium. We used a total of 727 nodules that were collected from 467 patients. The proposed system demonstrates the promise to be a valuable tool for the detection of lung cancer evidenced by achieving a nodule classification accuracy of 91.20%.",2018,10.1177/1533033818798800,cross-sectional,diagnosis,CT,Lung
A homological approach to a mathematical definition of pulmonary fibrosis and emphysema on computed tomography,"Three-dimensional imaging is essential to evaluate local abnormalities and understand structure-function relationships in an organ. However, quantifiable and interpretable methods to localize abnormalities remain unestablished. Visual assessments are prone to bias, machine learning methods depend on training images, and the underlying decision principle is usually difficult to interpret. Here, we developed a homological approach to mathematically define emphysema and fibrosis in the lungs on computed tomography (CT). With the use of persistent homology, the density of homological features, including connected components, tunnels, and voids, was extracted from the volumetric CT scans of lung diseases. A pair of CT values at which each homological feature appeared (birth) and disappeared (death) was computed by sweeping the threshold levels from higher to lower CT values. Consequently, fibrosis and emphysema were defined as voxels with dense voids having a longer lifetime (birth-death difference) and voxels with dense connected components having a lower birth, respectively. In an independent dataset including subjects with idiopathic pulmonary fibrosis (IPF), chronic obstructive pulmonary disease (COPD), and combined pulmonary fibrosis and emphysema (CPFE), the proposed definition enabled accurate segmentation with comparable quality to deep learning in terms of Dice coefficients. Persistent homology-defined fibrosis was closely associated with physiological abnormalities such as impaired diffusion capacity and long-term mortality in subjects with IPF and CPFE, and persistent homology-defined emphysema was associated with impaired diffusion capacity in subjects with COPD. The present persistent homology-based evaluation of structural abnormalities could help explore the clinical and physiological impacts of structural changes and morphological mechanisms of disease progression.NEW & NOTEWORTHY This study proposes a homological approach to mathematically define a three-dimensional texture feature of emphysema and fibrosis on chest computed tomography using persistent homology. The proposed definition enabled accurate segmentation with comparable quality to deep learning while offering higher interpretability than deep learning-based methods.",2021,10.1152/japplphysiol.00150.2021,cross-sectional,diagnosis,CT,Lung
A human-computer collaboration for COVID-19 differentiation: combining a radiomics model with deep learning and human auditing,"BACKGROUND: This study aimed to build a radiomics model with deep learning (DL) and human auditing and examine its diagnostic value in differentiating between coronavirus disease 2019 (COVID-19) and community-acquired pneumonia (CAP). METHODS: Forty-three COVID-19 patients, whose diagnoses had been confirmed with reverse-transcriptase polymerase-chain-reaction (RT-PCR) tests, and 60 CAP patients, whose diagnoses had been confirmed with sputum cultures, were enrolled in this retrospective study. The candidate regions of interest (ROIs) on the computed tomography (CT) images of the 103 patients were determined using a DL-based segmentation model powered by transfer learning. These ROIs were manually audited and corrected by 3 radiologists (with an average of 12 years of experience; range 6-17 years) to check the segmentation acceptance for the radiomics analysis. ROI-derived radiomics features were subsequently extracted to build the classification model and processed using 4 different algorithms (L1 regularization, Lasso, Ridge, and Z test) and 4 classifiers, including the logistic regression (LR), multi-layer perceptron (MLP), support vector machine (SVM), and extreme Gradient Boosting (XGboost). A receiver operating characteristic curve (ROC) analysis was conducted to evaluate the performance of the model. RESULTS: Quantitative CT measurements derived from human-audited segmentation results showed that COVID-19 patients had significantly decreased numbers of infected lobes compared to patients in the CAP group {median [interquartile range (IQR)]: 4 [3, 4] and 4 [4, 5]; P=0.031}. The infected percentage (%) of the whole lung was significantly more elevated in the CAP group [6.40 (2.77, 11.11)] than the COVID-19 group [1.83 (0.65, 4.42); P<0.001], and the same trend applied to each lobe, except for the superior lobe of the right lung [1.81 (0.09, 5.28) for COVID-19 vs. 1.32 (0.14, 7.02) for CAP; P=0.649]. Additionally, the highest proportion of infected lesions were observed in the CT value range of (-470, -370) Hounsfield units (HU) in the COVID-19 group. Conversely, the CAP group had a value range of (30, 60) HU. Radiomic model using corrected ROIs exhibited the highest area under ROC (AUC) of 0.990 [95% confidence interval (CI): 0.962-1.000] using Lasso for feature selection and MLP for classification. CONCLUSIONS: The proposed radiomics model based on human-audited segmentation made accurate differential diagnoses of COVID-19 and CAP. The quantification of CT measurements derived from DL could potentially be used as effective biomarkers in current clinical practice.",2021,10.21037/apm-20-2625,cross-sectional,diagnosis,CT,Lung
A hybrid CNN feature model for pulmonary nodule malignancy risk differentiation,"The malignancy risk differentiation of pulmonary nodule is one of the most challenge tasks of computer-aided diagnosis (CADx). Most recently reported CADx methods or schemes based on texture and shape estimation have shown relatively satisfactory on differentiating the risk level of malignancy among the nodules detected in lung cancer screening. However, the existing CADx schemes tend to detect and analyze characteristics of pulmonary nodules from a statistical perspective according to local features only. Enlightened by the currently prevailing learning ability of convolutional neural network (CNN), which simulates human neural network for target recognition and our previously research on texture features, we present a hybrid model that takes into consideration of both global and local features for pulmonary nodule differentiation using the largest public database founded by the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). By comparing three types of CNN models in which two of them were newly proposed by us, we observed that the multi-channel CNN model yielded the best discrimination in capacity of differentiating malignancy risk of the nodules based on the projection of distributions of extracted features. Moreover, CADx scheme using the new multi-channel CNN model outperformed our previously developed CADx scheme using the 3D texture feature analysis method, which increased the computed area under a receiver operating characteristic curve (AUC) from 0.9441 to 0.9702.",2018,10.3233/xst-17302,cross-sectional,diagnosis,CT,Lung
A Hybrid Convolutional Neural Network Model for Diagnosis of COVID-19 Using Chest X-ray Images,"COVID-19 declared as a pandemic that has a faster rate of infection and has impacted the lives and the country's economy due to forced lockdowns. Its detection using RT-PCR is required long time and due to which its infection has grown exponentially. This creates havoc for the shortage of testing kits in many countries. This work has proposed a new image processing-based technique for the health care systems named ""C19D-Net"", to detect ""COVID-19"" infection from ""Chest X-Ray"" (XR) images, which can help radiologists to improve their accuracy of detection COVID-19. The proposed system extracts deep learning (DL) features by applying the InceptionV4 architecture and Multiclass SVM classifier to classify and detect COVID-19 infection into four different classes. The dataset of 1900 Chest XR images has been collected from two publicly accessible databases. Images are pre-processed with proper scaling and regular feeding to the proposed model for accuracy attainments. Extensive tests are conducted with the proposed model (""C19D-Net"") and it has succeeded to achieve the highest COVID-19 detection accuracy as 96.24% for 4-classes, 95.51% for three-classes, and 98.1% for two-classes. The proposed method has outperformed well in expressions of ""precision"", ""accuracy"", ""F1-score"" and ""recall"" in comparison with most of the recent previously published methods. As a result, for the present situation of COVID-19, the proposed ""C19D-Net"" can be employed in places where test kits are in short supply, to help the radiologists to improve their accuracy of detection of COVID-19 patients through XR-Images.",2021,10.3390/ijerph182212191,cross-sectional,diagnosis,X-ray,Lung
A joint ROI extraction filter for computer aided lung nodule detection,"Extraction of regions of interest plays an important rule in computer aided lung nodules detection. However, because of the complex background and structure, accurate and robust extraction of ROIs in medical image still remains a problem. Aim at this problem, a two-stage operations joint filter: Hessian-LoB, is proposed. The first stage is blobs (which being taken as candidate ROIs) detection and the second stage is ROIs extraction. In the first stage, the derivatives of a Hessian matrix at multiple scales are convolved with input images to localize blobs. Then in the second stage, Laplacian of bilateral filter (LoB) is convolved with the detected blobs to extract the final ROIs. Experiments show that the proposed filter can deal with images with noise and low brightness contrast, and is effectively in ROI extraction for lung nodule detection.",2015,10.3233/bme-151448,cross-sectional,diagnosis,CT,Lung
A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19,"The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.",2021,10.1016/j.compbiomed.2021.104927,cross-sectional,diagnosis,X-ray,Lung
A Lightweight Multi-Section CNN for Lung Nodule Classification and Malignancy Estimation,"The size and shape of a nodule are the essential indicators of malignancy in lung cancer diagnosis. However, effectively capturing the nodule's structural information from CT scans in a computer-aided system is a challenging task. Unlike previous models that proposed computationally intensive deep ensemble models or three-dimensional CNN models, we propose a lightweight, multiple view sampling based multi-section CNN architecture. The model obtains a nodule's cross sections from multiple view angles and encodes the nodule's volumetric information into a compact representation by aggregating information from its different cross sections via a view pooling layer. The compact feature is subsequently used for the task of nodule classification. The method does not require the nodule's spatial annotation and works directly on the cross sections generated from volume enclosing the nodule. We evaluated the proposed method on lung image database consortium (LIDC) and image database resource initiative (IDRI) dataset. It achieved the state-of-the-art performance with a mean 93.18% classification accuracy. The architecture could also be used to select the representative cross sections determining the nodule's malignancy that facilitates in the interpretation of results. Because of being lightweight, the model could be ported to mobile devices, which brings the power of artificial intelligence (AI) driven application directly into the practitioner's hand.",2019,10.1109/jbhi.2018.2879834,cross-sectional,prognosis,CT,Lung
A Machine Learning Model Based on PET/CT Radiomics and Clinical Characteristics Predicts Tumor Immune Profiles in Non-Small Cell Lung Cancer: A Retrospective Multicohort Study,"BACKGROUND: The tumor immune microenvironment (TIME) phenotypes have been reported to mainly impact the efficacy of immunotherapy. Given the increasing use of immunotherapy in cancers, knowing an individual's TIME phenotypes could be helpful in screening patients who are more likely to respond to immunotherapy. Our study intended to establish, validate, and apply a machine learning model to predict TIME profiles in non-small cell lung cancer (NSCLC) by using (18)F-FDG PET/CT radiomics and clinical characteristics. METHODS: The RNA-seq data of 1145 NSCLC patients from The Cancer Genome Atlas (TCGA) cohort were analyzed. Then, 221 NSCLC patients from Daping Hospital (DPH) cohort received(18)F-FDG PET/CT scans before treatment and CD8 expression of the tumor samples were tested. The Artificial Intelligence Kit software was used to extract radiomic features of PET/CT images and develop a radiomics signature. The models were established by radiomics, clinical features, and radiomics-clinical combination, respectively, the performance of which was calculated by receiver operating curves (ROCs) and compared by DeLong test. Moreover, based on radiomics score (Rad-score) and clinical features, a nomogram was established. Finally, we applied the combined model to evaluate TIME phenotypes of NSCLC patients in The Cancer Imaging Archive (TCIA) cohort (n = 39). RESULTS: TCGA data showed CD8 expression could represent the TIME profiles in NSCLC. In DPH cohort, PET/CT radiomics model outperformed CT model (AUC: 0.907 vs. 0.861, P = 0.0314) to predict CD8 expression. Further, PET/CT radiomics-clinical combined model (AUC = 0.932) outperformed PET/CT radiomics model (AUC = 0.907, P = 0.0326) or clinical model (AUC = 0.868, P = 0.0036) to predict CD8 expression. In the TCIA cohort, the predicted CD8-high group had significantly higher immune scores and more activated immune pathways than the predicted CD8-low group (P = 0.0421). CONCLUSION: Our study indicates that (18)F-FDG PET/CT radiomics-clinical combined model could be a clinically practical method to non-invasively detect the tumor immune status in NSCLCs.",2022,10.3389/fimmu.2022.859323,cross-sectional,prognosis,PET/CT,Lung
A machine learning texture model for classifying lung cancer subtypes using preliminary bronchoscopic findings,"PURPOSE: Bronchoscopy is useful in lung cancer detection, but cannot be used to differentiate cancer types. A computer-aided diagnosis (CAD) system was proposed to distinguish malignant cancer types to achieve objective diagnoses. METHODS: Bronchoscopic images of 12 adenocarcinoma and 10 squamous cell carcinoma patients were collected. The images were transformed from a red-blue-green (RGB) to a hue-saturation-value (HSV) color space to obtain more meaningful color textures. By combining significant textural features (P < 0.05) in a machine learning classifier, a prediction model of malignant types was established. RESULTS: The performance of the CAD system achieved an accuracy of 86% (19/22), a sensitivity of 90% (9/10), a specificity of 83% (10/12), a positive predictive value of 82% (9/11), and a negative predictive value of 91% (10/11) in distinguishing lung cancer types. The area under the receiver operating characteristic curve was 0.82. CONCLUSIONS: On the basis of extracted HSV textures of bronchoscopic images, the CAD system can provide recommendations for clinical diagnoses of lung cancer types.",2018,10.1002/mp.13241,,,,
A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images,"Corona virus disease (COVID-19) acknowledged as a pandemic by the WHO and mankind all over the world is vulnerable to this virus. Alternative tools are needed that can help in diagnosis of the coronavirus. Researchers of this article investigated the potential of machine learning methods for automatic diagnosis of corona virus with high accuracy from X-ray images. Two most commonly used classifiers were selected: logistic regression (LR) and convolutional neural networks (CNN). The main reason was to make the system fast and efficient. Moreover, a dimensionality reduction approach was also investigated based on principal component analysis (PCA) to further speed up the learning process and improve the classification accuracy by selecting the highly discriminate features. The deep learning-based methods demand large amount of training samples compared to conventional approaches, yet adequate amount of labelled training samples was not available for COVID-19 X-ray images. Therefore, data augmentation technique using generative adversarial network (GAN) was employed to further increase the training samples and reduce the overfitting problem. We used the online available dataset and incorporated GAN to have 500 X-ray images in total for this study. Both CNN and LR showed encouraging results for COVID-19 patient identification. The LR and CNN models showed 95.2-97.6% overall accuracy without PCA and 97.6-100% with PCA for positive cases identification, respectively.",2021,10.1007/s12539-020-00403-6,cross-sectional,diagnosis,X-ray,Lung
A machine learning-based real-time tumor tracking system for fluoroscopic gating of lung radiotherapy,"To improve respiratory-gated radiotherapy accuracy, we developed a machine learning approach for markerless tumor tracking and evaluated it using lung cancer patient data. Digitally reconstructed radiography (DRR) datasets were generated using planning 4DCT data. Tumor positions were selected on respective DRR images to place the GTV center of gravity in the center of each DRR. DRR subimages around the tumor regions were cropped so that the subimage size was defined by tumor size. Training data were then classified into two groups: positive (including tumor) and negative (not including tumor) samples. Machine learning parameters were optimized by the extremely randomized tree method. For the tracking stage, a machine learning algorithm was generated to provide a tumor likelihood map using fluoroscopic images. Prior probability tumor positions were also calculated using the previous two frames. Tumor position was then estimated by calculating maximum probability on the tumor likelihood map and prior probability tumor positions. We acquired treatment planning 4DCT images in eight patients. Digital fluoroscopic imaging systems on either side of the vertical irradiation port allowed fluoroscopic image acquisition during treatment delivery. Each fluoroscopic dataset was acquired at 15 frames per second. We evaluated the tracking accuracy and computation times. Tracking positional accuracy averaged over all patients was 1.03 ± 0.34 mm (mean ± standard deviation, Euclidean distance) and 1.76 ± 0.71 mm ([Formula: see text] percentile). Computation time was 28.66 ± 1.89 ms/frame averaged over all frames. Our markerless algorithm successfully estimated tumor position in real time.",2020,10.1088/1361-6560/ab79c5,cross-sectional,treatment,4D-CT,Lung
A Machine-Learning Approach to Developing a Predictive Signature Based on Transcriptome Profiling of Ground-Glass Opacities for Accurate Classification and Exploring the Immune Microenvironment of Early-Stage LUAD,"Screening for early-stage lung cancer with low-dose computed tomography is recommended for high-risk populations; consequently, the incidence of pure ground-glass opacity (pGGO) is increasing. Ground-glass opacity (GGO) is considered the appearance of early lung cancer, and there remains an unmet clinical need to understand the pathology of small GGO (<1 cm in diameter). The objective of this study was to use the transcriptome profiling of pGGO specimens <1 cm in diameter to construct a pGGO-related gene risk signature to predict the prognosis of early-stage lung adenocarcinoma (LUAD) and explore the immune microenvironment of GGO. pGGO-related differentially expressed genes (DEGs) were screened to identify prognostic marker genes with two machine learning algorithms. A 15-gene risk signature was constructed from the DEGs that were shared between the algorithms. Risk scores were calculated using the regression coefficients for the pGGO-related DEGs. Patients with Stage I/II LUAD or Stage IA LUAD and high-risk scores had a worse prognosis than patients with low-risk scores. The prognosis of high-risk patients with Stage IA LUAD was almost identical to that of patients with Stage II LUAD, suggesting that treatment strategies for patients with Stage II LUAD may be beneficial in high-risk patients with Stage IA LUAD. pGGO-related DEGs were mainly enriched in immune-related pathways. Patients with high-risk scores and high tumor mutation burden had a worse prognosis and may benefit from immunotherapy. A nomogram was constructed to facilitate the clinical application of the 15-gene risk signature. Receiver operating characteristic curves and decision curve analysis validated the predictive ability of the nomogram in patients with Stage I LUAD in the TCGA-LUAD cohort and GEO datasets.",2022,10.3389/fimmu.2022.872387,,,,
A Machine-Learning Approach Using PET-Based Radiomics to Predict the Histological Subtypes of Lung Cancer,"PURPOSE: We sought to distinguish lung adenocarcinoma (ADC) from squamous cell carcinoma using a machine-learning algorithm with PET-based radiomic features. METHODS: A total of 396 patients with 210 ADCs and 186 squamous cell carcinomas who underwent FDG PET/CT prior to treatment were retrospectively analyzed. Four clinical features (age, sex, tumor size, and smoking status) and 40 radiomic features were investigated in terms of lung ADC subtype prediction. Radiomic features were extracted from the PET images of segmented tumors using the LIFEx package. The clinical and radiomic features were ranked, and a subset of useful features was selected based on Gini coefficient scores in terms of associations with histological class. The areas under the receiver operating characteristic curves (AUCs) of classifications afforded by several machine-learning algorithms (random forest, neural network, naive Bayes, logistic regression, and a support vector machine) were compared and validated via random sampling. RESULTS: We developed and validated a PET-based radiomic model predicting the histological subtypes of lung cancer. Sex, SUVmax, gray-level zone length nonuniformity, gray-level nonuniformity for zone, and total lesion glycolysis were the 5 best predictors of lung ADC. The logistic regression model outperformed all other classifiers (AUC = 0.859, accuracy = 0.769, F1 score = 0.774, precision = 0.804, recall = 0.746) followed by the neural network model (AUC = 0.854, accuracy = 0.772, F1 score = 0.777, precision = 0.807, recall = 0.750). CONCLUSIONS: A machine-learning approach successfully identified the histological subtypes of lung cancer. A PET-based radiomic features may help clinicians improve the histopathologic diagnosis in a noninvasive manner.",2019,10.1097/rlu.0000000000002810,cross-sectional,diagnosis,CT/PET,Lung
A manifold learning regularization approach to enhance 3D CT image-based lung nodule classification,"PURPOSE: Diagnosis of lung cancer requires radiologists to review every lung nodule in CT images. Such a process can be very time-consuming, and the accuracy is affected by many factors, such as experience of radiologists and available diagnosis time. To address this problem, we proposed to develop a deep learning-based system to automatically classify benign and malignant lung nodules. METHODS: The proposed method automatically determines benignity or malignancy given the 3D CT image patch of a lung nodule to assist diagnosis process. Motivated by the fact that real structure among data is often embedded on a low-dimensional manifold, we developed a novel manifold regularized classification deep neural network (MRC-DNN) to perform classification directly based on the manifold representation of lung nodule images. The concise manifold representation revealing important data structure is expected to benefit the classification, while the manifold regularization enforces strong, but natural constraints on network training, preventing over-fitting. RESULTS: The proposed method achieves accurate manifold learning with reconstruction error of ~ 30 HU on real lung nodule CT image data. In addition, the classification accuracy on testing data is 0.90 with sensitivity of 0.81 and specificity of 0.95, which outperforms state-of-the-art deep learning methods. CONCLUSION: The proposed MRC-DNN facilitates an accurate manifold learning approach for lung nodule classification based on 3D CT images. More importantly, MRC-DNN suggests a new and effective idea of enforcing regularization for network training, possessing the potential impact to a board range of applications.",2020,10.1007/s11548-019-02097-8,cross-sectional,,,
A method establishment and comparison of in vivo lung cancer model development platforms for evaluation of tumour metabolism and pharmaceutical efficacy,"BACKGROUND: Currently, the identification of accurate biomarkers for the diagnosis of patients with early-stage lung cancer remains difficult. Fortunately, metabolomics technology can be used to improve the detection of plasma metabolic biomarkers for lung cancer. In a previous study, we successfully utilised machine learning methods to identify significant metabolic markers for early-stage lung cancer diagnosis. However, a related research platform for the investigation of tumour metabolism and drug efficacy is still lacking. HYPOTHESIS/PURPOSE: A novel methodology for the comprehensive evaluation of the internal tumour-metabolic profile and drug evaluation needs to be established. METHODS: The optimal location for tumour cell inoculation was identified in mouse chest for the non-traumatic orthotopic lung cancer mouse model. Microcomputed tomography (micro-CT) was applied to monitor lung tumour growth. Proscillaridin A (P.A) and cisplatin (CDDP) were utilised to verify the anti-lung cancer efficacy of the platform. The top five clinically valid biomarkers, including proline, L-kynurenine, spermidine, taurine and palmitoyl-L-carnitine, were selected as the evaluation indices to obtain a suitable lung cancer mouse model for clinical metabolomics research by ultra-performance liquid chromatography-tandem mass spectrometry (UPLC-MS/MS). RESULTS: The platform was successfully established, achieving 100% tumour development rate and 0% surgery mortality. P.A and CDDP had significant anti-lung cancer efficacy in the platform. Compared with the control group, four biomarkers in the orthotopic model and two biomarkers in the metastatic model had significantly higher abundance. Principal component analysis (PCA) showed a significant separation between the orthotopic/metastatic model and the control/subcutaneous/KRAS transgenic model. The platform was mainly involved in arginine and proline metabolism, tryptophan metabolism, and taurine and hypotaurine metabolism. CONCLUSION: This study is the first to simulate clinical metabolomics by comparing the metabolic phenotype of plasma in different lung cancer mouse models. We found that the orthotopic model was the most suitable for tumour metabolism. Furthermore, the anti-tumour drug efficacy was verified in the platform. The platform can very well match the clinical reality, providing better lung cancer diagnosis and securing more precise evidence for drug evaluation in the future.",2022,10.1016/j.phymed.2021.153831,,,,
A Method for Optimal Detection of Lung Cancer Based on Deep Learning Optimized by Marine Predators Algorithm,"Lung cancer is the uncontrolled growth of cells in the lung that are made up of two spongy organs located in the chest. These cells may penetrate outside the lungs in a process called metastasis and spread to tissues and organs in the body. In this paper, using image processing, deep learning, and metaheuristic, an optimal methodology is proposed for early detection of this cancer. Here, we design a new convolutional neural network for this purpose. Marine predators algorithm is also used for optimal arrangement and better network accuracy. The method finally applied to RIDER dataset, and the results are compared with some pretrained deep networks, including CNN ResNet-18, GoogLeNet, AlexNet, and VGG-19. Final results showed higher results of the proposed method toward the compared techniques. The results showed that the proposed MPA-based method with 93.4% accuracy, 98.4% sensitivity, and 97.1% specificity provides the highest efficiency with the least error (1.6) toward the other state of the art methods.",2021,10.1155/2021/3694723,cross-sectional,diagnosis,CT,Lung
A method for volumetric imaging in radiotherapy using single x-ray projection,"PURPOSE: It is an intriguing problem to generate an instantaneous volumetric image based on the corresponding x-ray projection. The purpose of this study is to develop a new method to achieve this goal via a sparse learning approach. METHODS: To extract motion information hidden in projection images, the authors partitioned a projection image into small rectangular patches. The authors utilized a sparse learning method to automatically select patches that have a high correlation with principal component analysis (PCA) coefficients of a lung motion model. A model that maps the patch intensity to the PCA coefficients was built along with the patch selection process. Based on this model, a measured projection can be used to predict the PCA coefficients, which are then further used to generate a motion vector field and hence a volumetric image. The authors have also proposed an intensity baseline correction method based on the partitioned projection, in which the first and the second moments of pixel intensities at a patch in a simulated projection image are matched with those in a measured one via a linear transformation. The proposed method has been validated in both simulated data and real phantom data. RESULTS: The algorithm is able to identify patches that contain relevant motion information such as the diaphragm region. It is found that an intensity baseline correction step is important to remove the systematic error in the motion prediction. For the simulation case, the sparse learning model reduced the prediction error for the first PCA coefficient to 5%, compared to the 10% error when sparse learning was not used, and the 95th percentile error for the predicted motion vector was reduced from 2.40 to 0.92 mm. In the phantom case with a regular tumor motion, the predicted tumor trajectory was successfully reconstructed with a 0.82 mm error for tumor center localization compared to a 1.66 mm error without using the sparse learning method. When the tumor motion was driven by a real patient breathing signal with irregular periods and amplitudes, the average tumor center error was 0.6 mm. The algorithm robustness with respect to sparsity level, patch size, and presence or absence of diaphragm, as well as computation time, has also been studied. CONCLUSIONS: The authors have developed a new method that automatically identifies motion information from an x-ray projection, based on which a volumetric image is generated.",2015,10.1118/1.4918577,,,,
A method to combine target volume data from 3D and 4D planned thoracic radiotherapy patient cohorts for machine learning applications,"BACKGROUND AND PURPOSE: The gross tumour volume (GTV) is predictive of clinical outcome and consequently features in many machine-learned models. 4D-planning, however, has prompted substitution of the GTV with the internal gross target volume (iGTV). We present and validate a method to synthesise GTV data from the iGTV, allowing the combination of 3D and 4D planned patient cohorts for modelling. MATERIAL AND METHODS: Expert delineations in 40 non-small cell lung cancer patients were used to develop linear fit and erosion methods to synthesise the GTV volume and shape. Quality was assessed using Dice Similarity Coefficients (DSC) and closest point measurements; by calculating dosimetric features; and by assessing the quality of random forest models built on patient populations with and without synthetic GTVs. RESULTS: Volume estimates were within the magnitudes of inter-observer delineation variability. Shape comparisons produced mean DSCs of 0.8817 and 0.8584 for upper and lower lobe cases, respectively. A model trained on combined true and synthetic data performed significantly better than models trained on GTV alone, or combined GTV and iGTV data. CONCLUSIONS: Accurate synthesis of GTV size from the iGTV permits the combination of lung cancer patient cohorts, facilitating machine learning applications in thoracic radiotherapy.",2018,10.1016/j.radonc.2017.11.015,cross-sectional,treatment,CT,Lung
A model based on CT radiomic features for predicting RT-PCR becoming negative in coronavirus disease 2019 (COVID-19) patients,"BACKGROUND: Coronavirus disease 2019 (COVID-19) has emerged as a global pandemic. According to the diagnosis and treatment guidelines of China, negative reverse transcription-polymerase chain reaction (RT-PCR) is the key criterion for discharging COVID-19 patients. However, repeated RT-PCR tests lead to medical waste and prolonged hospital stays for COVID-19 patients during the recovery period. Our purpose is to assess a model based on chest computed tomography (CT) radiomic features and clinical characteristics to predict RT-PCR negativity during clinical treatment. METHODS: From February 10 to March 10, 2020, 203 mild COVID-19 patients in Fangcang Shelter Hospital were retrospectively included (training: n = 141; testing: n = 62), and clinical characteristics were collected. Lung abnormalities on chest CT images were segmented with a deep learning algorithm. CT quantitative features and radiomic features were automatically extracted. Clinical characteristics and CT quantitative features were compared between RT-PCR-negative and RT-PCR-positive groups. Univariate logistic regression and Spearman correlation analyses identified the strongest features associated with RT-PCR negativity, and a multivariate logistic regression model was established. The diagnostic performance was evaluated for both cohorts. RESULTS: The RT-PCR-negative group had a longer time interval from symptom onset to CT exams than the RT-PCR-positive group (median 23 vs. 16 days, p < 0.001). There was no significant difference in the other clinical characteristics or CT quantitative features. In addition to the time interval from symptom onset to CT exams, nine CT radiomic features were selected for the model. ROC curve analysis revealed AUCs of 0.811 and 0.812 for differentiating the RT-PCR-negative group, with sensitivity/specificity of 0.765/0.625 and 0.784/0.600 in the training and testing datasets, respectively. CONCLUSION: The model combining CT radiomic features and clinical data helped predict RT-PCR negativity during clinical treatment, indicating the proper time for RT-PCR retesting.",2020,10.1186/s12880-020-00521-z,cross-sectional,diagnosis,CT,Lung
A model for the effective COVID-19 identification in uncertainty environment using primary symptoms and CT scans,"The rapid spread of the COVID-19 virus around the world poses a real threat to public safety. Some COVID-19 symptoms are similar to other viral chest diseases, which makes it challenging to develop models for effective detection of COVID-19 infection. This article advocates a model to differentiate between COVID-19 and other four viral chest diseases under uncertainty environment using the viruses primary symptoms and CT scans. The proposed model is based on a plithogenic set, which provides higher accurate evaluation results in an uncertain environment. The proposed model employs the best-worst method (BWM) and the technique in order of preference by similarity to ideal solution (TOPSIS). Besides, this study discusses how smart Internet of Things technology can assist medical staff in monitoring the spread of COVID-19. Experimental evaluation of the proposed model was conducted on five different chest diseases. Evaluation results demonstrate that the proposed model effectiveness in detecting the COVID-19 in all five cases achieving detection accuracy of up to 98%.",2020,10.1177/1460458220952918,cross-sectional,diagnosis,CT,Lung
A multi-center study of COVID-19 patient prognosis using deep learning-based CT image analysis and electronic health records,"PURPOSE: As of August 30th, there were in total 25.1 million confirmed cases and 845 thousand deaths caused by coronavirus disease of 2019 (COVID-19) worldwide. With overwhelming demands on medical resources, patient stratification based on their risks is essential. In this multi-center study, we built prognosis models to predict severity outcomes, combining patients' electronic health records (EHR), which included vital signs and laboratory data, with deep learning- and CT-based severity prediction. METHOD: We first developed a CT segmentation network using datasets from multiple institutions worldwide. Two biomarkers were extracted from the CT images: total opacity ratio (TOR) and consolidation ratio (CR). After obtaining TOR and CR, further prognosis analysis was conducted on datasets from INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3. For each data cohort, generalized linear model (GLM) was applied for prognosis prediction. RESULTS: For the deep learning model, the correlation coefficient of the network prediction and manual segmentation was 0.755, 0.919, and 0.824 for the three cohorts, respectively. The AUC (95 % CI) of the final prognosis models was 0.85(0.77,0.92), 0.93(0.87,0.98), and 0.86(0.75,0.94) for INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3 cohorts, respectively. Either TOR or CR exist in all three final prognosis models. Age, white blood cell (WBC), and platelet (PLT) were chosen predictors in two cohorts. Oxygen saturation (SpO2) was a chosen predictor in one cohort. CONCLUSION: The developed deep learning method can segment lung infection regions. Prognosis results indicated that age, SpO2, CT biomarkers, PLT, and WBC were the most important prognostic predictors of COVID-19 in our prognosis model.",2021,10.1016/j.ejrad.2021.109583,cross-sectional,diagnosis,CT,Lung
A multi-scale framework with unsupervised joint training of convolutional neural networks for pulmonary deformable image registration,"To achieve accurate and fast deformable image registration (DIR) for pulmonary CT, we proposed a Multi-scale DIR framework with unsupervised Joint training of Convolutional Neural Network (MJ-CNN). MJ-CNN contains three models at multi-scale levels for a coarse-to-fine DIR to avoid being trapped in a local minimum. It is trained based on image similarity and deformation vector field (DVF) smoothness, requiring no supervision of ground-truth DVF. The three models are first trained sequentially and separately for their own registration tasks, and then are trained jointly for an end-to-end optimization under the multi-scale framework. In this study, MJ-CNN was trained using public SPARE 4D-CT data. The trained MJ-CNN was then evaluated on public DIR-LAB 4D-CT dataset as well as clinical CT-to-CBCT and CBCT-to-CBCT registration. For 4D-CT inter-phase registration, MJ-CNN achieved comparable accuracy to conventional iteration optimization-based methods, and showed the smallest registration errors compared to recently published deep learning-based DIR methods, demonstrating the efficacy of the proposed multi-scale joint training scheme. Besides, MJ-CNN trained using one dataset (SPARE) could generalize to a different dataset (DIR-LAB) acquired by different scanners and imaging protocols. Furthermore, MJ-CNN trained on 4D-CTs also performed well on CT-to-CBCT and CBCT-to-CBCT registration without any re-training or fine-tuning, demonstrating MJ-CNN's robustness against applications and imaging techniques. MJ-CNN took about 1.4 s for DVF estimation and required no manual-tuning of parameters during the evaluation. MJ-CNN is able to perform accurate DIR for pulmonary CT with nearly real-time speed, making it very applicable for clinical tasks.",2020,10.1088/1361-6560/ab5da0,cross-sectional,informatics,4D-CT,Lung
A multiplexed microfluidic system for evaluation of dynamics of immune-tumor interactions,"Recapitulation of the tumor microenvironment is critical for probing mechanisms involved in cancer, and for evaluating the tumor-killing potential of chemotherapeutic agents, targeted therapies and immunotherapies. Microfluidic devices have emerged as valuable tools for both mechanistic studies and for preclinical evaluation of therapeutic agents, due to their ability to precisely control drug concentrations and gradients of oxygen and other species in a scalable and potentially high throughput manner. Most existing in vitro microfluidic cancer models are comprised of cultured cancer cells embedded in a physiologically relevant matrix, collocated with vascular-like structures. However, the recent emergence of immune checkpoint inhibitors (ICI) as a powerful therapeutic modality against many cancers has created a need for preclinical in vitro models that accommodate interactions between tumors and immune cells, particularly for assessment of unprocessed tumor fragments harvested directly from patient biopsies. Here we report on a microfluidic model, termed EVIDENT (ex vivo immuno-oncology dynamic environment for tumor biopsies), that accommodates up to 12 separate tumor biopsy fragments interacting with flowing tumor-infiltrating lymphocytes (TILs) in a dynamic microenvironment. Flow control is achieved with a single pump in a simple and scalable configuration, and the entire system is constructed using low-sorption materials, addressing two principal concerns with existing microfluidic cancer models. The system sustains tumor fragments for multiple days, and permits real-time, high-resolution imaging of the interaction between autologous TILs and tumor fragments, enabling mapping of TIL-mediated tumor killing and testing of various ICI treatments versus tumor response. Custom image analytic algorithms based on machine learning reported here provide automated and quantitative assessment of experimental results. Initial studies indicate that the system is capable of quantifying temporal levels of TIL infiltration and tumor death, and that the EVIDENT model mimics the known in vivo tumor response to anti-PD-1 ICI treatment of flowing TILs relative to isotype control treatments for syngeneic mouse MC38 tumors.",2018,10.1039/c8lc00256h,,,,
A multistage discriminative model for tumor and lymph node detection in thoracic images,"Analysis of primary lung tumors and disease in regional lymph nodes is important for lung cancer staging, and an automated system that can detect both types of abnormalities will be helpful for clinical routine. In this paper, we present a new method to automatically detect both tumors and abnormal lymph nodes simultaneously from positron emission tomography-computed tomography thoracic images. We perform the detection in a multistage approach, by first detecting all potential abnormalities, then differentiate between tumors and lymph nodes, and finally refine the detected tumors for false positive reduction. Each stage is designed with a discriminative model based on support vector machines and conditional random fields, exploiting intensity, spatial and contextual features. The method is designed to handle a wide and complex variety of abnormal patterns found in clinical datasets, consisting of different spatial contexts of tumors and abnormal lymph nodes. We evaluated the proposed method thoroughly on clinical datasets, and encouraging results were obtained.",2012,10.1109/tmi.2012.2185057,cross-sectional,diagnosis,PET/CT,Lung
A new method based on MTANNs for cutting down false-positives: an evaluation on different versions of commercial pulmonary nodule detection CAD software,"One of the major problems for computer-aided pulmonary nodule detection in chest radiographs is that a high false-positive (FP) rate exists. In an effort to overcome this problem, a new method based on the MTANN (Massive Training Artificial Neural Network) is proposed in this paper. An MTANN comprises a multi-layer neural network where a linear function rather than a sigmoid function is used as its activity function in the output layer. In this work, a mixture of multiple MTANNs were employed rather than only a single MTANN. 50 MTANNs for 50 different types of FPs were prepared firstly. Then, several effective MTANNs that had higher performances were selected to construct the MTANNs mixture. Finally, the outputs of the multiple MTANNs were combined with a mixing neural network to reduce various different types of FPs. The performance of this MTANNs mixture in FPs reduction is validated on three different versions of commercial CAD software with a validation database consisting of 52 chest radiographs. Experimental results demonstrate that the proposed MTANN approach is useful in cutting down FPs in different CAD software for detecting pulmonary nodules in chest radiographs.",2014,10.3233/bme-141102,cross-sectional,diagnosis,X-ray,Lung
A new method of detecting pulmonary nodules with PET/CT based on an improved watershed algorithm,"BACKGROUND: Integrated 18F-fluorodeoxyglucose positron emission tomography/computed tomography (18F-FDG PET/CT) is widely performed for staging solitary pulmonary nodules (SPNs). However, the diagnostic efficacy of SPNs based on PET/CT is not optimal. Here, we propose a method of detection based on PET/CT that can differentiate malignant and benign SPNs with few false-positives. METHOD: Our proposed method combines the features of positron-emission tomography (PET) and computed tomography (CT). A dynamic threshold segmentation method was used to identify lung parenchyma in CT images and suspicious areas in PET images. Then, an improved watershed method was used to mark suspicious areas on the CT image. Next, the support vector machine (SVM) method was used to classify SPNs based on textural features of CT images and metabolic features of PET images to validate the proposed method. RESULTS: Our proposed method was more efficient than traditional methods and methods based on the CT or PET features alone (sensitivity 95.6%; average of 2.9 false positives per scan).",2015,10.1371/journal.pone.0123694,cross-sectional,diagnosis,PET/CT,Lung
A New Optimal Diagnosis System for Coronavirus (COVID-19) Diagnosis Based on Archimedes Optimization Algorithm on Chest X-Ray Images,"The new coronavirus, COVID-19, has affected people all over the world. Coronaviruses are a large group of viruses that can infect animals and humans and cause respiratory distress; these discomforts may be as mild as a cold or as severe as pneumonia. Correct detection of this disease can help to avoid its spreading increasingly. In this paper, a new CAD-based approach is suggested for the optimal diagnosis of this disease from chest X-ray images. The proposed method starts with a min-max normalization to scale all data into a normal scale, and then, histogram equalization is performed to improve the quality of the image before main processing. Afterward, 18 different features are extracted from the image. To decrease the method difficulty, the minimum features are selected based on a metaheuristic called Archimedes optimization algorithm (AOA). The model is then implemented on three datasets, and its results are compared with four other state-of-the-art methods. The final results indicated that the proposed method with 86% accuracy and 96% precision has the highest balance between accuracy and reliability with the compared methods as a diagnostic system for COVID-19.",2021,10.1155/2021/7788491,cross-sectional,diagnosis,X-ray,Lung
A Noise-Robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions From CT Images,"Segmentation of pneumonia lesions from CT scans of COVID-19 patients is important for accurate diagnosis and follow-up. Deep learning has a potential to automate this task but requires a large set of high-quality annotations that are difficult to collect. Learning from noisy training labels that are easier to obtain has a potential to alleviate this problem. To this end, we propose a novel noise-robust framework to learn from noisy labels for the segmentation task. We first introduce a noise-robust Dice loss that is a generalization of Dice loss for segmentation and Mean Absolute Error (MAE) loss for robustness against noise, then propose a novel COVID-19 Pneumonia Lesion segmentation network (COPLE-Net) to better deal with the lesions with various scales and appearances. The noise-robust Dice loss and COPLE-Net are combined with an adaptive self-ensembling framework for training, where an Exponential Moving Average (EMA) of a student model is used as a teacher model that is adaptively updated by suppressing the contribution of the student to EMA when the student has a large training loss. The student model is also adaptive by learning from the teacher only when the teacher outperforms the student. Experimental results showed that: (1) our noise-robust Dice loss outperforms existing noise-robust loss functions, (2) the proposed COPLE-Net achieves higher performance than state-of-the-art image segmentation networks, and (3) our framework with adaptive self-ensembling significantly outperforms a standard training process and surpasses other noise-robust training approaches in the scenario of learning from noisy labels for COVID-19 pneumonia lesion segmentation.",2020,10.1109/tmi.2020.3000314,cross-sectional,diagnosis,CT,Lung
A Noninvasive Multianalytical Approach for Lung Cancer Diagnosis of Patients with Pulmonary Nodules,"Addressing the high false-positive rate of conventional low-dose computed tomography (LDCT) for lung cancer diagnosis, the efficacy of incorporating blood-based noninvasive testing for assisting practicing clinician's decision making in diagnosis of pulmonary nodules (PNs) is investigated. In this prospective observative study, next generation sequencing- (NGS-) based cell-free DNA (cfDNA) mutation profiling, NGS-based cfDNA methylation profiling, and blood-based protein cancer biomarker testing are performed for patients with PNs, who are diagnosed as high-risk patients through LDCT and subsequently undergo surgical resections, with tissue sections pathologically examined and classified. Using pathological classification as the gold standard, statistical and machine learning methods are used to select molecular markers associated with tissue's malignant classification based on a 98-patient discovery cohort (28 benign and 70 malignant), and to construct an integrative multianalytical model for tissue malignancy prediction. Predictive models based on individual testing platforms have shown varying levels of performance, while their final integrative model produces an area under the receiver operating characteristic curve (AUC) of 0.85. The model's performance is further confirmed on a 29-patient independent validation cohort (14 benign and 15 malignant, with power > 0.90), reproducing AUC of 0.86, which translates to an overall sensitivity of 80% and specificity of 85.7%.",2021,10.1002/advs.202100104,cross-sectional,diagnosis,CT,Lung
A novel adaptive momentum method for medical image classification using convolutional neural network,"BACKGROUND: AI for medical diagnosis has made a tremendous impact by applying convolutional neural networks (CNNs) to medical image classification and momentum plays an essential role in stochastic gradient optimization algorithms for accelerating or improving training convolutional neural networks. In traditional optimizers in CNNs, the momentum is usually weighted by a constant. However, tuning hyperparameters for momentum can be computationally complex. In this paper, we propose a novel adaptive momentum for fast and stable convergence. METHOD: Applying adaptive momentum rate proposes increasing or decreasing based on every epoch's error changes, and it eliminates the need for momentum hyperparameter optimization. We tested the proposed method with 3 different datasets: REMBRANDT Brain Cancer, NIH Chest X-ray, COVID-19 CT scan. We compared the performance of a novel adaptive momentum optimizer with Stochastic gradient descent (SGD) and other adaptive optimizers such as Adam and RMSprop. RESULTS: Proposed method improves SGD performance by reducing classification error from 6.12 to 5.44%, and it achieved the lowest error and highest accuracy compared with other optimizers. To strengthen the outcomes of this study, we investigated the performance comparison for the state-of-the-art CNN architectures with adaptive momentum. The results shows that the proposed method achieved the highest with 95% compared to state-of-the-art CNN architectures while using the same dataset. The proposed method improves convergence performance by reducing classification error and achieves high accuracy compared with other optimizers.",2022,10.1186/s12880-022-00755-z,cross-sectional,diagnosis,CT and X-ray,Lung
A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography,"The deposits of fat on the surroundings of the heart are correlated to several health risk factors such as atherosclerosis, carotid stiffness, coronary artery calcification, atrial fibrillation and many others. These deposits vary unrelated to obesity, which reinforces its direct segmentation for further quantification. However, manual segmentation of these fats has not been widely deployed in clinical practice due to the required human workload and consequential high cost of physicians and technicians. In this work, we propose a unified method for an autonomous segmentation and quantification of two types of cardiac fats. The segmented fats are termed epicardial and mediastinal, and stand apart from each other by the pericardium. Much effort was devoted to achieve minimal user intervention. The proposed methodology mainly comprises registration and classification algorithms to perform the desired segmentation. We compare the performance of several classification algorithms on this task, including neural networks, probabilistic models and decision tree algorithms. Experimental results of the proposed methodology have shown that the mean accuracy regarding both epicardial and mediastinal fats is 98.5% (99.5% if the features are normalized), with a mean true positive rate of 98.0%. In average, the Dice similarity index was equal to 97.6%.",2016,10.1016/j.cmpb.2015.09.017,cross-sectional,diagnosis,CT,Heart
"A Novel Block Imaging Technique Using Nine Artificial Intelligence Models for COVID-19 Disease Classification, Characterization and Severity Measurement in Lung Computed Tomography Scans on an Italian Cohort","Computer Tomography (CT) is currently being adapted for visualization of COVID-19 lung damage. Manual classification and characterization of COVID-19 may be biased depending on the expert's opinion. Artificial Intelligence has recently penetrated COVID-19, especially deep learning paradigms. There are nine kinds of classification systems in this study, namely one deep learning-based CNN, five kinds of transfer learning (TL) systems namely VGG16, DenseNet121, DenseNet169, DenseNet201 and MobileNet, three kinds of machine-learning (ML) systems, namely artificial neural network (ANN), decision tree (DT), and random forest (RF) that have been designed for classification of COVID-19 segmented CT lung against Controls. Three kinds of characterization systems were developed namely (a) Block imaging for COVID-19 severity index (CSI); (b) Bispectrum analysis; and (c) Block Entropy. A cohort of Italian patients with 30 controls (990 slices) and 30 COVID-19 patients (705 slices) was used to test the performance of three types of classifiers. Using K10 protocol (90% training and 10% testing), the best accuracy and AUC was for DCNN and RF pairs were 99.41 ± 5.12%, 0.991 (p < 0.0001), and 99.41 ± 0.62%, 0.988 (p < 0.0001), respectively, followed by other ML and TL classifiers. We show that diagnostics odds ratio (DOR) was higher for DL compared to ML, and both, Bispecturm and Block Entropy shows higher values for COVID-19 patients. CSI shows an association with Ground Glass Opacities (0.9146, p < 0.0001). Our hypothesis holds true that deep learning shows superior performance compared to machine learning models. Block imaging is a powerful novel approach for pinpointing COVID-19 severity and is clinically validated.",2021,10.1007/s10916-021-01707-w,cross-sectional,diagnosis,CT,Lung
A novel bone suppression method that improves lung nodule detection : Suppressing dedicated bone shadows in radiographs while preserving the remaining signal,"PURPOSE: Suppressing thoracic bone shadows in chest radiographs has been previously reported to improve the detection rates for solid lung nodules, however at the cost of increased false detection rates. These bone suppression methods are based on an artificial neural network that was trained using dual-energy subtraction images in order to mimic their appearance. METHOD: Here, a novel approach is followed where all bone shadows crossing the lung field are suppressed sequentially leaving the intercostal space unaffected. Given a contour delineating a bone, its image region is spatially transferred to separate normal image gradient components from tangential component. Smoothing the normal partial gradient along the contour results in a reconstruction of the image representing the bone shadow only, because all other overlaid signals tend to cancel out each other in this representation. RESULTS: The method works even with highly contrasted overlaid objects such as a pacemaker. The approach was validated in a reader study with two experienced chest radiologists, and these images helped improving both the sensitivity and the specificity of the readers for the detection and localization of solid lung nodules. The AUC improved significantly from 0.596 to 0.655 on a basis of 146 images from patients and normals with a total of 123 confirmed lung nodules. CONCLUSION: Subtracting all reconstructed bone shadows from the original image results in a soft image where lung nodules are no longer obscured by bone shadows. Both the sensitivity and the specificity of experienced radiologists increased.",2016,10.1007/s11548-015-1278-y,,,,
A Novel Computer-Aided Diagnosis Scheme on Small Annotated Set: G2C-CAD,"PURPOSE: Computer-aided diagnosis (CAD) can aid in improving diagnostic level; however, the main problem currently faced by CAD is that it cannot obtain sufficient labeled samples. To solve this problem, in this study, we adopt a generative adversarial network (GAN) approach and design a semisupervised learning algorithm, named G2C-CAD. METHODS: From the National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) dataset, we extracted four types of pulmonary nodule sign images closely related to lung cancer: noncentral calcification, lobulation, spiculation, and nonsolid/ground-glass opacity (GGO) texture, obtaining a total of 3,196 samples. In addition, we randomly selected 2,000 non-lesion image blocks as negative samples. We split the data 90% for training and 10% for testing. We designed a DCGAN generative adversarial framework and trained it on the small sample set. We also trained our designed CNN-based fuzzy Co-forest on the labeled small sample set and obtained a preliminary classifier. Then, coupled with the simulated unlabeled samples generated by the trained DCGAN, we conducted iterative semisupervised learning, which continually improved the classification performance of the fuzzy Co-forest until the termination condition was reached. Finally, we tested the fuzzy Co-forest and compared its performance with that of a C4.5 random decision forest and the G2C-CAD system without the fuzzy scheme, using ROC and confusion matrix for evaluation. RESULTS: Four different types of lung cancer-related signs were used in the classification experiment: noncentral calcification, lobulation, spiculation, and nonsolid/ground-glass opacity (GGO) texture, along with negative image samples. For these five classes, the G2C-CAD system obtained AUCs of 0.946, 0.912, 0.908, 0.887, and 0.939, respectively. The average accuracy of G2C-CAD exceeded that of the C4.5 random decision tree by 14%. G2C-CAD also obtained promising test results on the LISS signs dataset; its AUCs for GGO, lobulation, spiculation, pleural indentation, and negative image samples were 0.972, 0.964, 0.941, 0.967, and 0.953, respectively. CONCLUSION: The experimental results show that G2C-CAD is an appropriate method for addressing the problem of insufficient labeled samples in the medical image analysis field. Moreover, our system can be used to establish a training sample library for CAD classification diagnosis, which is important for future medical image analysis.",2019,10.1155/2019/6425963,cross-sectional,diagnosis,CT,Lung
A Novel COVID-19 Diagnosis Support System Using the Stacking Approach and Transfer Learning Technique on Chest X-Ray Images,"COVID-19 is an infectious disease-causing flu-like respiratory problem with various symptoms such as cough or fever, which in severe cases can cause pneumonia. The aim of this paper is to develop a rapid and accurate medical diagnosis support system to detect COVID-19 in chest X-ray images using a stacking approach combining transfer learning techniques and KNN algorithm for selection of the best model. In deep learning, we have multiple approaches for building a classification system for analyzing radiographic images. In this work, we used the transfer learning technique. This approach makes it possible to store and use the knowledge acquired from a pretrained convolutional neural network to solve a new problem. To ensure the robustness of the proposed system for diagnosing patients with COVID-19 using X-ray images, we used a machine learning method called the stacking approach to combine the performances of the many transfer learning-based models. The generated model was trained on a dataset containing four classes, namely, COVID-19, tuberculosis, viral pneumonia, and normal cases. The dataset used was collected from a six-source dataset of X-ray images. To evaluate the performance of the proposed system, we used different common evaluation measures. Our proposed system achieves an extremely good accuracy of 99.23% exceeding many previous related studies.",2021,10.1155/2021/9437538,cross-sectional,diagnosis,CT,Lung
A Novel CT-Based Radiomics Features Analysis for Identification and Severity Staging of COPD,"RATIONALE AND OBJECTIVES: To evaluate the role of radiomics based on Chest Computed Tomography (CT) in the identification and severity staging of chronic obstructive pulmonary disease (COPD). MATERIALS AND METHODS: This retrospective analysis included 322 participants (249 COPD patients and 73 control subjects). In total, 1395 chest CT-based radiomics features were extracted from each participant's CT images. Three feature selection methods, including variance threshold, Select K Best method, and least absolute shrinkage and selection operator (LASSO), and two classification methods, including support vector machine (SVM) and logistic regression (LR), were used as identification and severity classification of COPD. Performance was compared by AUC, accuracy, sensitivity, specificity, precision, and F1-score. RESULTS: 38 and 10 features were selected to construct radiomics models to detect and stage COPD, respectively. For COPD identification, SVM classifier achieved AUCs of 0.992 and 0.970, while LR classifier achieved AUCs of 0.993 and 0.972 in the training set and test set, respectively. For the severity staging of COPD, the mentioned two machine learning classifiers can better differentiate less severity (GOLD1 + GOLD2) group from greater severity (GOLD3 + GOLD4) group. The AUCs of SVM and LR is 0.907 and 0.903 in the training set, and that of 0.799 and 0.797 in the test set. CONCLUSION: The present study showed that the novel radiomics approach based on chest CT images that can be used for COPD identification and severity classification, and the constructed radiomics model demonstrated acceptable performance.",2022,10.1016/j.acra.2022.01.004,cross-sectional,combined,CT,Lung
A novel deep learning approach to extract Chinese clinical entities for lung cancer screening and staging,"BACKGROUND: Computed tomography (CT) reports record a large volume of valuable information about patients' conditions and the interpretations of radiology images from radiologists, which can be used for clinical decision-making and further academic study. However, the free-text nature of clinical reports is a critical barrier to use this data more effectively. In this study, we investigate a novel deep learning method to extract entities from Chinese CT reports for lung cancer screening and TNM staging. METHODS: The proposed approach presents a new named entity recognition algorithm, namely the BERT-based-BiLSTM-Transformer network (BERT-BTN) with pre-training, to extract clinical entities for lung cancer screening and staging. Specifically, instead of traditional word embedding methods, BERT is applied to learn the deep semantic representations of characters. Following the long short-term memory layer, a Transformer layer is added to capture the global dependencies between characters. Besides, pre-training technique is employed to alleviate the problem of insufficient labeled data. RESULTS: We verify the effectiveness of the proposed approach on a clinical dataset containing 359 CT reports collected from the Department of Thoracic Surgery II of Peking University Cancer Hospital. The experimental results show that the proposed approach achieves an 85.96% macro-F1 score under exact match scheme, which improves the performance by 1.38%, 1.84%, 3.81%,4.29%,5.12%,5.29% and 8.84% compared to BERT-BTN, BERT-LSTM, BERT-fine-tune, BERT-Transformer, FastText-BTN, FastText-BiLSTM and FastText-Transformer, respectively. CONCLUSIONS: In this study, we developed a novel deep learning method, i.e., BERT-BTN with pre-training, to extract the clinical entities from Chinese CT reports. The experimental results indicate that the proposed approach can efficiently recognize various clinical entities about lung cancer screening and staging, which shows the potential for further clinical decision-making and academic research.",2021,10.1186/s12911-021-01575-x,cross-sectional,combined,CT,Lung
A Novel Deep Learning Network and Its Application for Pulmonary Nodule Segmentation,"Pulmonary nodules are the early manifestation of lung cancer, which appear as circular shadow of no more than 3 cm on the computed tomography (CT) image. Accurate segmentation of the contours of pulmonary nodules can help doctors improve the efficiency of diagnosis. Deep learning has achieved great success in computer vision. In this study, we propose a novel network for pulmonary nodule segmentation from CT images based on U-NET. The proposed network has two merits: one is that it introduces dense connection to transfer and utilize features. Additionally, the problem of gradient disappearance can be avoided. The second is that it introduces a new loss function which is tolerance on the pixels near the borders of the nodule. Experimental results show that the proposed network at least achieves 1% improvement compared with other state-of-art networks in terms of different criteria.",2022,10.1155/2022/7124902,cross-sectional,diagnosis,CT,Lung
A novel deep learning-based quantification of serial chest computed tomography in Coronavirus Disease 2019 (COVID-19),"This study aims to explore and compare a novel deep learning-based quantification with the conventional semi-quantitative computed tomography (CT) scoring for the serial chest CT scans of COVID-19. 95 patients with confirmed COVID-19 and a total of 465 serial chest CT scans were involved, including 61 moderate patients (moderate group, 319 chest CT scans) and 34 severe patients (severe group, 146 chest CT scans). Conventional CT scoring and deep learning-based quantification were performed for all chest CT scans for two study goals: (1) Correlation between these two estimations; (2) Exploring the dynamic patterns using these two estimations between moderate and severe groups. The Spearman's correlation coefficient between these two estimation methods was 0.920 (p < 0.001). predicted pulmonary involvement (CT score and percent of pulmonary lesions calculated using deep learning-based quantification) increased more rapidly and reached a higher peak on 23rd days from symptom onset in severe group, which reached a peak on 18th days in moderate group with faster absorption of the lesions. The deep learning-based quantification for COVID-19 showed a good correlation with the conventional CT scoring and demonstrated a potential benefit in the estimation of disease severities of COVID-19.",2021,10.1038/s41598-020-80261-w,cross-sectional,prognosis,CT,Lung
A novel deep neuroevolution-based image classification method to diagnose coronavirus disease (COVID-19),"COVID-19 has had a detrimental impact on normal activities, public safety, and the global financial system. To identify the presence of this disease within communities and to commence the management of infected patients early, positive cases should be diagnosed as quickly as possible. New results from X-ray imaging indicate that images provide key information about COVID-19. Advanced deep-learning (DL) models can be applied to X-ray radiological images to accurately diagnose this disease and to mitigate the effects of a shortage of skilled medical personnel in rural areas. However, the performance of DL models strongly depends on the methodology used to design their architectures. Therefore, deep neuroevolution (DNE) techniques are introduced to automatically design DL architectures accurately. In this paper, a new paradigm is proposed for the automated diagnosis of COVID-19 from chest X-ray images using a novel two-stage improved DNE Algorithm. The proposed DNE framework is evaluated on a real-world dataset and the results demonstrate that it provides the highest classification performance in terms of different evaluation metrics.",2021,10.1016/j.compbiomed.2021.104994,cross-sectional,diagnosis,X-ray,Lung
A Novel Framework Based on Deep Learning and ANOVA Feature Selection Method for Diagnosis of COVID-19 Cases from Chest X-Ray Images,"Background and Objective. The new coronavirus disease (known as COVID-19) was first identified in Wuhan and quickly spread worldwide, wreaking havoc on the economy and people's everyday lives. As the number of COVID-19 cases is rapidly increasing, a reliable detection technique is needed to identify affected individuals and care for them in the early stages of COVID-19 and reduce the virus's transmission. The most accessible method for COVID-19 identification is Reverse Transcriptase-Polymerase Chain Reaction (RT-PCR); however, it is time-consuming and has false-negative results. These limitations encouraged us to propose a novel framework based on deep learning that can aid radiologists in diagnosing COVID-19 cases from chest X-ray images. Methods. In this paper, a pretrained network, DenseNet169, was employed to extract features from X-ray images. Features were chosen by a feature selection method, i.e., analysis of variance (ANOVA), to reduce computations and time complexity while overcoming the curse of dimensionality to improve accuracy. Finally, selected features were classified by the eXtreme Gradient Boosting (XGBoost). The ChestX-ray8 dataset was employed to train and evaluate the proposed method. Results and Conclusion. The proposed method reached 98.72% accuracy for two-class classification (COVID-19, No-findings) and 92% accuracy for multiclass classification (COVID-19, No-findings, and Pneumonia). The proposed method's precision, recall, and specificity rates on two-class classification were 99.21%, 93.33%, and 100%, respectively. Also, the proposed method achieved 94.07% precision, 88.46% recall, and 100% specificity for multiclass classification. The experimental results show that the proposed framework outperforms other methods and can be helpful for radiologists in the diagnosis of COVID-19 cases.",2022,10.1155/2022/4694567,cross-sectional,diagnosis,X-ray,Lung
A Novel Hybrid Feature Extraction Model for Classification on Pulmonary Nodules,"In this paper an improved Computer Aided Design system can offer a second opinion to radiologists on early diagnosis of pulmonary nodules on CT (Computer Tomography) images. A Deep Convolutional Neural Network (DCNN) method is used for feature extraction and hybridize as combination of Convolutional Neural Network (CNN), Histogram of Oriented Gradient (HOG), Extended Histogram of Oriented Gradients (ExHOG) and Local Binary Pattern (LBP). A combination of shape, texture, scaling, rotation, translation features extracted using HOG, LBP and CNN. The Homogeneous descriptors used to extract the feature of lung images from Lung Image Database Consortium (LIDC) are given to classifiers Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Decision Tree and Random Forest to classify nodules and non-nodules. Experimental results demonstrate the effectiveness of the proposed method in terms of accuracy which gives best result than the competing methods.",2019,10.31557/apjcp.2019.20.2.457,cross-sectional,diagnosis,CT,Lung
A Novel Machine Learning-derived Radiomic Signature of the Whole Lung Differentiates Stable From Progressive COVID-19 Infection: A Retrospective Cohort Study,"OBJECTIVE: This study aimed to use the radiomics signatures of a machine learning-based tool to evaluate the prognosis of patients with coronavirus disease 2019 (COVID-19) infection. METHODS: The clinical and imaging data of 64 patients with confirmed diagnoses of COVID-19 were retrospectively selected and divided into a stable group and a progressive group according to the data obtained from the ongoing treatment process. Imaging features from whole-lung images from baseline computed tomography (CT) scans were extracted and dimensionality reduction was performed. Support vector machines were used to construct radiomics signatures and to compare differences between the 2 groups. We also compared the differences of signature scores in the clinical, laboratory, and CT image feature subgroups and finally analyzed the correlation between the radiomics features of the constructed signature and the other features including clinical, laboratory, and CT imaging features. RESULTS: The signature has a good classification effect for the stable group and the progressive group, with area under curve, sensitivity, and specificity of 0.833, 80.95%, and 74.42%, respectively. Signature score differences in laboratory and CT imaging features between subgroups were not statistically significant (P>0.05); cough was negatively correlated with GLCM Entropy_angle 90_offset4 (r=-0.578), but was positively correlated with ShortRunEmphhasis_AllDirect_offset4_SD (r=0.454); C-reactive protein was positively correlated with Cluster Prominence_ AllDirect_offset 4_ SD (r=0.47). CONCLUSION: The radiomics signature of the whole lung based on machine learning may reveal the changes of lung microstructure in the early stage and help to indicate the progression of the disease.",2020,10.1097/rti.0000000000000544,cross-sectional,prognosis,CT,Lung
A Novel Multicolor-thresholding Auto-detection Method to Detect the Location and Severity of Inflammation in Confirmed SARS-COV-2 Cases using Chest X-Ray Images,"OBJECTIVES: Since late 2019, Coronavirus Disease 2019 (COVID-19) has spread around the world. It has been determined that the disease is very contagious and can cause Acute Respiratory Distress (ARD). Medical imaging has the potential to help identify, detect, and quantify the severity of this infection. This work seeks to develop a novel auto-detection technique for verified COVID-19 cases that can detect aberrant alterations in traditional X-ray pictures. METHODS: Nineteen separately colored layers were created from X-ray scans of patients diagnosed with COVID-19. Each layer represents objects that have a similar contrast and can be represented by a single color. In a single layer, objects with similar contrasts are formed. A single color image was created by extracting all the objects from all the layers. The prototype model could recognize a wide range of abnormal changes in the image texture based on color differentiation. This was true even when the contrast values of the detected unclear abnormalities varied slightly. RESULTS: The results indicate that the proposed novel method is 91% accurate in detecting and grading COVID-19 lung infections compared to the opinions of three experienced radiologists evaluating chest X-ray images. Additionally, the method can be used to determine the infection site and severity of the disease by categorizing X-rays into five severity levels. CONCLUSION: By comparing affected tissue to healthy tissue, the proposed COVID-19 auto-detection method can identify locations and indicate the severity of the disease, as well as predict where the disease may spread.",2022,10.2174/1573405617666210910150119,cross-sectional,diagnosis,X-ray,Lung
A novel multiple instance learning framework for COVID-19 severity assessment via data augmentation and self-supervised learning,"How to fast and accurately assess the severity level of COVID-19 is an essential problem, when millions of people are suffering from the pandemic around the world. Currently, the chest CT is regarded as a popular and informative imaging tool for COVID-19 diagnosis. However, we observe that there are two issues - weak annotation and insufficient data that may obstruct automatic COVID-19 severity assessment with CT images. To address these challenges, we propose a novel three-component method, i.e., 1) a deep multiple instance learning component with instance-level attention to jointly classify the bag and also weigh the instances, 2) a bag-level data augmentation component to generate virtual bags by reorganizing high confidential instances, and 3) a self-supervised pretext component to aid the learning process. We have systematically evaluated our method on the CT images of 229 COVID-19 cases, including 50 severe and 179 non-severe cases. Our method could obtain an average accuracy of 95.8%, with 93.6% sensitivity and 96.4% specificity, which outperformed previous works.",2021,10.1016/j.media.2021.101978,cross-sectional,diagnosis,CT,Lung
A novel technology to integrate imaging and clinical markers for non-invasive diagnosis of lung cancer,"This study presents a non-invasive, automated, clinical diagnostic system for early diagnosis of lung cancer that integrates imaging data from a single computed tomography scan and breath bio-markers obtained from a single exhaled breath to quickly and accurately classify lung nodules. CT imaging and breath volatile organic compounds data were collected from 47 patients. Spherical Harmonics-based shape features to quantify the shape complexity of the pulmonary nodules, 7th-Order Markov Gibbs Random Field based appearance model to describe the spatial non-homogeneities in the pulmonary nodule, and volumetric features (size) of pulmonary nodules were calculated from CT images. 27 VOCs in exhaled breath were captured by a micro-reactor approach and quantied using mass spectrometry. CT and breath markers were input into a deep-learning autoencoder classifier with a leave-one-subject-out cross validation for nodule classification. To mitigate the limitation of a small sample size and validate the methodology for individual markers, retrospective CT scans from 467 patients with 727 pulmonary nodules, and breath samples from 504 patients were analyzed. The CAD system achieved 97.8% accuracy, 97.3% sensitivity, 100% specificity, and 99.1% area under curve in classifying pulmonary nodules.",2021,10.1038/s41598-021-83907-5,cross-sectional,diagnosis,CT,Lung
A PET Radiomics Model to Predict Refractory Mediastinal Hodgkin Lymphoma,"First-order radiomic features, such as metabolic tumor volume (MTV) and total lesion glycolysis (TLG), are associated with disease progression in early-stage classical Hodgkin lymphoma (HL). We hypothesized that a model incorporating first- and second-order radiomic features would more accurately predict outcome than MTV or TLG alone. We assessed whether radiomic features extracted from baseline PET scans predicted relapsed or refractory disease status in a cohort of 251 patients with stage I-II HL who were managed at a tertiary cancer center. Models were developed and tested using a machine-learning algorithm. Features extracted from mediastinal sites were highly predictive of primary refractory disease. A model incorporating 5 of the most predictive features had an area under the curve (AUC) of 95.2% and total error rate of 1.8%. By comparison, the AUC was 78% for both MTV and TLG and was 65% for maximum standardize uptake value (SUV(max)). Furthermore, among the patients with refractory mediastinal disease, our model distinguished those who were successfully salvaged from those who ultimately died of HL. We conclude that our PET radiomic model may improve upfront stratification of early-stage HL patients with mediastinal disease and thus contribute to risk-adapted, individualized management.",2019,10.1038/s41598-018-37197-z,cross-sectional,diagnosis,PET,Lung
A physics-guided modular deep-learning based automated framework for tumor segmentation in PET,"An important need exists for reliable positron emission tomography (PET) tumor-segmentation methods for tasks such as PET-based radiation-therapy planning and reliable quantification of volumetric and radiomic features. To address this need, we propose an automated physics-guided deep-learning-based three-module framework to segment PET images on a per-slice basis. The framework is designed to help address the challenges of limited spatial resolution and lack of clinical training data with known ground-truth tumor boundaries in PET. The first module generates PET images containing highly realistic tumors with known ground-truth using a new stochastic and physics-based approach, addressing lack of training data. The second module trains a modified U-net using these images, helping it learn the tumor-segmentation task. The third module fine-tunes this network using a small-sized clinical dataset with radiologist-defined delineations as surrogate ground-truth, helping the framework learn features potentially missed in simulated tumors. The framework was evaluated in the context of segmenting primary tumors in (18)F-fluorodeoxyglucose (FDG)-PET images of patients with lung cancer. The framework's accuracy, generalizability to different scanners, sensitivity to partial volume effects (PVEs) and efficacy in reducing the number of training images were quantitatively evaluated using Dice similarity coefficient (DSC) and several other metrics. The framework yielded reliable performance in both simulated (DSC: 0.87 (95% confidence interval (CI): 0.86, 0.88)) and patient images (DSC: 0.73 (95% CI: 0.71, 0.76)), outperformed several widely used semi-automated approaches, accurately segmented relatively small tumors (smallest segmented cross-section was 1.83 cm(2)), generalized across five PET scanners (DSC: 0.74 (95% CI: 0.71, 0.76)), was relatively unaffected by PVEs, and required low training data (training with data from even 30 patients yielded DSC of 0.70 (95% CI: 0.68, 0.71)). In conclusion, the proposed automated physics-guided deep-learning-based PET-segmentation framework yielded reliable performance in delineating tumors in FDG-PET images of patients with lung cancer.",2020,10.1088/1361-6560/ab8535,cross-sectional,treatment,PET,Lung
A pilot study using kernelled support tensor machine for distant failure prediction in lung SBRT,"We developed a kernelled support tensor machine (KSTM)-based model with tumor tensors derived from pre-treatment PET and CT imaging as input to predict distant failure in early stage non-small cell lung cancer (NSCLC) treated with stereotactic body radiation therapy (SBRT). The patient cohort included 110 early stage NSCLC patients treated with SBRT, 25 of whom experienced failure at distant sites. Three-dimensional tumor tensors were constructed and used as input for the KSTM-based classifier. A KSTM iterative algorithm with a convergent proof was developed to train the weight vectors for every mode of the tensor for the classifier. In contrast to conventional radiomics approaches that rely on handcrafted imaging features, the KSTM-based classifier uses 3D imaging as input, taking full advantage of the imaging information. The KSTM-based classifier preserves the intrinsic 3D geometry structure of the medical images and the correlation in the original images and trains the classification hyper-plane in an adaptive feature tensor space. The KSTM-based predictive algorithm was compared with three conventional machine learning models and three radiomics approaches. For PET and CT, the KSTM-based predictive method achieved the highest prediction results among the seven methods investigated in this study based on 10-fold cross validation and independent testing.",2018,10.1016/j.media.2018.09.004,cross-sectional,treatment,PET/CT,Lung
A pilot study: Quantify lung volume and emphysema extent directly from two-dimensional scout images,"PURPOSE: The potential to compute volume metrics of emphysema from planar scout images was investigated in this study. The successful implementation of this concept will have a wide impact in different fields, and specifically, maximize the diagnostic potential of the planar medical images. METHODS: We investigated our premise using a well-characterized chronic obstructive pulmonary disease (COPD) cohort. In this cohort, planar scout images from computed tomography (CT) scans were used to compute lung volume and percentage of emphysema. Lung volume and percentage of emphysema were quantified on the volumetric CT images and used as the ""ground truth"" for developing the models to compute the variables from the corresponding scout images. We trained two classical convolutional neural networks (CNNs), including VGG19 and InceptionV3, to compute lung volume and the percentage of emphysema from the scout images. The scout images (n = 1,446) were split into three subgroups: (1) training (n = 1,235), (2) internal validation (n = 99), and (3) independent test (n = 112) at the subject level in a ratio of 8:1:1. The mean absolute difference (MAD) and R-square (R2) were the performance metrics to evaluate the prediction performance of the developed models. RESULTS: The lung volumes and percentages of emphysema computed from a single planar scout image were significantly linear correlated with the measures quantified using volumetric CT images (VGG19: R2 = 0.934 for lung volume and R2 = 0.751 for emphysema percentage, and InceptionV3: R2 = 0.977 for lung volume and R2 = 0.775 for emphysema percentage). The mean absolute differences (MADs) for lung volume and percentage of emphysema were 0.302 ± 0.247L and 2.89 ± 2.58%, respectively, for VGG19, and 0.366 ± 0.287L and 3.19 ± 2.14, respectively, for InceptionV3. CONCLUSIONS: Our promising results demonstrated the feasibility of inferring volume metrics from planar images using CNNs.",2021,10.1002/mp.15019,cross-sectional,diagnosis,CT,Lung
A prediction model based on DNA methylation biomarkers and radiological characteristics for identifying malignant from benign pulmonary nodules,"BACKGROUND: Lung cancer remains the leading cause of cancer deaths across the world. Early detection of lung cancer by low-dose computed tomography (LDCT) can reduce the mortality rate. However, making a definitive preoperative diagnosis of malignant pulmonary nodules (PNs) found by LDCT is a clinical challenge. This study aimed to develop a prediction model based on DNA methylation biomarkers and radiological characteristics for identifying malignant pulmonary nodules from benign PNs. METHODS: We assessed three DNA methylation biomarkers (PTGER4, RASSF1A, and SHOX2) and clinically-relevant variables in a training cohort of 110 individuals with PNs. Four machine-learning-based prediction models were established and compared, including the K-nearest neighbors (KNN), random forest (RF), support vector machine (SVM), and logistic regression (LR) algorithms. Variables of the best-performing algorithm (LR) were selected through stepwise use of Akaike's information criterion (AIC). The constructed prediction model was compared with the methylation biomarkers and the Mayo Clinic model using the non-parametric approach of DeLong et al. with the area under a receiver operator characteristic curve (AUC) analysis. RESULTS: A prediction model was finally constructed based on three DNA methylation biomarkers and one radiological characteristic for identifying malignant from benign PNs. The developed prediction model achieved an AUC value of 0.951 in malignant PNs diagnosis, significantly higher than the three DNA methylation biomarkers (0.912, 95% CI:0.843-0.958, p = 0.013) or Mayo Clinic model (0.823, 95% CI:0.739-0.890, p = 0.001). Validation of the prediction model in the testing cohort of 100 subjects with PNs confirmed the diagnostic value. CONCLUSION: We have shown that integrating DNA methylation biomarkers and radiological characteristics could more accurately identify lung cancer in subjects with CT-found PNs. The prediction model developed in our study may provide clinical utility in combination with LDCT to improve the over-all diagnosis of lung cancer.",2021,10.1186/s12885-021-08002-4,,,,
A prediction model of outcome of SARS-CoV-2 pneumonia based on laboratory findings,"The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has resulted in thousands of deaths in the world. Information about prediction model of prognosis of SARS-CoV-2 infection is scarce. We used machine learning for processing laboratory findings of 110 patients with SARS-CoV-2 pneumonia (including 51 non-survivors and 59 discharged patients). The maximum relevance minimum redundancy (mRMR) algorithm and the least absolute shrinkage and selection operator logistic regression model were used for selection of laboratory features. Seven laboratory features selected in the model were: prothrombin activity, urea, white blood cell, interleukin-2 receptor, indirect bilirubin, myoglobin, and fibrinogen degradation products. The signature constructed using the seven features had 98% [93%, 100%] sensitivity and 91% [84%, 99%] specificity in predicting outcome of SARS-CoV-2 pneumonia. Thus it is feasible to establish an accurate prediction model of outcome of SARS-CoV-2 pneumonia based on laboratory findings.",2020,10.1038/s41598-020-71114-7,,,,
A preliminary study of a photon dose calculation algorithm using a convolutional neural network,"The aim of dose calculation algorithm research is to improve the calculation accuracy while maximizing the calculation efficiency. In this study, the three-dimensional distribution of total energy release per unit mass (TERMA) and the electron density (ED) distribution are considered inputs in a method for calculating the three-dimensional dose distribution based on a convolutional neural network (CNN). Attempts are made to improve the efficiency of the collapsed cone convolution/superposition (CCCS) algorithm while providing an approach to improve the efficiency of other traditional dose calculation algorithms. Twelve sets of computed tomography (CT) images were employed for training. Data sets were generated by the CCCS algorithm with a random beam configuration. For each monoenergetic photon model, 7500 samples were generated for the training set, and 1500 samples were generated for the validation set. Training occurred for 0.5 MeV, 1 MeV, 2 MeV, 3 MeV, 4 MeV, 5 MeV, and 6 MeV monoenergetic photon models. To evaluate the usability under linac conditions, a comparison between CCCS and CNN-Dose was performed for the Mohan 6-MV spectrum for 12 additional new sets of CT images with different anatomies. A total of 1512 test samples were generated. For all anatomies, the mean value, 95% lower confidence limit (LCL) and 95% upper confidence limit (UCL) were 99.56%, 99.51% and 99.61%, respectively, at the 3%/2 mm criteria. The mean value, 95% LCL and 95% UCL were 98.57%, 98.46% and 98.67%, respectively, at the 2%/2 mm criteria. The results meet the relevant clinical requirements. In the proposed methods, the dose distribution of clinical energy can be obtained by TERMA, and the electronic density can be obtained with a CNN. This method can also be used for other traditional dose algorithms and displays potential in treatment planning, adaptive radiation therapy, and in vivo verification.",2020,10.1088/1361-6560/abb1d7,cross-sectional,treatment,CT,Lung
A prognostic analysis method for non-small cell lung cancer based on the computed tomography radiomics,"In order to assist doctors in arranging the postoperative treatments and re-examinations for non-small cell lung cancer (NSCLC) patients, this study was initiated to explore a prognostic analysis method for NSCLC based on computed tomography (CT) radiomics. The data of 173 NSCLC patients were collected retrospectively and the clinically meaningful 3-year survival was used as the predictive limit to predict the patient's prognosis survival time range. Firstly, lung tumors were segmented and the radiomics features were extracted. Secondly, the feature weighting algorithm was used to screen and optimize the extracted original feature data. Then, the selected feature data combining with the prognosis survival of patients were used to train machine learning classification models. Finally, a prognostic survival prediction model and radiomics prognostic factors were obtained to predict the prognosis survival time range of NSCLC patients. The classification accuracy rate under cross-validation was up to 88.7% in the prognosis survival analysis model. When verifying on an independent data set, the model also yielded a high prediction accuracy which is up to 79.6%. Inverse different moment, lobulation sign and angular second moment were NSCLC prognostic factors based on radiomics. This study proved that CT radiomics features could effectively assist doctors to make more accurate prognosis survival prediction for NSCLC patients, so as to help doctors to optimize treatment and re-examination for NSCLC patients to extend their survival time.",2020,10.1088/1361-6560/ab6e51,cross-sectional,prognosis,CT,Lung
A promising approach for screening pulmonary hypertension based on frontal chest radiographs using deep learning: A retrospective study,"BACKGROUND: To date, the missed diagnosis rate of pulmonary hypertension (PH) was high, and there has been limited development of a rapid, simple, and effective way to screen the disease. The purpose of this study is to develop a deep learning approach to achieve rapid detection of possible abnormalities in chest radiographs suggesting PH for screening patients suspected of PH. METHODS: We retrospectively collected frontal chest radiographs and the pulmonary artery systolic pressure (PASP) value measured by Doppler transthoracic echocardiography from 762 patients (357 healthy controls and 405 with PH) from three institutes in China from January 2013 to May 2019. The wohle sample comprised 762 images (641 for training, 80 for internal test, and 41 for external test). We firstly performed a 8-fold cross-validation on the 641 images selected for training (561 for pre-training, 80 for validation), then decided to tune learning rate to 0.0008 according to the best score on validation data. Finally, we used all the pre-training and validation data (561+80 = 641) to train our models (Resnet50, Xception, and Inception V3), evaluated them on internal and external test dataset to classify the images as having manifestations of PH or healthy according to the area under the receiver operating characteristic curve (AUC/ROC). After that, the three deep learning models were further used for prediction of PASP using regression algorithm. Moreover, we invited an experienced chest radiologist to classify the images in the test dataset as having PH or not, and compared the prediction accuracy performed by deep learing models with that of manual classification. RESULTS: The AUC performed by the best model (Inception V3) achieved 0.970 in the internal test, and slightly declined in the external test (0.967) when using deep learning algorithms to classify PH from normal based on chest X-rays. The mean absolute error (MAE) of the best model for prediction of PASP value was smaller in the internal test (7.45) compared to 9.95 in the external test. Manual classification of PH based on chest X-rays showed much lower AUCs compared to that performed by deep learning models both in the internal and external test. CONCLUSIONS: The present study used deep learning algorithms to classify abnormalities suggesting PH in chest radiographs with high accuracy and good generalizability. Once tested prospectively in clinical settings, the technology could provide a non-invasive and easy-to-use method to screen patients suspected of having PH.",2020,10.1371/journal.pone.0236378,cross-sectional,diagnosis,X-ray,Lung vessels
A quantitative imaging biomarker for predicting disease-free-survival-associated histologic subgroups in lung adenocarcinoma,"OBJECTIVES: Classification of histologic subgroups has significant prognostic value for lung adenocarcinoma patients who undergo surgical resection. However, clinical histopathology assessment is generally performed on only a small portion of the overall tumor from biopsy or surgery. Our objective is to identify a noninvasive quantitative imaging biomarker (QIB) for the classification of histologic subgroups in lung adenocarcinoma patients. METHODS: We retrospectively collected and reviewed 1313 CT scans of patients with resected lung adenocarcinomas from two geographically distant institutions who were seen between January 2014 and October 2017. Three study cohorts, the training, internal validation, and external validation cohorts, were created, within which lung adenocarcinomas were divided into two disease-free-survival (DFS)-associated histologic subgroups, the mid/poor and good DFS groups. A comprehensive machine learning- and deep learning-based analytical system was adopted to identify reproducible QIBs and help to understand QIBs' significance. RESULTS: Intensity-Skewness, a QIB quantifying tumor density distribution, was identified as the optimal biomarker for predicting histologic subgroups. Intensity-Skewness achieved high AUCs (95% CI) of 0.849(0.813,0.881), 0.820(0.781,0.856) and 0.863(0.827,0.895) on the training, internal validation, and external validation cohorts, respectively. A criterion of Intensity-Skewness ≤ 1.5, which indicated high tumor density, showed high specificity of 96% (sensitivity 46%) and 99% (sensitivity 53%) on predicting the mid/poor DFS group in the training and external validation cohorts, respectively. CONCLUSIONS: A QIB derived from routinely acquired CT was able to predict lung adenocarcinoma histologic subgroups, providing a noninvasive method that could potentially benefit personalized treatment decision-making for lung cancer patients. KEY POINTS: • A noninvasive imaging biomarker, Intensity-Skewness, which described the distortion of pixel-intensity distribution within lesions on CT images, was identified as a biomarker to predict disease-free-survival-associated histologic subgroups in lung adenocarcinoma. • An Intensity-Skewness of ≤ 1.5 has high specificity in predicting the mid/poor disease-free survival histologic patient group in both the training cohort and the external validation cohort. • The Intensity-Skewness is a feature that can be automatically computed with high reproducibility and robustness.",2020,10.1007/s00330-020-06663-6,cross-sectional,prognosis,CT,Lung
A Radiogenomics Ensemble to Predict EGFR and KRAS Mutations in NSCLC,"Lung cancer causes more deaths globally than any other type of cancer. To determine the best treatment, detecting EGFR and KRAS mutations is of interest. However, non-invasive ways to obtain this information are not available. Furthermore, many times there is a lack of big enough relevant public datasets, so the performance of single classifiers is not outstanding. In this paper, an ensemble approach is applied to increase the performance of EGFR and KRAS mutation prediction using a small dataset. A new voting scheme, Selective Class Average Voting (SCAV), is proposed and its performance is assessed both for machine learning models and CNNs. For the EGFR mutation, in the machine learning approach, there was an increase in the sensitivity from 0.66 to 0.75, and an increase in AUC from 0.68 to 0.70. With the deep learning approach, an AUC of 0.846 was obtained, and with SCAV, the accuracy of the model was increased from 0.80 to 0.857. For the KRAS mutation, both in the machine learning models (0.65 to 0.71 AUC) and the deep learning models (0.739 to 0.778 AUC), a significant increase in performance was found. The results obtained in this work show how to effectively learn from small image datasets to predict EGFR and KRAS mutations, and that using ensembles with SCAV increases the performance of machine learning classifiers and CNNs. The results provide confidence that as large datasets become available, tools to augment clinical capabilities can be fielded.",2021,10.3390/tomography7020014,cross-sectional,diagnosis,CT,Lung
A radiomics approach for lung nodule detection in thoracic CT images based on the dynamic patterns of morphological variation,"OBJECTIVES: To propose and evaluate a set of radiomic features, called morphological dynamics features, for pulmonary nodule detection, which were rooted in the dynamic patterns of morphological variation and needless precise lesion segmentation. MATERIALS AND METHODS: Two datasets were involved, namely, university hospital (UH) and LIDC datasets, comprising 72 CT scans (360 nodules) and 888 CT scans (2230 nodules), respectively. Each nodule was annotated by multiple radiologists. Denoted the category of nodules identified by at least k radiologists as ALk. A nodule detection algorithm, called CAD-MD algorithm, was proposed based on the morphological dynamics radiomic features, characterizing a lesion by ten sets of the same features with different values extracted from ten different thresholding results. Each nodule candidate was classified by a two-level classifier, including ten decision trees and a random forest, respectively. The CAD-MD algorithm was compared with a deep learning approach, the N-Net, using the UH dataset. RESULTS: On the AL1 and AL2 of the UH dataset, the AUC of the AFROC curves were 0.777 and 0.851 for the CAD-MD algorithm and 0.478 and 0.472 for the N-Net, respectively. The CAD-MD algorithm achieved the sensitivities of 84.4% and 91.4% with 2.98 and 3.69 FPs/scan and the N-Net 74.4% and 80.7% with 3.90 and 4.49 FPs/scan, respectively. On the LIDC dataset, the CAD-MD algorithm attained the sensitivities of 87.6%, 89.2%, 92.2%, and 95.0% with 4 FPs/scan for AL1-AL4, respectively. CONCLUSION: The morphological dynamics radiomic features might serve as an effective set of radiomic features for lung nodule detection. KEY POINTS: • Texture features varied with such CT system settings as reconstruction kernels of CT images, CT scanner models, and parameter settings, and so on. • Shape and first-order statistics were shown to be the most robust features against variation in CT imaging parameters. • The morphological dynamics radiomic features, which mainly characterized the dynamic patterns of morphological variation, were shown to be effective for lung nodule detection.",2022,10.1007/s00330-021-08456-x,cross-sectional,diagnosis,CT,Lung
A radiomics evaluation of 2D and 3D MRI texture features to classify brain metastases from lung cancer and melanoma,"Brain metastases are occasionally detected before diagnosing their primary site of origin. In these cases, simple visual examination of medical images of the metastases is not enough to identify the primary cancer, so an extensive evaluation is needed. To avoid this procedure, a radiomics approach on magnetic resonance (MR) images of the metastatic lesions is proposed to classify two of the most frequent origins (lung cancer and melanoma). In this study, 50 T1-weighted MR images of brain metastases from 30 patients were analyzed: 27 of lung cancer and 23 of melanoma origin. A total of 43 statistical texture features were extracted from the segmented lesions in 2D and 3D. Five predictive models were evaluated using a nested cross-validation scheme. The best classification results were achieved using 3D texture features for all the models, obtaining an average AUC > 0.9 in all cases and an AUC = 0.947 ± 0.067 when using the best model (naïve Bayes).",2017,10.1109/embc.2017.8036869,,,,
A radiomics study to predict invasive pulmonary adenocarcinoma appearing as pure ground-glass nodules,"AIM: To establish a machine-learning model to differentiate adenocarcinoma in situ (AIS) or minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC) appearing as pure ground-glass nodules (pGGNs). MATERIALS AND METHODS: This retrospective study enrolled 136 patients with histopathologically diagnosed with AIS, MIA, and IAC. All pGGNs were divided randomly into a training and a testing dataset at a ratio of 7 : 3. Radiomics features were extracted based on the unenhanced computed tomography (CT) images derived from the last preoperative CT examination of each patient. The F-test and least absolute shrinkage and selection operator (LASSO) logistic regression were applied to select the most valuable features to establish a support vector machine (SVM) model. The performance of the model was evaluated using the area under the receiver operating characteristic curve (AUROC), and the accuracy, sensitivity, and specificity were calculated to compare the diagnostic performance of radiologists and the SVM model. RESULTS: Six significant radiomics features were selected to develop the SVM model and showed excellent ability to differentiate AIS/MIA from IAC in both the training dataset (AUROC=0.950, 95% confidence interval [CI]: 0.886-0.984) and the testing dataset (AUROC=0.945, 95% CI: 0.826-0.992). Compared with two radiologists, the proposed model possessed significant advantages with higher accuracy (90.24% versus 75.61% and 80.49%), sensitivity (91.67% versus 50% and 75%), and specificity (89.66% versus 86.21% and 82.76%). CONCLUSION: A machine-learning model based on radiomics features exhibits superior diagnostic performance in differentiating AIS/MIA from IAC appearing as pGGNs.",2021,10.1016/j.crad.2020.10.005,cross-sectional,diagnosis,CT,Lung
A radiomics-boosted deep-learning model for COVID-19 and non-COVID-19 pneumonia classification using chest x-ray images,"PURPOSE: To develop a deep learning model design that integrates radiomics analysis for enhanced performance of COVID-19 and non-COVID-19 pneumonia detection using chest x-ray images. METHODS: As a novel radiomics approach, a 2D sliding kernel was implemented to map the impulse response of radiomic features throughout the entire chest x-ray image; thus, each feature is rendered as a 2D map in the same dimension as the x-ray image. Based on each of the three investigated deep neural network architectures, including VGG-16, VGG-19, and DenseNet-121, a pilot model was trained using x-ray images only. Subsequently, two radiomic feature maps (RFMs) were selected based on cross-correlation analysis in reference to the pilot model saliency map results. The radiomics-boosted model was then trained based on the same deep neural network architecture using x-ray images plus the selected RFMs as input. The proposed radiomics-boosted design was developed using 812 chest x-ray images with 262/288/262 COVID-19/non-COVID-19 pneumonia/healthy cases, and 649/163 cases were assigned as training-validation/independent test sets. For each model, 50 runs were trained with random assignments of training/validation cases following the 7:1 ratio in the training-validation set. Sensitivity, specificity, accuracy, and ROC curves together with area-under-the-curve (AUC) from all three deep neural network architectures were evaluated. RESULTS: After radiomics-boosted implementation, all three investigated deep neural network architectures demonstrated improved sensitivity, specificity, accuracy, and ROC AUC results in COVID-19 and healthy individual classifications. VGG-16 showed the largest improvement in COVID-19 classification ROC (AUC from 0.963 to 0.993), and DenseNet-121 showed the largest improvement in healthy individual classification ROC (AUC from 0.962 to 0.989). The reduced variations suggested improved robustness of the model to data partition. For the challenging non-COVID-19 pneumonia classification task, radiomics-boosted implementation of VGG-16 (AUC from 0.918 to 0.969) and VGG-19 (AUC from 0.964 to 0.970) improved ROC results, while DenseNet-121 showed a slight yet insignificant ROC performance reduction (AUC from 0.963 to 0.949). The achieved highest accuracy of COVID-19/non-COVID-19 pneumonia/healthy individual classifications were 0.973 (VGG-19)/0.936 (VGG-19)/ 0.933 (VGG-16), respectively. CONCLUSIONS: The inclusion of radiomic analysis in deep learning model design improved the performance and robustness of COVID-19/non-COVID-19 pneumonia/healthy individual classification, which holds great potential for clinical applications in the COVID-19 pandemic.",2022,10.1002/mp.15582,cross-sectional,diagnosis,X-ray,Lung
A Rapid Artificial Intelligence-Based Computer-Aided Diagnosis System for COVID-19 Classification from CT Images,"The excessive number of COVID-19 cases reported worldwide so far, supplemented by a high rate of false alarms in its diagnosis using the conventional polymerase chain reaction method, has led to an increased number of high-resolution computed tomography (CT) examinations conducted. The manual inspection of the latter, besides being slow, is susceptible to human errors, especially because of an uncanny resemblance between the CT scans of COVID-19 and those of pneumonia, and therefore demands a proportional increase in the number of expert radiologists. Artificial intelligence-based computer-aided diagnosis of COVID-19 using the CT scans has been recently coined, which has proven its effectiveness in terms of accuracy and computation time. In this work, a similar framework for classification of COVID-19 using CT scans is proposed. The proposed method includes four core steps: (i) preparing a database of three different classes such as COVID-19, pneumonia, and normal; (ii) modifying three pretrained deep learning models such as VGG16, ResNet50, and ResNet101 for the classification of COVID-19-positive scans; (iii) proposing an activation function and improving the firefly algorithm for feature selection; and (iv) fusing optimal selected features using descending order serial approach and classifying using multiclass supervised learning algorithms. We demonstrate that once this method is performed on a publicly available dataset, this system attains an improved accuracy of 97.9% and the computational time is almost 34 (sec).",2021,10.1155/2021/2560388,cross-sectional,diagnosis,CT,Lung
A rapid screening classifier for diagnosing COVID-19,"Rationale: Coronavirus disease 2019 (COVID-19) has caused a global pandemic. A classifier combining chest X-ray (CXR) with clinical features may serve as a rapid screening approach. Methods: The study included 512 patients with COVID-19 and 106 with influenza A/B pneumonia. A deep neural network (DNN) was applied, and deep features derived from CXR and clinical findings formed fused features for diagnosis prediction. Results: The clinical features of COVID-19 and influenza showed different patterns. Patients with COVID-19 experienced less fever, more diarrhea, and more salient hypercoagulability. Classifiers constructed using the clinical features or CXR had an area under the receiver operating curve (AUC) of 0.909 and 0.919, respectively. The diagnostic efficacy of the classifier combining the clinical features and CXR was dramatically improved and the AUC was 0.952 with 91.5% sensitivity and 81.2% specificity. Moreover, combined classifier was functional in both severe and non-serve COVID-19, with an AUC of 0.971 with 96.9% sensitivity in non-severe cases, which was on par with the computed tomography (CT)-based classifier, but had relatively inferior efficacy in severe cases compared to CT. In extension, we performed a reader study involving three experienced pulmonary physicians, artificial intelligence (AI) system demonstrated superiority in turn-around time and diagnostic accuracy compared with experienced pulmonary physicians. Conclusions: The classifier constructed using clinical and CXR features is efficient, economical, and radiation safe for distinguishing COVID-19 from influenza A/B pneumonia, serving as an ideal rapid screening tool during the COVID-19 pandemic.",2021,10.7150/ijbs.53982,cross-sectional,diagnosis,X-ray,Lung
"A Rapid, Accurate and Machine-Agnostic Segmentation and Quantification Method for CT-Based COVID-19 Diagnosis","COVID-19 has caused a global pandemic and become the most urgent threat to the entire world. Tremendous efforts and resources have been invested in developing diagnosis, prognosis and treatment strategies to combat the disease. Although nucleic acid detection has been mainly used as the gold standard to confirm this RNA virus-based disease, it has been shown that such a strategy has a high false negative rate, especially for patients in the early stage, and thus CT imaging has been applied as a major diagnostic modality in confirming positive COVID-19. Despite the various, urgent advances in developing artificial intelligence (AI)-based computer-aided systems for CT-based COVID-19 diagnosis, most of the existing methods can only perform classification, whereas the state-of-the-art segmentation method requires a high level of human intervention. In this paper, we propose a fully-automatic, rapid, accurate, and machine-agnostic method that can segment and quantify the infection regions on CT scans from different sources. Our method is founded upon two innovations: 1) the first CT scan simulator for COVID-19, by fitting the dynamic change of real patients' data measured at different time points, which greatly alleviates the data scarcity issue; and 2) a novel deep learning algorithm to solve the large-scene-small-object problem, which decomposes the 3D segmentation problem into three 2D ones, and thus reduces the model complexity by an order of magnitude and, at the same time, significantly improves the segmentation accuracy. Comprehensive experimental results over multi-country, multi-hospital, and multi-machine datasets demonstrate the superior performance of our method over the existing ones and suggest its important application value in combating the disease.",2020,10.1109/tmi.2020.3001810,cross-sectional,diagnosis,CT,Lung
A segmentation tool for pulmonary nodules in lung cancer screening: Testing and clinical usage,"PURPOSE: With the future goal of defining a large dataset based on low-dose CT with labelled pulmonary lesions for lung cancer screening (LCS) research, the aim of this work is to propose and evaluate into a clinical context a tool for semi-automatic segmentation able to facilitate the process of labels collection from a LCS study (COSMOS, Continuous Observation of SMOking Subjects). METHODS: Considering a preliminary set of manual annotations, a segmentation model based on a 2D-Unet was trained from scratch. Contour quality of the final 2D-Unet was assessed on an internal test set of manual annotations and on a subset of the public available LIDC dataset used as external test set. The tool for semi-automatic segmentation was then designed integrating the tested model into a Graphical User Interface. According to the opinion of two clinical users, the percentage of lesions properly contoured through the tool was quantified (Acceptance Rate, AR). The variability between segmentations derived by the two readers was estimated as mean percentage of difference (MPD) between the two sets of volumes and comparing the likelihood of malignancy derived from Volume Doubling Time (VDT). RESULTS: Performance in test sets were found similar (DICE ~ 0.75(0.15)). Accordingly, a good mean AR (80.1%) resulted from the two readers. Variability in terms of MPD was equal to 23.6% while 2.7% was the VDTs percentage of disagreement. CONCLUSIONS: A semi-automatic segmentation tool was developed and its applicability evaluated into a clinical context demonstrating the efficacy of the tool in facilitating the collection of labelled data.",2021,10.1016/j.ejmp.2021.08.011,cross-sectional,diagnosis,CT,Lung
A semi-supervised learning framework for micropapillary adenocarcinoma detection,"PURPOSE: Micropapillary adenocarcinoma is a distinctive histological subtype of lung adenocarcinoma with poor prognosis. Computer-aided diagnosis method has the potential to provide help for its early diagnosis. But the implementation of the existing methods largely relies on massive manually labeled data and consumes a lot of time and energy. To tackle these problems, we propose a framework that applies semi-supervised learning method to detect micropapillary adenocarcinoma, which aims to utilize labeled and unlabeled data better. METHODS: The framework consists of a teacher model and a student model. The teacher model is first obtained by using the labeled data. Then, it makes predictions on unlabeled data as pseudo-labels for students. Finally, high-quality pseudo-labels are selected and associated with the labeled data to train the student model. During the learning process of the student model, augmentation is added so that the student model generalizes better than the teacher model. RESULTS: Experiments are conducted on our own whole slide micropapillary lung adenocarcinoma histopathology image dataset and we selected 3527 patches for the experiment. In the supervised learning, our detector achieves a precision of 0.762 and recall of 0.884. In the semi-supervised learning, our method achieves a precision of 0.775 and recall of 0.896; it is superior to other methods. CONCLUSION: We proposed a semi-supervised learning framework for micropapillary adenocarcinoma detection, which has better performance in utilizing both labeled and unlabeled data. In addition, the detector we designed improves the detection accuracy and speed and achieves promising results in detecting micropapillary adenocarcinoma.",2022,10.1007/s11548-022-02565-8,,,,
A short-term follow-up CT based radiomics approach to predict response to immunotherapy in advanced non-small-cell lung cancer,"To develop a short-term follow-up CT-based radiomics approach to predict response to immunotherapy in advanced non-small-cell lung cancer (NSCLC) and investigate the prognostic value of radiomics features in predicting progression-free survival (PFS) and overall survival (OS). We first retrospectively collected 224 advanced NSCLC patients from two centers, and divided them into a primary cohort and two validation cohorts respectively. Then, we processed CT scans with a series of image preprocessing techniques namely, tumor segmentation, image resampling, feature extraction and normalization. To select the optimal features, we applied the feature ranking with recursive feature elimination method. After resampling the training dataset with a synthetic minority oversampling technique, we applied the support vector machine classifier to build a machine-learning-based classification model to predict response to immunotherapy. Finally, we used Kaplan-Meier (KM) survival analysis method to evaluate prognostic value of rad-score generated by CT-radiomics model. In two validation cohorts, the delta-radiomics model significantly improved the area under receiver operating characteristic curve from 0.64 and 0.52 to 0.82 and 0.87, respectively (P < .05). In sub-group analysis, pre- and delta-radiomics model yielded higher performance for adenocarcinoma (ADC) patients than squamous cell carcinoma (SCC) patients. Through the KM survival analysis, the rad-score of delta-radiomics model had a significant prognostic for PFS and OS in validation cohorts (P < .05). Our results demonstrated that (1) delta-radiomics model could improve the prediction performance, (2) radiomics model performed better on ADC patients than SCC patients, (3) delta-radiomics model had prognostic values in predicting PFS and OS of NSCLC patients.",2022,10.1080/2162402x.2022.2028962,cross-sectional,prognosis,CT,Lung
A Simple Method to Train the AI Diagnosis Model of Pulmonary Nodules,"BACKGROUND: The differential diagnosis of subcentimetre lung nodules with a diameter of less than 1 cm has always been one of the problems of imaging doctors and thoracic surgeons. We plan to create a deep learning model for the diagnosis of pulmonary nodules in a simple method. METHODS: Image data and pathological diagnosis of patients come from the First Affiliated Hospital of Zhejiang University School of Medicine from October 1, 2016, to October 1, 2019. After data preprocessing and data augmentation, the training set is used to train the model. The test set is used to evaluate the trained model. At the same time, the clinician will also diagnose the test set. RESULTS: A total of 2,295 images of 496 lung nodules and their corresponding pathological diagnosis were selected as a training set and test set. After data augmentation, the number of training set images reached 12,510 images, including 6,648 malignant nodular images and 5,862 benign nodular images. The area under the P-R curve of the trained model is 0.836 in the classification of malignant and benign nodules. The area under the ROC curve of the trained model is 0.896 (95% CI: 78.96%~100.18%), which is higher than that of three doctors. However, the P value is not less than 0.05. CONCLUSION: With the help of an automatic machine learning system, clinicians can create a deep learning pulmonary nodule pathology classification model without the help of deep learning experts. The diagnostic efficiency of this model is not inferior to that of the clinician.",2020,10.1155/2020/2812874,cross-sectional,diagnosis,CT,Lung
A simplified cluster model and a tool adapted for collaborative labeling of lung cancer CT scans,"BACKGROUND AND OBJECTIVE: Lung cancer is the most common type of cancer with a high mortality rate. Early detection using medical imaging is critically important for the long-term survival of the patients. Computer-aided diagnosis (CAD) tools can potentially reduce the number of incorrect interpretations of medical image data by radiologists. Datasets with adequate sample size, annotation, and truth are the dominant factors in developing and training effective CAD algorithms. The objective of this study was to produce a practical approach and a tool for the creation of medical image datasets. METHODS: The proposed model uses the modified maximum transverse diameter approach to mark a putative lung nodule. The modification involves the possibility to use a set of overlapping spheres of appropriate size to approximate the shape of the nodule. The algorithm embedded in the model also groups the marks made by different readers for the same lesion. We used the data of 536 randomly selected patients of Moscow outpatient clinics to create a dataset of standard-dose chest computed tomography (CT) scans utilizing the double-reading approach with arbitration. Six volunteer radiologists independently produced a report for each scan using the proposed model with the main focus on the detection of lesions with sizes ranging from 3 to 30 mm. After this, an arbitrator reviewed their marks and annotations. RESULTS: The maximum transverse diameter approach outperformed the alternative methods (3D box, ellipsoid, and complete outline construction) in a study of 10,000 computer-generated tumor models of different shapes in terms of accuracy and speed of nodule shape approximation. The markup and annotation of the CTLungCa-500 dataset revealed 72 studies containing no lung nodules. The remaining 464 CT scans contained 3151 lesions marked by at least one radiologist: 56%, 14%, and 29% of the lesions were malignant, benign, and non-nodular, respectively. 2887 lesions have the target size of 3-30 mm. Only 70 nodules were uniformly identified by all the six readers. An increase in the number of independent readers providing CT scans interpretations led to an accuracy increase associated with a decrease in agreement. The dataset markup process took three working weeks. CONCLUSIONS: The developed cluster model simplifies the collaborative and crowdsourced creation of image repositories and makes it time-efficient. Our proof-of-concept dataset provides a valuable source of annotated medical imaging data for training CAD algorithms aimed at early detection of lung nodules. The tool and the dataset are publicly available at https://github.com/Center-of-Diagnostics-and-Telemedicine/FAnTom.git and https://mosmed.ai/en/datasets/ct_lungcancer_500/, respectively.",2021,10.1016/j.cmpb.2021.106111,cross-sectional,diagnosis,CT,Lung
A stacked ensemble for the detection of COVID-19 with high recall and accuracy,"The main challenges for the automatic detection of the coronavirus disease (COVID-19) from computed tomography (CT) scans of an individual are: a lack of large datasets, ambiguity in the characteristics of COVID-19 and the detection techniques having low sensitivity (or recall). Hence, developing diagnostic techniques with high recall and automatic feature extraction using the available data are crucial for controlling the spread of COVID-19. This paper proposes a novel stacked ensemble capable of detecting COVID-19 from a patient's chest CT scans with high recall and accuracy. A systematic approach for designing a stacked ensemble from pre-trained computer vision models using transfer learning (TL) is presented. A novel diversity measure that results in the stacked ensemble with high recall and accuracy is proposed. The stacked ensemble proposed in this paper considers four pre-trained computer vision models: the visual geometry group (VGG)-19, residual network (ResNet)-101, densely connected convolutional network (DenseNet)-169 and wide residual network (WideResNet)-50-2. The proposed model was trained and evaluated with three different chest CT scans. As recall is more important than precision, the trade-offs between recall and precision were explored in relevance to COVID-19. The optimal recommended threshold values were found for each dataset.",2021,10.1016/j.compbiomed.2021.104608,cross-sectional,diagnosis,CT,Lung
A study of computer-aided diagnosis for pulmonary nodule: comparison between classification accuracies using calculated image features and imaging findings annotated by radiologists,"PURPOSE: In our previous study, we developed a computer-aided diagnosis (CADx) system using imaging findings annotated by radiologists. The system, however, requires radiologists to input many imaging findings. In order to reduce such an interaction of radiologists, we further developed a CADx system using derived imaging findings based on calculated image features, in which the system only requires few user operations. The purpose of this study is to check whether calculated image features (CFT) or derived imaging findings (DFD) can represent information in imaging findings annotated by radiologists (AFD). METHODS: We calculate 2282 image features and derive 39 imaging findings by using information on a nodule position and its type (solid or ground-glass). These image features are categorized into shape features, texture features and imaging findings-specific features. Each imaging finding is derived based on each corresponding classifier using random forest. To check whether CFT or DFD can represent information in AFD, under an assumption that the accuracies of classifiers are the same if information included in input is the same, we constructed classifiers by using various types of information (CTT, DFD and AFD) and compared accuracies on an inferred diagnosis of a nodule. We employ SVM with RBF kernel as classifier to infer a diagnosis name. RESULTS: Accuracies of classifiers using DFD, CFT, AFD and CFT [Formula: see text] AFD were 0.613, 0.577, 0.773 and 0.790, respectively. Concordance rates between DFD and AFD of shape findings, texture findings and surrounding findings were 0.644, 0.871 and 0.768, respectively. CONCLUSIONS: The results suggest that CFT and AFD are similar information and CFT represent only a portion of AFD. Particularly, CFT did not contain shape information in AFD. In order to decrease an interaction of radiologists, a development of a method which overcomes these problems is necessary.",2017,10.1007/s11548-017-1554-0,,,,
A Super-Learner Model for Tumor Motion Prediction and Management in Radiation Therapy: Development and Feasibility Evaluation,"In cancer radiation therapy, large tumor motion due to respiration can lead to uncertainties in tumor target delineation and treatment delivery, thus making active motion management an essential step in thoracic and abdominal tumor treatment. In current practice, patients with tumor motion may be required to receive two sets of CT scans - the initial free-breathing 4-dimensional CT (4DCT) scan for tumor motion estimation and a second CT scan under appropriate motion management such as breath-hold or abdominal compression. The aim of this study is to assess the feasibility of a predictive model for tumor motion estimation in three-dimensional space based on machine learning algorithms. The model was developed based on sixteen imaging features extracted from non-4D diagnostic CT images and eleven clinical features extracted from the Electronic Health Record (EHR) database of 150 patients to characterize the lung tumor motion. A super-learner model was trained to combine four base machine learning models including the Random Forest, Multi-Layer Perceptron, LightGBM and XGBoost, the hyper-parameters of which were also optimized to obtain the best performance. The outputs of the super-learner model consist of tumor motion predictions in the Superior-Inferior (SI), Anterior-Posterior (AP) and Left-Right (LR) directions, and were compared against tumor motions measured in the free-breathing 4DCT scans. The accuracy of predictions was evaluated using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) through ten rounds of independent tests. The MAE and RMSE of predictions in the SI direction were 1.23 mm and 1.70 mm; the MAE and RMSE of predictions in the AP direction were 0.81 mm and 1.19 mm, and the MAE and RMSE of predictions in the LR direction were 0.70 mm and 0.95 mm. In addition, the relative feature importance analysis demonstrated that the imaging features are of great importance in the tumor motion prediction compared to the clinical features. Our findings indicate that a super-learner model can accurately predict tumor motion ranges as measured in the 4DCT, and could provide a machine learning framework to assist radiation oncologists in determining the active motion management strategy for patients with large tumor motion.",2019,10.1038/s41598-019-51338-y,cross-sectional,treatment,CT,Lung
A Two-Stage Convolutional Neural Networks for Lung Nodule Detection,"Early detection of lung cancer is an effective way to improve the survival rate of patients. It is a critical step to have accurate detection of lung nodules in computed tomography (CT) images for the diagnosis of lung cancer. However, due to the heterogeneity of the lung nodules and the complexity of the surrounding environment, it is a challenge to develop a robust nodule detection method. In this study, we propose a two-stage convolutional neural networks (TSCNN) for lung nodule detection. The first stage based on the improved U-Net segmentation network is to establish an initial detection of lung nodules. During this stage, in order to obtain a high recall rate without introducing excessive false positive nodules, we propose a new sampling strategy for training. Simultaneously, a two-phase prediction method is also proposed in this stage. The second stage in the TSCNN architecture based on the proposed dual pooling structure is built into three 3D-CNN classification networks for false positive reduction. Since the network training requires a significant amount of training data, we designed a random mask as the data augmentation method in this study. Furthermore, we have improved the generalization ability of the false positive reduction model by means of ensemble learning. We verified the proposed architecture on the LUNA dataset in our experiments, which showed that the proposed TSCNN architecture did obtain competitive detection performance.",2020,10.1109/jbhi.2019.2963720,cross-sectional,diagnosis,CT,Lung
A Weakly-Supervised Framework for COVID-19 Classification and Lesion Localization From Chest CT,"Accurate and rapid diagnosis of COVID-19 suspected cases plays a crucial role in timely quarantine and medical treatment. Developing a deep learning-based model for automatic COVID-19 diagnosis on chest CT is helpful to counter the outbreak of SARS-CoV-2. A weakly-supervised deep learning framework was developed using 3D CT volumes for COVID-19 classification and lesion localization. For each patient, the lung region was segmented using a pre-trained UNet; then the segmented 3D lung region was fed into a 3D deep neural network to predict the probability of COVID-19 infectious; the COVID-19 lesions are localized by combining the activation regions in the classification network and the unsupervised connected components. 499 CT volumes were used for training and 131 CT volumes were used for testing. Our algorithm obtained 0.959 ROC AUC and 0.976 PR AUC. When using a probability threshold of 0.5 to classify COVID-positive and COVID-negative, the algorithm obtained an accuracy of 0.901, a positive predictive value of 0.840 and a very high negative predictive value of 0.982. The algorithm took only 1.93 seconds to process a single patient's CT volume using a dedicated GPU. Our weakly-supervised deep learning model can accurately predict the COVID-19 infectious probability and discover lesion regions in chest CT without the need for annotating the lesions for training. The easily-trained and high-performance deep learning algorithm provides a fast way to identify COVID-19 patients, which is beneficial to control the outbreak of SARS-CoV-2. The developed deep learning software is available at https://github.com/sydney0zq/covid-19-detection.",2020,10.1109/tmi.2020.2995965,cross-sectional,diagnosis,CT,Lung
A weighted rule based method for predicting malignancy of pulmonary nodules by nodule characteristics,"Predicting malignancy of solitary pulmonary nodules from computer tomography scans is a difficult and important problem in the diagnosis of lung cancer. This paper investigates the contribution of nodule characteristics in the prediction of malignancy. Using data from Lung Image Database Consortium (LIDC) database, we propose a weighted rule based classification approach for predicting malignancy of pulmonary nodules. LIDC database contains CT scans of nodules and information about nodule characteristics evaluated by multiple annotators. In the first step of our method, votes for nodule characteristics are obtained from ensemble classifiers by using image features. In the second step, votes and rules obtained from radiologist evaluations are used by a weighted rule based method to predict malignancy. The rule based method is constructed by using radiologist evaluations on previous cases. Correlations between malignancy and other nodule characteristics and agreement ratio of radiologists are considered in rule evaluation. To handle the unbalanced nature of LIDC, ensemble classifiers and data balancing methods are used. The proposed approach is compared with the classification methods trained on image features. Classification accuracy, specificity and sensitivity of classifiers are measured. The experimental results show that using nodule characteristics for malignancy prediction can improve classification results.",2015,10.1016/j.jbi.2015.05.011,cross-sectional,diagnosis,CT,Lung
AAR-LN-DQ: Automatic anatomy recognition based disease quantification in thoracic lymph node zones via FDG PET/CT images without Nodal Delineation,"PURPOSE: The derivation of quantitative information from medical images in a practical manner is essential for quantitative radiology (QR) to become a clinical reality, but still faces a major hurdle because of image segmentation challenges. With the goal of performing disease quantification in lymph node (LN) stations without explicit nodal delineation, this paper presents a novel approach for disease quantification (DQ) by automatic recognition of LN zones and detection of malignant lymph nodes within thoracic LN zones via positron emission tomography/computed tomography (PET/CT) images. Named AAR-LN-DQ, this approach decouples DQ methods from explicit nodal segmentation via an LN recognition strategy involving a novel globular filter and a deep neural network called SegNet. METHOD: The methodology consists of four main steps: (a) Building lymph node zone models by automatic anatomy recognition (AAR) method. It incorporates novel aspects of model building that relate to finding an optimal hierarchy for organs and lymph node zones in the thorax. (b) Recognizing lymph node zones by the built lymph node models. (c) Detecting pathologic LNs in the recognized zones by using a novel globular filter (g-filter) and a multi-level support vector machine (SVM) classifier. Here, we make use of the general globular shape of LNs to first localize them and then use a multi-level SVM classifier to identify pathologic LNs from among the LNs localized by the g-filter. Alternatively, we designed a deep neural network called SegNet which is trained to directly recognize pathologic nodes within AAR localized LN zones. (d) Disease quantification based on identified pathologic LNs within localized zones. A fuzzy disease map is devised to express the degree of disease burden at each voxel within the identified LNs to simultaneously handle several uncertain phenomena such as PET partial volume effects, uncertainty in localization of LNs, and gradation of disease content at the voxel level. We focused on the task of disease quantification in patients with lymphoma based on PET/CT acquisitions and devised a method of evaluation. Model building was carried out using 42 near-normal patient datasets via contrast-enhanced CT examinations of their thorax. PET/CT datasets from an additional 63 lymphoma patients were utilized for evaluating the AAR-LN-DQ methodology. We assess the accuracy of the three main processes involved in AAR-LN-DQ via fivefold cross validation: lymph node zone recognition, abnormal lymph node localization, and disease quantification. RESULTS: The recognition and scale error for LN zones were 12.28 mm ± 1.99 and 0.94 ± 0.02, respectively, on normal CT datasets. On abnormal PET/CT datasets, the sensitivity and specificity of pathologic LN recognition were 84.1% ± 0.115 and 98.5% ± 0.003, respectively, for the g-filter-SVM strategy, and 91.3% ± 0.110 and 96.1% ± 0.016, respectively, for the SegNet method. Finally, the mean absolute percent errors for disease quantification of the recognized abnormal LNs were 8% ± 0.09 and 14% ± 0.10 for the g-filter-SVM method and the best SegNet strategy, respectively. CONCLUSIONS: Accurate disease quantification on PET/CT images without performing explicit delineation of lymph nodes is feasible following lymph node zone and pathologic LN localization. It is very useful to perform LN zone recognition by AAR as this step can cover most (95.8%) of the abnormal LNs and drastically reduce the regions to search for abnormal LNs. This also improves the specificity of deep networks such as SegNet significantly. It is possible to utilize general shape information about LNs such as their globular nature via g-filter and to arrive at high recognition rates for abnormal LNs in conjunction with a traditional classifier such as SVM. Finally, the disease map concept is effective for estimating disease burden, irrespective of how the LNs are identified, to handle various uncertainties without having to address them explicitly one by one.",2020,10.1002/mp.14240,cross-sectional,diagnosis,CT,Lymph nodes
Accelerate gas diffusion-weighted MRI for lung morphometry with deep learning,"OBJECTIVES: Multiple b-value gas diffusion-weighted MRI (DW-MRI) enables non-invasive and quantitative assessment of lung morphometry, but its long acquisition time is not well-tolerated by patients. We aimed to accelerate multiple b-value gas DW-MRI for lung morphometry using deep learning. METHODS: A deep cascade of residual dense network (DC-RDN) was developed to reconstruct high-quality DW images from highly undersampled k-space data. Hyperpolarized (129)Xe lung ventilation images were acquired from 101 participants and were retrospectively collected to generate synthetic DW-MRI data to train the DC-RDN. Afterwards, the performance of the DC-RDN was evaluated on retrospectively and prospectively undersampled multiple b-value (129)Xe MRI datasets. RESULTS: Each slice with size of 64 × 64 × 5 could be reconstructed within 7.2 ms. For the retrospective test data, the DC-RDN showed significant improvement on all quantitative metrics compared with the conventional reconstruction methods (p < 0.05). The apparent diffusion coefficient (ADC) and morphometry parameters were not significantly different between the fully sampled and DC-RDN reconstructed images (p > 0.05). For the prospectively accelerated acquisition, the required breath-holding time was reduced from 17.8 to 4.7 s with an acceleration factor of 4. Meanwhile, the prospectively reconstructed results showed good agreement with the fully sampled images, with a mean difference of -0.72% and -0.74% regarding global mean ADC and mean linear intercept (L(m)) values. CONCLUSIONS: DC-RDN is effective in accelerating multiple b-value gas DW-MRI while maintaining accurate estimation of lung microstructural morphometry, facilitating the clinical potential of studying lung diseases with hyperpolarized DW-MRI. KEY POINTS: • The deep cascade of residual dense network allowed fast and high-quality reconstruction of multiple b-value gas diffusion-weighted MRI at an acceleration factor of 4. • The apparent diffusion coefficient and morphometry parameters were not significantly different between the fully sampled images and the reconstructed results (p > 0.05). • The required breath-holding time was reduced from 17.8 to 4.7 s and each slice with size of 64 × 64 × 5 could be reconstructed within 7.2 ms.",2022,10.1007/s00330-021-08126-y,cross-sectional,informatics,gas diffusion-weighted MR,Lung
Accelerating bioinformatics research with International Conference on Intelligent Biology and Medicine 2020,"The International Association for Intelligent Biology and Medicine (IAIBM) is a nonprofit organization that promotes intelligent biology and medical science. It hosts an annual International Conference on Intelligent Biology and Medicine (ICIBM), which was initially established in 2012. Due to the coronavirus (COVID-19) pandemic, the ICIBM 2020 was held for the first time as a virtual online conference on August 9 to 10. The virtual conference had ~ 300 registered participants and featured 41 online real-time presentations. ICIBM 2020 received a total of 75 manuscript submissions, and 12 were selected to be published in this special issue of BMC Bioinformatics. These 12 manuscripts cover a wide range of bioinformatics topics including network analysis, imaging analysis, machine learning, gene expression analysis, and sequence analysis.",2020,10.1186/s12859-020-03890-y,,,,
Accuracy of deep learning-based computed tomography diagnostic system for COVID-19: A consecutive sampling external validation cohort study,"Ali-M3, an artificial intelligence program, analyzes chest computed tomography (CT) and detects the likelihood of coronavirus disease (COVID-19) based on scores ranging from 0 to 1. However, Ali-M3 has not been externally validated. Our aim was to evaluate the accuracy of Ali-M3 for detecting COVID-19 and discuss its clinical value. We evaluated the external validity of Ali-M3 using sequential Japanese sampling data. In this retrospective cohort study, COVID-19 infection probabilities for 617 symptomatic patients were determined using Ali-M3. In 11 Japanese tertiary care facilities, these patients underwent reverse transcription-polymerase chain reaction (RT-PCR) testing. They also underwent chest CT to confirm a diagnosis of COVID-19. Of the 617 patients, 289 (46.8%) were RT-PCR-positive. The area under the curve (AUC) of Ali-M3 for predicting a COVID-19 diagnosis was 0.797 (95% confidence interval: 0.762‒0.833) and the goodness-of-fit was P = 0.156. With a cut-off probability of a diagnosis of COVID-19 by Ali-M3 set at 0.5, the sensitivity and specificity were 80.6% and 68.3%, respectively. A cut-off of 0.2 yielded a sensitivity and specificity of 89.2% and 43.2%, respectively. Among the 223 patients who required oxygen, the AUC was 0.825. Sensitivity at a cut-off of 0.5% and 0.2% was 88.7% and 97.9%, respectively. Although the sensitivity was lower when the days from symptom onset were fewer, the sensitivity increased for both cut-off values after 5 days. We evaluated Ali-M3 using external validation with symptomatic patient data from Japanese tertiary care facilities. As Ali-M3 showed sufficient sensitivity performance, despite a lower specificity performance, Ali-M3 could be useful in excluding a diagnosis of COVID-19.",2021,10.1371/journal.pone.0258760,cross-sectional,diagnosis,CT,Lung
Accurate auto-labeling of chest X-ray images based on quantitative similarity to an explainable AI model,"The inability to accurately, efficiently label large, open-access medical imaging datasets limits the widespread implementation of artificial intelligence models in healthcare. There have been few attempts, however, to automate the annotation of such public databases; one approach, for example, focused on labor-intensive, manual labeling of subsets of these datasets to be used to train new models. In this study, we describe a method for standardized, automated labeling based on similarity to a previously validated, explainable AI (xAI) model-derived-atlas, for which the user can specify a quantitative threshold for a desired level of accuracy (the probability-of-similarity, pSim metric). We show that our xAI model, by calculating the pSim values for each clinical output label based on comparison to its training-set derived reference atlas, can automatically label the external datasets to a user-selected, high level of accuracy, equaling or exceeding that of human experts. We additionally show that, by fine-tuning the original model using the automatically labelled exams for retraining, performance can be preserved or improved, resulting in a highly accurate, more generalized model.",2022,10.1038/s41467-022-29437-8,cross-sectional,diagnosis,CT,Lung
Accurate detection of COVID-19 using deep features based on X-Ray images and feature selection methods,"COVID-19 is a severe epidemic affecting the whole world. This epidemic, which has a high mortality rate, affects the health systems and the economies of countries significantly. Therefore, ending the epidemic is one of the most important priorities of all states. For this, automatic diagnosis and detection systems are very important to control the epidemic. In addition to the recommendation of the ""reverse transcription-polymerase chain reaction (RT-PCR)"" test, additional diagnosis and detection systems are required. Hence, based on the fact that the COVID-19 virus attacks the lungs, automatic diagnosis and detection systems developed using X-ray and CT images come to the fore. In this study, a high-performance detection system was implemented with three different CNN (ResNet50, ResNet101, InceptionResNetV2) models and X-ray images of three different classes (COVID-19, Normal, Pneumonia). The particle swarm optimization (PSO) algorithm and ant colony algorithm (ACO) was applied among the feature selection methods, and their performances were compared. The results were obtained using support vector machines (SVM) and a k-nearest neighbor (k-NN) classifier using the 10-fold cross-validation method. The highest overall accuracy performance was 99.83% with the SVM algorithm without feature selection. The highest performance was achieved after the feature selection process with the SVM + PSO method as 99.86%. As a result, higher performance with less computational load has been achieved by realizing the feature selection. Based on the high results obtained, it is thought that this study will benefit radiologists as a decision support system.",2021,10.1016/j.compbiomed.2021.104771,cross-sectional,diagnosis,X-ray,Lung
Accurate recognition of colorectal cancer with semi-supervised deep learning on pathological images,"Machine-assisted pathological recognition has been focused on supervised learning (SL) that suffers from a significant annotation bottleneck. We propose a semi-supervised learning (SSL) method based on the mean teacher architecture using 13,111 whole slide images of colorectal cancer from 8803 subjects from 13 independent centers. SSL (~3150 labeled, ~40,950 unlabeled; ~6300 labeled, ~37,800 unlabeled patches) performs significantly better than the SL. No significant difference is found between SSL (~6300 labeled, ~37,800 unlabeled) and SL (~44,100 labeled) at patch-level diagnoses (area under the curve (AUC): 0.980 ± 0.014 vs. 0.987 ± 0.008, P value = 0.134) and patient-level diagnoses (AUC: 0.974 ± 0.013 vs. 0.980 ± 0.010, P value = 0.117), which is close to human pathologists (average AUC: 0.969). The evaluation on 15,000 lung and 294,912 lymph node images also confirm SSL can achieve similar performance as that of SL with massive annotations. SSL dramatically reduces the annotations, which has great potential to effectively build expert-level pathological artificial intelligence platforms in practice.",2021,10.1038/s41467-021-26643-8,,,,
Accurate Screening of COVID-19 Using Attention-Based Deep 3D Multiple Instance Learning,"Automated Screening of COVID-19 from chest CT is of emergency and importance during the outbreak of SARS-CoV-2 worldwide in 2020. However, accurate screening of COVID-19 is still a massive challenge due to the spatial complexity of 3D volumes, the labeling difficulty of infection areas, and the slight discrepancy between COVID-19 and other viral pneumonia in chest CT. While a few pioneering works have made significant progress, they are either demanding manual annotations of infection areas or lack of interpretability. In this paper, we report our attempt towards achieving highly accurate and interpretable screening of COVID-19 from chest CT with weak labels. We propose an attention-based deep 3D multiple instance learning (AD3D-MIL) where a patient-level label is assigned to a 3D chest CT that is viewed as a bag of instances. AD3D-MIL can semantically generate deep 3D instances following the possible infection area. AD3D-MIL further applies an attention-based pooling approach to 3D instances to provide insight into each instance's contribution to the bag label. AD3D-MIL finally learns Bernoulli distributions of the bag-level labels for more accessible learning. We collected 460 chest CT examples: 230 CT examples from 79 patients with COVID-19, 100 CT examples from 100 patients with common pneumonia, and 130 CT examples from 130 people without pneumonia. A series of empirical studies show that our algorithm achieves an overall accuracy of 97.9%, AUC of 99.0%, and Cohen kappa score of 95.7%. These advantages endow our algorithm as an efficient assisted tool in the screening of COVID-19.",2020,10.1109/tmi.2020.2996256,cross-sectional,diagnosis,CT,Lung
Accurate segmentation for different types of lung nodules on CT images using improved U-Net convolutional network,"Since lung nodules on computed tomography images can have different shapes, contours, textures or locations and may be attached to neighboring blood vessels or pleural surfaces, accurate segmentation is still challenging. In this study, we propose an accurate segmentation method based on an improved U-Net convolutional network for different types of lung nodules on computed tomography images.The first phase is to segment lung parenchyma and correct the lung contour by applying α-hull algorithm. The second phase is to extract image pairs of patches containing lung nodules in the center and the corresponding ground truth and build an improved U-Net network with introduction of batch normalization.A large number of experiments manifest that segmentation performance of Dice loss has superior results than mean square error and Binary_crossentropy loss. The α-hull algorithm and batch normalization can improve the segmentation performance effectively. Our best result for Dice similar coefficient (0.8623) is also more competitive than other state-of-the-art segmentation algorithms.In order to segment different types of lung nodules accurately, we propose an improved U-Net network, which can improve the segmentation accuracy effectively. Moreover, this work also has practical value in helping radiologists segment lung nodules and diagnose lung cancer.",2021,10.1097/md.0000000000027491,cross-sectional,diagnosis,CT,Lung
"Accurately Differentiating Between Patients With COVID-19, Patients With Other Viral Infections, and Healthy Individuals: Multimodal Late Fusion Learning Approach","BACKGROUND: Effectively identifying patients with COVID-19 using nonpolymerase chain reaction biomedical data is critical for achieving optimal clinical outcomes. Currently, there is a lack of comprehensive understanding in various biomedical features and appropriate analytical approaches for enabling the early detection and effective diagnosis of patients with COVID-19. OBJECTIVE: We aimed to combine low-dimensional clinical and lab testing data, as well as high-dimensional computed tomography (CT) imaging data, to accurately differentiate between healthy individuals, patients with COVID-19, and patients with non-COVID viral pneumonia, especially at the early stage of infection. METHODS: In this study, we recruited 214 patients with nonsevere COVID-19, 148 patients with severe COVID-19, 198 noninfected healthy participants, and 129 patients with non-COVID viral pneumonia. The participants' clinical information (ie, 23 features), lab testing results (ie, 10 features), and CT scans upon admission were acquired and used as 3 input feature modalities. To enable the late fusion of multimodal features, we constructed a deep learning model to extract a 10-feature high-level representation of CT scans. We then developed 3 machine learning models (ie, k-nearest neighbor, random forest, and support vector machine models) based on the combined 43 features from all 3 modalities to differentiate between the following 4 classes: nonsevere, severe, healthy, and viral pneumonia. RESULTS: Multimodal features provided substantial performance gain from the use of any single feature modality. All 3 machine learning models had high overall prediction accuracy (95.4%-97.7%) and high class-specific prediction accuracy (90.6%-99.9%). CONCLUSIONS: Compared to the existing binary classification benchmarks that are often focused on single-feature modality, this study's hybrid deep learning-machine learning framework provided a novel and effective breakthrough for clinical applications. Our findings, which come from a relatively large sample size, and analytical workflow will supplement and assist with clinical decision support for current COVID-19 diagnostic methods and other clinical applications with high-dimensional multimodal biomedical features.",2021,10.2196/25535,cross-sectional,diagnosis,CT,Lung
Active Contour Based Segmentation and Classification for Pleura Diseases Based on Otsu’s Thresholding and Support Vector Machine (SVM),"Objective: Generally, lung cancer is the abnormal growth of cells that originates in one or both lungs. Finding the pulmonary nodule helps in the diagnosis of lung cancer in early stage and also increase the lifetime of the individual. Accurate segmentation of normal and abnormal portion in segmentation is challenging task in computer-aided diagnostics. Methods: The article proposes an innovative method to spot the cancer portion using Otsu’s segmentation algorithm. It is followed by a Support Vector Machine (SVM) classifier to classify the abnormal portion of the lung image. Results: The suggested methods use the Otsu’s thresholding and active contour based segmentation techniques to locate the affected lung nodule of CT images. The segmentation is followed by an SVM classifier in order to categorize the affected portion is normal or abnormal. The proposed method is suitable to provide good and accurate segmentation and classification results for complex images. Conclusion: The comparative analysis between the two segmentation methods along with SVM classifier was performed. A classification process based on active contour and SVM techniques provides better than Otsu’s segmentation for complex lung images.",2019,10.31557/apjcp.2019.20.1.167,cross-sectional,diagnosis,CT,Lung
ADA-COVID: Adversarial Deep Domain Adaptation-Based Diagnosis of COVID-19 from Lung CT Scans Using Triplet Embeddings,"Rapid diagnosis of COVID-19 with high reliability is essential in the early stages. To this end, recent research often uses medical imaging combined with machine vision methods to diagnose COVID-19. However, the scarcity of medical images and the inherent differences in existing datasets that arise from different medical imaging tools, methods, and specialists may affect the generalization of machine learning-based methods. Also, most of these methods are trained and tested on the same dataset, reducing the generalizability and causing low reliability of the obtained model in real-world applications. This paper introduces an adversarial deep domain adaptation-based approach for diagnosing COVID-19 from lung CT scan images, termed ADA-COVID. Domain adaptation-based training process receives multiple datasets with different input domains to generate domain-invariant representations for medical images. Also, due to the excessive structural similarity of medical images compared to other image data in machine vision tasks, we use the triplet loss function to generate similar representations for samples of the same class (infected cases). The performance of ADA-COVID is evaluated and compared with other state-of-the-art COVID-19 diagnosis algorithms. The obtained results indicate that ADA-COVID achieves classification improvements of at least 3%, 20%, 20%, and 11% in accuracy, precision, recall, and F(1) score, respectively, compared to the best results of competitors, even without directly training on the same data. The implementation source code of the ADA-COVID is publicly available at https://github.com/MehradAria/ADA-COVID.",2022,10.1155/2022/2564022,cross-sectional,diagnosis,CT,Lung
"Adapting for the COVID-19 pandemic in Ecuador, a characterization of hospital strategies and patients","The World Health Organization (WHO) declared coronavirus disease-2019 (COVID-19) a global pandemic on 11 March 2020. In Ecuador, the first case of COVID-19 was recorded on 29 February 2020. Despite efforts to control its spread, SARS-CoV-2 overran the Ecuadorian public health system, which became one of the most affected in Latin America on 24 April 2020. The Hospital General del Sur de Quito (HGSQ) had to transition from a general to a specific COVID-19 health center in a short period of time to fulfill the health demand from patients with respiratory afflictions. Here, we summarized the implementations applied in the HGSQ to become a COVID-19 exclusive hospital, including the rearrangement of hospital rooms and a triage strategy based on a severity score calculated through an artificial intelligence (AI)-assisted chest computed tomography (CT). Moreover, we present clinical, epidemiological, and laboratory data from 75 laboratory tested COVID-19 patients, which represent the first outbreak of Quito city. The majority of patients were male with a median age of 50 years. We found differences in laboratory parameters between intensive care unit (ICU) and non-ICU cases considering C-reactive protein, lactate dehydrogenase, and lymphocytes. Sensitivity and specificity of the AI-assisted chest CT were 21.4% and 66.7%, respectively, when considering a score >70%; regardless, this system became a cornerstone of hospital triage due to the lack of RT-PCR testing and timely results. If health workers act as vectors of SARS-CoV-2 at their domiciles, they can seed outbreaks that might put 1,879,047 people at risk of infection within 15 km around the hospital. Despite our limited sample size, the information presented can be used as a local example that might aid future responses in low and middle-income countries facing respiratory transmitted epidemics.",2021,10.1371/journal.pone.0251295,,,,
Adaptive Feature Selection Guided Deep Forest for COVID-19 Classification With Chest CT,"Chest computed tomography (CT) becomes an effective tool to assist the diagnosis of coronavirus disease-19 (COVID-19). Due to the outbreak of COVID-19 worldwide, using the computed-aided diagnosis technique for COVID-19 classification based on CT images could largely alleviate the burden of clinicians. In this paper, we propose an Adaptive Feature Selection guided Deep Forest (AFS-DF) for COVID-19 classification based on chest CT images. Specifically, we first extract location-specific features from CT images. Then, in order to capture the high-level representation of these features with the relatively small-scale data, we leverage a deep forest model to learn high-level representation of the features. Moreover, we propose a feature selection method based on the trained deep forest model to reduce the redundancy of features, where the feature selection could be adaptively incorporated with the COVID-19 classification model. We evaluated our proposed AFS-DF on COVID-19 dataset with 1495 patients of COVID-19 and 1027 patients of community acquired pneumonia (CAP). The accuracy (ACC), sensitivity (SEN), specificity (SPE), AUC, precision and F1-score achieved by our method are 91.79%, 93.05%, 89.95%, 96.35%, 93.10% and 93.07%, respectively. Experimental results on the COVID-19 dataset suggest that the proposed AFS-DF achieves superior performance in COVID-19 vs. CAP classification, compared with 4 widely used machine learning methods.",2020,10.1109/jbhi.2020.3019505,cross-sectional,diagnosis,CT,Lung
Added Value of Computer-aided CT Image Features for Early Lung Cancer Diagnosis with Small Pulmonary Nodules: A Matched Case-Control Study,"Purpose To test whether computer-aided diagnosis (CAD) approaches can increase the positive predictive value (PPV) and reduce the false-positive rate in lung cancer screening for small nodules compared with human reading by thoracic radiologists. Materials and Methods A matched case-control sample of low-dose computed tomography (CT) studies in 186 participants with 4-20-mm noncalcified lung nodules who underwent biopsy in the National Lung Screening Trial (NLST) was selected. Variables used for matching were age, sex, smoking status, chronic obstructive pulmonary disease status, body mass index, study year of the positive screening test, and screening results. Studies before lung biopsy were randomly split into a training set (70 cancers plus 70 benign controls) and a validation set (20 cancers plus 26 benign controls). Image features from within and outside dominant nodules were extracted. A CAD algorithm developed from the training set and a random forest classifier were applied to the validation set to predict biopsy outcomes. Receiver operating characteristic analysis was used to compare the prediction accuracy of CAD with the NLST investigator's diagnosis and readings from three experienced and board-certified thoracic radiologists who used contemporary clinical practice guidelines. Results In the validation cohort, the area under the receiver operating characteristic curve for CAD was 0.9154. By default, the sensitivity, specificity, and PPV of the NLST investigators were 1.00, 0.00, and 0.43, respectively. The sensitivity, specificity, PPV, and negative predictive value of CAD and the three radiologists' combined reading were 0.95, 0.88, 0.86, and 0.96 and 0.70, 0.69, 0.64, and 0.75, respectively. Conclusion CAD could increase PPV and reduce the false-positive rate in the early diagnosis of lung cancer. (©) RSNA, 2017 Online supplemental material is available for this article.",2018,10.1148/radiol.2017162725,cross-sectional,diagnosis,CT,Lung
Advanced Deep Learning Algorithms for Infectious Disease Modeling Using Clinical Data: A Case Study on COVID-19,"BACKGROUND: Dealing with the COVID-19 pandemic has been one of the most important objectives of many countries.Intently observing the growth dynamics of the cases is one way to accomplish the solution for the pandemic. INTRODUCTION: Infectious diseases are caused by a micro-organism/virus from another person or an animal. It causes difficulty at both the individual and collective levels. The ongoing episode of COVID-19 ailment, brought about by the new coronavirus first detected in Wuhan, China, and its quick spread far and wide revived the consideration of the world towards the impact of such plagues on an individual's everyday existence. We suggested that a basic structure be developed to work with the progressive examination of the development rate (cases/day) and development speed (cases/day2) of COVID-19 cases. METHODS: We attempt to exploit the effectiveness of advanced deep learning algorithms to predict the growth of infectious diseases based on time series data and classification based on symptoms text data and X-ray image data. The goal is to identify the nature of the phenomenon represented by the sequence of observations and forecasting. RESULTS: We concluded that our good habits and healthy lifestyle prevent the risk of COVID-19. We observed that by simply using masks in our daily lives, we could flatten the curve of increasing cases.Limiting human mobility resulted in a significant decrease in the development speed within a few days, a deceleration within two weeks, and a close to fixed development within six weeks. CONCLUSION: These outcomes authenticate that mass social isolation is a profoundly viable measure against the spread of SARS-CoV-2, as recently recommended. Aside from the research of country- by-country predominance, the proposed structure is useful for city, state, district, and discretionary region information, serving as a resource for screening COVID-19 cases in the area.",2022,10.2174/1573405617666210908125911,cross-sectional,diagnosis,X-ray,Lung
Agile convolutional neural network for pulmonary nodule classification using CT images,"OBJECTIVE: To distinguish benign from malignant pulmonary nodules using CT images is critical for their precise diagnosis and treatment. A new Agile convolutional neural network (CNN) framework is proposed to conquer the challenges of a small-scale medical image database and the small size of the nodules, and it improves the performance of pulmonary nodule classification using CT images. METHODS: A hybrid CNN of LeNet and AlexNet is constructed through combining the layer settings of LeNet and the parameter settings of AlexNet. A dataset with 743 CT image nodule samples is built up based on the 1018 CT scans of LIDC to train and evaluate the Agile CNN model. Through adjusting the parameters of the kernel size, learning rate, and other factors, the effect of these parameters on the performance of the CNN model is investigated, and an optimized setting of the CNN is obtained finally. RESULTS: After finely optimizing the settings of the CNN, the estimation accuracy and the area under the curve can reach 0.822 and 0.877, respectively. The accuracy of the CNN is significantly dependent on the kernel size, learning rate, training batch size, dropout, and weight initializations. The best performance is achieved when the kernel size is set to [Formula: see text], the learning rate is 0.005, the batch size is 32, and dropout and Gaussian initialization are used. CONCLUSIONS: This competitive performance demonstrates that our proposed CNN framework and the optimization strategy of the CNN parameters are suitable for pulmonary nodule classification characterized by small medical datasets and small targets. The classification model might help diagnose and treat pulmonary nodules effectively.",2018,10.1007/s11548-017-1696-0,,,,
AI detection of mild COVID-19 pneumonia from chest CT scans,"OBJECTIVES: An artificial intelligence model was adopted to identify mild COVID-19 pneumonia from computed tomography (CT) volumes, and its diagnostic performance was then evaluated. METHODS: In this retrospective multicenter study, an atrous convolution-based deep learning model was established for the computer-assisted diagnosis of mild COVID-19 pneumonia. The dataset included 2087 chest CT exams collected from four hospitals between 1 January 2019 and 31 May 2020. The true positive rate, true negative rate, receiver operating characteristic curve, area under the curve (AUC) and convolutional feature map were used to evaluate the model. RESULTS: The proposed deep learning model was trained on 1538 patients and tested on an independent testing cohort of 549 patients. The overall sensitivity was 91.5% (195/213; p < 0.001, 95% CI: 89.2-93.9%), the overall specificity was 90.5% (304/336; p < 0.001, 95% CI: 88.0-92.9%) and the general AUC value was 0.955 (p < 0.001). CONCLUSIONS: A deep learning model can accurately detect COVID-19 and serve as an important supplement to the COVID-19 reverse transcription-polymerase chain reaction (RT-PCR) test. KEY POINTS: • The implementation of a deep learning model to identify mild COVID-19 pneumonia was confirmed to be effective and feasible. • The strategy of using a binary code instead of the region of interest label to identify mild COVID-19 pneumonia was verified. • This AI model can assist in the early screening of COVID-19 without interfering with normal clinical examinations.",2021,10.1007/s00330-021-07797-x,cross-sectional,diagnosis,CT,Lung
AI Lung Segmentation and Perfusion Analysis of Dual-Energy CT Can Help to Distinguish COVID-19 Infiltrates from Visually Similar Immunotherapy-Related Pneumonitis Findings and Can Optimize Radiological Workflows,"(1) To explore the potential impact of an AI dual-energy CT (DECT) prototype on decision making and workflows by investigating its capabilities to differentiate COVID-19 from immunotherapy-related pneumonitis. (2) Methods: From 3 April 2020 to 12 February 2021, DECT from biometrically matching patients with COVID-19, pneumonitis, and inconspicuous findings were selected from our clinical routine. Three blinded readers independently scored each pulmonary lobe analogous to CO-RADS. Inter-rater agreement was determined with an intraclass correlation coefficient (ICC). Averaged perfusion metrics per lobe (iodine uptake in mg, volume without vessels in ml, iodine concentration in mg/mL) were extracted using manual segmentation and an AI DECT prototype. A generalized linear mixed model was used to investigate metric validity and potential distinctions at equal CO-RADS scores. Multinomial regression measured the contribution ""Reader"", ""CO-RADS score"", and ""perfusion metrics"" to diagnosis. The time to diagnosis was measured for manual vs. AI segmentation. (3) Results: We included 105 patients (62 ± 13 years, mean BMI 27 ± 2). There were no significant differences between manually and AI-extracted perfusion metrics (p = 0.999). Regardless of the CO-RADS score, iodine uptake and concentration per lobe were significantly higher in COVID-19 than in pneumonitis (p < 0.001). In regression, iodine uptake had a greater contribution to diagnosis than CO-RADS scoring (Odds Ratio (OR) = 1.82 [95%CI 1.10-2.99] vs. OR = 0.20 [95%CI 0.14-0.29]). The AI prototype extracted the relevant perfusion metrics significantly faster than radiologists (10 ± 1 vs. 15 ± 2 min, p < 0.001). (4) Conclusions: The investigated AI prototype positively impacts decision making and workflows by extracting perfusion metrics that differentiate COVID-19 from visually similar pneumonitis significantly faster than radiologists.",2021,10.3390/tomography8010003,cross-sectional,diagnosis,Dual energy CT,Lung
AI recognition of patient race in medical imaging: a modelling study,"BACKGROUND: Previous studies in medical imaging have shown disparate abilities of artificial intelligence (AI) to detect a person's race, yet there is no known correlation for race on medical imaging that would be obvious to human experts when interpreting the images. We aimed to conduct a comprehensive evaluation of the ability of AI to recognise a patient's racial identity from medical images. METHODS: Using private (Emory CXR, Emory Chest CT, Emory Cervical Spine, and Emory Mammogram) and public (MIMIC-CXR, CheXpert, National Lung Cancer Screening Trial, RSNA Pulmonary Embolism CT, and Digital Hand Atlas) datasets, we evaluated, first, performance quantification of deep learning models in detecting race from medical images, including the ability of these models to generalise to external environments and across multiple imaging modalities. Second, we assessed possible confounding of anatomic and phenotypic population features by assessing the ability of these hypothesised confounders to detect race in isolation using regression models, and by re-evaluating the deep learning models by testing them on datasets stratified by these hypothesised confounding variables. Last, by exploring the effect of image corruptions on model performance, we investigated the underlying mechanism by which AI models can recognise race. FINDINGS: In our study, we show that standard AI deep learning models can be trained to predict race from medical images with high performance across multiple imaging modalities, which was sustained under external validation conditions (x-ray imaging [area under the receiver operating characteristics curve (AUC) range 0·91-0·99], CT chest imaging [0·87-0·96], and mammography [0·81]). We also showed that this detection is not due to proxies or imaging-related surrogate covariates for race (eg, performance of possible confounders: body-mass index [AUC 0·55], disease distribution [0·61], and breast density [0·61]). Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting the efforts to control this behaviour when it is undesirable will be challenging and demand further study. INTERPRETATION: The results from our study emphasise that the ability of AI deep learning models to predict self-reported race is itself not the issue of importance. However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging. FUNDING: National Institute of Biomedical Imaging and Bioengineering, MIDRC grant of National Institutes of Health, US National Science Foundation, National Library of Medicine of the National Institutes of Health, and Taiwan Ministry of Science and Technology.",2022,10.1016/s2589-7500(22)00063-2,,,,
AI-based diagnosis of COVID-19 patients using X-ray scans with stochastic ensemble of CNNs,"According to the World Health Organization (WHO), novel coronavirus (COVID-19) is an infectious disease and has a significant social and economic impact. The main challenge in fighting against this disease is its scale. Due to the outbreak, medical facilities are under pressure due to case numbers. A quick diagnosis system is required to address these challenges. To this end, a stochastic deep learning model is proposed. The main idea is to constrain the deep-representations over a Gaussian prior to reinforce the discriminability in feature space. The model can work on chest X-ray or CT-scan images. It provides a fast diagnosis of COVID-19 and can scale seamlessly. The work presents a comprehensive evaluation of previously proposed approaches for X-ray based disease diagnosis. The approach works by learning a latent space over X-ray image distribution from the ensemble of state-of-the-art convolutional-nets, and then linearly regressing the predictions from an ensemble of classifiers which take the latent vector as input. We experimented with publicly available datasets having three classes: COVID-19, normal and pneumonia yielding an overall accuracy and AUC of 0.91 and 0.97, respectively. Moreover, for robust evaluation, experiments were performed on a large chest X-ray dataset to classify among Atelectasis, Effusion, Infiltration, Nodule, and Pneumonia classes. The results demonstrate that the proposed model has better understanding of the X-ray images which make the network more generic to be later used with other domains of medical image analysis.",2021,10.1007/s13246-021-01060-9,cross-sectional,diagnosis,X-ray and CT,Lung
AI-based improvement in lung cancer detection on chest radiographs: results of a multi-reader study in NLST dataset,"OBJECTIVE: Assess if deep learning-based artificial intelligence (AI) algorithm improves reader performance for lung cancer detection on chest X-rays (CXRs). METHODS: This reader study included 173 images from cancer-positive patients (n = 98) and 346 images from cancer-negative patients (n = 196) selected from National Lung Screening Trial (NLST). Eight readers, including three radiology residents, and five board-certified radiologists, participated in the observer performance test. AI algorithm provided image-level probability of pulmonary nodule or mass on CXRs and a heatmap of detected lesions. Reader performance was compared with AUC, sensitivity, specificity, false-positives per image (FPPI), and rates of chest CT recommendations. RESULTS: With AI, the average sensitivity of readers for the detection of visible lung cancer increased for residents, but was similar for radiologists compared to that without AI (0.61 [95% CI, 0.55-0.67] vs. 0.72 [95% CI, 0.66-0.77], p = 0.016 for residents, and 0.76 [95% CI, 0.72-0.81] vs. 0.76 [95% CI, 0.72-0.81, p = 1.00 for radiologists), while false-positive findings per image (FPPI) was similar for residents, but decreased for radiologists (0.15 [95% CI, 0.11-0.18] vs. 0.12 [95% CI, 0.09-0.16], p = 0.13 for residents, and 0.24 [95% CI, 0.20-0.29] vs. 0.17 [95% CI, 0.13-0.20], p < 0.001 for radiologists). With AI, the average rate of chest CT recommendation in patients positive for visible cancer increased for residents, but was similar for radiologists (54.7% [95% CI, 48.2-61.2%] vs. 70.2% [95% CI, 64.2-76.2%], p < 0.001 for residents and 72.5% [95% CI, 68.0-77.1%] vs. 73.9% [95% CI, 69.4-78.3%], p = 0.68 for radiologists), while that in cancer-negative patients was similar for residents, but decreased for radiologists (11.2% [95% CI, 9.6-13.1%] vs. 9.8% [95% CI, 8.0-11.6%], p = 0.32 for residents and 16.4% [95% CI, 14.7-18.2%] vs. 11.7% [95% CI, 10.2-13.3%], p < 0.001 for radiologists). CONCLUSIONS: AI algorithm can enhance the performance of readers for the detection of lung cancers on chest radiographs when used as second reader. KEY POINTS: • Reader study in the NLST dataset shows that AI algorithm had sensitivity benefit for residents and specificity benefit for radiologists for the detection of visible lung cancer. • With AI, radiology residents were able to recommend more chest CT examinations (54.7% vs 70.2%, p < 0.001) for patients with visible lung cancer. • With AI, radiologists recommended significantly less proportion of unnecessary chest CT examinations (16.4% vs. 11.7%, p < 0.001) in cancer-negative patients.",2021,10.1007/s00330-021-08074-7,cross-sectional,diagnosis,X-ray,Lung
AI-Based Quantitative CT Analysis of Temporal Changes According to Disease Severity in COVID-19 Pneumonia,"OBJECTIVE: To quantitatively evaluate computed tomography (CT) parameters of coronavirus disease 2019 (COVID-19) pneumonia an artificial intelligence (AI)-based software in different clinical severity groups during the disease course. METHODS: From March 11 to April 15, 2020, 51 patients (age, 18-84 years; 28 men) diagnosed and hospitalized with COVID-19 pneumonia with a total of 116 CT scans were enrolled in the study. Patients were divided into mild (n = 12), moderate (n = 31), and severe (n = 8) groups based on clinical severity. An AI-based quantitative CT analysis, including lung volume, opacity score, opacity volume, percentage of opacity, and mean lung density, was performed in initial and follow-up CTs obtained at different time points. Receiver operating characteristic analysis was performed to find the diagnostic ability of quantitative CT parameters for discriminating severe from nonsevere pneumonia. RESULTS: In baseline assessment, the severe group had significantly higher opacity score, opacity volume, higher percentage of opacity, and higher mean lung density than the moderate group (all P ≤ 0.001). Through consecutive time points, the severe group had a significant decrease in lung volume (P = 0.006), a significant increase in total opacity score (P = 0.003), and percentage of opacity (P = 0.007). A significant increase in total opacity score was also observed for the mild group (P = 0.011). Residual opacities were observed in all groups. The involvement of more than 4 lobes (sensitivity, 100%; specificity, 65.26%), total opacity score greater than 4 (sensitivity, 100%; specificity, 64.21), total opacity volume greater than 337.4 mL (sensitivity, 80.95%; specificity, 84.21%), percentage of opacity greater than 11% (sensitivity, 80.95%; specificity, 88.42%), total high opacity volume greater than 10.5 mL (sensitivity, 95.24%; specificity, 66.32%), percentage of high opacity greater than 0.8% (sensitivity, 85.71%; specificity, 80.00%) and mean lung density HU greater than -705 HU (sensitivity, 57.14%; specificity, 90.53%) were related to severe pneumonia. CONCLUSIONS: An AI-based quantitative CT analysis is an objective tool in demonstrating disease severity and can also assist the clinician in follow-up by providing information about the disease course and prognosis according to different clinical severity groups.",2021,10.1097/rct.0000000000001224,cross-sectional,diagnosis,CT,Lung
ai-corona: Radiologist-assistant deep learning framework for COVID-19 diagnosis in chest CT scans,"The development of medical assisting tools based on artificial intelligence advances is essential in the global fight against COVID-19 outbreak and the future of medical systems. In this study, we introduce ai-corona, a radiologist-assistant deep learning framework for COVID-19 infection diagnosis using chest CT scans. Our framework incorporates an EfficientNetB3-based feature extractor. We employed three datasets; the CC-CCII set, the MasihDaneshvari Hospital (MDH) cohort, and the MosMedData cohort. Overall, these datasets constitute 7184 scans from 5693 subjects and include the COVID-19, non-COVID abnormal (NCA), common pneumonia (CP), non-pneumonia, and Normal classes. We evaluate ai-corona on test sets from the CC-CCII set, MDH cohort, and the entirety of the MosMedData cohort, for which it gained AUC scores of 0.997, 0.989, and 0.954, respectively. Our results indicates ai-corona outperforms all the alternative models. Lastly, our framework's diagnosis capabilities were evaluated as assistant to several experts. Accordingly, We observed an increase in both speed and accuracy of expert diagnosis when incorporating ai-corona's assistance.",2021,10.1371/journal.pone.0250952,cross-sectional,diagnosis,CT,Lung
"AI-Driven COVID-19 Tools to Interpret, Quantify Lung Images","Qualitative interpretation is a good thing when it comes to reading lung images in the fight against coronavirus 2019 disease (COVID-19), but quantitative analysis makes radiology reporting much more comprehensive. To that end, several research groups have begun looking to artificial intelligence (AI) as a tool for reading and analyzing X-rays and computed tomography (CT) scans, and helping to diagnose and monitor COVID-19.",2020,10.1109/mpuls.2020.3008354,,,,
AI-driven deep convolutional neural networks for chest X-ray pathology identification,"BACKGROUND: Chest X-ray images are widely used to detect many different lung diseases. However, reading chest X-ray images to accurately detect and classify different lung diseases by doctors is often difficult with large inter-reader variability. Thus, there is a huge demand for developing computer-aided automated schemes of chest X-ray images to help doctors more accurately and efficiently detect lung diseases depicting on chest X-ray images. OBJECTIVE: To develop convolution neural network (CNN) based deep learning models and compare their feasibility and performance to classify 14 chest diseases or pathology patterns based on chest X-rays. METHOD: Several CNN models pre-trained using ImageNet dataset are modified as transfer learning models and applied to classify between 14 different chest pathology and normal chest patterns depicting on chest X-ray images. In this process, a deep convolution generative adversarial network (DC-GAN) is also trained to mitigate the effects of small or imbalanced dataset and generate synthetic images to balance the dataset of different diseases. The classification models are trained and tested using a large dataset involving 91,324 frontal-view chest X-ray images. RESULTS: In this study, eight models are trained and compared. Among them, ResNet-152 model achieves an accuracy of 67% and 62% with and without data augmentation, respectively. Inception-V3, NasNetLarge, Xcaption, ResNet-50 and InceptionResNetV2 achieve accuracy of 68%, 62%, 66%, 66% and 54% respectively. Additionally, Resnet-152 with data augmentation achieves an accuracy of 83% but only for six classes. CONCLUSION: This study solves the problem of having fewer data by using GAN-based techniques to add synthetic images and demonstrates the feasibility of applying transfer learning CNN method to help classify 14 types of chest diseases depicting on chest X-ray images.",2022,10.3233/xst-211082,cross-sectional,diagnosis,X-ray,Lung
AI-Driven Model for Automatic Emphysema Detection in Low-Dose Computed Tomography Using Disease-Specific Augmentation,"The objective of this study is to evaluate the feasibility of a disease-specific deep learning (DL) model based on minimum intensity projection (minIP) for automated emphysema detection in low-dose computed tomography (LDCT) scans. LDCT scans of 240 individuals from a population-based cohort in the Netherlands (ImaLife study, mean age ± SD = 57 ± 6 years) were retrospectively chosen for training and internal validation of the DL model. For independent testing, LDCT scans of 125 individuals from a lung cancer screening cohort in the USA (NLST study, mean age ± SD = 64 ± 5 years) were used. Dichotomous emphysema diagnosis based on radiologists' annotation was used to develop the model. The automated model included minIP processing (slab thickness range: 1 mm to 11 mm), classification, and detection maps generation. The data-split for the pipeline evaluation involved class-balanced and imbalanced settings. The proposed DL pipeline showed the highest performance (area under receiver operating characteristics curve) for 11 mm slab thickness in both the balanced (ImaLife = 0.90 ± 0.05) and the imbalanced dataset (NLST = 0.77 ± 0.06). For ImaLife subcohort, the variation in minIP slab thickness from 1 to 11 mm increased the DL model's sensitivity from 75 to 88% and decreased the number of false-negative predictions from 10 to 5. The minIP-based DL model can automatically detect emphysema in LDCTs. The performance of thicker minIP slabs was better than that of thinner slabs. LDCT can be leveraged for emphysema detection by applying disease specific augmentation.",2022,10.1007/s10278-022-00599-7,cross-sectional,diagnosis,CT,Lung
"AI-driven quantification, staging and outcome prediction of COVID-19 pneumonia","Coronavirus disease 2019 (COVID-19) emerged in 2019 and disseminated around the world rapidly. Computed tomography (CT) imaging has been proven to be an important tool for screening, disease quantification and staging. The latter is of extreme importance for organizational anticipation (availability of intensive care unit beds, patient management planning) as well as to accelerate drug development through rapid, reproducible and quantified assessment of treatment response. Even if currently there are no specific guidelines for the staging of the patients, CT together with some clinical and biological biomarkers are used. In this study, we collected a multi-center cohort and we investigated the use of medical imaging and artificial intelligence for disease quantification, staging and outcome prediction. Our approach relies on automatic deep learning-based disease quantification using an ensemble of architectures, and a data-driven consensus for the staging and outcome prediction of the patients fusing imaging biomarkers with clinical and biological attributes. Highly promising results on multiple external/independent evaluation cohorts as well as comparisons with expert human readers demonstrate the potentials of our approach.",2021,10.1016/j.media.2020.101860,cross-sectional,prognosis,CT,Lung
AIforCOVID: Predicting the clinical outcomes in patients with COVID-19 applying AI to chest-X-rays. An Italian multicentre study,"Recent epidemiological data report that worldwide more than 53 million people have been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease has been spreading very rapidly and few months after the identification of the first infected, shortage of hospital resources quickly became a problem. In this work we investigate whether artificial intelligence working with chest X-ray (CXR) scans and clinical data can be used as a possible tool for the early identification of patients at risk of severe outcome, like intensive care or death. Indeed, further to induce lower radiation dose than computed tomography (CT), CXR is a simpler and faster radiological technique, being also more widespread. In this respect, we present three approaches that use features extracted from CXR images, either handcrafted or automatically learnt by convolutional neuronal networks, which are then integrated with the clinical data. As a further contribution, this work introduces a repository that collects data from 820 patients enrolled in six Italian hospitals in spring 2020 during the first COVID-19 emergency. The dataset includes CXR images, several clinical attributes and clinical outcomes. Exhaustive evaluation shows promising performance both in 10-fold and leave-one-centre-out cross-validation, suggesting that clinical data and images have the potential to provide useful information for the management of patients and hospital resources.",2021,10.1016/j.media.2021.102216,cross-sectional,prognosis,X-ray,Lung
AIR-Net: A novel multi-task learning method with auxiliary image reconstruction for predicting EGFR mutation status on CT images of NSCLC patients,"Automated and accurate EGFR mutation status prediction using computed tomography (CT) imagery is of great value for tailoring optimal treatments to non-small cell lung cancer (NSCLC) patients. However, existing deep learning based methods usually adopt a single task learning strategy to design and train EGFR mutation status prediction models with limited training data, which may be insufficient to learn distinguishable representations for promoting prediction performance. In this paper, a novel multi-task learning method named AIR-Net is proposed to precisely predict EGFR mutation status on CT images. First, an auxiliary image reconstruction task is effectively integrated with EGFR mutation status prediction, aiming at providing extra supervision at the training phase. Particularly, we adequately employ multi-level information in a shared encoder to generate more comprehensive representations of tumors. Second, a powerful feature consistency loss is further introduced to constrain semantic consistency of original and reconstructed images, which contributes to enhanced image reconstruction and offers more effective regularization to AIR-Net during training. Performance analysis of AIR-Net indicates that auxiliary image reconstruction plays an essential role in identifying EGFR mutation status. Furthermore, extensive experimental results demonstrate that our method achieves favorable performance against other competitive prediction methods. All the results executed in this study suggest that the effectiveness and superiority of AIR-Net in precisely predicting EGFR mutation status of NSCLC.",2022,10.1016/j.compbiomed.2021.105157,cross-sectional,diagnosis,CT,Lung
ALK molecular phenotype in non-small cell lung cancer: CT radiogenomic characterization,"PURPOSE: To present a radiogenomic computed tomographic (CT) characterization of anaplastic lymphoma kinase (ALK)-rearranged non-small cell lung cancer (NSCLC) (ALK+). MATERIALS AND METHODS: In this HIPAA-compliant institutional review board-approved retrospective study, CT studies, ALK status, and clinical-pathologic data in 172 patients with NSCLC from three institutions were analyzed. A screen of 24 CT image traits was performed in a training set of 59 patients, followed by random forest variable selection incorporating 24 CT traits plus six clinical-pathologic covariates to identify a radiogenomic predictor of ALK+ status. This predictor was then validated in an independent cohort (n = 113). Test-for-accuracy and subset analyses were performed. A similar analysis was performed to identify a biomarker associated with shorter progression-free survival (PFS) after therapy with the ALK inhibitor crizotinib. RESULTS: ALK+ status was associated with central tumor location, absence of pleural tail, and large pleural effusion. An ALK+ radiogenomic CT status biomarker consisting of these three imaging traits with patient age of younger than 60 years showed strong discriminatory power for ALK+ status, with a sensitivity of 83.3% (15 of 18), a specificity of 77.9% (74 of 95), and an accuracy of 78.8% (89 of 113) in independent testing. The discriminatory power was particularly strong in patients with operable disease (stage IIIA or lower), with a sensitivity of 100.0% (five of five), a specificity of 88.1% (37 of 42), and an accuracy of 89.4% (42 of 47). Tumors with a disorganized vessel pattern had a shorter PFS with crizotinib therapy than tumors without this trait (11.4 vs 20.2 months, P = .041). CONCLUSION: ALK+ NSCLC has distinct characteristics at CT imaging that, when combined with clinical covariates, discriminate ALK+ from non-ALK tumors and can potentially identify patients with a shorter durable response to crizotinib.",2014,10.1148/radiol.14140789,cross-sectional,prognosis,CT,Lung
Alterations of the Human Lung and Gut Microbiomes in Non-Small Cell Lung Carcinomas and Distant Metastasis,"Non-small cell lung cancer (NSCLC) is the leading cause of cancer-related deaths worldwide. Although dysbiosis of the lung and gut microbiota have been associated with NSCLC, their relative contributions are unclear; in addition, their roles in distant metastasis (DM) are still illusive. We recruited in total 121 participants, including 87 newly diagnosed treatment-naive NSCLC patients of various stages and 34 healthy volunteers, and surveyed their fecal and sputum microbiota. We compared the microbial profiles between groups, identified microbial biomarkers, and generated machine learning models for distinguishing healthy individuals from patients with NSCLC and patients of various stages. We found significant perturbations of gut and sputum microbiota in patients with NSCLC and DM. A machine learning model combining both microbiota (combined model) performed better than an individual data set in patient stratification, with the highest area under the curve (AUC) value of 0.896. Sputum and gut microbiota both contributed to the combined model; in most cases, sputum-only models performed similar to the combined models. Several microbial biomarkers were shared by both microbiotas, indicating their similar roles at distinct body sites. Microbial biomarkers of distinct disease stages were mostly shared, suggesting biomarkers for DM could be acquired early. Furthermore, Pseudomonas aeruginosa, a species previously associated with wound infections, was significantly more abundant in brain metastasis, indicating that distinct types of DMs could have different microbes. Our results indicate that alterations of the sputum microbiota have stronger relationships with NSCLC and DM than the gut and strongly support the feasibility of metagenome-based noninvasive disease diagnosis and risk evaluation. (This study has been registered at ClinicalTrials.gov under registration no. NCT03454685). IMPORTANCE Our survey on gut and sputum microbiota revealed that both were significantly disturbed in non-small cell lung cancer (NSCLC) and associated with distant metastasis (DM) while only the sputum microbiota was associated with non-DM NSCLC. The lung microbiota could therefore have a stronger association with (and thus may contribute more to) disease development than the gut microbiota. Mathematic models using both microbiotas performed better in patient stratification than using individual microbiota. Sputum models, however, performed similar to the combined models, suggesting a convenient, noninvasive diagnostic for NSCLC. Microbial biomarkers of distinct disease stages were mostly shared, suggesting that the same set of microbes were underlying disease progression, and the signals for distant metastasis could be acquired at early stages of the disease. Our results strongly support the feasibility of noninvasive diagnosis of NSCLC, including distant metastasis, are of clinical importance, and should warrant further research on the underlying molecular mechanisms.",2021,10.1128/Spectrum.00802-21,,,,
An [18F]FDG-PET/CT deep learning method for fully automated detection of pathological mediastinal lymph nodes in lung cancer patients,"PURPOSE: The identification of pathological mediastinal lymph nodes is an important step in the staging of lung cancer, with the presence of metastases significantly affecting survival rates. Nodes are currently identified by a physician, but this process is time-consuming and prone to errors. In this paper, we investigate the use of artificial intelligence-based methods to increase the accuracy and consistency of this process. METHODS: Whole-body (18)F-labelled fluoro-2-deoxyglucose ([18F]FDG) positron emission tomography/computed tomography ([18F]FDG-PET/CT) scans (Philips Gemini TF) from 134 patients were retrospectively analysed. The thorax was automatically located, and then slices were fed into a U-Net to identify candidate regions. These regions were split into overlapping 3D cubes, which were individually predicted as positive or negative using a 3D CNN. From these predictions, pathological mediastinal nodes could be identified. A second cohort of 71 patients was then acquired from a different, newer scanner (GE Discovery MI), and the performance of the model on this dataset was tested with and without transfer learning. RESULTS: On the test set from the first scanner, our model achieved a sensitivity of 0.87 (95% confidence intervals [0.74, 0.94]) with 0.41 [0.22, 0.71] false positives/patient. This was comparable to the performance of an expert. Without transfer learning, on the test set from the second scanner, the corresponding results were 0.53 [0.35, 0.70] and 0.24 [0.10, 0.49], respectively. With transfer learning, these metrics were 0.88 [0.73, 0.97] and 0.69 [0.43, 1.04], respectively. CONCLUSION: Model performance was comparable to that of an expert on data from the same scanner. With transfer learning, the model can be applied to data from a different scanner. To our knowledge it is the first study of its kind to go directly from whole-body [18F]FDG-PET/CT scans to pathological mediastinal lymph node localisation.",2022,10.1007/s00259-021-05513-x,cross-sectional,diagnosis,PET/CT,Lung
An adaptive pulmonary nodule detection algorithm,"Recently, lung cancer has been paid more and more attention. People have reached a consensus that early detection and early treatment can improve the survival rate of patients. Among them, pulmonary nodules are the important reference for doctors to determine the lung health. With the continuous improvement of CT image resolution, more suspected pulmonary nodule information appears from the impact of chest CT. How to relatively and accurately locate the suspected nodule location from a large number of CT images has brought challenges to the doctor's daily diagnosis. To solve the problem that the original DBSCAN clustering algorithm needs manual setting of the threshold, this paper proposes a region growing algorithm and an adaptive DBSCAN clustering algorithm to improve the accuracy of pulmonary nodule detection. The image is roughly processed and ROI (Regions of Interest) region is roughly extracted by CLAHE transform. The region growing algorithm is used to roughly process the adjacent region's expansibility and the suspected region in ROI, and mark the center point in the region and the boundary point of its point set. The mean value of region range is taken as the threshold value of DBSCAN clustering algorithm. The center of the point domain is used as the starting point of clustering, and the rough set of points is used as the MinPts threshold. Finally, the clustering results are labeled in the initial CT image. Experiments show that the pulmonary nodule detection method proposed in this paper effectively improves the accuracy of the detection results.",2020,10.3233/xst-200656,cross-sectional,diagnosis,CT,Lung
An aggregate method for thorax diseases classification,"A common problem found in real-word medical image classification is the inherent imbalance of the positive and negative patterns in the dataset where positive patterns are usually rare. Moreover, in the classification of multiple classes with neural network, a training pattern is treated as a positive pattern in one output node and negative in all the remaining output nodes. In this paper, the weights of a training pattern in the loss function are designed based not only on the number of the training patterns in the class but also on the different nodes where one of them treats this training pattern as positive and the others treat it as negative. We propose a combined approach of weights calculation algorithm for deep network training and the training optimization from the state-of-the-art deep network architecture for thorax diseases classification problem. Experimental results on the Chest X-Ray image dataset demonstrate that this new weighting scheme improves classification performances, also the training optimization from the EfficientNet improves the performance furthermore. We compare the aggregate method with several performances from the previous study of thorax diseases classifications to provide the fair comparisons against the proposed method.",2021,10.1038/s41598-021-81765-9,cross-sectional,diagnosis,X-ray,Thorax
An analysis of the development of digital health technologies to fight COVID-19 in Brazil and the world,"The coronavirus pandemic that struck the world in late 2019 continues to break records of new cases and deaths from the disease. Guidelines for clinical management of infected patients and prevention of new cases focus on measures to control symptoms, hygiene habits, social distancing, and decrease in human crowding. This forced a change in the way health services provide care, generating the incorporation of new health technologies. The Essay thus aims to compile and analyze experiences in the use of digital health technologies to minimize the impacts of COVID-19. The authors identified the development of technological solutions for clinical management of patients, imaging diagnosis, use of artificial intelligence for risk analysis, geolocation apps, data analysis and reports, self-diagnosis, and even orientation for decision-making. The great majority of the initiatives listed here prove effective in minimizing the impacts of COVID-19 on health systems and aim to decrease human crowding and thus facilitate access to services, besides contributing to the incorporation of new health practices and modes of care.",2021,10.1590/0102-311x00243220,,,,
An annotation-free whole-slide training approach to pathological classification of lung cancer types using deep learning,"Deep learning for digital pathology is hindered by the extremely high spatial resolution of whole-slide images (WSIs). Most studies have employed patch-based methods, which often require detailed annotation of image patches. This typically involves laborious free-hand contouring on WSIs. To alleviate the burden of such contouring and obtain benefits from scaling up training with numerous WSIs, we develop a method for training neural networks on entire WSIs using only slide-level diagnoses. Our method leverages the unified memory mechanism to overcome the memory constraint of compute accelerators. Experiments conducted on a data set of 9662 lung cancer WSIs reveal that the proposed method achieves areas under the receiver operating characteristic curve of 0.9594 and 0.9414 for adenocarcinoma and squamous cell carcinoma classification on the testing set, respectively. Furthermore, the method demonstrates higher classification performance than multiple-instance learning as well as strong localization results for small lesions through class activation mapping.",2021,10.1038/s41467-021-21467-y,,,,
An approach to the classification of COVID-19 based on CT scans using convolutional features and genetic algorithms,"COVID-19 is a respiratory disease that, as of July 15th, 2021, has infected more than 187 million people worldwide and is responsible for more than 4 million deaths. An accurate diagnosis of COVID-19 is essential for the treatment and control of the disease. The use of computed tomography (CT) has shown to be promising for evaluating patients suspected of COVID-19 infection. The analysis of a CT examination is complex, and requires attention from a specialist. This paper presents a methodology for detecting COVID-19 from CT images. We first propose a convolutional neural network architecture to extract features from CT images, and then optimize the hyperparameters of the network using a tree Parzen estimator to choose the best parameters. Following this, we apply a selection of features using a genetic algorithm. Finally, classification is performed using four classifiers with different approaches. The proposed methodology achieved an accuracy of 0.997, a kappa index of 0.995, an AUROC of 0.997, and an AUPRC of 0.997 on the SARS-CoV-2 CT-Scan dataset, and an accuracy of 0.987, a kappa index of 0.975, an AUROC of 0.989, and an AUPRC of 0.987 on the COVID-CT dataset, using our CNN after optimization of the hyperparameters, the selection of features and the multi-layer perceptron classifier. Compared with pretrained CNNs and related state-of-the-art works, the results achieved by the proposed methodology were superior. Our results show that the proposed method can assist specialists in screening and can aid in diagnosing patients with suspected COVID-19.",2021,10.1016/j.compbiomed.2021.104744,cross-sectional,diagnosis,CT,Lung
An Artificial Intelligence-Based Chest X-ray Model on Human Nodule Detection Accuracy From a Multicenter Study,"IMPORTANCE: Most early lung cancers present as pulmonary nodules on imaging, but these can be easily missed on chest radiographs. OBJECTIVE: To assess if a novel artificial intelligence (AI) algorithm can help detect pulmonary nodules on radiographs at different levels of detection difficulty. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included 100 posteroanterior chest radiograph images taken between 2000 and 2010 of adult patients from an ambulatory health care center in Germany and a lung image database in the US. Included images were selected to represent nodules with different levels of detection difficulties (from easy to difficult), and comprised both normal and nonnormal control. EXPOSURES: All images were processed with a novel AI algorithm, the AI Rad Companion Chest X-ray. Two thoracic radiologists established the ground truth and 9 test radiologists from Germany and the US independently reviewed all images in 2 sessions (unaided and AI-aided mode) with at least a 1-month washout period. MAIN OUTCOMES AND MEASURES: Each test radiologist recorded the presence of 5 findings (pulmonary nodules, atelectasis, consolidation, pneumothorax, and pleural effusion) and their level of confidence for detecting the individual finding on a scale of 1 to 10 (1 representing lowest confidence; 10, highest confidence). The analyzed metrics for nodules included sensitivity, specificity, accuracy, and receiver operating characteristics curve area under the curve (AUC). RESULTS: Images from 100 patients were included, with a mean (SD) age of 55 (20) years and including 64 men and 36 women. Mean detection accuracy across the 9 radiologists improved by 6.4% (95% CI, 2.3% to 10.6%) with AI-aided interpretation compared with unaided interpretation. Partial AUCs within the effective interval range of 0 to 0.2 false positive rate improved by 5.6% (95% CI, -1.4% to 12.0%) with AI-aided interpretation. Junior radiologists saw greater improvement in sensitivity for nodule detection with AI-aided interpretation as compared with their senior counterparts (12%; 95% CI, 4% to 19% vs 9%; 95% CI, 1% to 17%) while senior radiologists experienced similar improvement in specificity (4%; 95% CI, -2% to 9%) as compared with junior radiologists (4%; 95% CI, -3% to 5%). CONCLUSIONS AND RELEVANCE: In this diagnostic study, an AI algorithm was associated with improved detection of pulmonary nodules on chest radiographs compared with unaided interpretation for different levels of detection difficulty and for readers with different experience.",2021,10.1001/jamanetworkopen.2021.41096,cross-sectional,diagnosis,X-ray,Lung
An artificial neural network (ANN)-based lung-tumor motion predictor for intrafractional MR tumor tracking,"PURPOSE: To address practical issues of implementing artificial neural networks (ANN) for lung-tumor motion prediction in MRI-based intrafractional lung-tumor tracking. METHODS: A feedforward four-layered ANN structure is used to predict future tumor positions. A back-propagation algorithm is used for ANN learning. Adaptive learning is incorporated by continuously updating weights and learning rate during prediction. An ANN training scheme specific for MRI-based tracking is developed. A multiple-ANN structure is developed to reduce tracking failures caused by the lower imaging rates of MRI. We used particle swarm optimization to optimize the ANN structure and initial weights (IW) for each patient and treatment fraction. Prediction accuracy is evaluated using the 1D superior-inferior lung-tumor motions of 29 lung cancer patients for system delays of 120-520 ms, in increments of 80 ms. The result is compared with four different scenarios: (1), (2) ANN structure optimization + with∕without IW optimization, and (3), (4) no ANN structure optimization + with∕without IW optimization, respectively. An additional simulation is performed to assess the value of optimizing the ANN structure for each treatment fraction. RESULTS: For 120-520 ms system delays, mean RMSE values (ranges 0.0-2.8 mm from 29 patients) of 0.5-0.9 mm are observed, respectively. Using patient specific ANN structures, a 30%-60% decrease in mean RMSE values is observed as a result of IW optimization, alone. No significant advantages in prediction performance are observed, however, by optimizing for each fraction. CONCLUSIONS: A new ANN-based lung-tumor motion predictor is developed for MRI-based intrafractional tumor tracking. The prediction accuracy of our predictor is evaluated using a realistic simulated MR imaging rate and system delays. For 120-520 ms system delays, mean RMSE values of 0.5-0.9 mm (ranges 0.0-2.8 mm from 29 patients) are achieved. Further, the advantage of patient specific ANN structure and IW in lung-tumor motion prediction is demonstrated by a 30%-60% decrease in mean RMSE values.",2012,10.1118/1.4730294,cross-sectional,treatment,MRI,Lung
An artificial-intelligence lung imaging analysis system (ALIAS) for population-based nodule computing in CT scans,"Computed tomography (CT) screening is essential for early lung cancer detection. With the development of artificial intelligence techniques, it is particularly desirable to explore the ability of current state-of-the-art methods and to analyze nodule features in terms of a large population. In this paper, we present an artificial-intelligence lung image analysis system (ALIAS) for nodule detection and segmentation. And after segmenting the nodules, the locations, sizes, as well as imaging features are computed at the population level for studying the differences between benign and malignant nodules. The results provide better understanding of the underlying imaging features and their ability for early lung cancer diagnosis.",2021,10.1016/j.compmedimag.2021.101899,cross-sectional,diagnosis,CT,Lung
An Assisted Diagnosis System for Detection of Early Pulmonary Nodule in Computed Tomography Images,"Lung cancer is still the most concerned disease around the world. Lung nodule generates in the pulmonary parenchyma which indicates the latent risk of lung cancer. Computer-aided pulmonary nodules detection system is necessary, which can reduce diagnosis time and decrease mortality of patients. In this study, we have proposed a new computer aided diagnosis (CAD) system for detection of early pulmonary nodule, which can help radiologists quickly locate suspected nodules and make judgments. This system consists of four main sections: pulmonary parenchyma segmentation, nodule candidate detection, features extraction (total 22 features) and nodule classification. The publicly available data set created by the Lung Image Database Consortium (LIDC) is used for training and testing. This study selects 6400 slices from 80 CT scans containing totally 978 nodules, which is labeled by four radiologists. Through a fast segmentation method proposed in this paper, pulmonary nodules including 888 true nodules and 11,379 false positive nodules are segmented. By means of an ensemble classifier, Random Forest (RF), this study acquires 93.2, 92.4, 94.8, 97.6% of accuracy, sensitivity, specificity, area under the curve (AUC), respectively. Compared with support vector machine (SVM) classifier, RF can reduce more false positive nodules and acquire larger AUC. With the help of this CAD system, radiologist can be provided with a great reference for pulmonary nodule diagnosis timely.",2017,10.1007/s10916-016-0669-0,,,,
An automated COVID-19 detection based on fused dynamic exemplar pyramid feature extraction and hybrid feature selection using deep learning,"The new coronavirus disease known as COVID-19 is currently a pandemic that is spread out the whole world. Several methods have been presented to detect COVID-19 disease. Computer vision methods have been widely utilized to detect COVID-19 by using chest X-ray and computed tomography (CT) images. This work introduces a model for the automatic detection of COVID-19 using CT images. A novel handcrafted feature generation technique and a hybrid feature selector are used together to achieve better performance. The primary goal of the proposed framework is to achieve a higher classification accuracy than convolutional neural networks (CNN) using handcrafted features of the CT images. In the proposed framework, there are four fundamental phases, which are preprocessing, fused dynamic sized exemplars based pyramid feature generation, ReliefF, and iterative neighborhood component analysis based feature selection and deep neural network classifier. In the preprocessing phase, CT images are converted into 2D matrices and resized to 256 × 256 sized images. The proposed feature generation network uses dynamic-sized exemplars and pyramid structures together. Two basic feature generation functions are used to extract statistical and textural features. The selected most informative features are forwarded to artificial neural networks (ANN) and deep neural network (DNN) for classification. ANN and DNN models achieved 94.10% and 95.84% classification accuracies respectively. The proposed fused feature generator and iterative hybrid feature selector achieved the best success rate, according to the results obtained by using CT images.",2021,10.1016/j.compbiomed.2021.104356,cross-sectional,diagnosis,CT,Lung
An automated diagnosis and classification of COVID-19 from chest CT images using a transfer learning-based convolutional neural network,"Researchers have developed more intelligent, highly responsive, and efficient detection methods owing to the COVID-19 demands for more widespread diagnosis. The work done deals with developing an AI-based framework that can help radiologists and other healthcare professionals diagnose COVID-19 cases at a high level of accuracy. However, in the absence of publicly available CT datasets, the development of such AI tools can prove challenging. Therefore, an algorithm for performing automatic and accurate COVID-19 classification using Convolutional Neural Network (CNN), pre-trained model, and Sparrow search algorithm (SSA) on CT lung images was proposed. The pre-trained CNN models used are SeresNext50, SeresNext101, SeNet154, MobileNet, MobileNetV2, MobileNetV3Small, and MobileNetV3Large. In addition, the SSA will be used to optimize the different CNN and transfer learning(TL) hyperparameters to find the best configuration for the pre-trained model used and enhance its performance. Two datasets are used in the experiments. There are two classes in the first dataset, while three in the second. The authors combined two publicly available COVID-19 datasets as the first dataset, namely the COVID-19 Lung CT Scans and COVID-19 CT Scan Dataset. In total, 14,486 images were included in this study. The authors analyzed the Large COVID-19 CT scan slice dataset in the second dataset, which utilized 17,104 images. Compared to other pre-trained models on both classes datasets, MobileNetV3Large pre-trained is the best model. As far as the three-classes dataset is concerned, a model trained on SeNet154 is the best available. Results show that, when compared to other CNN models like LeNet-5 CNN, COVID faster R-CNN, Light CNN, Fuzzy + CNN, Dynamic CNN, CNN and Optimized CNN, the proposed Framework achieves the best accuracy of 99.74% (two classes) and 98% (three classes).",2022,10.1016/j.compbiomed.2022.105383,cross-sectional,diagnosis,CT,Lung
An Automatic Detection System of Lung Nodule Based on Multigroup Patch-Based Deep Learning Network,"High-efficiency lung nodule detection dramatically contributes to the risk assessment of lung cancer. It is a significant and challenging task to quickly locate the exact positions of lung nodules. Extensive work has been done by researchers around this domain for approximately two decades. However, previous computer-aided detection (CADe) schemes are mostly intricate and time-consuming since they may require more image processing modules, such as the computed tomography image transformation, the lung nodule segmentation, and the feature extraction, to construct a whole CADe system. It is difficult for these schemes to process and analyze enormous data when the medical images continue to increase. Besides, some state of the art deep learning schemes may be strict in the standard of database. This study proposes an effective lung nodule detection scheme based on multigroup patches cut out from the lung images, which are enhanced by the Frangi filter. Through combining two groups of images, a four-channel convolution neural networks model is designed to learn the knowledge of radiologists for detecting nodules of four levels. This CADe scheme can acquire the sensitivity of 80.06% with 4.7 false positives per scan and the sensitivity of 94% with 15.1 false positives per scan. The results demonstrate that the multigroup patch-based learning system is efficient to improve the performance of lung nodule detection and greatly reduce the false positives under a huge amount of image data.",2018,10.1109/jbhi.2017.2725903,cross-sectional,diagnosis,CT,Lung
An effective approach for CT lung segmentation using mask region-based convolutional neural networks,"Computer vision systems have numerous tools to assist in various medical fields, notably in image diagnosis. Computed tomography (CT) is the principal imaging method used to assist in the diagnosis of diseases such as bone fractures, lung cancer, heart disease, and emphysema, among others. Lung cancer is one of the four main causes of death in the world. The lung regions in the CT images are marked manually by a specialist as this initial step is a significant challenge for computer vision techniques. Once defined, the lung regions are segmented for clinical diagnoses. This work proposes an automatic segmentation of the lungs in CT images, using the Convolutional Neural Network (CNN) Mask R-CNN, to specialize the model for lung region mapping, combined with supervised and unsupervised machine learning methods (Bayes, Support Vectors Machine (SVM), K-means and Gaussian Mixture Models (GMMs)). Our approach using Mask R-CNN with the K-means kernel produced the best results for lung segmentation reaching an accuracy of 97.68 ± 3.42% and an average runtime of 11.2 s. We compared our results against other works for validation purposes, and our approach had the highest accuracy and was faster than some state-of-the-art methods.",2020,10.1016/j.artmed.2020.101792,cross-sectional,diagnosis,CT,Lung
An Efficient Deep Learning Model to Detect COVID-19 Using Chest X-ray Images,"The tragic pandemic of COVID-19, due to the Severe Acute Respiratory Syndrome coronavirus-2 or SARS-CoV-2, has shaken the entire world, and has significantly disrupted healthcare systems in many countries. Because of the existing challenges and controversies to testing for COVID-19, improved and cost-effective methods are needed to detect the disease. For this purpose, machine learning (ML) has emerged as a strong forecasting method for detecting COVID-19 from chest X-ray images. In this paper, we used a Deep Learning Method (DLM) to detect COVID-19 using chest X-ray (CXR) images. Radiographic images are readily available and can be used effectively for COVID-19 detection compared to other expensive and time-consuming pathological tests. We used a dataset of 10,040 samples, of which 2143 had COVID-19, 3674 had pneumonia (but not COVID-19), and 4223 were normal (not COVID-19 or pneumonia). Our model had a detection accuracy of 96.43% and a sensitivity of 93.68%. The area under the ROC curve was 99% for COVID-19, 97% for pneumonia (but not COVID-19 positive), and 98% for normal cases. In conclusion, ML approaches may be used for rapid analysis of CXR images and thus enable radiologists to filter potential candidates in a time-effective manner to detect COVID-19.",2022,10.3390/ijerph19042013,cross-sectional,diagnosis,X-ray,Lung
An Efficient Method for Coronavirus Detection Through X-rays Using Deep Neural Network,"BACKGROUND: Coronavirus (COVID-19) is a group of infectious diseases caused by related viruses called coronaviruses. In humans, the seriousness of infection caused by a coronavirus in the respiratory tract can vary from mild to lethal. A serious illness can be developed in old people and those with underlying medical problems like diabetes, cardiovascular disease, cancer, and chronic respiratory disease. For the diagnosis of coronavirus disease, due to the growing number of cases, a limited number of test kits for COVID-19 are available in the hospitals. Hence, it is important to implement an automated system as an immediate alternative diagnostic option to pause the spread of COVID-19 in the population. OBJECTIVE: This paper proposes a deep learning model for the classification of coronavirus infected patient detection using chest X-ray radiographs. METHODS: A fully connected convolutional neural network model is developed to classify healthy and diseased X-ray radiographs. The proposed neural network model consists of seven convolutional layers with the rectified linear unit, softmax (last layer) activation functions, and max-pooling layers which were trained using the publicly available COVID-19 dataset. RESULTS AND CONCLUSION: For validation of the proposed model, the publicly available chest X-ray radiograph dataset consisting of COVID-19 and normal patient's images were used. Considering the performance of the results that are evaluated based on various evaluation metrics such as precision, recall, MSE, RMSE and accuracy, it is seen that the accuracy of the proposed CNN model is 98.07%.",2022,10.2174/1573405617999210112193220,cross-sectional,diagnosis,X-ray,Lung
An efficient mixture of deep and machine learning models for COVID-19 diagnosis in chest X-ray images,"A newly emerged coronavirus (COVID-19) seriously threatens human life and health worldwide. In coping and fighting against COVID-19, the most critical step is to effectively screen and diagnose infected patients. Among them, chest X-ray imaging technology is a valuable imaging diagnosis method. The use of computer-aided diagnosis to screen X-ray images of COVID-19 cases can provide experts with auxiliary diagnosis suggestions, which can reduce the burden of experts to a certain extent. In this study, we first used conventional transfer learning methods, using five pre-trained deep learning models, which the Xception model showed a relatively ideal effect, and the diagnostic accuracy reached 96.75%. In order to further improve the diagnostic accuracy, we propose an efficient diagnostic method that uses a combination of deep features and machine learning classification. It implements an end-to-end diagnostic model. The proposed method was tested on two datasets and performed exceptionally well on both of them. We first evaluated the model on 1102 chest X-ray images. The experimental results show that the diagnostic accuracy of Xception + SVM is as high as 99.33%. Compared with the baseline Xception model, the diagnostic accuracy is improved by 2.58%. The sensitivity, specificity and AUC of this model reached 99.27%, 99.38% and 99.32%, respectively. To further illustrate the robustness of our method, we also tested our proposed model on another dataset. Finally also achieved good results. Compared with related research, our proposed method has higher classification accuracy and efficient diagnostic performance. Overall, the proposed method substantially advances the current radiology based methodology, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis and follow-up of COVID-19 cases.",2020,10.1371/journal.pone.0242535,cross-sectional,diagnosis,X-ray,Lung
An Embedded Multi-branch 3D Convolution Neural Network for False Positive Reduction in Lung Nodule Detection,"Numerous lung nodule candidates can be produced through an automated lung nodule detection system. Classifying these candidates to reduce false positives is an important step in the detection process. The objective during this paper is to predict real nodules from a large number of pulmonary nodule candidates. Facing the challenge of the classification task, we propose a novel 3D convolution neural network (CNN) to reduce false positives in lung nodule detection. The novel 3D CNN includes embedded multiple branches in its structure. Each branch processes a feature map from a layer with different depths. All of these branches are cascaded at their ends; thus, features from different depth layers are combined to predict the categories of candidates. The proposed method obtains a competitive score in lung nodule candidate classification on LUNA16 dataset with an accuracy of 0.9783, a sensitivity of 0.8771, a precision of 0.9426, and a specificity of 0.9925. Moreover, a good performance on the competition performance metric (CPM) is also obtained with a score of 0.830. As a 3D CNN, the proposed model can learn complete and three-dimensional discriminative information about nodules and non-nodules to avoid some misidentification problems caused due to lack of spatial correlation information extracted from traditional methods or 2D networks. As an embedded multi-branch structure, the model is also more effective in recognizing the nodules of various shapes and sizes. As a result, the proposed method gains a competitive score on the false positive reduction in lung nodule detection and can be used as a reference for classifying nodule candidates.",2020,10.1007/s10278-020-00326-0,cross-sectional,diagnosis,CT,Lung
An ensemble learning approach to digital corona virus preliminary screening from cough sounds,"This work develops a robust classifier for a COVID-19 pre-screening model from crowdsourced cough sound data. The crowdsourced cough recordings contain a variable number of coughs, with some input sound files more informative than the others. Accurate detection of COVID-19 from the sound datasets requires overcoming two main challenges (i) the variable number of coughs in each recording and (ii) the low number of COVID-positive cases compared to healthy coughs in the data. We use two open datasets of crowdsourced cough recordings and segment each cough recording into non-overlapping coughs. The segmentation enriches the original data without oversampling by splitting the original cough sound files into non-overlapping segments. Splitting the sound files enables us to increase the samples of the minority class (COVID-19) without changing the feature distribution of the COVID-19 samples resulted from applying oversampling techniques. Each cough sound segment is transformed into six image representations for further analyses. We conduct extensive experiments with shallow machine learning, Convolutional Neural Network (CNN), and pre-trained CNN models. The results of our models were compared to other recently published papers that apply machine learning to cough sound data for COVID-19 detection. Our method demonstrated a high performance using an ensemble model on the testing dataset with area under receiver operating characteristics curve = 0.77, precision = 0.80, recall = 0.71, F1 measure = 0.75, and Kappa = 0.53. The results show an improvement in the prediction accuracy of our COVID-19 pre-screening model compared to the other models.",2021,10.1038/s41598-021-95042-2,,,,
An ensemble learning method based on ordinal regression for COVID-19 diagnosis from chest CT,"Coronavirus disease 2019 (COVID-19) has brought huge losses to the world, and it remains a great threat to public health. X-ray computed tomography (CT) plays a central role in the management of COVID-19. Traditional diagnosis with pulmonary CT images is time-consuming and error-prone, which could not meet the need for precise and rapid COVID-19 screening. Nowadays, deep learning (DL) has been successfully applied to CT image analysis, which assists radiologists in workflow scheduling and treatment planning for patients with COVID-19. Traditional methods use cross-entropy as the loss function with a Softmax classifier following a fully-connected layer. Most DL-based classification methods target intraclass relationships in a certain class (early, progressive, severe, or dissipative phases), ignoring the natural order of different phases of the disease progression,i.e.,from an early stage and progress to a late stage. To learn both intraclass and interclass relationships among different stages and improve the accuracy of classification, this paper proposes an ensemble learning method based on ordinal regression, which leverages the ordinal information on COVID-19 phases. The proposed method uses multi-binary, neuron stick-breaking (NSB), and soft labels (SL) techniques, and ensembles the ordinal outputs through a median selection. To evaluate our method, we collected 172 confirmed cases. In a 2-fold cross-validation experiment, the accuracy is increased by 22% compared with traditional methods when we use modified ResNet-18 as the backbone. And precision, recall, andF1-score are also improved. The experimental results show that our proposed method achieves a better classification performance than the traditional methods, which helps establish guidelines for the classification of COVID-19 chest CT images.",2021,10.1088/1361-6560/ac34b2,cross-sectional,diagnosis,CT,Lung
An ensemble of neural networks provides expert-level prenatal detection of complex congenital heart disease,"Congenital heart disease (CHD) is the most common birth defect. Fetal screening ultrasound provides five views of the heart that together can detect 90% of complex CHD, but in practice, sensitivity is as low as 30%. Here, using 107,823 images from 1,326 retrospective echocardiograms and screening ultrasounds from 18- to 24-week fetuses, we trained an ensemble of neural networks to identify recommended cardiac views and distinguish between normal hearts and complex CHD. We also used segmentation models to calculate standard fetal cardiothoracic measurements. In an internal test set of 4,108 fetal surveys (0.9% CHD, >4.4 million images), the model achieved an area under the curve (AUC) of 0.99, 95% sensitivity (95% confidence interval (CI), 84-99%), 96% specificity (95% CI, 95-97%) and 100% negative predictive value in distinguishing normal from abnormal hearts. Model sensitivity was comparable to that of clinicians and remained robust on outside-hospital and lower-quality images. The model's decisions were based on clinically relevant features. Cardiac measurements correlated with reported measures for normal and abnormal hearts. Applied to guideline-recommended imaging, ensemble learning models could significantly improve detection of fetal CHD, a critical and global diagnostic challenge.",2021,10.1038/s41591-021-01342-5,cross-sectional,diagnosis,Echocardiograph images,Heart 
An explainable AI system for automated COVID-19 assessment and lesion categorization from CT-scans,"COVID-19 infection caused by SARS-CoV-2 pathogen has been a catastrophic pandemic outbreak all over the world, with exponential increasing of confirmed cases and, unfortunately, deaths. In this work we propose an AI-powered pipeline, based on the deep-learning paradigm, for automated COVID-19 detection and lesion categorization from CT scans. We first propose a new segmentation module aimed at automatically identifying lung parenchyma and lobes. Next, we combine the segmentation network with classification networks for COVID-19 identification and lesion categorization. We compare the model's classification results with those obtained by three expert radiologists on a dataset of 166 CT scans. Results showed a sensitivity of 90.3% and a specificity of 93.5% for COVID-19 detection, at least on par with those yielded by the expert radiologists, and an average lesion categorization accuracy of about 84%. Moreover, a significant role is played by prior lung and lobe segmentation, that allowed us to enhance classification performance by over 6 percent points. The interpretation of the trained AI models reveals that the most significant areas for supporting the decision on COVID-19 identification are consistent with the lesions clinically associated to the virus, i.e., crazy paving, consolidation and ground glass. This means that the artificial models are able to discriminate a positive patient from a negative one (both controls and patients with interstitial pneumonia tested negative to COVID) by evaluating the presence of those lesions into CT scans. Finally, the AI models are integrated into a user-friendly GUI to support AI explainability for radiologists, which is publicly available at http://perceivelab.com/covid-ai. The whole AI system is unique since, to the best of our knowledge, it is the first AI-based software, publicly available, that attempts to explain to radiologists what information is used by AI methods for making decisions and that proactively involves them in the decision loop to further improve the COVID-19 understanding.",2021,10.1016/j.artmed.2021.102114,cross-sectional,diagnosis,CT,Lung
An image-based deep learning framework for individualizing radiotherapy dose,"BACKGROUND: Radiotherapy continues to be delivered uniformly without consideration of individual tumor characteristics. To advance toward more precise treatments in radiotherapy, we queried the lung computed tomography (CT)-derived feature space to identify radiation sensitivity parameters that can predict treatment failure and hence guide the individualization of radiotherapy dose. METHODS: We used a cohort-based registry of 849 patients with cancer in the lung treated with high dose radiotherapy using stereotactic body radiotherapy. We input pre-therapy lung CT images into a multi-task deep neural network, Deep Profiler, to generate an image fingerprint that primarily predicts time to event treatment outcomes and secondarily approximates classical radiomic features. We validated our findings in an independent study population (n = 95). Deep Profiler was combined with clinical variables to derive iGray, an individualized dose that estimates treatment failure probability to be <5%. FINDINGS: Radiation treatments in patients with high Deep Profiler scores fail at a significantly higher rate than in those with low scores. The 3-year cumulative incidences of local failure were 20.3% (95% CI: 16.0-24.9) and 5.7% (95% CI: 3.5-8.8), respectively. Deep Profiler independently predicted local failure (hazard ratio 1.65, 95% 1.02-2.66, p = 0.04). Models that included Deep Profiler and clinical variables predicted treatment failures with a concordance index of 0.72 (95% CI: 0.67-0.77), a significant improvement compared to classical radiomics or clinical variables alone (p = <0.001 and <0.001, respectively). Deep Profiler performed well in an external study population (n = 95), accurately predicting treatment failures across diverse clinical settings and CT scanner types (concordance index = 0.77 [95% CI: 0.69-0.92]). iGray had a wide dose range (21.1-277 Gy, BED), suggested dose reductions in 23.3% of patients and can be safely delivered in the majority of cases. INTERPRETATION: Our results indicate that there are image-distinct subpopulations that have differential sensitivity to radiotherapy. The image-based deep learning framework proposed herein is the first opportunity to use medical images to individualize radiotherapy dose.",2019,10.1016/s2589-7500(19)30058-5,cross-sectional,treatment,CT,Lung
An integrated autoencoder-based hybrid CNN-LSTM model for COVID-19 severity prediction from lung ultrasound,"The COVID-19 pandemic has become one of the biggest threats to the global healthcare system, creating an unprecedented condition worldwide. The necessity of rapid diagnosis calls for alternative methods to predict the condition of the patient, for which disease severity estimation on the basis of Lung Ultrasound (LUS) can be a safe, radiation-free, flexible, and favorable option. In this paper, a frame-based 4-score disease severity prediction architecture is proposed with the integration of deep convolutional and recurrent neural networks to consider both spatial and temporal features of the LUS frames. The proposed convolutional neural network (CNN) architecture implements an autoencoder network and separable convolutional branches fused with a modified DenseNet-201 network to build a vigorous, noise-free classification model. A five-fold cross-validation scheme is performed to affirm the efficacy of the proposed network. In-depth result analysis shows a promising improvement in the classification performance by introducing the Long Short-Term Memory (LSTM) layers after the proposed CNN architecture by an average of 7-12%, which is approximately 17% more than the traditional DenseNet architecture alone. From an extensive analysis, it is found that the proposed end-to-end scheme is very effective in detecting COVID-19 severity scores from LUS images.",2021,10.1016/j.compbiomed.2021.104296,cross-sectional,prognosis,Ultrasound,Lung
An investigation of CNN models for differentiating malignant from benign lesions using small pathologically proven datasets,"Cancer has been one of the most threatening diseases to human health. There have been many efforts devoted to the advancement of radiology and transformative tools (e.g. non-invasive computed tomographic or CT imaging) to detect cancer in early stages. One of the major goals is to identify malignant from benign lesions. In recent years, machine deep learning (DL), e.g. convolutional neural network (CNN), has shown encouraging classification performance on medical images. However, DL algorithms always need large datasets with ground truth. Yet in the medical imaging field, especially for cancer imaging, it is difficult to collect such large volume of images with pathological information. Therefore, strategies are needed to learn effectively from small datasets via CNN models. To forward that goal, this paper explores two CNN models by focusing extensively on expansion of training samples from two small pathologically proven datasets (colorectal polyp dataset and lung nodule dataset) and then differentiating malignant from benign lesions. Experimental outcomes indicate that even in very small datasets of less than 70 subjects, malignance can be successfully differentiated from benign via the proposed CNN models, the average AUCs (area under the receiver operating curve) of differentiating colorectal polyps and pulmonary nodules are 0.86 and 0.71, respectively. Our experiments further demonstrate that for these two small datasets, instead of only studying the original raw CT images, feeding additional image features, such as the local binary pattern of the lesions, into the CNN models can significantly improve classification performance. In addition, we find that our explored voxel level CNN model has better performance when facing the small and unbalanced datasets.",2019,10.1016/j.compmedimag.2019.101645,,,,
An IoT Based Predictive Modelling for Predicting Lung Cancer Using Fuzzy Cluster Based Segmentation and Classification,"In this paper, we propose a new Internet of Things (IoT) based predictive modelling by using fuzzy cluster based augmentation and classification for predicting the lung cancer disease through continuous monitoring and also to improve the healthcare by providing medical instructions. Here, the fuzzy clustering method is used and which is based on transition region extraction for effective image segmentation. Moreover, Fuzzy C-Means Clustering algorithm is used to categorize the transitional region features from the feature of lung cancer image. In this work, Otsu thresholding method is used for extracting the transition region from lung cancer image. Moreover, the right edge image and the morphological thinning operation are used for enhancing the performance of segmentation. In addition, the morphological cleaning and the image region filling operations are performed over an edge lung cancer image for getting the object regions. In addition, we also propose a new incremental classification algorithm which combines the existing Association Rule Mining (ARM), the standard Decision Tree (DT) with temporal features and the CNN. The experiments have been conducted by using the standard images that are collected from database and the current health data which are collected from patient through IoT devices. The results proved that the performance of the proposed prediction model which is able to achieve the better accuracy when it is compared with other existing prediction model.",2018,10.1007/s10916-018-1139-7,,,,
An original deep learning model using limited data for COVID-19 discrimination: A multicenter study,"OBJECTIVES: Artificial intelligence (AI) has been proved to be a highly efficient tool for COVID-19 diagnosis, but the large data size and heavy label force required for algorithm development and the poor generalizability of AI algorithms, to some extent, limit the application of AI technology in clinical practice. The aim of this study is to develop an AI algorithm with high robustness using limited chest CT data for COVID-19 discrimination. METHODS: A three dimensional algorithm that combined multi-instance learning with the LSTM architecture (3DMTM) was developed for differentiating COVID-19 from community acquired pneumonia (CAP) while logistic regression (LR), k-nearest neighbor (KNN), support vector machine (SVM), and a three dimensional convolutional neural network set for comparison. Totally, 515 patients with or without COVID-19 between December 2019 and March 2020 from five different hospitals were recruited and divided into relatively large (150 COVID-19 and 183 CAP cases) and relatively small datasets (17 COVID-19 and 35 CAP cases) for either training or validation and another independent dataset (37 COVID-19 and 93 CAP cases) for external test. Area under the receiver operating characteristic curve (AUC), sensitivity, specificity, precision, accuracy, F1 score, and G-mean were utilized for performance evaluation. RESULTS: In the external test cohort, the relatively large data-based 3DMTM-LD achieved an AUC of 0.956 (95% confidence interval, 95% CI, 0.929∼0.982) with 86.2% and 98.0% for its sensitivity and specificity. 3DMTM-SD got an AUC of 0.937 (95% CI, 0.909∼0.965), while the AUC of 3DCM-SD decreased dramatically to 0.714 (95% CI, 0.649∼0.780) with training data reduction. KNN-MMSD, LR-MMSD, SVM-MMSD, and 3DCM-MMSD benefited significantly from the inclusion of clinical information while models trained with relatively large dataset got slight performance improvement in COVID-19 discrimination. 3DMTM, trained with either CT or multi-modal data, presented comparably excellent performance in COVID-19 discrimination. CONCLUSIONS: The 3DMTM algorithm presented excellent robustness for COVID-19 discrimination with limited CT data. 3DMTM based on CT data performed comparably in COVID-19 discrimination with that trained with multi-modal information. Clinical information could improve the performance of KNN, LR, SVM, and 3DCM in COVID-19 discrimination, especially in the scenario with limited data for training.",2022,10.1002/mp.15549,cross-sectional,diagnosis,CT,Lung
An Uncertainty-Aware Transfer Learning-Based Framework for COVID-19 Diagnosis,"The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries, and also, there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this article proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs), including VGG16, ResNet50, DenseNet121, and InceptionResNetV2, are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modeling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image data sets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC). Also, it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.",2021,10.1109/tnnls.2021.3054306,cross-sectional,diagnosis,CT and X-ray,Lung
An unsupervised multi-scale framework with attention-based network (MANet) for lung 4D-CT registration,"Deformable image registration (DIR) of 4D-CT is very important in many radiotherapeutic applications including tumor target definition, image fusion, dose accumulation and response evaluation. It is a challenging task to performing accurate and fast DIR of lung 4D-CT images due to its large and complicated deformations. In this study, we propose an unsupervised multi-scale DIR framework with attention-based network (MANet). Three cascaded models used for aligning CT images in different resolution levels were involved and trained by minimizing the loss functions, which were defined as the combination of dissimilarity between the fixed image and the deformed image and DVF regularization term. In addition, attention gates were incorporated into the three models to distinguish the moving structures from non-moving or minimal-moving structures during registration. The three models were trained sequentially and separately to minimize the loss function in each scale to initialize the MANet, and then trained jointly to minimize the total loss function which incorporated an additional dissimilarity between fixed image and deformed image. Besides, an adversarial network was integrated into MANet to enforce the DVF regularization by penalizing the unrealistic deformed images. The proposed MANet was evaluated on the public dir-lab dataset, and the target registration errors (TREs) of the model were compared with convention iterative optimization-based methods and three recently published deep learning-based methods. The initial results showed that the MANet with an average of TRE of 1.53 ± 1.02 mm outperformed other registration methods, and its execution time took about 1 s for DVF estimation with no requirement of manual-tuning for parameters, which demonstrating that our proposed method had the ability of performing superior registration for 4D-CT.",2021,10.1088/1361-6560/ac0afc,cross-sectional,others,4D-CT,Lung
Analysis of clinical features and imaging signs of COVID-19 with the assistance of artificial intelligence,"OBJECTIVE: To explore the CT imaging features/signs of patients with different clinical types of Coronavirus Disease 2019 (COVID-19) via the application of artificial intelligence (AI), thus improving the understanding of COVID-19. PANTIENTS AND METHODS: Clinical data and chest CT imaging features of 58 patients confirmed with COVID-19 in the Fifth Medical Center of PLA General Hospital were retrospectively analyzed. According to the Guidelines on Novel Coronavirus-Infected Pneumonia Diagnosis and Treatment (Provisional 6th Edition), COVID-19 patients were divided into mild type (7), common type (34), severe type (7) and critical type (10 patients). The CT imaging features of the patients with different clinical types of COVID-19 types were analyzed, and the volume percentage of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung was calculated with the use of AI software. SPSS 21.0 software was used for statistical analysis. RESULTS: Common clinical manifestations of COVID-19 patients: fever was found in 47 patients (81.0%), cough in 31 (53.4%) and weakness in 10 (17.2%). Laboratory examinations: normal or decreased white blood cell (WBC) counts were observed in 52 patients (89.7%), decreased lymphocyte counts (LCs) in 14 (24.1%) and increased C-reactive protein (CRP) levels in 18 (31.0%). CT imaging features: there were 48 patients (94.1%) with lesions distributed in both lungs and 46 patients (90.2%) had lesions most visible in the lower lungs; the primary manifestations in patients with common type COVID-19 were ground-glass opacities (GGOs) (23/34, 67.6%) or mixed type (17/34, 50.0%), with lesions mainly distributed in the periphery of the lungs (28/34, 82.4%); the primary manifestations of patients with severe/critical type COVID-19 were consolidations (13/17, 76.5%) or mixed type (14/17, 82.4%), with lesions distributed in both the peripheral and central areas of lungs (14/17,82.4%); other common signs, including pleural parallel signs, halo signs, vascular thickening signs, crazy-paving signs and air bronchogram signs, were visible in patients with different clinical types, and pleural effusion was found in 5 patients with severe/critical COVID-19. AI software was used to calculate the volume percentages of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung. There were significant differences in the volume percentages of pneumonia lesions for the superior lobe of the left lung, the inferior lobe of the left lung, the superior lobe of the right lung, the inferior lobe of the right lung and the whole lung among patients with different clinical types (p<0.05). The area under the ROC curve (AUC) of the volume percentage of pneumonia lesions for the whole lung for the diagnosis of severe/critical type COVID-19 was 0.740, with sensitivity and specificity of 91.2% and 58.8%, respectively. CONCLUSIONS: The clinical and CT imaging features of COVID-19 patients were characteristic to a certain degree; thus, the clinical course and severity of COVID-19 could be evaluated with a combination of an analysis of clinical features and CT imaging features and assistant diagnosis by AI software.",2020,10.26355/eurrev_202008_22510,cross-sectional,diagnosis,CT,Lung
Analysis of COVID-19 Infections on a CT Image Using DeepSense Model,"In this paper, a data mining model on a hybrid deep learning framework is designed to diagnose the medical conditions of patients infected with the coronavirus disease 2019 (COVID-19) virus. The hybrid deep learning model is designed as a combination of convolutional neural network (CNN) and recurrent neural network (RNN) and named as DeepSense method. It is designed as a series of layers to extract and classify the related features of COVID-19 infections from the lungs. The computerized tomography image is used as an input data, and hence, the classifier is designed to ease the process of classification on learning the multidimensional input data using the Expert Hidden layers. The validation of the model is conducted against the medical image datasets to predict the infections using deep learning classifiers. The results show that the DeepSense classifier offers accuracy in an improved manner than the conventional deep and machine learning classifiers. The proposed method is validated against three different datasets, where the training data are compared with 70%, 80%, and 90% training data. It specifically provides the quality of the diagnostic method adopted for the prediction of COVID-19 infections in a patient.",2020,10.3389/fpubh.2020.599550,cross-sectional,diagnosis,CT,Lung
Analysis of high-resolution reconstruction of medical images based on deep convolutional neural networks in lung cancer diagnostics,"BACKGROUND AND OBJECTIVE: To study the diagnostic effect of 64-slice spiral CT and MRI high-resolution images based on deep convolutional neural networks(CNN) in lung cancer. METHODS: In this paper, we Select 74 patients with highly suspected lung cancer who were treated in our hospital from January 2017 to January 2021 as the research objects. The enhanced 64-slice spiral CT and MRI were used to detect and diagnose respectively, and the images and accuracy of CT diagnosis and MRI diagnosis were retrospectively analyzed. RESULTS: The accuracy of CT diagnosis is 94.6% (70/74), and the accuracy of MRI diagnosis is 89.2% (66/74). CT examination has the advantages of non-invasive, convenient operation and fast examination. MRI is showing there are advantages in the relationship between the chest wall and the mediastinum, and the relationship between the lesion and the large blood vessels. CONCLUSION: Enhanced CT and MRI examinations based on convolutional neural networks(CNN) to improve image clarity have high application value in the diagnosis of lung cancer patients, but the focus of performance is different.",2022,10.1016/j.cmpb.2021.106592,cross-sectional,diagnosis,CT and MRI,Lung
Analysis of Inducing Factors of Chronic Pulmonary Heart Disease Caused by Chronic Obstructive Pulmonary Disease at High Altitude through Epidemiological Investigation under Intelligent Medicine and Big Data,"This study explores the risk factors of chronic pulmonary heart disease (CPHD) induced by plateau chronic obstructive pulmonary disease (COPD) based on intelligent medical treatment and big data of electrocardiogram (ECG) signal. Based on GPU, a wavelet algorithm is introduced to extract features of ECG signal, and it was combined with generalized regression neural network (GRNN) to improve classification accuracy. From June 2018 to December 2020, 10,185 patients diagnosed with COPD in the plateau area by pulmonary function testing, ECG, and chest X-ray at X Hospital are taken as the research objects to evaluate the distribution of CPHD incidence at different ages and altitudes. The running time of GTX780Ti is about 15 times shorter than that of CPU. The accuracy of N detection based on the GPU-accelerated neural network model reached 98.06%. Accuracy (Acc), sensitivity (Se), specificity (Sp), and positive rate (PR) of V were 99.03%, 89.17%, 98.92%, and 93.18%, respectively. The Acc, Se, Sp, and PR of S were 99.54%, 86.22%, 99.74%, and 92.56%, respectively. The GRNN classification accuracy was up to 98%. 19% of COPD patients were diagnosed with CPHD, including 1,409 males (72.82%) and 526 females (36.24%). The highest prevalence of CPHD was 64.60% when the altitude was 1,900-2,499 m, and the prevalence was only 2.43% when the altitude was ≥3,500 m. The highest prevalence of CPHD was 63.77% at the age of 61-70 years, and the lowest prevalence at the age of 15∼20 years was only 0.26%. Therefore, the GPU-based neural network model improved the classification accuracy of ECG signals. Age and altitude were risk factors for CPHD induced by high-altitude COPD, which provided a reference for the prevention, diagnosis, and treatment of CPHD in high-altitude areas.",2022,10.1155/2022/2612074,,,,
Analysis of segmentation of lung parenchyma based on deep learning methods,"Precise segmentation of lung parenchyma is essential for effective analysis of the lung. Due to the obvious contrast and large regional area compared to other tissues in the chest, lung tissue is less difficult to segment. Special attention to details of lung segmentation is also needed. To improve the quality and speed of segmentation of lung parenchyma based on computed tomography (CT) or computed tomography angiography (CTA) images, the 4th International Symposium on Image Computing and Digital Medicine (ISICDM 2020) provides interesting and valuable research ideas and approaches. For the work of lung parenchyma segmentation, 9 of the 12 participating teams used the U-Net network or its modified forms, and others used the methods to improve the segmentation accuracy include attention mechanism, multi-scale feature information fusion. Among them, U-Net achieves the best results including that the final dice coefficient of CT segmentation is 0.991 and the final dice coefficient of CTA segmentation is 0.984. In addition, attention U-Net and nnU-Net network also performs well. In this paper, the methods chosen by 12 teams from different research groups are evaluated and their segmentation results are analyzed for the study and references to those involved.",2021,10.3233/xst-210956,cross-sectional,diagnosis,CT,Lung
Analysis of Stroke Detection during the COVID-19 Pandemic Using Natural Language Processing of Radiology Reports,"BACKGROUND AND PURPOSE: The coronavirus disease 2019 (COVID-19) pandemic has led to decreases in neuroimaging volume. Our aim was to quantify the change in acute or subacute ischemic strokes detected on CT or MR imaging during the pandemic using natural language processing of radiology reports. MATERIALS AND METHODS: We retrospectively analyzed 32,555 radiology reports from brain CTs and MRIs from a comprehensive stroke center, performed from March 1 to April 30 each year from 2017 to 2020, involving 20,414 unique patients. To detect acute or subacute ischemic stroke in free-text reports, we trained a random forest natural language processing classifier using 1987 randomly sampled radiology reports with manual annotation. Natural language processing classifier generalizability was evaluated using 1974 imaging reports from an external dataset. RESULTS: The natural language processing classifier achieved a 5-fold cross-validation classification accuracy of 0.97 and an F1 score of 0.74, with a slight underestimation (-5%) of actual numbers of acute or subacute ischemic strokes in cross-validation. Importantly, cross-validation performance stratified by year was similar. Applying the classifier to the complete study cohort, we found an estimated 24% decrease in patients with acute or subacute ischemic strokes reported on CT or MR imaging from March to April 2020 compared with the average from those months in 2017-2019. Among patients with stroke-related order indications, the estimated proportion who underwent neuroimaging with acute or subacute ischemic stroke detection significantly increased from 16% during 2017-2019 to 21% in 2020 (P = .01). The natural language processing classifier performed worse on external data. CONCLUSIONS: Acute or subacute ischemic stroke cases detected by neuroimaging decreased during the COVID-19 pandemic, though a higher proportion of studies ordered for stroke were positive for acute or subacute ischemic strokes. Natural language processing approaches can help automatically track acute or subacute ischemic stroke numbers for epidemiologic studies, though local classifier training is important due to radiologist reporting style differences.",2021,10.3174/ajnr.A6961,,,,
Analysis of the Diagnosis Model of Peripheral Non-Small-Cell Lung Cancer under Computed Tomography Images,"This study aimed to explore the effect of deep learning models on lung CT image lung parenchymal segmentation (LPS) and the application value of CT image texture features in the diagnosis of peripheral non-small-cell lung cancer (NSCLC). Data of peripheral lung cancer (PLC) patients was collected retrospectively and was divided into peripheral SCLC group and peripheral NSCLC group according to the pathological examination results, ResNet50 model and feature pyramid network (FPN) algorithm were undertaken to improve the Mask-RCNN model, and after the MaZda software extracted the texture features of the CT images of PLC patients, the Fisher coefficient was used to reduce the dimensionality, and the texture features of the CT images were analyzed and compared. The results showed that the average Dice coefficients of the 2D CH algorithm, Faster-RCNN, Mask-RCNN, and the algorithm proposed in the validation set were 0.882, 0.953, 0.961, and 0.986, respectively. The accuracy rates were 88.3%, 93.5%, 94.4%, and 97.2%. The average segmentation speeds in lung CT images were 0.289 s/sheet, 0.115 s/sheet, 0.108 s/sheet, and 0.089 s/sheet. The improved deep learning model showed higher accuracy, better robustness, and faster speed than other algorithms in the LPS of CT images. In summary, deep learning can achieve the LPS of CT images and show excellent segmentation efficiency. The texture parameters of GLCM in CT images have excellent differential diagnosis performance for NSCLC and SCLC and potential clinical application value.",2022,10.1155/2022/3107965,cross-sectional,diagnosis,CT,Lung
Analyzing inter-reader variability affecting deep ensemble learning for COVID-19 detection in chest radiographs,"Data-driven deep learning (DL) methods using convolutional neural networks (CNNs) demonstrate promising performance in natural image computer vision tasks. However, their use in medical computer vision tasks faces several limitations, viz., (i) adapting to visual characteristics that are unlike natural images; (ii) modeling random noise during training due to stochastic optimization and backpropagation-based learning strategy; (iii) challenges in explaining DL black-box behavior to support clinical decision-making; and (iv) inter-reader variability in the ground truth (GT) annotations affecting learning and evaluation. This study proposes a systematic approach to address these limitations through application to the pandemic-caused need for Coronavirus disease 2019 (COVID-19) detection using chest X-rays (CXRs). Specifically, our contribution highlights significant benefits obtained through (i) pretraining specific to CXRs in transferring and fine-tuning the learned knowledge toward improving COVID-19 detection performance; (ii) using ensembles of the fine-tuned models to further improve performance over individual constituent models; (iii) performing statistical analyses at various learning stages for validating results; (iv) interpreting learned individual and ensemble model behavior through class-selective relevance mapping (CRM)-based region of interest (ROI) localization; and, (v) analyzing inter-reader variability and ensemble localization performance using Simultaneous Truth and Performance Level Estimation (STAPLE) methods. We find that ensemble approaches markedly improved classification and localization performance, and that inter-reader variability and performance level assessment helps guide algorithm design and parameter optimization. To the best of our knowledge, this is the first study to construct ensembles, perform ensemble-based disease ROI localization, and analyze inter-reader variability and algorithm performance for COVID-19 detection in CXRs.",2020,10.1371/journal.pone.0242301,cross-sectional,diagnosis,X-ray,Lung
AND-rPPG: A novel denoising-rPPG network for improving remote heart rate estimation,"Heart rate (HR) estimation is an essential physiological parameter in the field of biomedical imaging. Remote Photoplethysmography (r-PPG) is a pathbreaking development in this field wherein the PPG signal is extracted from non-contact face videos. In the COVID-19 pandemic, rPPG plays a vital role for doctors and patients to perform telehealthcare. Existing rPPG methods provide incorrect HR estimation when face video contains facial deformations induced by facial expression. These methods process the entire face and utilize the same knowledge to mitigate different noises. It limits the performance of these methods because different facial expressions induce different noise characteristics depending on the facial region. Another limitation is that these methods neglect the facial expression for denoising even though it is the prominent noise source in temporal signals. These issues are mitigated in this paper by proposing a novel HR estimation method AND-rPPG, that is, A Novel Denoising-rPPG. We initiate the utilization of Action Units (AUs) for denoising temporal signals. Our denoising network models the temporal signals better than sequential architectures and mitigate the AUs-based (or face expression-based) noises effectively. The experiments performed on publicly available datasets reveal that our proposed method outperforms state-of-the-art HR estimation methods, and our denoising model can be easily integrated with existing methods to improve their HR estimation.",2022,10.1016/j.compbiomed.2021.105146,,,,
Ant colony optimization approaches to clustering of lung nodules from CT images,"Lung cancer is becoming a threat to mankind. Applying machine learning algorithms for detection and segmentation of irregular shaped lung nodules remains a remarkable milestone in CT scan image analysis research. In this paper, we apply ACO algorithm for lung nodule detection. We have compared the performance against three other algorithms, namely, Otsu algorithm, watershed algorithm, and global region based segmentation. In addition, we suggest a novel approach which involves variations of ACO, namely, refined ACO, logical ACO, and variant ACO. Variant ACO shows better reduction in false positives. In addition we propose black circular neighborhood approach to detect nodule centers from the edge detected image. Genetic algorithm based clustering is performed to cluster the nodules based on intensity, shape, and size. The performance of the overall approach is compared with hierarchical clustering to establish the improvisation in the proposed approach.",2014,10.1155/2014/572494,cross-sectional,diagnosis,CT,Lung
Any unique image biomarkers associated with COVID-19?,"OBJECTIVE: To define the uniqueness of chest CT infiltrative features associated with COVID-19 image characteristics as potential diagnostic biomarkers. METHODS: We retrospectively collected chest CT exams including n = 498 on 151 unique patients RT-PCR positive for COVID-19 and n = 497 unique patients with community-acquired pneumonia (CAP). Both COVID-19 and CAP image sets were partitioned into three groups for training, validation, and testing respectively. In an attempt to discriminate COVID-19 from CAP, we developed several classifiers based on three-dimensional (3D) convolutional neural networks (CNNs). We also asked two experienced radiologists to visually interpret the testing set and discriminate COVID-19 from CAP. The classification performance of the computer algorithms and the radiologists was assessed using the receiver operating characteristic (ROC) analysis, and the nonparametric approaches with multiplicity adjustments when necessary. RESULTS: One of the considered models showed non-trivial, but moderate diagnostic ability overall (AUC of 0.70 with 99% CI 0.56-0.85). This model allowed for the identification of 8-50% of CAP patients with only 2% of COVID-19 patients. CONCLUSIONS: Professional or automated interpretation of CT exams has a moderately low ability to distinguish between COVID-19 and CAP cases. However, the automated image analysis is promising for targeted decision-making due to being able to accurately identify a sizable subsect of non-COVID-19 cases. KEY POINTS: • Both human experts and artificial intelligent models were used to classify the CT scans. • ROC analysis and the nonparametric approaches were used to analyze the performance of the radiologists and computer algorithms. • Unique image features or patterns may not exist for reliably distinguishing all COVID-19 from CAP; however, there may be imaging markers that can identify a sizable subset of non-COVID-19 cases.",2020,10.1007/s00330-020-06956-w,cross-sectional,diagnosis,CT,Lung
Applicability of a prognostic CT-based radiomic signature model trained on stage I-III non-small cell lung cancer in stage IV non-small cell lung cancer,"OBJECTIVES: Recently it has been shown that radiomic features of computed tomography (CT) have prognostic information in stage I-III non-small cell lung cancer (NSCLC) patients. We aim to validate this prognostic radiomic signature in stage IV adenocarcinoma patients undergoing chemotherapy. MATERIALS AND METHODS: Two datasets of chemo-naive stage IV adenocarcinoma patients were investigated, dataset 1: 285 patients with CTs performed in a single center; dataset 2: 223 patients included in a multicenter clinical trial. The main exclusion criteria were EGFR mutation or unknown mutation status and non-delineated primary tumor. Radiomic features were calculated for the primary tumor. The c-index of cox regression was calculated and compared to the signature performance for overall survival (OS). RESULTS: In total CT scans from 195 patients were eligible for analysis. Patients having a prognostic index (PI) lower than the signature median (n = 92) had a significantly better OS than patients with a PI higher than the median (n = 103, HR 1.445, 95% CI 1.07-1.95, p = 0.02, c-index 0.576, 95% CI 0.527-0.624). CONCLUSION: The radiomic signature, derived from daily practice CT scans, has prognostic value for stage IV NSCLC, however the signature performs less than previously described for stage I-III NSCLC stages. In the future, machine learning techniques can potentially lead to a better prognostic imaging based model for stage IV NSCLC.",2018,10.1016/j.lungcan.2018.07.023,cross-sectional,diagnosis,CT,Lung
Application of a classifier combining bronchial transcriptomics and chest computed tomography features facilitates the diagnostic evaluation of lung cancer in smokers and nonsmokers,"Lung cancer screening by computed tomography (CT) reduces mortality but exhibited high false-positive rates. We established a diagnostic classifier combining chest CT features with bronchial transcriptomics. Patients with CT-detected suspected lung cancer were enrolled. The sample collected by bronchial brushing was used for RNA sequencing. The e1071 and pROC packages in R software was applied to build the model. Eventually, a total of 283 patients, including 183 with lung cancer and 100 with benign lesions, were included into final analysis. When incorporating transcriptomic data with radiological characteristics, the advanced model yielded 0.903 AUC with 81.1% NPV. Moreover, the classifier performed well regardless of lesion size, location, stage, histologic type or smoking status. Pathway analysis showed enhanced epithelial differentiation, tumor metastasis, and impaired immunity were predominant in smokers with cancer, whereas tumorigenesis played a central role in nonsmokers with cancer. Apoptosis and oxidative stress contributed critically in metastatic lung cancer; by contrast, immune dysfunction was pivotal in locally advanced lung cancer. Collectively, we devised a minimal-to-noninvasive, efficient diagnostic classifier for smokers and nonsmokers with lung cancer, which provides evidence for different mechanisms of cancer development and metastasis associated with smoking. A negative classifier result will help the physician make conservative diagnostic decisions.",2021,10.1002/ijc.33675,cross-sectional,diagnosis,CT,Lung
Application of an Artificial Intelligence Trilogy to Accelerate Processing of Suspected Patients With SARS-CoV-2 at a Smart Quarantine Station: Observational Study,"BACKGROUND: As the COVID-19 epidemic increases in severity, the burden of quarantine stations outside emergency departments (EDs) at hospitals is increasing daily. To address the high screening workload at quarantine stations, all staff members with medical licenses are required to work shifts in these stations. Therefore, it is necessary to simplify the workflow and decision-making process for physicians and surgeons from all subspecialties. OBJECTIVE: The aim of this paper is to demonstrate how the National Cheng Kung University Hospital artificial intelligence (AI) trilogy of diversion to a smart quarantine station, AI-assisted image interpretation, and a built-in clinical decision-making algorithm improves medical care and reduces quarantine processing times. METHODS: This observational study on the emerging COVID-19 pandemic included 643 patients. An ""AI trilogy"" of diversion to a smart quarantine station, AI-assisted image interpretation, and a built-in clinical decision-making algorithm on a tablet computer was applied to shorten the quarantine survey process and reduce processing time during the COVID-19 pandemic. RESULTS: The use of the AI trilogy facilitated the processing of suspected cases of COVID-19 with or without symptoms; also, travel, occupation, contact, and clustering histories were obtained with the tablet computer device. A separate AI-mode function that could quickly recognize pulmonary infiltrates on chest x-rays was merged into the smart clinical assisting system (SCAS), and this model was subsequently trained with COVID-19 pneumonia cases from the GitHub open source data set. The detection rates for posteroanterior and anteroposterior chest x-rays were 55/59 (93%) and 5/11 (45%), respectively. The SCAS algorithm was continuously adjusted based on updates to the Taiwan Centers for Disease Control public safety guidelines for faster clinical decision making. Our ex vivo study demonstrated the efficiency of disinfecting the tablet computer surface by wiping it twice with 75% alcohol sanitizer. To further analyze the impact of the AI application in the quarantine station, we subdivided the station group into groups with or without AI. Compared with the conventional ED (n=281), the survey time at the quarantine station (n=1520) was significantly shortened; the median survey time at the ED was 153 minutes (95% CI 108.5-205.0), vs 35 minutes at the quarantine station (95% CI 24-56; P<.001). Furthermore, the use of the AI application in the quarantine station reduced the survey time in the quarantine station; the median survey time without AI was 101 minutes (95% CI 40-153), vs 34 minutes (95% CI 24-53) with AI in the quarantine station (P<.001). CONCLUSIONS: The AI trilogy improved our medical care workflow by shortening the quarantine survey process and reducing the processing time, which is especially important during an emerging infectious disease epidemic.",2020,10.2196/19878,,,,
Application of Artificial Intelligence in Emergency Nursing of Patients with Chronic Obstructive Pulmonary Disease,"The research achievements of artificial intelligence technology in the development of chronic obstructive pulmonary disease were explored, and the advantages and problems encountered in the development of intelligent nursing were analyzed. This paper presents the application of artificial intelligence in the emergency care of patients with chronic obstructive pulmonary disease. The method included 447 COPD patients in a randomized controlled trial to observe the improvement of quality of life at 4 and 12 months after artificial intelligence medical intervention. A prospective randomized controlled trial included 101 patients with COPD who underwent a 9-month web-based knowledge exercise on the prevention of acute exacerbation of COPD through artificial intelligence medicine and were randomly divided into two groups: the experimental group and the control group. The results show that, in the experimental group and the control group, after 4 months, the quality of life does not change; after 12 months, compared with controls, the quality of life and emotional and psychological conditions have improved obviously. 29 patients who participated in the experiment and were randomly divided into the experimental group and the control group showed satisfactory results. COPD hospitalized rate and length of hospital stay were decreased in the experimental group than in the control group. For single-factor analysis, artificial intelligence medical intervention has not achieved significant significance, and the experimental results have preliminarily confirmed the effectiveness of artificial intelligence medical treatment.",2021,10.1155/2021/6423398,,,,
Application of artificial intelligence in the diagnosis of multiple primary lung cancer,"Artificial intelligence (AI) based on deep learning, convolutional neural networks and big data has been increasingly effective in the diagnosis and treatment of multiple primary pulmonary nodules. In comparison to previous imaging systems, AI measures more objective parameters such as three-dimensional (3D) volume, probability of malignant nodules, and possible pathological patterns, making the access to the properties of nodules more objective. In our retrospective study, a total of 53 patients with synchronous and metachronous multiple pulmonary nodules were enrolled of which 33 patients were confirmed by pathological tests to have primary binodules, and nine to have primary trinodules. A total of 15 patients had only one focus removed. The statistical results showed that the agreement in the AI diagnosis and postoperative pathological tests was 88.8% in identifying benign or malignant lesions. In addition, the probability of malignancy of benign lesions, preinvasive lesions (AAH, AIS) and invasive lesions (MIA, IA) was totally different (49.40±38.41% vs 80.22±13.55% vs 88.17±17.31%). The purpose of our study was to provide references for the future application of AI in the diagnosis and follow-up of multiple pulmonary nodules. AI may represent a relevant diagnostic aid that shows more accurate and objective results in the diagnosis of multiple pulmonary nodules, reducing the time required for interpretation of results by directly displaying visual information to doctors and patients and together with the clinical conditions of MPLC patients, offering plans for follow-up and treatment that may be more beneficial and reasonable for patients. Despite the great application potential in pneumosurgery, further research is needed to verify the accuracy and range of the application of AI.",2019,10.1111/1759-7714.13185,cross-sectional,diagnosis,CT,Lung
Application of deep learning (3-dimensional convolutional neural network) for the prediction of pathological invasiveness in lung adenocarcinoma: A preliminary study,"To compare results for radiological prediction of pathological invasiveness in lung adenocarcinoma between radiologists and a deep learning (DL) system.Ninety patients (50 men, 40 women; mean age, 66 years; range, 40-88 years) who underwent pre-operative chest computed tomography (CT) with 0.625-mm slice thickness were included in this retrospective study. Twenty-four cases of adenocarcinoma in situ (AIS), 20 cases of minimally invasive adenocarcinoma (MIA), and 46 cases of invasive adenocarcinoma (IVA) were pathologically diagnosed. Three radiologists of different levels of experience diagnosed each nodule by using previously documented CT findings to predict pathological invasiveness. DL was structured using a 3-dimensional (3D) convolutional neural network (3D-CNN) constructed with 2 successive pairs of convolution and max-pooling layers, and 2 fully connected layers. The output layer comprises 3 nodes to recognize the 3 conditions of adenocarcinoma (AIS, MIA, and IVA) or 2 nodes for 2 conditions (AIS and MIA/IVA). Results from DL and the 3 radiologists were statistically compared.No significant differences in pathological diagnostic accuracy rates were seen between DL and the 3 radiologists (P >.11). Receiver operating characteristic analysis demonstrated that area under the curve for DL (0.712) was almost the same as that for the radiologist with extensive experience (0.714; P = .98). Compared with the consensus results from radiologists, DL offered significantly inferior sensitivity (P = .0005), but significantly superior specificity (P = .02).Despite the small training data set, diagnostic performance of DL was almost the same as the radiologist with extensive experience. In particular, DL provided higher specificity than radiologists.",2019,10.1097/md.0000000000016119,cross-sectional,prognosis,CT,Lung
Application of Deep Learning in Lung Cancer Imaging Diagnosis,"Lung cancer is one of the malignant tumors with the highest fatality rate and nearest to our lives. It poses a great threat to human health and it mainly occurs in smokers. In our country, with the acceleration of industrialization, environmental pollution, and population aging, the cancer burden of lung cancer is increasing day by day. In the diagnosis of lung cancer, Computed Tomography (CT) images are a fairly common visualization tool. CT images visualize all tissues based on the absorption of X-rays. The diseased parts of the lung are collectively referred to as pulmonary nodules, the shape of nodules is different, and the risk of cancer will vary with the shape of nodules. Computer-aided diagnosis (CAD) is a very suitable method to solve this problem because the computer vision model can quickly scan every part of the CT image of the same quality for analysis and will not be affected by fatigue and emotion. The latest advances in deep learning enable computer vision models to help doctors diagnose various diseases, and in some cases, models have shown greater competitiveness than doctors. Based on the opportunity of technological development, the application of computer vision in medical imaging diagnosis of diseases has important research significance and value. In this paper, we have used a deep learning-based model on CT images of lung cancer and verified its effectiveness in the timely and accurate prediction of lungs disease. The proposed model has three parts: (i) detection of lung nodules, (ii) False Positive Reduction of the detected nodules to filter out ""false nodules,"" and (iii) classification of benign and malignant lung nodules. Furthermore, different network structures and loss functions were designed and realized at different stages. Additionally, to fine-tune the proposed deep learning-based mode and improve its accuracy in the detection Lung Nodule Detection, Noudule-Net, which is a detection network structure that combines U-Net and RPN, is proposed. Experimental observations have verified that the proposed scheme has exceptionally improved the expected accuracy and precision ratio of the underlined disease.",2022,10.1155/2022/6107940,cross-sectional,diagnosis,CT,Lung
Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks,"Fast diagnostic methods can control and prevent the spread of pandemic diseases like coronavirus disease 2019 (COVID-19) and assist physicians to better manage patients in high workload conditions. Although a laboratory test is the current routine diagnostic tool, it is time-consuming, imposing a high cost and requiring a well-equipped laboratory for analysis. Computed tomography (CT) has thus far become a fast method to diagnose patients with COVID-19. However, the performance of radiologists in diagnosis of COVID-19 was moderate. Accordingly, additional investigations are needed to improve the performance in diagnosing COVID-19. In this study is suggested a rapid and valid method for COVID-19 diagnosis using an artificial intelligence technique based. 1020 CT slices from 108 patients with laboratory proven COVID-19 (the COVID-19 group) and 86 patients with other atypical and viral pneumonia diseases (the non-COVID-19 group) were included. Ten well-known convolutional neural networks were used to distinguish infection of COVID-19 from non-COVID-19 groups: AlexNet, VGG-16, VGG-19, SqueezeNet, GoogleNet, MobileNet-V2, ResNet-18, ResNet-50, ResNet-101, and Xception. Among all networks, the best performance was achieved by ResNet-101 and Xception. ResNet-101 could distinguish COVID-19 from non-COVID-19 cases with an AUC of 0.994 (sensitivity, 100%; specificity, 99.02%; accuracy, 99.51%). Xception achieved an AUC of 0.994 (sensitivity, 98.04%; specificity, 100%; accuracy, 99.02%). However, the performance of the radiologist was moderate with an AUC of 0.873 (sensitivity, 89.21%; specificity, 83.33%; accuracy, 86.27%). ResNet-101 can be considered as a high sensitivity model to characterize and diagnose COVID-19 infections, and can be used as an adjuvant tool in radiology departments.",2020,10.1016/j.compbiomed.2020.103795,cross-sectional,diagnosis,CT,Lung
Application of deep learning to identify COVID-19 infection in posteroanterior chest X-rays,"INTRODUCTION: The objective of this study was to assess seven configurations of six convolutional deep neural network architectures for classification of chest X-rays (CXRs) as COVID-19 positive or negative. METHODS: The primary dataset consisted of 294 COVID-19 positive and 294 COVID-19 negative CXRs, the latter comprising roughly equally many pneumonia, emphysema, fibrosis, and healthy images. We used six common convolutional neural network architectures, VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile and InceptionV3. We studied six models (one for each architecture) which were pre-trained on a vast repository of generic (non-CXR) images, as well as a seventh DenseNet121 model, which was pre-trained on a repository of CXR images. For each model, we replaced the output layers with custom fully connected layers for the task of binary classification of images as COVID-19 positive or negative. Performance metrics were calculated on a hold-out test set with CXRs from patients who were not included in the training/validation set. RESULTS: When pre-trained on generic images, the VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile, and InceptionV3 architectures respectively produced hold-out test set areas under the receiver operating characteristic (AUROCs) of 0.98, 0.95, 0.97, 0.95, 0.99, and 0.96 for the COVID-19 classification of CXRs. The X-ray pre-trained DenseNet121 model, in comparison, had a test set AUROC of 0.87. DISCUSSION: Common convolutional neural network architectures with parameters pre-trained on generic images yield high-performance and well-calibrated COVID-19 CXR classification.",2021,10.1016/j.clinimag.2021.07.004,cross-sectional,diagnosis,X-ray,Lung
Application of logistic regression and convolutional neural network in prediction and diagnosis of high-risk populations of lung cancer,"OBJECTIVES: The early detection, early diagnosis, and early treatment of lung cancer are the best strategies to improve the 5-year survival rate. Logistic regression analysis can be a helpful tool in the early detection of high-risk groups of lung cancer. Convolutional neural network (CNN) could distinguish benign from malignant pulmonary nodules, which is critical for early precise diagnosis and treatment. Here, we developed a risk assessment model of lung cancer and a high-precision classification diagnostic model using these technologies so as to provide a basis for early screening of lung cancer and for intelligent differential diagnosis. METHODS: A total of 355 lung cancer patients, 444 patients with benign lung disease and 472 healthy people from The First Affiliated Hospital of Zhengzhou University were included in this study. Moreover, the dataset of 607 lung computed tomography images was collected from the above patients. The logistic regression method was employed to screen the high-risk groups of lung cancer, and the CNN model was designed to classify pulmonary nodules into benign or malignant nodules. RESULTS: The area under the curve of the lung cancer risk assessment model in the training set and the testing set were 0.823 and 0.710, respectively. After finely optimizing the settings of the CNN, the area under the curve could reach 0.984. CONCLUSIONS: This performance demonstrated that the lung cancer risk assessment model could be used to screen for high-risk individuals with lung cancer and the CNN framework was suitable for the differential diagnosis of pulmonary nodules.",2022,10.1097/cej.0000000000000684,cross-sectional,prognosis,CT,Lung
Application of Super-Resolution Convolutional Neural Network for Enhancing Image Resolution in Chest CT,"In this study, the super-resolution convolutional neural network (SRCNN) scheme, which is the emerging deep-learning-based super-resolution method for enhancing image resolution in chest CT images, was applied and evaluated using the post-processing approach. For evaluation, 89 chest CT cases were sampled from The Cancer Imaging Archive. The 89 CT cases were divided randomly into 45 training cases and 44 external test cases. The SRCNN was trained using the training dataset. With the trained SRCNN, a high-resolution image was reconstructed from a low-resolution image, which was down-sampled from an original test image. For quantitative evaluation, two image quality metrics were measured and compared to those of the conventional linear interpolation methods. The image restoration quality of the SRCNN scheme was significantly higher than that of the linear interpolation methods (p < 0.001 or p < 0.05). The high-resolution image reconstructed by the SRCNN scheme was highly restored and comparable to the original reference image, in particular, for a ×2 magnification. These results indicate that the SRCNN scheme significantly outperforms the linear interpolation methods for enhancing image resolution in chest CT images. The results also suggest that SRCNN may become a potential solution for generating high-resolution CT images from standard CT images.",2018,10.1007/s10278-017-0033-z,cross-sectional,informatics,CT,Lung
Applying Machine Learning with Localized Surface Plasmon Resonance Sensors to Detect SARS-CoV-2 Particles,"The sudden outbreak of COVID-19 rapidly developed into a global pandemic, which caused tens of millions of infections and millions of deaths. Although SARS-CoV-2 is known to cause COVID-19, effective approaches to detect SARS-CoV-2 using a convenient, rapid, accurate, and low-cost method are lacking. To date, most of the diagnostic methods for patients with early infections are limited to the detection of viral nucleic acids via polymerase chain reaction (PCR), or antigens, using an enzyme-linked immunosorbent assay or a chemiluminescence immunoassay. This study developed a novel method that uses localized surface plasmon resonance (LSPR) sensors, optical imaging, and artificial intelligence methods to directly detect the SARS-CoV-2 virus particles without any sample preparation. The virus concentration can be qualitatively and quantitatively detected in the range of 125.28 to 10(6) vp/mL through a few steps within 12 min with a limit of detection (LOD) of 100 vp/mL. The accuracy of the SARS-CoV-2 positive or negative assessment was found to be greater than 97%, and this was demonstrated by establishing a regression machine learning model for the virus concentration prediction (R(2) &gt; 0.95).",2022,10.3390/bios12030173,,,,
Arbitrary Scale Super-Resolution for Medical Images,"Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalize over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in [Formula: see text]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation.",2021,10.1142/s0129065721500374,cross-sectional,informatics,CT and MRI,Lung and heart
Artificial intelligence analysis of three-dimensional imaging data derives factors associated with postoperative recurrence in patients with radiologically solid-predominant small-sized lung cancers,"OBJECTIVES: Indications of limited resection, such as segmentectomy, have recently been reported for patients with solid-predominant lung cancers ≤2 cm. This study aims to identify unfavourable prognostic factors using three-dimensional imaging analysis with artificial intelligence (AI) technology. METHODS: A total of 157 patients who had clinical N0 non-small cell lung cancer with a radiological size ≤2 cm, and a consolidation tumour ratio > 0.5, who underwent anatomical lung resection between 2011 and 2017 were enrolled. To evaluate the three-dimensional structure, the ground-glass nodule/Solid Automatic Identification AI software Beta Version (AI software; Fujifilm Corporation, Japan) was used. RESULTS: Maximum standardized uptake value (SUVmax) and solid-part volume measured by AI software (AI-SV) showed significant differences between the 139 patients with adenocarcinoma and the 18 patients with non-adenocarcinoma. Among the adenocarcinoma patients, 42 patients (30.2%) were found to be pathological upstaging. Multivariable analysis demonstrated that high SUVmax, high carcinoembryonic antigen level and high AI-SV were significant prognostic factors for recurrence-free survival (RFS; P < 0.05). The 5-year RFS was compared between patients with tumours showing high SUVmax and those showing low SUVmax (67.7% vs 95.4%, respectively, P < 0.001). The 5-year RFS was 91.0% in patients with small AI-SV and 68.1% in those with high AI-SV (P = 0.001). CONCLUSIONS: High AI-SV, high SUVmax and abnormal carcinoembryonic antigen level were unfavourable prognostic factors of patients with solid-predominant lung adenocarcinoma with a radiological size ≤2 cm. Our results suggest that lobectomy should be preferred to segmentectomy for patients with these prognostic factors.",2022,10.1093/ejcts/ezab541,cross-sectional,prognosis,CT,Lung 
Artificial Intelligence and Medical Internet of Things Framework for Diagnosis of Coronavirus Suspected Cases,"The world has been facing the COVID-19 pandemic since December 2019. Timely and efficient diagnosis of COVID-19 suspected patients plays a significant role in medical treatment. The deep transfer learning-based automated COVID-19 diagnosis on chest X-ray is required to counter the COVID-19 outbreak. This work proposes a real-time Internet of Things (IoT) framework for early diagnosis of suspected COVID-19 patients by using ensemble deep transfer learning. The proposed framework offers real-time communication and diagnosis of COVID-19 suspected cases. The proposed IoT framework ensembles four deep learning models such as InceptionResNetV2, ResNet152V2, VGG16, and DenseNet201. The medical sensors are utilized to obtain the chest X-ray modalities and diagnose the infection by using the deep ensemble model stored on the cloud server. The proposed deep ensemble model is compared with six well-known transfer learning models over the chest X-ray dataset. Comparative analysis revealed that the proposed model can help radiologists to efficiently and timely diagnose the COVID-19 suspected patients.",2021,10.1155/2021/3277988,cross-sectional,diagnosis,X-ray,Lung
Artificial intelligence and radiomics enhance the positive predictive value of digital chest tomosynthesis for lung cancer detection within SOS clinical trial,OBJECTIVE: To enhance the positive predictive value (PPV) of chest digital tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics features. METHOD: The investigation was carried out within the SOS clinical trial (NCT03645018) for lung cancer screening with DTS. Lung nodules were identified by visual analysis and then classified using the diameter and the radiological aspect of the nodule following lung-RADS. Haralick texture features were extracted from the segmented nodules. Both semantic variables and radiomics features were used to build a predictive model using logistic regression on a subset of variables selected with backward feature selection and using two machine learning: a Random Forest and a neural network with the whole subset of variables. The methods were applied to a train set and validated on a test set where diagnostic accuracy metrics were calculated. RESULTS: Binary visual analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS classification increased the PPV (0.19) but with an unacceptable low sensitivity (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a low sensitivity (0.30). Neural network demonstrated to be the best predictor with a high PPV (0.95) and a high sensitivity (0.90). CONCLUSIONS: The neural network demonstrated the best PPV. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS. KEY POINTS: • We investigated several approaches to enhance the positive predictive value of chest digital tomosynthesis in the lung cancer detection. • Neural network demonstrated to be the best predictor with a nearly perfect PPV. • Neural network could help radiologists to reduce the number of false positive in DTS.,2020,10.1007/s00330-020-06783-z,cross-sectional,diagnosis,CT,Lung
Artificial Intelligence Augmentation of Radiologist Performance in Distinguishing COVID-19 from Pneumonia of Other Origin at Chest CT,"Background Coronavirus disease 2019 (COVID-19) and pneumonia of other diseases share similar CT characteristics, which contributes to the challenges in differentiating them with high accuracy. Purpose To establish and evaluate an artificial intelligence (AI) system for differentiating COVID-19 and other pneumonia at chest CT and assessing radiologist performance without and with AI assistance. Materials and Methods A total of 521 patients with positive reverse transcription polymerase chain reaction results for COVID-19 and abnormal chest CT findings were retrospectively identified from 10 hospitals from January 2020 to April 2020. A total of 665 patients with non-COVID-19 pneumonia and definite evidence of pneumonia at chest CT were retrospectively selected from three hospitals between 2017 and 2019. To classify COVID-19 versus other pneumonia for each patient, abnormal CT slices were input into the EfficientNet B4 deep neural network architecture after lung segmentation, followed by a two-layer fully connected neural network to pool slices together. The final cohort of 1186 patients (132 583 CT slices) was divided into training, validation, and test sets in a 7:2:1 and equal ratio. Independent testing was performed by evaluating model performance in separate hospitals. Studies were blindly reviewed by six radiologists without and then with AI assistance. Results The final model achieved a test accuracy of 96% (95% confidence interval [CI]: 90%, 98%), a sensitivity of 95% (95% CI: 83%, 100%), and a specificity of 96% (95% CI: 88%, 99%) with area under the receiver operating characteristic curve of 0.95 and area under the precision-recall curve of 0.90. On independent testing, this model achieved an accuracy of 87% (95% CI: 82%, 90%), a sensitivity of 89% (95% CI: 81%, 94%), and a specificity of 86% (95% CI: 80%, 90%) with area under the receiver operating characteristic curve of 0.90 and area under the precision-recall curve of 0.87. Assisted by the probabilities of the model, the radiologists achieved a higher average test accuracy (90% vs 85%, Δ = 5, P < .001), sensitivity (88% vs 79%, Δ = 9, P < .001), and specificity (91% vs 88%, Δ = 3, P = .001). Conclusion Artificial intelligence assistance improved radiologists' performance in distinguishing coronavirus disease 2019 pneumonia from non-coronavirus disease 2019 pneumonia at chest CT. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020201491,cross-sectional,diagnosis,CT,Lung
Artificial Intelligence ECG to Detect Left Ventricular Dysfunction in COVID-19: A Case Series,"Coronavirus disease 2019 (COVID-19) can result in deterioration of cardiac function, which is associated with high mortality. A simple point-of-care diagnostic test to screen for ventricular dysfunction would be clinically useful to guide management. We sought to review the clinical experience with an artificial intelligence electrocardiogram (AI ECG) to screen for ventricular dysfunction in patients with documented COVID-19. We examined all patients in the Mayo Clinic system who underwent clinically indicated electrocardiography and echocardiography within 2 weeks following a positive COVID-19 test and had permitted use of their data for research were included. Of the 27 patients who met the inclusion criteria, one had a history of normal ventricular function who developed COVID-19 myocarditis with rapid clinical decline. The initial AI ECG in this patient indicated normal ventricular function. Repeat AI ECG showed a probability of ejection fraction (EF) less than or equal to 40% of 90.2%, corroborated with an echocardiographic EF of 35%. One other patient had a pre-existing EF less than or equal to 40%, accurately detected by the algorithm before and after COVID-19 diagnosis, and another was found to have a low EF by AI ECG and echocardiography with the COVID-19 diagnosis. The area under the curve for detection of EF less than or equal to 40% was 0.95. This case series suggests that the AI ECG, previously shown to detect ventricular dysfunction in a large general population, may be useful as a screening tool for the detection of cardiac dysfunction in patients with COVID-19.",2020,10.1016/j.mayocp.2020.09.020,,,,
Artificial intelligence for detecting small FDG-positive lung nodules in digital PET/CT: impact of image reconstructions on diagnostic performance,"OBJECTIVES: To evaluate the diagnostic performance of a deep learning algorithm for automated detection of small (18)F-FDG-avid pulmonary nodules in PET scans, and to assess whether novel block sequential regularized expectation maximization (BSREM) reconstruction affects detection accuracy as compared to ordered subset expectation maximization (OSEM) reconstruction. METHODS: Fifty-seven patients with 92 (18)F-FDG-avid pulmonary nodules (all ≤ 2 cm) undergoing PET/CT for oncological (re-)staging were retrospectively included and a total of 8824 PET images of the lungs were extracted using OSEM and BSREM reconstruction. Per-slice and per-nodule sensitivity of a deep learning algorithm was assessed, with an expert readout by a radiologist/nuclear medicine physician serving as standard of reference. Receiver-operator characteristic (ROC) curve of OSEM and BSREM were assessed and the areas under the ROC curve (AUC) were compared. A maximum standardized uptake value (SUV(max))-based sensitivity analysis and a size-based sensitivity analysis with subgroups defined by nodule size was performed. RESULTS: The AUC of the deep learning algorithm for nodule detection using OSEM reconstruction was 0.796 (CI 95%; 0.772-0.869), and 0.848 (CI 95%; 0.828-0.869) using BSREM reconstruction. The AUC was significantly higher for BSREM compared to OSEM (p = 0.001). On a per-slice analysis, sensitivity and specificity were 66.7% and 79.0% for OSEM, and 69.2% and 84.5% for BSREM. On a per-nodule analysis, the overall sensitivity of OSEM was 81.5% compared to 87.0% for BSREM. CONCLUSIONS: Our results suggest that machine learning algorithms may aid detection of small (18)F-FDG-avid pulmonary nodules in clinical PET/CT. AI performed significantly better on images with BSREM than OSEM. KEY POINTS: • The diagnostic value of deep learning for detecting small lung nodules (≤ 2 cm) in PET images using BSREM and OSEM reconstruction was assessed. • BSREM yields higher SUV(max)of small pulmonary nodules as compared to OSEM reconstruction. • The use of BSREM translates into a higher detectability of small pulmonary nodules in PET images as assessed with artificial intelligence.",2020,10.1007/s00330-019-06498-w,cross-sectional,diagnosis,PET/CT,Lung
Artificial intelligence for prediction of COVID-19 progression using CT imaging and clinical data,"OBJECTIVES: Early recognition of coronavirus disease 2019 (COVID-19) severity can guide patient management. However, it is challenging to predict when COVID-19 patients will progress to critical illness. This study aimed to develop an artificial intelligence system to predict future deterioration to critical illness in COVID-19 patients. METHODS: An artificial intelligence (AI) system in a time-to-event analysis framework was developed to integrate chest CT and clinical data for risk prediction of future deterioration to critical illness in patients with COVID-19. RESULTS: A multi-institutional international cohort of 1,051 patients with RT-PCR confirmed COVID-19 and chest CT was included in this study. Of them, 282 patients developed critical illness, which was defined as requiring ICU admission and/or mechanical ventilation and/or reaching death during their hospital stay. The AI system achieved a C-index of 0.80 for predicting individual COVID-19 patients' to critical illness. The AI system successfully stratified the patients into high-risk and low-risk groups with distinct progression risks (p < 0.0001). CONCLUSIONS: Using CT imaging and clinical data, the AI system successfully predicted time to critical illness for individual patients and identified patients with high risk. AI has the potential to accurately triage patients and facilitate personalized treatment. KEY POINT: • AI system can predict time to critical illness for patients with COVID-19 by using CT imaging and clinical data.",2022,10.1007/s00330-021-08049-8,cross-sectional,prognosis,CT,Lung
Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets,"Chest CT is emerging as a valuable diagnostic tool for clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to aid in rapid evaluation of CT scans for differentiation of COVID-19 findings from other clinical entities. Here we show that a series of deep learning algorithms, trained in a diverse multinational cohort of 1280 patients to localize parietal pleura/lung parenchyma followed by classification of COVID-19 pneumonia, can achieve up to 90.8% accuracy, with 84% sensitivity and 93% specificity, as evaluated in an independent test set (not included in training and validation) of 1337 patients. Normal controls included chest CTs from oncology, emergency, and pneumonia-related indications. The false positive rate in 140 patients with laboratory confirmed other (non COVID-19) pneumonias was 10%. AI-based algorithms can readily identify CT scans with COVID-19 associated pneumonia, as well as distinguish non-COVID related pneumonias with high specificity in diverse patient populations.",2020,10.1038/s41467-020-17971-2,cross-sectional,diagnosis,CT,Lung
Artificial intelligence on COVID-19 pneumonia detection using chest xray images,"Recent studies show the potential of artificial intelligence (AI) as a screening tool to detect COVID-19 pneumonia based on chest x-ray (CXR) images. However, issues on the datasets and study designs from medical and technical perspectives, as well as questions on the vulnerability and robustness of AI algorithms have emerged. In this study, we address these issues with a more realistic development of AI-driven COVID-19 pneumonia detection models by generating our own data through a retrospective clinical study to augment the dataset aggregated from external sources. We optimized five deep learning architectures, implemented development strategies by manipulating data distribution to quantitatively compare study designs, and introduced several detection scenarios to evaluate the robustness and diagnostic performance of the models. At the current level of data availability, the performance of the detection model depends on the hyperparameter tuning and has less dependency on the quantity of data. InceptionV3 attained the highest performance in distinguishing pneumonia from normal CXR in two-class detection scenario with sensitivity (Sn), specificity (Sp), and positive predictive value (PPV) of 96%. The models attained higher general performance of 91-96% Sn, 94-98% Sp, and 90-96% PPV in three-class compared to four-class detection scenario. InceptionV3 has the highest general performance with accuracy, F1-score, and g-mean of 96% in the three-class detection scenario. For COVID-19 pneumonia detection, InceptionV3 attained the highest performance with 86% Sn, 99% Sp, and 91% PPV with an AUC of 0.99 in distinguishing pneumonia from normal CXR. Its capability of differentiating COVID-19 pneumonia from normal and non-COVID-19 pneumonia attained 0.98 AUC and a micro-average of 0.99 for other classes.",2021,10.1371/journal.pone.0257884,cross-sectional,diagnosis,X-ray,Lung
"Artificial Intelligence Predicts Severity of COVID-19 Based on Correlation of Exaggerated Monocyte Activation, Excessive Organ Damage and Hyperinflammatory Syndrome: A Prospective Clinical Study","BACKGROUND: Prediction of the severity of COVID-19 at its onset is important for providing adequate and timely management to reduce mortality. OBJECTIVE: To study the prognostic value of damage parameters and cytokines as predictors of severity of COVID-19 using an extensive immunologic profiling and unbiased artificial intelligence methods. METHODS: Sixty hospitalized COVID-19 patients (30 moderate and 30 severe) and 17 healthy controls were included in the study. The damage indicators high mobility group box 1 (HMGB1), lactate dehydrogenase (LDH), aspartate aminotransferase (AST), alanine aminotransferase (ALT), extensive biochemical analyses, a panel of 47 cytokines and chemokines were analyzed at weeks 1, 2 and 7 along with clinical complaints and CT scans of the lungs. Unbiased artificial intelligence (AI) methods (logistic regression and Support Vector Machine and Random Forest algorithms) were applied to investigate the contribution of each parameter to prediction of the severity of the disease. RESULTS: On admission, the severely ill patients had significantly higher levels of LDH, IL-6, monokine induced by gamma interferon (MIG), D-dimer, fibrinogen, glucose than the patients with moderate disease. The levels of macrophage derived cytokine (MDC) were lower in severely ill patients. Based on artificial intelligence analysis, eight parameters (creatinine, glucose, monocyte number, fibrinogen, MDC, MIG, C-reactive protein (CRP) and IL-6 have been identified that could predict with an accuracy of 83-87% whether the patient will develop severe disease. CONCLUSION: This study identifies the prognostic factors and provides a methodology for making prediction for COVID-19 patients based on widely accepted biomarkers that can be measured in most conventional clinical laboratories worldwide.",2021,10.3389/fimmu.2021.715072,cross-sectional,prognosis,CT,Lung
Artificial intelligence solution to classify pulmonary nodules on CT,"PURPOSE: The purpose of this study was to create an algorithm to detect and classify pulmonary nodules in two categories based on their volume greater than 100 mm(3) or not, using machine learning and deep learning techniques. MATERIALS AND METHOD: The dataset used to train the model was provided by the organization team of the SFR (French Radiological Society) Data Challenge 2019. An asynchronous and parallel 3-stages pipeline was developed to process all the data (a data ""pre-processing"" stage; a ""nodule detection"" stage; a ""classifier"" stage). Lung segmentation was achieved using 3D U-NET algorithm; nodule detection was done using 3D Retina-UNET and classifier stage with a support vector machine algorithm on selected features. Performances were assessed using area under receiver operating characteristics curve (AUROC). RESULTS: The pipeline showed good performance for pathological nodule detection and patient diagnosis. With the preparation dataset, an AUROC of 0.9058 (95% confidence interval [CI]: 0.8746-0.9362) was obtained, 87% yielding accuracy (95% CI: 84.83%-91.03%) for the ""nodule detection"" stage, corresponding to 86% specificity (95% CI: 82%-92%) and 89% sensitivity (95% CI: 84.83%-91.03%). CONCLUSION: A fully functional pipeline using 3D U-NET, 3D Retina-UNET and classifier stage with a support vector machine algorithm was developed, resulting in high capabilities for pulmonary nodule classification.",2020,10.1016/j.diii.2020.10.004,cross-sectional,diagnosis,CT,Lung
Artificial intelligence to codify lung CT in Covid-19 patients,"The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already assumed pandemic proportions, affecting over 100 countries in few weeks. A global response is needed to prepare health systems worldwide. Covid-19 can be diagnosed both on chest X-ray and on computed tomography (CT). Asymptomatic patients may also have lung lesions on imaging. CT investigation in patients with suspicion Covid-19 pneumonia involves the use of the high-resolution technique (HRCT). Artificial intelligence (AI) software has been employed to facilitate CT diagnosis. AI software must be useful categorizing the disease into different severities, integrating the structured report, prepared according to subjective considerations, with quantitative, objective assessments of the extent of the lesions. In this communication, we present an example of a good tool for the radiologist (Thoracic VCAR software, GE Healthcare, Italy) in Covid-19 diagnosis (Pan et al. in Radiology, 2020. https://doi.org/10.1148/radiol.2020200370). Thoracic VCAR offers quantitative measurements of the lung involvement. Thoracic VCAR can generate a clear, fast and concise report that communicates vital medical information to referring physicians. In the post-processing phase, software, thanks to the help of a colorimetric map, recognizes the ground glass and differentiates it from consolidation and quantifies them as a percentage with respect to the healthy parenchyma. AI software therefore allows to accurately calculate the volume of each of these areas. Therefore, keeping in mind that CT has high diagnostic sensitivity in identifying lesions, but not specific for Covid-19 and similar to other infectious viral diseases, it is mandatory to have an AI software that expresses objective evaluations of the percentage of ventilated lung parenchyma compared to the affected one.",2020,10.1007/s11547-020-01195-x,cross-sectional,diagnosis,CT,Lung
Artificial Intelligence-Aided Multiple Tumor Detection Method Based on Immunohistochemistry-Enhanced Dark-Field Imaging,"The immunohistochemical method serves as one of the most practical tools in clinical cancer detection and thus has great application value to overcome the existing limits of the conventional method and further improve the detecting efficiency and sensitivity. This study employed 3,3'-diaminobenzidine (DAB), a conventional color indicator for immunohistochemistry, as a novel high-sensitive scattering reagent to provide a multidimensional image signal varying with the overexpression rate of tumor markers. Based on the scattering properties of DAB aggregates, an efficient and robust artificial intelligence-aided immunohistochemical method based on dark-field imaging has been established, with improvement in both the imaging quality and interpretation efficiency in comparison with the conventional manual-operated immunohistochemical method. Referencing the diagnosis from three independent pathologists, this method succeeded in detecting HER2 overexpressed breast tumors with a sensitivity of 95.2% and a specificity of 100.0%; meanwhile, it was found to be applicable for non-small-cell lung tumors and malignant lymphoma as well. As demonstrated, this study provided an effective and reliable means for making diagnostic suggestions, which exhibited great potential in multiple tumor pathological detection at low cost.",2022,10.1021/acs.analchem.1c04000,,,,
Artificial Intelligence-assisted chest X-ray assessment scheme for COVID-19,"OBJECTIVES: To study whether a trained convolutional neural network (CNN) can be of assistance to radiologists in differentiating Coronavirus disease (COVID)-positive from COVID-negative patients using chest X-ray (CXR) through an ambispective clinical study. To identify subgroups of patients where artificial intelligence (AI) can be of particular value and analyse what imaging features may have contributed to the performance of AI by means of visualisation techniques. METHODS: CXR of 487 patients were classified into [4] categories-normal, classical COVID, indeterminate, and non-COVID by consensus opinion of 2 radiologists. CXR which were classified as ""normal"" and ""indeterminate"" were then subjected to analysis by AI, and final categorisation provided as guided by prediction of the network. Precision and recall of the radiologist alone and radiologist assisted by AI were calculated in comparison to reverse transcriptase-polymerase chain reaction (RT-PCR) as the gold standard. Attention maps of the CNN were analysed to understand regions in the CXR important to the AI algorithm in making a prediction. RESULTS: The precision of radiologists improved from 65.9 to 81.9% and recall improved from 17.5 to 71.75 when assistance with AI was provided. AI showed 92% accuracy in classifying ""normal"" CXR into COVID or non-COVID. Analysis of attention maps revealed attention on the cardiac shadow in these ""normal"" radiographs. CONCLUSION: This study shows how deployment of an AI algorithm can complement a human expert in the determination of COVID status. Analysis of the detected features suggests possible subtle cardiac changes, laying ground for further investigative studies into possible cardiac changes. KEY POINTS: • Through an ambispective clinical study, we show how assistance with an AI algorithm can improve recall (sensitivity) and precision (positive predictive value) of radiologists in assessing CXR for possible COVID in comparison to RT-PCR. • We show that AI achieves the best results in images classified as ""normal"" by radiologists. We conjecture that possible subtle cardiac in the CXR, imperceptible to the human eye, may have contributed to this prediction. • The reported results may pave the way for a human computer collaboration whereby the expert with some help from the AI algorithm achieves higher accuracy in predicting COVID status on CXR than previously thought possible when considering either alone.",2021,10.1007/s00330-020-07628-5,cross-sectional,diagnosis,x-ray,Lung
Artificial Intelligence-based Fully Automated Per Lobe Segmentation and Emphysema-quantification Based on Chest Computed Tomography Compared With Global Initiative for Chronic Obstructive Lung Disease Severity of Smokers,"OBJECTIVES: The objective of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for the fully automated per lobe segmentation and emphysema quantification (EQ) on chest-computed tomography as it compares to the Global Initiative for Chronic Obstructive Lung Disease (GOLD) severity classification of chronic obstructive pulmonary disease (COPD) patients. METHODS: Patients (n=137) who underwent chest-computed tomography acquisition and spirometry within 6 months were retrospectively included in this Institutional Review Board-approved and Health Insurance Portability and Accountability Act-compliant study. Patient-specific spirometry data, which included forced expiratory volume in 1 second, forced vital capacity, and the forced expiratory volume in 1 second/forced vital capacity ratio (Tiffeneau-Index), were used to assign patients to their respective GOLD stage I to IV. Lung lobe segmentation was carried out using AI-RAD Companion software prototype (Siemens Healthineers), a deep convolution image-to-image network and emphysema was quantified in each lung lobe to detect the low attenuation volume. RESULTS: A strong correlation between the whole-lung-EQ and the GOLD stages was found (ρ=0.88, P<0.0001). The most significant correlation was noted in the left upper lobe (ρ=0.85, P<0.0001), and the weakest in the left lower lobe (ρ=0.72, P<0.0001) and right middle lobe (ρ=0.72, P<0.0001). CONCLUSIONS: AI-based per lobe segmentation and its EQ demonstrate a very strong correlation with the GOLD severity stages of COPD patients. Furthermore, the low attenuation volume of the left upper lobe not only showed the strongest correlation to GOLD severity but was also able to most clearly distinguish mild and moderate forms of COPD. This is particularly relevant due to the fact that early disease processes often elude conventional pulmonary function diagnostics. Earlier detection of COPD is a crucial element for positively altering the course of disease progression through various therapeutic options.",2020,10.1097/rti.0000000000000500,cross-sectional,diagnosis,CT,Lung
Artificial intelligence-based imaging analytics and lung cancer diagnostics: Considerations for health system leaders,"Lung cancer is a leading cause of cancer death in Canada, and accurate, early diagnosis are critical to improving clinical outcomes. Artificial Intelligence (AI)-based imaging analytics are a promising healthcare innovation that aim to improve the accuracy and efficiency of lung cancer diagnosis. Maximizing their clinical potential while mitigating their risks and limitations will require focused leadership informed by interdisciplinary expertise and system-wide insight. We convened a knowledge exchange workshop with diverse Saskatchewan health system leaders and stakeholders to explore issues surrounding the use of AI in diagnostic imaging for lung cancer, including implementation opportunities, challenges, and priorities. This technology is anticipated to improve patient outcomes, reduce unnecessary healthcare spending, and increase knowledge. However, health system leaders must also address the needs for robust data, financial investment, effective communication and collaboration between healthcare sectors, privacy and data protections, and continued interdisciplinary research to achieve this technology's potential benefits.",2021,10.1177/0840470420975062,,,,
Artificial intelligence-enabled rapid diagnosis of patients with COVID-19,"For diagnosis of coronavirus disease 2019 (COVID-19), a SARS-CoV-2 virus-specific reverse transcriptase polymerase chain reaction (RT-PCR) test is routinely used. However, this test can take up to 2 d to complete, serial testing may be required to rule out the possibility of false negative results and there is currently a shortage of RT-PCR test kits, underscoring the urgent need for alternative methods for rapid and accurate diagnosis of patients with COVID-19. Chest computed tomography (CT) is a valuable component in the evaluation of patients with suspected SARS-CoV-2 infection. Nevertheless, CT alone may have limited negative predictive value for ruling out SARS-CoV-2 infection, as some patients may have normal radiological findings at early stages of the disease. In this study, we used artificial intelligence (AI) algorithms to integrate chest CT findings with clinical symptoms, exposure history and laboratory testing to rapidly diagnose patients who are positive for COVID-19. Among a total of 905 patients tested by real-time RT-PCR assay and next-generation sequencing RT-PCR, 419 (46.3%) tested positive for SARS-CoV-2. In a test set of 279 patients, the AI system achieved an area under the curve of 0.92 and had equal sensitivity as compared to a senior thoracic radiologist. The AI system also improved the detection of patients who were positive for COVID-19 via RT-PCR who presented with normal CT scans, correctly identifying 17 of 25 (68%) patients, whereas radiologists classified all of these patients as COVID-19 negative. When CT scans and associated clinical history are available, the proposed AI system can help to rapidly diagnose COVID-19 patients.",2020,10.1038/s41591-020-0931-3,cross-sectional,diagnosis,CT,Lung
Artificial intelligence-supported lung cancer detection by multi-institutional readers with multi-vendor chest radiographs: a retrospective clinical validation study,"BACKGROUND: We investigated the performance improvement of physicians with varying levels of chest radiology experience when using a commercially available artificial intelligence (AI)-based computer-assisted detection (CAD) software to detect lung cancer nodules on chest radiographs from multiple vendors. METHODS: Chest radiographs and their corresponding chest CT were retrospectively collected from one institution between July 2017 and June 2018. Two author radiologists annotated pathologically proven lung cancer nodules on the chest radiographs while referencing CT. Eighteen readers (nine general physicians and nine radiologists) from nine institutions interpreted the chest radiographs. The readers interpreted the radiographs alone and then reinterpreted them referencing the CAD output. Suspected nodules were enclosed with a bounding box. These bounding boxes were judged correct if there was significant overlap with the ground truth, specifically, if the intersection over union was 0.3 or higher. The sensitivity, specificity, accuracy, PPV, and NPV of the readers' assessments were calculated. RESULTS: In total, 312 chest radiographs were collected as a test dataset, including 59 malignant images (59 nodules of lung cancer) and 253 normal images. The model provided a modest boost to the reader's sensitivity, particularly helping general physicians. The performance of general physicians was improved from 0.47 to 0.60 for sensitivity, from 0.96 to 0.97 for specificity, from 0.87 to 0.90 for accuracy, from 0.75 to 0.82 for PPV, and from 0.89 to 0.91 for NPV while the performance of radiologists was improved from 0.51 to 0.60 for sensitivity, from 0.96 to 0.96 for specificity, from 0.87 to 0.90 for accuracy, from 0.76 to 0.80 for PPV, and from 0.89 to 0.91 for NPV. The overall increase in the ratios of sensitivity, specificity, accuracy, PPV, and NPV were 1.22 (1.14-1.30), 1.00 (1.00-1.01), 1.03 (1.02-1.04), 1.07 (1.03-1.11), and 1.02 (1.01-1.03) by using the CAD, respectively. CONCLUSION: The AI-based CAD was able to improve the ability of physicians to detect nodules of lung cancer in chest radiographs. The use of a CAD model can indicate regions physicians may have overlooked during their initial assessment.",2021,10.1186/s12885-021-08847-9,cross-sectional,diagnosis,X-ray,Lung
Artificial Neural Network-Based Deep Learning Model for COVID-19 Patient Detection Using X-Ray Chest Images,"The world is experiencing an unprecedented crisis due to the coronavirus disease (COVID-19) outbreak that has affected nearly 216 countries and territories across the globe. Since the pandemic outbreak, there is a growing interest in computational model-based diagnostic technologies to support the screening and diagnosis of COVID-19 cases using medical imaging such as chest X-ray (CXR) scans. It is discovered in initial studies that patients infected with COVID-19 show abnormalities in their CXR images that represent specific radiological patterns. Still, detection of these patterns is challenging and time-consuming even for skilled radiologists. In this study, we propose a novel convolutional neural network- (CNN-) based deep learning fusion framework using the transfer learning concept where parameters (weights) from different models are combined into a single model to extract features from images which are then fed to a custom classifier for prediction. We use gradient-weighted class activation mapping to visualize the infected areas of CXR images. Furthermore, we provide feature representation through visualization to gain a deeper understanding of the class separability of the studied models with respect to COVID-19 detection. Cross-validation studies are used to assess the performance of the proposed models using open-access datasets containing healthy and both COVID-19 and other pneumonia infected CXR images. Evaluation results show that the best performing fusion model can attain a classification accuracy of 95.49% with a high level of sensitivity and specificity.",2021,10.1155/2021/5513679,cross-sectional,diagnosis,X-ray,Lung
Artificial neural networks improve LDCT lung cancer screening: a comparative validation study,"BACKGROUND: This study proposes a prediction model for the automatic assessment of lung cancer risk based on an artificial neural network (ANN) with a data-driven approach to the low-dose computed tomography (LDCT) standardized structure report. METHODS: This comparative validation study analysed a prospective cohort from Chiayi Chang Gung Memorial Hospital, Taiwan. In total, 836 asymptomatic patients who had undergone LDCT scans between February 2017 and August 2018 were included, comprising 27 lung cancer cases and 809 controls. A derivation cohort of 602 participants (19 lung cancer cases and 583 controls) was collected to construct the ANN prediction model. A comparative validation of the ANN and Lung-RADS was conducted with a prospective cohort of 234 participants (8 lung cancer cases and 226 controls). The areas under the curves (AUCs) of the receiver operating characteristic (ROC) curves were used to compare the prediction models. RESULTS: At the cut-off of category 3, the Lung-RADS had a sensitivity of 12.5%, specificity of 96.0%, positive predictive value of 10.0%, and negative predictive value of 96.9%. At its optimal cut-off value, the ANN had a sensitivity of 75.0%, specificity of 85.0%, positive predictive value of 15.0%, and negative predictive value of 99.0%. The area under the ROC curve was 0.764 for the Lung-RADS and 0.873 for the ANN (P = 0.01). The two most important predictors used by the ANN for predicting lung cancer were the documented sizes of partially solid nodules and ground-glass nodules. CONCLUSIONS: Compared to the Lung-RADS, the ANN provided better sensitivity for the detection of lung cancer in an Asian population. In addition, the ANN provided a more refined discriminative ability than the Lung-RADS for lung cancer risk stratification with population-specific demographic characteristics. When lung nodules are detected and documented in a standardized structured report, ANNs may better provide important insights for lung cancer prediction than conventional rule-based criteria.",2020,10.1186/s12885-020-07465-1,cross-sectional,diagnosis,CT,Lung
Assessing Lung Cancer Absolute Risk Trajectory Based on a Polygenic Risk Model,"Lung cancer is the leading cause of cancer-related death globally. An improved risk stratification strategy can increase efficiency of low-dose CT (LDCT) screening. Here we assessed whether individual's genetic background has clinical utility for risk stratification in the context of LDCT screening. On the basis of 13,119 patients with lung cancer and 10,008 controls with European ancestry in the International Lung Cancer Consortium, we constructed a polygenic risk score (PRS) via 10-fold cross-validation with regularized penalized regression. The performance of risk model integrating PRS, including calibration and ability to discriminate, was assessed using UK Biobank data (N = 335,931). Absolute risk was estimated on the basis of age-specific lung cancer incidence and all-cause mortality as competing risk. To evaluate its potential clinical utility, the PRS distribution was simulated in the National Lung Screening Trial (N = 50,772 participants). The lung cancer ORs for individuals at the top decile of the PRS distribution versus those at bottom 10% was 2.39 [95% confidence interval (CI) = 1.92-3.00; P = 1.80 × 10(-14)] in the validation set (P (trend) = 5.26 × 10(-20)). The OR per SD of PRS increase was 1.26 (95% CI = 1.20-1.32; P = 9.69 × 10(-23)) for overall lung cancer risk in the validation set. When considering absolute risks, individuals at different PRS deciles showed differential trajectories of 5-year and cumulative absolute risk. The age reaching the LDCT screening recommendation threshold can vary by 4 to 8 years, depending on the individual's genetic background, smoking status, and family history. Collectively, these results suggest that individual's genetic background may inform the optimal lung cancer LDCT screening strategy. SIGNIFICANCE: Three large-scale datasets reveal that, after accounting for risk factors, an individual's genetics can affect their lung cancer risk trajectory, thus may inform the optimal timing for LDCT screening.",2021,10.1158/0008-5472.Can-20-1237,,,,
Assessing micrometastases as a target for nanoparticles using 3D microscopy and machine learning,"Metastasis of solid tumors is a key determinant of cancer patient survival. Targeting micrometastases using nanoparticles could offer a way to stop metastatic tumor growth before it causes excessive patient morbidity. However, nanoparticle delivery to micrometastases is difficult to investigate because micrometastases are small in size and lie deep within tissues. Here, we developed an imaging and image analysis workflow to analyze nanoparticle-cell interactions in metastatic tumors. This technique combines tissue clearing and 3D microscopy with machine learning-based image analysis to assess the physiology of micrometastases with single-cell resolution and quantify the delivery of nanoparticles within them. We show that nanoparticles access a higher proportion of cells in micrometastases (50% nanoparticle-positive cells) compared with primary tumors (17% nanoparticle-positive cells) because they reside close to blood vessels and require a small diffusion distance to reach all tumor cells. Furthermore, the high-throughput nature of our image analysis workflow allowed us to profile the physiology and nanoparticle delivery of 1,301 micrometastases. This enabled us to use machine learning-based modeling to predict nanoparticle delivery to individual micrometastases based on their physiology. Our imaging method allows researchers to measure nanoparticle delivery to micrometastases and highlights an opportunity to target micrometastases with nanoparticles. The development of models to predict nanoparticle delivery based on micrometastasis physiology could enable personalized treatments based on the specific physiology of a patient's micrometastases.",2019,10.1073/pnas.1907646116,,,,
Assessing PD-L1 expression in non-small cell lung cancer and predicting responses to immune checkpoint inhibitors using deep learning on computed tomography images,"Rationale: This study aimed to use computed tomography (CT) images to assess PD-L1 expression in non-small cell lung cancer (NSCLC) and predict response to immunotherapy. Methods: We retrospectively analyzed a PD-L1 expression dataset that consisted of 939 consecutive stage IIIB-IV NSCLC patients with pretreatment CT images. A deep convolutional neural network was trained and optimized with CT images from the training cohort (n = 750) and validation cohort (n = 93) to obtain a PD-L1 expression signature (PD-L1ES), which was evaluated using the test cohort (n = 96). Finally, a separate immunotherapy cohort (n = 94) was used to assess the prognostic value of PD-L1ES with respect to clinical outcome. Results: PD-L1ES was able to predict high PD-L1 expression (PD-L1 ≥ 50%) with areas under the receiver operating characteristic curve (AUC) of 0.78 (95% confidence interval (CI): 0.75~0.80), 0.71 (95% CI: 0.59~0.81), and 0.76 (95% CI: 0.66~0.85) in the training, validation, and test cohorts, respectively. In patients treated with anti-PD-1 antibody, low PD-L1ES was associated with improved progression-free survival (PFS) (median PFS 363 days in low score group vs 183 days in high score group; hazard ratio [HR]: 2.57, 95% CI: 1.22~5.44; P = 0.010). Additionally, when PD-L1ES was combined with a clinical model that was trained using age, sex, smoking history and family history of malignancy, the response to immunotherapy could be better predicted compared to either PD-L1ES or the clinical model alone. Conclusions: The deep learning model provides a noninvasive method to predict high PD-L1 expression of NSCLC and to infer clinical outcomes in response to immunotherapy. Additionally, this deep learning model combined with clinical models demonstrated improved stratification capabilities.",2021,10.7150/thno.48027,cross-sectional,diagnosis,CT,Lung
Assessing the Accuracy of a Deep Learning Method to Risk Stratify Indeterminate Pulmonary Nodules,"Rationale: The management of indeterminate pulmonary nodules (IPNs) remains challenging, resulting in invasive procedures and delays in diagnosis and treatment. Strategies to decrease the rate of unnecessary invasive procedures and optimize surveillance regimens are needed.Objectives: To develop and validate a deep learning method to improve the management of IPNs.Methods: A Lung Cancer Prediction Convolutional Neural Network model was trained using computed tomography images of IPNs from the National Lung Screening Trial, internally validated, and externally tested on cohorts from two academic institutions.Measurements and Main Results: The areas under the receiver operating characteristic curve in the external validation cohorts were 83.5% (95% confidence interval [CI], 75.4-90.7%) and 91.9% (95% CI, 88.7-94.7%), compared with 78.1% (95% CI, 68.7-86.4%) and 81.9 (95% CI, 76.1-87.1%), respectively, for a commonly used clinical risk model for incidental nodules. Using 5% and 65% malignancy thresholds defining low- and high-risk categories, the overall net reclassifications in the validation cohorts for cancers and benign nodules compared with the Mayo model were 0.34 (Vanderbilt) and 0.30 (Oxford) as a rule-in test, and 0.33 (Vanderbilt) and 0.58 (Oxford) as a rule-out test. Compared with traditional risk prediction models, the Lung Cancer Prediction Convolutional Neural Network was associated with improved accuracy in predicting the likelihood of disease at each threshold of management and in our external validation cohorts.Conclusions: This study demonstrates that this deep learning algorithm can correctly reclassify IPNs into low- or high-risk categories in more than a third of cancers and benign nodules when compared with conventional risk models, potentially reducing the number of unnecessary invasive procedures and delays in diagnosis.",2020,10.1164/rccm.201903-0505OC,cross-sectional,diagnosis,CT,Lung
Assessing the utility of autofluorescence-based pulmonary optical endomicroscopy to predict the malignant potential of solitary pulmonary nodules in humans,"Solitary pulmonary nodules are common, often incidental findings on chest CT scans. The investigation of pulmonary nodules is time-consuming and often leads to protracted follow-up with ongoing radiological surveillance, however, clinical calculators that assess the risk of the nodule being malignant exist to help in the stratification of patients. Furthermore recent advances in interventional pulmonology include the ability to both navigate to nodules and also to perform autofluorescence endomicroscopy. In this study we assessed the efficacy of incorporating additional information from label-free fibre-based optical endomicrosopy of the nodule on assessing risk of malignancy. Using image analysis and machine learning approaches, we find that this information does not yield any gain in predictive performance in a cohort of patients. Further advances with pulmonary endomicroscopy will require the addition of molecular tracers to improve information from this procedure.",2016,10.1038/srep31372,cross-sectional,diagnosis,CT,Lung
Assessment of associations between clinical and immune microenvironmental factors and tumor mutation burden in resected nonsmall cell lung cancer by applying machine learning to whole-slide images,"BACKGROUND: It is unclear whether clinical factors and immune microenvironment (IME) factors are associated with tumor mutation burden (TMB) in patients with nonsmall cell lung cancer (NSCLC). MATERIALS AND METHODS: We assessed TMB in surgical tumor specimens by performing whole exome sequencing. IME profiles, including PD-L1 tumor proportion score (TPS), stromal CD8 tumor-infiltrating lymphocyte (TIL) density, and stromal Foxp3 TIL density, were quantified by digital pathology using a machine learning algorithm. To detect factors associated with TMB, clinical data, and IME factors were assessed by means of a multiple regression model. RESULTS: We analyzed tumors from 200 of the 246 surgically resected NSCLC patients between September 2014 and September 2015. Patient background: median age (range) 70 years (39-87); male 37.5%; smoker 27.5%; pathological stage (p-stage) I/II/III, 63.5/22.5/14.0%; histological type Ad/Sq, 77.0/23.0%; primary tumor location upper/lower, 58.5/41.5%; median PET SUV 7.5 (0.86-29.8); median serum CEA (sCEA) level 3.4 ng/mL (0.5-144.3); median serum CYFRA 21-1 (sCYFRA) level 1.2 ng/mL (1.0-38.0); median TMB 2.19/ Mb (0.12-64.38); median PD-L1 TPS 15.1% (0.09-77.4); median stromal CD8 TIL density 582.1/mm(2) (120.0-4967.6);, and median stromal Foxp3 TIL density 183.7/mm(2) (6.3-544.0). The multiple regression analysis identified three factors associated with higher TMB: smoking status: smoker, increase PET SUV, and sCEA level: >5 ng/mL (P < .001, P < .001, and P = .006, respectively). CONCLUSIONS: The IME factors assessed were not associated with TMB, but our findings showed that, in addition to smoking, PET SUV and sCEA levels may be independent predictors of TMB. TMB and IME factors are independent factors in resected NSCLC.",2020,10.1002/cam4.3107,,,,
Assisting scalable diagnosis automatically via CT images in the combat against COVID-19,"The pandemic of Coronavirus Disease 2019 (COVID-19) is causing enormous loss of life globally. Prompt case identification is critical. The reference method is the real-time reverse transcription PCR (RT-PCR) assay, whose limitations may curb its prompt large-scale application. COVID-19 manifests with chest computed tomography (CT) abnormalities, some even before the onset of symptoms. We tested the hypothesis that the application of deep learning (DL) to 3D CT images could help identify COVID-19 infections. Using data from 920 COVID-19 and 1,073 non-COVID-19 pneumonia patients, we developed a modified DenseNet-264 model, COVIDNet, to classify CT images to either class. When tested on an independent set of 233 COVID-19 and 289 non-COVID-19 pneumonia patients, COVIDNet achieved an accuracy rate of 94.3% and an area under the curve of 0.98. As of March 23, 2020, the COVIDNet system had been used 11,966 times with a sensitivity of 91.12% and a specificity of 88.50% in six hospitals with PCR confirmation. Application of DL to CT images may improve both efficiency and capacity of case detection and long-term surveillance.",2021,10.1038/s41598-021-83424-5,cross-sectional,diagnosis,CT,Lung
Association of AI quantified COVID-19 chest CT and patient outcome,"PURPOSE: Severity scoring is a key step in managing patients with COVID-19 pneumonia. However, manual quantitative analysis by radiologists is a time-consuming task, while qualitative evaluation may be fast but highly subjective. This study aims to develop artificial intelligence (AI)-based methods to quantify disease severity and predict COVID-19 patient outcome. METHODS: We develop an AI-based framework that employs deep neural networks to efficiently segment lung lobes and pulmonary opacities. The volume ratio of pulmonary opacities inside each lung lobe gives the severity scores of the lobes, which are then used to predict ICU admission and mortality with three different machine learning methods. The developed methods were evaluated on datasets from two hospitals (site A: Firoozgar Hospital, Iran, 105 patients; site B: Massachusetts General Hospital, USA, 88 patients). RESULTS: AI-based severity scores are strongly associated with those evaluated by radiologists (Spearman's rank correlation 0.837, [Formula: see text]). Using AI-based scores produced significantly higher ([Formula: see text]) area under the ROC curve (AUC) values. The developed AI method achieved the best performance of AUC = 0.813 (95% CI [0.729, 0.886]) in predicting ICU admission and AUC = 0.741 (95% CI [0.640, 0.837]) in mortality estimation on the two datasets. CONCLUSIONS: Accurate severity scores can be obtained using the developed AI methods over chest CT images. The computed severity scores achieved better performance than radiologists in predicting COVID-19 patient outcome by consistently quantifying image features. Such developed techniques of severity assessment may be extended to other lung diseases beyond the current pandemic.",2021,10.1007/s11548-020-02299-5,cross-sectional,diagnosis,CT,Lung
Association of Omics Features with Histopathology Patterns in Lung Adenocarcinoma,"Adenocarcinoma accounts for more than 40% of lung malignancy, and microscopic pathology evaluation is indispensable for its diagnosis. However, how histopathology findings relate to molecular abnormalities remains largely unknown. Here, we obtained H&E-stained whole-slide histopathology images, pathology reports, RNA sequencing, and proteomics data of 538 lung adenocarcinoma patients from The Cancer Genome Atlas and used these to identify molecular pathways associated with histopathology patterns. We report cell-cycle regulation and nucleotide binding pathways underpinning tumor cell dedifferentiation, and we predicted histology grade using transcriptomics and proteomics signatures (area under curve >0.80). We built an integrative histopathology-transcriptomics model to generate better prognostic predictions for stage I patients (p = 0.0182 ± 0.0021) compared with gene expression or histopathology studies alone, and the results were replicated in an independent cohort (p = 0.0220 ± 0.0070). These results motivate the integration of histopathology and omics data to investigate molecular mechanisms of pathology findings and enhance clinical prognostic prediction.",2017,10.1016/j.cels.2017.10.014,,,,
Asymptomatic Transmissibility Calls for Implementing a Zero-COVID Strategy to End the Current Global Crisis,"The coronavirus disease 2019 (COVID-19) pandemic has led to unprecedented global challenges. A zero-COVID strategy is needed to end the crisis, but there is a lack of biological evidence. In the present study, we collected available data on SARS, MERS, and COVID-19 to perform a comprehensive comparative analysis and visualization. The study results revealed that the fatality rate of COVID-19 is low, whereas its death toll is high compared to SARS and MERS. Moreover, COVID-19 had a higher asymptomatic rate. In particular, COVID-19 exhibited unique asymptomatic transmissibility. Further, we developed a foolproof operating software in Python language to simulate COVID-19 spread in Wuhan, showing that the cumulative cases of existing asymptomatic spread would be over 100 times higher than that of only symptomatic spread. This confirmed the essential role of asymptomatic transmissibility in the uncontrolled global spread of COVID-19, which enables the necessity of implementing the zero-COVID policy. In conclusion, we revealed the triggering role of the asymptomatic transmissibility of COVID-19 in this unprecedented global crisis, which offers support to the zero-COVID strategy against the recurring COVID-19 spread.",2022,10.3389/fcimb.2022.836409,,,,
Attention-embedded complementary-stream CNN for false positive reduction in pulmonary nodule detection,"False positive reduction plays a key role in computer-aided detection systems for pulmonary nodule detection in computed tomography (CT) scans. However, this remains a challenge owing to the heterogeneity and similarity of anisotropic pulmonary nodules. In this study, a novel attention-embedded complementary-stream convolutional neural network (AECS-CNN) is proposed to obtain more representative features of nodules for false positive reduction. The proposed network comprises three function blocks: 1) attention-guided multi-scale feature extraction, 2) complementary-stream block with an attention module for feature integration, and 3) classification block. The inputs of the network are multi-scale 3D CT volumes due to variations in nodule sizes. Subsequently, a gradual multi-scale feature extraction block with an attention module was applied to acquire more contextual information regarding the nodules. A subsequent complementary-stream integration block with an attention module was utilized to learn the significantly complementary features. Finally, the candidates were classified using a fully connected layer block. An exhaustive experiment on the LUNA16 challenge dataset was conducted to verify the effectiveness and performance of the proposed network. The AECS-CNN achieved a sensitivity of 0.92 with 4 false positives per scan. The results indicate that the attention mechanism can improve the network performance in false positive reduction, the proposed AECS-CNN can learn more representative features, and the attention module can guide the network to learn the discriminated feature channels and the crucial information embedded in the data, thereby effectively enhancing the performance of the detection system.",2021,10.1016/j.compbiomed.2021.104357,cross-sectional,diagnosis,CT,Lung
Attribute-guided image generation of three-dimensional computed tomography images of lung nodules using a generative adversarial network,"PURPOSE: To develop and evaluate a three-dimensional (3D) generative model of computed tomography (CT) images of lung nodules using a generative adversarial network (GAN). To guide the GAN, lung nodule size was used. MATERIALS AND METHODS: A public CT dataset of lung nodules was used, from where 1182 lung nodules were obtained. Our proposed GAN model used masked 3D CT images and nodule size information to generate images. To evaluate the generated CT images, two radiologists visually evaluated whether the CT images with lung nodule were true or generated, and the diagnostic ability was evaluated using receiver-operating characteristic analysis and area under the curves (AUC). Then, two models for classifying nodule size into five categories were trained, one using the true and the other using the generated CT images of lung nodules. Using true CT images, the classification accuracy of the sizes of the true lung nodules was calculated for the two classification models. RESULTS: The sensitivity, specificity, and AUC of the two radiologists were respectively as follows: radiologist 1: 81.3%, 37.7%, and 0.592; radiologist 2: 77.1%, 30.2%, and 0.597. For categorization of nodule size, the mean accuracy of the classification model constructed with true CT images was 85% (range 83.2-86.1%), and that with generated CT images was 85% (range 82.2-88.1%). CONCLUSIONS: Our results show that it was possible to generate 3D CT images of lung nodules that could be used to construct a classification model of lung nodule size without true CT images.",2020,10.1016/j.compbiomed.2020.104032,cross-sectional,diagnosis,CT,Lung
Augmentation of CBCT Reconstructed From Under-Sampled Projections Using Deep Learning,"Edges tend to be over-smoothed in total variation (TV) regularized under-sampled images. In this paper, symmetric residual convolutional neural network (SR-CNN), a deep learning based model, was proposed to enhance the sharpness of edges and detailed anatomical structures in under-sampled cone-beam computed tomography (CBCT). For training, CBCT images were reconstructed using TV-based method from limited projections simulated from the ground truth CT, and were fed into SR-CNN, which was trained to learn a restoring pattern from under-sampled images to the ground truth. For testing, under-sampled CBCT was reconstructed using TV regularization and was then augmented by SR-CNN. Performance of SR-CNN was evaluated using phantom and patient images of various disease sites acquired at different institutions both qualitatively and quantitatively using structure similarity (SSIM) and peak signal-to-noise ratio (PSNR). SR-CNN substantially enhanced image details in the TV-based CBCT across all experiments. In the patient study using real projections, SR-CNN augmented CBCT images reconstructed from as low as 120 half-fan projections to image quality comparable to the reference fully-sampled FDK reconstruction using 900 projections. In the tumor localization study, improvements in the tumor localization accuracy were made by the SR-CNN augmented images compared with the conventional FDK and TV-based images. SR-CNN demonstrated robustness against noise levels and projection number reductions and generalization for various disease sites and datasets from different institutions. Overall, the SR-CNN-based image augmentation technique was efficient and effective in considerably enhancing edges and anatomical structures in under-sampled 3D/4D-CBCT, which can be very valuable for image-guided radiotherapy.",2019,10.1109/tmi.2019.2912791,cross-sectional,informatics,CT,Lung
Augmenting existing deterioration indices with chest radiographs to predict clinical deterioration,"IMPORTANCE: When hospitals are at capacity, accurate deterioration indices could help identify low-risk patients as potential candidates for home care programs and alleviate hospital strain. To date, many existing deterioration indices are based entirely on structured data from the electronic health record (EHR) and ignore potentially useful information from other sources. OBJECTIVE: To improve the accuracy of existing deterioration indices by incorporating unstructured imaging data from chest radiographs. DESIGN, SETTING, AND PARTICIPANTS: Machine learning models were trained to predict deterioration of patients hospitalized with acute dyspnea using existing deterioration index scores and chest radiographs. Models were trained on hospitalized patients without coronavirus disease 2019 (COVID-19) and then subsequently tested on patients with COVID-19 between January 2020 and December 2020 at a single tertiary care center who had at least one radiograph taken within 48 hours of hospital admission. MAIN OUTCOMES AND MEASURES: Patient deterioration was defined as the need for invasive or non-invasive mechanical ventilation, heated high flow nasal cannula, IV vasopressor administration or in-hospital mortality at any time following admission. The EPIC deterioration index was augmented with unstructured data from chest radiographs to predict risk of deterioration. We compared discriminative performance of the models with and without incorporating chest radiographs using area under the receiver operating curve (AUROC), focusing on comparing the fraction and total patients identified as low risk at different negative predictive values (NPV). RESULTS: Data from 6278 hospitalizations were analyzed, including 5562 hospitalizations without COVID-19 (training cohort) and 716 with COVID-19 (216 in validation, 500 in held-out test cohort). At a NPV of 0.95, the best-performing image-augmented deterioration index identified 49 more (9.8%) individuals as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission. At a NPV of 0.9, the EPIC image-augmented deterioration index identified 26 more individuals (5.2%) as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission. CONCLUSION AND RELEVANCE: Augmenting existing deterioration indices with chest radiographs results in better identification of low-risk patients. The model augmentation strategy could be used in the future to incorporate other forms of unstructured data into existing disease models.",2022,10.1371/journal.pone.0263922,cross-sectional,diagnosis,X-ray,Lung
Augmenting lung cancer diagnosis on chest radiographs: positioning artificial intelligence to improve radiologist performance,"AIM: To evaluate the role that artificial intelligence (AI) could play in assisting radiologists as the first reader of chest radiographs (CXRs), to increase the accuracy and efficiency of lung cancer diagnosis by flagging positive cases before passing the remaining examinations to standard reporting. MATERIALS AND METHODS: A dataset of 400 CXRs including 200 difficult lung cancer cases was curated. Examinations were reviewed by three FRCR radiologists and an AI algorithm to establish performance in tumour identification. AI and radiologist labels were combined retrospectively to simulate the proposed AI triage workflow. RESULTS: When used as a standalone algorithm, AI classification was equivalent to the average radiologist performance. The best overall performances were achieved when AI was combined with radiologists, with an average reduction of missed cancers of 60%. Combination with AI also standardised the performance of radiologists. The greatest improvements were observed when common sources of errors were present, such as distracting findings. DISCUSSION: The proposed AI implementation pathway stands to reduce radiologist errors and improve clinician reporting performance. Furthermore, taking a radiologist-centric approach in the development of clinical AI holds promise for catching systematically missed lung cancers. This represents a tremendous opportunity to improve patient outcomes for lung cancer diagnosis.",2021,10.1016/j.crad.2021.03.021,cross-sectional,diagnosis,X-ray,Lung
Auto informing COVID-19 detection result from x-ray/CT images based on deep learning,"It is no secret to all that the corona pandemic has caused a decline in all aspects of the world. Therefore, offering an accurate automatic diagnostic system is very important. This paper proposed an accurate COVID-19 system by testing various deep learning models for x-ray/computed tomography (CT) medical images. A deep preprocessing procedure was done with two filters and segmentation to increase classification results. According to the results obtained, 99.94% of accuracy, 98.70% of sensitivity, and 100% of specificity scores were obtained by the Xception model in the x-ray dataset and the InceptionV3 model for CT scan images. The compared results have demonstrated that the proposed model is proven to be more successful than the deep learning algorithms in previous studies. Moreover, it has the ability to automatically notify the examination results to the patients, the health authority, and the community after taking any x-ray or CT images.",2021,10.1063/5.0059829,,,,
Automated approach for segmenting gross tumor volumes for lung cancer stereotactic body radiation therapy using CT-based dense V-networks,"The aim of this study was to develop an automated segmentation approach for small gross tumor volumes (GTVs) in 3D planning computed tomography (CT) images using dense V-networks (DVNs) that offer more advantages in segmenting smaller structures than conventional V-networks. Regions of interest (ROI) with dimensions of 50 × 50 × 6-72 pixels in the planning CT images were cropped based on the GTV centroids when applying stereotactic body radiotherapy (SBRT) to patients. Segmentation accuracy of GTV contours for 192 lung cancer patients [with the following tumor types: 118 solid, 53 part-solid types and 21 pure ground-glass opacity (pure GGO)], who underwent SBRT, were evaluated based on a 10-fold cross-validation test using Dice's similarity coefficient (DSC) and Hausdorff distance (HD). For each case, 11 segmented GTVs consisting of three single outputs, four logical AND outputs, and four logical OR outputs from combinations of two or three outputs from DVNs were obtained by three runs with different initial weights. The AND output (combination of three outputs) achieved the highest values of average 3D-DSC (0.832 ± 0.074) and HD (4.57 ± 2.44 mm). The average 3D DSCs from the AND output for solid, part-solid and pure GGO types were 0.838 ± 0.074, 0.822 ± 0.078 and 0.819 ± 0.059, respectively. This study suggests that the proposed approach could be useful in segmenting GTVs for planning lung cancer SBRT.",2021,10.1093/jrr/rraa132,cross-sectional,treatment,CT,Lung
Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence,"Background The coronavirus disease 2019 (COVID-19) pandemic has spread across the globe with alarming speed, morbidity, and mortality. Immediate triage of patients with chest infections suspected to be caused by COVID-19 using chest CT may be of assistance when results from definitive viral testing are delayed. Purpose To develop and validate an artificial intelligence (AI) system to score the likelihood and extent of pulmonary COVID-19 on chest CT scans using the COVID-19 Reporting and Data System (CO-RADS) and CT severity scoring systems. Materials and Methods The CO-RADS AI system consists of three deep-learning algorithms that automatically segment the five pulmonary lobes, assign a CO-RADS score for the suspicion of COVID-19, and assign a CT severity score for the degree of parenchymal involvement per lobe. This study retrospectively included patients who underwent a nonenhanced chest CT examination because of clinical suspicion of COVID-19 at two medical centers. The system was trained, validated, and tested with data from one of the centers. Data from the second center served as an external test set. Diagnostic performance and agreement with scores assigned by eight independent observers were measured using receiver operating characteristic analysis, linearly weighted κ values, and classification accuracy. Results A total of 105 patients (mean age, 62 years ± 16 [standard deviation]; 61 men) and 262 patients (mean age, 64 years ± 16; 154 men) were evaluated in the internal and external test sets, respectively. The system discriminated between patients with COVID-19 and those without COVID-19, with areas under the receiver operating characteristic curve of 0.95 (95% CI: 0.91, 0.98) and 0.88 (95% CI: 0.84, 0.93), for the internal and external test sets, respectively. Agreement with the eight human observers was moderate to substantial, with mean linearly weighted κ values of 0.60 ± 0.01 for CO-RADS scores and 0.54 ± 0.01 for CT severity scores. Conclusion With high diagnostic performance, the CO-RADS AI system correctly identified patients with COVID-19 using chest CT scans and assigned standardized CO-RADS and CT severity scores that demonstrated good agreement with findings from eight independent observers and generalized well to external data. © RSNA, 2020 Supplemental material is available for this article.",2021,10.1148/radiol.2020202439,cross-sectional,diagnosis,CT,Lung
Automated Classification of Lung Cancer Types from Cytological Images Using Deep Convolutional Neural Networks,"Lung cancer is a leading cause of death worldwide. Currently, in differential diagnosis of lung cancer, accurate classification of cancer types (adenocarcinoma, squamous cell carcinoma, and small cell carcinoma) is required. However, improving the accuracy and stability of diagnosis is challenging. In this study, we developed an automated classification scheme for lung cancers presented in microscopic images using a deep convolutional neural network (DCNN), which is a major deep learning technique. The DCNN used for classification consists of three convolutional layers, three pooling layers, and two fully connected layers. In evaluation experiments conducted, the DCNN was trained using our original database with a graphics processing unit. Microscopic images were first cropped and resampled to obtain images with resolution of 256 × 256 pixels and, to prevent overfitting, collected images were augmented via rotation, flipping, and filtering. The probabilities of three types of cancers were estimated using the developed scheme and its classification accuracy was evaluated using threefold cross validation. In the results obtained, approximately 71% of the images were classified correctly, which is on par with the accuracy of cytotechnologists and pathologists. Thus, the developed scheme is useful for classification of lung cancers from microscopic images.",2017,10.1155/2017/4067832,,,,
Automated coronary artery calcification scoring in non-gated chest CT: agreement and reliability,"OBJECTIVE: To determine the agreement and reliability of fully automated coronary artery calcium (CAC) scoring in a lung cancer screening population. MATERIALS AND METHODS: 1793 low-dose chest CT scans were analyzed (non-contrast-enhanced, non-gated). To establish the reference standard for CAC, first automated calcium scoring was performed using a preliminary version of a method employing coronary calcium atlas and machine learning approach. Thereafter, each scan was inspected by one of four trained raters. When needed, the raters corrected initially automaticity-identified results. In addition, an independent observer subsequently inspected manually corrected results and discarded scans with gross segmentation errors. Subsequently, fully automatic coronary calcium scoring was performed. Agatston score, CAC volume and number of calcifications were computed. Agreement was determined by calculating proportion of agreement and examining Bland-Altman plots. Reliability was determined by calculating linearly weighted kappa (κ) for Agatston strata and intraclass correlation coefficient (ICC) for continuous values. RESULTS: 44 (2.5%) scans were excluded due to metal artifacts or gross segmentation errors. In the remaining 1749 scans, median Agatston score was 39.6 (P25-P75∶0-345.9), median volume score was 60.4 mm3 (P25-P75∶0-361.4) and median number of calcifications was 2 (P25-P75∶0-4) for the automated scores. The κ demonstrated very good reliability (0.85) for Agatston risk categories between the automated and reference scores. The Bland-Altman plots showed underestimation of calcium score values by automated quantification. Median difference was 2.5 (p25-p75∶0.0-53.2) for Agatston score, 7.6 (p25-p75∶0.0-94.4) for CAC volume and 1 (p25-p75∶0-5) for number of calcifications. The ICC was very good for Agatston score (0.90), very good for calcium volume (0.88) and good for number of calcifications (0.64). DISCUSSION: Fully automated coronary calcium scoring in a lung cancer screening setting is feasible with acceptable reliability and agreement despite an underestimation of the amount of calcium when compared to reference scores.",2014,10.1371/journal.pone.0091239,cross-sectional,diagnosis,CT,Heart
Automated detection and classification of tumor histotypes on dynamic PET imaging data through machine-learning driven voxel classification,"2-deoxy-2-fluorine-((18)F)fluoro-d-glucose Positron Emission Tomography/Computed Tomography ((18)F-FDG-PET/CT) is widely used in oncology mainly for diagnosis and staging of various cancer types, including lung cancer, which is the most common cancer worldwide. Since histopathologic subtypes of lung cancer show different degree of (18)F-FDG uptake, to date there are some diagnostic limits and uncertainties, hindering an (18)F-FDG-PET-driven classification of histologic subtypes of lung cancers. On the other hand, since activated macrophages, neutrophils, fibroblasts and granulation tissues also show an increased (18)F-FDG activity, infectious and/or inflammatory processes and post-surgical and post-radiation changes may cause false-positive results, especially for lymph-nodes assessment. Here we propose a model-free, machine-learning based algorithm for the automated classification of adenocarcinoma, the most common type of lung cancer, and other types of tumors. Input for the algorithm are dynamic acquisitions of PET data (dPET), providing for a spatially and temporally resolved characterization of the uptake kinetic. The algorithm consists in a trained Random Forest classifier which, relying contextually on several spatial and temporal features of (18)F-FDG uptake, generates as an outcome probability maps allowing to distinguish adenocarcinoma from other lung histotype and to identify metastatic lymph-nodes, ultimately increasing the specificity of the technique. Its performance, evaluated on a dPET dataset of 19 patients affected by primary lung cancer, provides a probability 0.943 ± 0.090 for the detection of adenocarcinoma. The use of this algorithm will guarantee an automatic and more accurate localization and discrimination of tumors, also providing a powerful tool for detecting at which extent tumor has spread beyond a primary tumor into lymphatic system.",2022,10.1016/j.compbiomed.2022.105423,cross-sectional,diagnosis,PET,Lung
Automated detection and quantification of COVID-19 pneumonia: CT imaging analysis by a deep learning-based software,"BACKGROUND: The novel coronavirus disease 2019 (COVID-19) is an emerging worldwide threat to public health. While chest computed tomography (CT) plays an indispensable role in its diagnosis, the quantification and localization of lesions cannot be accurately assessed manually. We employed deep learning-based software to aid in detection, localization and quantification of COVID-19 pneumonia. METHODS: A total of 2460 RT-PCR tested SARS-CoV-2-positive patients (1250 men and 1210 women; mean age, 57.7 ± 14.0 years (age range, 11-93 years) were retrospectively identified from Huoshenshan Hospital in Wuhan from February 11 to March 16, 2020. Basic clinical characteristics were reviewed. The uAI Intelligent Assistant Analysis System was used to assess the CT scans. RESULTS: CT scans of 2215 patients (90%) showed multiple lesions of which 36 (1%) and 50 patients (2%) had left and right lung infections, respectively (> 50% of each affected lung's volume), while 27 (1%) had total lung infection (> 50% of the total volume of both lungs). Overall, 298 (12%), 778 (32%) and 1300 (53%) patients exhibited pure ground glass opacities (GGOs), GGOs with sub-solid lesions and GGOs with both sub-solid and solid lesions, respectively. Moreover, 2305 (94%) and 71 (3%) patients presented primarily with GGOs and sub-solid lesions, respectively. Elderly patients (≥ 60 years) were more likely to exhibit sub-solid lesions. The generalized linear mixed model showed that the dorsal segment of the right lower lobe was the favoured site of COVID-19 pneumonia. CONCLUSION: Chest CT combined with analysis by the uAI Intelligent Assistant Analysis System can accurately evaluate pneumonia in COVID-19 patients.",2020,10.1007/s00259-020-04953-1,cross-sectional,diagnosis,CT,Lung
Automated detection and segmentation of thoracic lymph nodes from CT using 3D foveal fully convolutional neural networks,"BACKGROUND: In oncology, the correct determination of nodal metastatic disease is essential for patient management, as patient treatment and prognosis are closely linked to the stage of the disease. The aim of the study was to develop a tool for automatic 3D detection and segmentation of lymph nodes (LNs) in computed tomography (CT) scans of the thorax using a fully convolutional neural network based on 3D foveal patches. METHODS: The training dataset was collected from the Computed Tomography Lymph Nodes Collection of the Cancer Imaging Archive, containing 89 contrast-enhanced CT scans of the thorax. A total number of 4275 LNs was segmented semi-automatically by a radiologist, assessing the entire 3D volume of the LNs. Using this data, a fully convolutional neuronal network based on 3D foveal patches was trained with fourfold cross-validation. Testing was performed on an unseen dataset containing 15 contrast-enhanced CT scans of patients who were referred upon suspicion or for staging of bronchial carcinoma. RESULTS: The algorithm achieved a good overall performance with a total detection rate of 76.9% for enlarged LNs during fourfold cross-validation in the training dataset with 10.3 false-positives per volume and of 69.9% in the unseen testing dataset. In the training dataset a better detection rate was observed for enlarged LNs compared to smaller LNs, the detection rate for LNs with a short-axis diameter (SAD) ≥ 20 mm and SAD 5-10 mm being 91.6% and 62.2% (p < 0.001), respectively. Best detection rates were obtained for LNs located in Level 4R (83.6%) and Level 7 (80.4%). CONCLUSIONS: The proposed 3D deep learning approach achieves an overall good performance in the automatic detection and segmentation of thoracic LNs and shows reasonable generalizability, yielding the potential to facilitate detection during routine clinical work and to enable radiomics research without observer-bias.",2021,10.1186/s12880-021-00599-z,cross-sectional,diagnosis,CT,Lymph nodes
Automated Detection of COVID-19 Cases on Radiographs using Shape-Dependent Fibonacci-p Patterns,"The coronavirus (COVID-19) pandemic has been adversely affecting people's health globally. To diminish the effect of this widespread pandemic, it is essential to detect COVID-19 cases as quickly as possible. Chest radiographs are less expensive and are a widely available imaging modality for detecting chest pathology compared with CT images. They play a vital role in early prediction and developing treatment plans for suspected or confirmed COVID-19 chest infection patients. In this paper, a novel shape-dependent Fibonacci-p patterns-based feature descriptor using a machine learning approach is proposed. Computer simulations show that the presented system (1) increases the effectiveness of differentiating COVID-19, viral pneumonia, and normal conditions, (2) is effective on small datasets, and (3) has faster inference time compared to deep learning methods with comparable performance. Computer simulations are performed on two publicly available datasets; (a) the Kaggle dataset, and (b) the COVIDGR dataset. To assess the performance of the presented system, various evaluation parameters, such as accuracy, recall, specificity, precision, and f1-score are used. Nearly 100% differentiation between normal and COVID-19 radiographs is observed for the three-class classification scheme using the lung area-specific Kaggle radiographs. While Recall of 72.65 ± 6.83 and specificity of 77.72 ± 8.06 is observed for the COVIDGR dataset.",2021,10.1109/jbhi.2021.3069798,cross-sectional,diagnosis,X-ray,Lung
Automated detection of COVID-19 cases using deep neural networks with X-ray images,"The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.",2020,10.1016/j.compbiomed.2020.103792,cross-sectional,diagnosis,X-ray,Lung
Automated detection of COVID-19 through convolutional neural network using chest x-ray images,"The COVID-19 epidemic has a catastrophic impact on global well-being and public health. More than 27 million confirmed cases have been reported worldwide until now. Due to the growing number of confirmed cases, and challenges to the variations of the COVID-19, timely and accurate classification of healthy and infected patients is essential to control and treat COVID-19. We aim to develop a deep learning-based system for the persuasive classification and reliable detection of COVID-19 using chest radiography. Firstly, we evaluate the performance of various state-of-the-art convolutional neural networks (CNNs) proposed over recent years for medical image classification. Secondly, we develop and train CNN from scratch. In both cases, we use a public X-Ray dataset for training and validation purposes. For transfer learning, we obtain 100% accuracy for binary classification (i.e., Normal/COVID-19) and 87.50% accuracy for tertiary classification (Normal/COVID-19/Pneumonia). With the CNN trained from scratch, we achieve 93.75% accuracy for tertiary classification. In the case of transfer learning, the classification accuracy drops with the increased number of classes. The results are demonstrated by comprehensive receiver operating characteristics (ROC) and confusion metric analysis with 10-fold cross-validation.",2022,10.1371/journal.pone.0262052,cross-sectional,diagnosis,X-ray,Lung
Automated detection of COVID-19 using ensemble of transfer learning with deep convolutional neural network based on CT scans,"PURPOSE: COVID-19 has infected millions of people worldwide. One of the most important hurdles in controlling the spread of this disease is the inefficiency and lack of medical tests. Computed tomography (CT) scans are promising in providing accurate and fast detection of COVID-19. However, determining COVID-19 requires highly trained radiologists and suffers from inter-observer variability. To remedy these limitations, this paper introduces an automatic methodology based on an ensemble of deep transfer learning for the detection of COVID-19. METHODS: A total of 15 pre-trained convolutional neural networks (CNNs) architectures: EfficientNets(B0-B5), NasNetLarge, NasNetMobile, InceptionV3, ResNet-50, SeResnet 50, Xception, DenseNet121, ResNext50 and Inception_resnet_v2 are used and then fine-tuned on the target task. After that, we built an ensemble method based on majority voting of the best combination of deep transfer learning outputs to further improve the recognition performance. We have used a publicly available dataset of CT scans, which consists of 349 CT scans labeled as being positive for COVID-19 and 397 negative COVID-19 CT scans that are normal or contain other types of lung diseases. RESULTS: The experimental results indicate that the majority voting of 5 deep transfer learning architecture with EfficientNetB0, EfficientNetB3, EfficientNetB5, Inception_resnet_v2, and Xception has the higher results than the individual transfer learning structure and among the other models based on precision (0.857), recall (0.854) and accuracy (0.85) metrics in diagnosing COVID-19 from CT scans. CONCLUSION: Our study based on an ensemble deep transfer learning system with different pre-trained CNNs architectures can work well on a publicly available dataset of CT images for the diagnosis of COVID-19 based on CT scans.",2021,10.1007/s11548-020-02286-w,cross-sectional,diagnosis,CT,Lung
Automated detection of lung cancer at ultralow dose PET/CT by deep neural networks - Initial results,"OBJECTIVES: We evaluated whether machine learning may be helpful for the detection of lung cancer in FDG-PET imaging in the setting of ultralow dose PET scans. MATERIALS AND METHODS: We studied the performance of an artificial neural network discriminating lung cancer patients (n = 50) from controls (n = 50) without pulmonary malignancies. A total of 3936 PET slices including images in which the lung tumor is visually present and image slices of patients with no lung cancer were exported. The diagnostic performance of the artificial neural network based on clinical standard dose PET images (PET(100%)) as well as with a tenfold (PET(10%)) and thirtyfold (PET(3.3%)) reduced radiation dose (∼0.11 mSv) was assessed. RESULTS: The area under the curve of the deep learning algorithm for lung cancer detection was 0.989, 0.983 and 0.970 for standard dose images (PET(100%)), and reduced dose PET(10%), and PET(3.3%) reconstruction, respectively. The artificial neural network achieved a sensitivity of 95.9% and 91.5% and a specificity of 98.1% and 94.2%, at standard dose and ultralow dose PET(3.3%), respectively. CONCLUSION: Our results suggest that machine learning algorithms may aid fully automated lung cancer detection even at very low effective radiation doses of 0.11 mSv. Further improvement of this technology might improve the specificity of lung cancer screening efforts and could lead to new applications of FDG-PET.",2018,10.1016/j.lungcan.2018.11.001,cross-sectional,diagnosis,PET/CT,Lung
Automated detection of lung nodules and coronary artery calcium using artificial intelligence on low-dose CT scans for lung cancer screening: accuracy and prognostic value,"BACKGROUND: Artificial intelligence (AI) in diagnostic radiology is undergoing rapid development. Its potential utility to improve diagnostic performance for cardiopulmonary events is widely recognized, but the accuracy and precision have yet to be demonstrated in the context of current screening modalities. Here, we present findings on the performance of an AI convolutional neural network (CNN) prototype (AI-RAD Companion, Siemens Healthineers) that automatically detects pulmonary nodules and quantifies coronary artery calcium volume (CACV) on low-dose chest CT (LDCT), and compare results to expert radiologists. We also correlate AI findings with adverse cardiopulmonary outcomes in a retrospective cohort of 117 patients who underwent LDCT. METHODS: A total of 117 patients were enrolled in this study. Two CNNs were used to identify lung nodules and CACV on LDCT scans. All subjects were used for lung nodule analysis, and 96 subjects met the criteria for coronary artery calcium volume analysis. Interobserver concordance was measured using ICC and Cohen's kappa. Multivariate logistic regression and partial least squares regression were used for outcomes analysis. RESULTS: Agreement of the AI findings with experts was excellent (CACV ICC = 0.904, lung nodules Cohen's kappa = 0.846) with high sensitivity and specificity (CACV: sensitivity = .929, specificity = .960; lung nodules: sensitivity = 1, specificity = 0.708). The AI findings improved the prediction of major cardiopulmonary outcomes at 1-year follow-up including major adverse cardiac events and lung cancer (AUC(MACE) = 0.911, AUC(Lung Cancer) = 0.942). CONCLUSION: We conclude the AI prototype rapidly and accurately identifies significant risk factors for cardiopulmonary disease on standard screening low-dose chest CT. This information can be used to improve diagnostic ability, facilitate intervention, improve morbidity and mortality, and decrease healthcare costs. There is also potential application in countries with limited numbers of cardiothoracic radiologists.",2021,10.1186/s12916-021-01928-3,cross-sectional,diagnosis,CT,Lung and Heart 
Automated detection of pulmonary nodules in PET/CT images: Ensemble false-positive reduction using a convolutional neural network technique,"PURPOSE: Automated detection of solitary pulmonary nodules using positron emission tomography (PET) and computed tomography (CT) images shows good sensitivity; however, it is difficult to detect nodules in contact with normal organs, and additional efforts are needed so that the number of false positives (FPs) can be further reduced. In this paper, the authors propose an improved FP-reduction method for the detection of pulmonary nodules in PET/CT images by means of convolutional neural networks (CNNs). METHODS: The overall scheme detects pulmonary nodules using both CT and PET images. In the CT images, a massive region is first detected using an active contour filter, which is a type of contrast enhancement filter that has a deformable kernel shape. Subsequently, high-uptake regions detected by the PET images are merged with the regions detected by the CT images. FP candidates are eliminated using an ensemble method; it consists of two feature extractions, one by shape/metabolic feature analysis and the other by a CNN, followed by a two-step classifier, one step being rule based and the other being based on support vector machines. RESULTS: The authors evaluated the detection performance using 104 PET/CT images collected by a cancer-screening program. The sensitivity in detecting candidates at an initial stage was 97.2%, with 72.8 FPs/case. After performing the proposed FP-reduction method, the sensitivity of detection was 90.1%, with 4.9 FPs/case; the proposed method eliminated approximately half the FPs existing in the previous study. CONCLUSIONS: An improved FP-reduction scheme using CNN technique has been developed for the detection of pulmonary nodules in PET/CT images. The authors' ensemble FP-reduction method eliminated 93% of the FPs; their proposed method using CNN technique eliminates approximately half the FPs existing in the previous study. These results indicate that their method may be useful in the computer-aided detection of pulmonary nodules using PET/CT images.",2016,10.1118/1.4948498,cross-sectional,diagnosis,PET/CT,Lung
Automated detection of skeletal metastasis of lung cancer with bone scans using convolutional nuclear network,"A bone scan is widely used for surveying bone metastases caused by various solid tumors. Scintigraphic images are characterized by inferior spatial resolution, bringing a significant challenge to manual analysis of images by nuclear medicine physicians. We present in this work a new framework for automatically classifying scintigraphic images collected from patients clinically diagnosed with lung cancer. The framework consists of data preparation and image classification. In the data preparation stage, data augmentation is used to enlarge the dataset, followed by image fusion and thoracic region extraction. In the image classification stage, we use a self-defined convolutional neural network consisting of feature extraction, feature aggregation, and feature classification sub-networks. The developed multi-class classification network can not only predict whether a bone scan image contains bone metastasis but also tell which subcategory of lung cancer that a bone metastasis metastasized from is present in the image. Experimental evaluations on a set of clinical bone scan images have shown that the proposed multi-class classification network is workable for automated classification of metastatic images, with achieving average scores of 0.7392, 0.7592, 0.7242, and 0.7292 for accuracy, precision, recall, and F-1 score, respectively.",2022,10.1088/1361-6560/ac4565,,,,
Automated Diagnosis of Chest X-Ray for Early Detection of COVID-19 Disease,"In March 2020, the World Health Organization announced the COVID-19 pandemic, its dangers, and its rapid spread throughout the world. In March 2021, the second wave of the pandemic began with a new strain of COVID-19, which was more dangerous for some countries, including India, recording 400,000 new cases daily and more than 4,000 deaths per day. This pandemic has overloaded the medical sector, especially radiology. Deep-learning techniques have been used to reduce the burden on hospitals and assist physicians for accurate diagnoses. In our study, two models of deep learning, ResNet-50 and AlexNet, were introduced to diagnose X-ray datasets collected from many sources. Each network diagnosed a multiclass (four classes) and a two-class dataset. The images were processed to remove noise, and a data augmentation technique was applied to the minority classes to create a balance between the classes. The features extracted by convolutional neural network (CNN) models were combined with traditional Gray-level Cooccurrence Matrix (GLCM) and Local Binary Pattern (LBP) algorithms in a 1-D vector of each image, which produced more representative features for each disease. Network parameters were tuned for optimum performance. The ResNet-50 network reached accuracy, sensitivity, specificity, and Area Under the Curve (AUC) of 95%, 94.5%, 98%, and 97.10%, respectively, with the multiclasses (COVID-19, viral pneumonia, lung opacity, and normal), while it reached accuracy, sensitivity, specificity, and AUC of 99%, 98%, 98%, and 97.51%, respectively, with the binary classes (COVID-19 and normal).",2021,10.1155/2021/6919483,cross-sectional,diagnosis,X-ray,Lung
Automated Diagnosis of COVID-19 Using Deep Features and Parameter Free BAT Optimization,"Background: Accurate and fast diagnosis of COVID-19 is very important to manage the medical conditions of affected persons. The task is challenging owing to shortage and ineffectiveness of clinical testing kits. However, the existing problems can be improved by employing computational intelligent techniques on radiological images like CT-Scans (Computed Tomography) of lungs. Extensive research has been reported using deep learning models to diagnose the severity of COVID-19 from CT images. This has undoubtedly minimized the manual involvement in abnormality identification but reported detection accuracy is limited. Methods: The present work proposes an expert model based on deep features and Parameter Free BAT (PF-BAT) optimized Fuzzy K-nearest neighbor (PF-FKNN) classifier to diagnose novel coronavirus. In this proposed model, features are extracted from the fully connected layer of transfer learned MobileNetv2 followed by FKNN training. The hyperparameters of FKNN are fine-tuned using PF-BAT. Results: The experimental results on the benchmark COVID CT scan data reveal that the proposed algorithm attains a validation accuracy of 99.38% which is better than the existing state-of-the-art methods proposed in past. Conclusion: The proposed model will help in timely and accurate identification of the coronavirus at the various phases. Such kind of rapid diagnosis will assist clinicians to manage the healthcare condition of patients well and will help in speedy recovery from the diseases. Clinical and Translational Impact Statement - The proposed automated system can provide accurate and fast detection of COVID-19 signature from lung radiographs. Also, the usage of lighter MobileNetv2 architecture makes it practical for deployment in real-time.",2021,10.1109/jtehm.2021.3077142,cross-sectional,diagnosis,CT,Lung
Automated identification of pulmonary arteries and veins depicted in non-contrast chest CT scans,"We present a novel integrative computerized solution to automatically identify and differentiate pulmonary arteries and veins depicted on chest computed tomography (CT) without iodinated contrast agents. We first identified the central extrapulmonary arteries and veins using a convolutional neural network (CNN) model. Then, a computational differential geometry method was used to automatically identify the tubular-like structures in the lungs with high densities, which we believe are the intrapulmonary vessels. Beginning with the extrapulmonary arteries and veins, we progressively traced the intrapulmonary vessels by following their skeletons and differentiated them into arteries and veins. Instead of manually labeling the numerous arteries and veins in the lungs for machine learning, this integrative strategy limits the manual effort only to the large extrapulmonary vessels. We used a dataset consisting of 120 chest CT scans acquired on different subjects using various protocols to develop, train, and test the algorithms. Our experiments on an independent test set (n = 15) showed promising performance. The computer algorithm achieved a sensitivity of ∼98% in labeling the pulmonary artery and vein branches when compared with a human expert's results, demonstrating the feasibility of our computerized solution in pulmonary artery/vein labeling.",2022,10.1016/j.media.2022.102367,cross-sectional,diagnosis,CT,Veins and arteries
Automated lung cancer diagnosis using three-dimensional convolutional neural networks,"Lung cancer is the deadliest cancer worldwide. It has been shown that early detection using low-dose computer tomography (LDCT) scans can reduce deaths caused by this disease. We present a general framework for the detection of lung cancer in chest LDCT images. Our method consists of a nodule detector trained on the LIDC-IDRI dataset followed by a cancer predictor trained on the Kaggle DSB 2017 dataset and evaluated on the IEEE International Symposium on Biomedical Imaging (ISBI) 2018 Lung Nodule Malignancy Prediction test set. Our candidate extraction approach is effective to produce accurate candidates with a recall of 99.6%. In addition, our false positive reduction stage classifies successfully the candidates and increases precision by a factor of 2000. Our cancer predictor obtained a ROC AUC of 0.913 and was ranked 1st place at the ISBI 2018 Lung Nodule Malignancy Prediction challenge. Graphical abstract.",2020,10.1007/s11517-020-02197-7,cross-sectional,diagnosis,CT,Lung
Automated Lung Nodule Detection and Classification Using Deep Learning Combined with Multiple Strategies,"Lung cancer is one of the major causes of cancer-related deaths due to its aggressive nature and delayed detections at advanced stages. Early detection of lung cancer is very important for the survival of an individual, and is a significant challenging problem. Generally, chest radiographs (X-ray) and computed tomography (CT) scans are used initially for the diagnosis of the malignant nodules; however, the possible existence of benign nodules leads to erroneous decisions. At early stages, the benign and the malignant nodules show very close resemblance to each other. In this paper, a novel deep learning-based model with multiple strategies is proposed for the precise diagnosis of the malignant nodules. Due to the recent achievements of deep convolutional neural networks (CNN) in image analysis, we have used two deep three-dimensional (3D) customized mixed link network (CMixNet) architectures for lung nodule detection and classification, respectively. Nodule detections were performed through faster R-CNN on efficiently-learned features from CMixNet and U-Net like encoder-decoder architecture. Classification of the nodules was performed through a gradient boosting machine (GBM) on the learned features from the designed 3D CMixNet structure. To reduce false positives and misdiagnosis results due to different types of errors, the final decision was performed in connection with physiological symptoms and clinical biomarkers. With the advent of the internet of things (IoT) and electro-medical technology, wireless body area networks (WBANs) provide continuous monitoring of patients, which helps in diagnosis of chronic diseases-especially metastatic cancers. The deep learning model for nodules' detection and classification, combined with clinical factors, helps in the reduction of misdiagnosis and false positive (FP) results in early-stage lung cancer diagnosis. The proposed system was evaluated on LIDC-IDRI datasets in the form of sensitivity (94%) and specificity (91%), and better results were obatined compared to the existing methods.",2019,10.3390/s19173722,cross-sectional,diagnosis,CT,Lung
Automated mapping and N-Staging of thoracic lymph nodes in contrast-enhanced CT scans of the chest using a fully convolutional neural network,"PURPOSE: To develop a deep-learning (DL)-based approach for thoracic lymph node (LN) mapping based on their anatomical location. METHOD: The training-and validation-dataset included 89 contrast-enhanced computed tomography (CT) scans of the chest. 4201 LNs were semi-automatically segmented and then assigned to LN levels according to their anatomical location. The LN level classification task was addressed by a multi-class segmentation procedure using a fully convolutional neural network. Mapping was performed by firstly determining potential level affiliation for each voxel and then performing majority voting over all voxels belonging to each LN. Mean classification accuracies on the validation data were calculated separately for each level and overall Top-1, Top-2 and Top-3 scores were determined, where a Top-X score describes how often the annotated class was within the top-X predictions. To demonstrate the clinical applicability of our model, we tested its N-staging capabilities in a simulated clinical use case scenario assuming a patient diseased with lung cancer. RESULTS: The artificial intelligence(AI)-based assignment revealed mean classification accuracies of 86.36 % (Top-1), 94.48 % (Top-2) and 96.10 % (Top-3). Best accuracies were achieved for LNs in the subcarinal level 7 (98.31 %) and axillary region (98.74 %). The highest misclassification rates were observed among LNs in adjacent levels. The proof-of-principle application in a simulated clinical use case scenario for automated tumor N-staging showed a mean classification accuracy of up to 96.14 % (Top-1). CONCLUSIONS: The proposed AI approach for automatic classification of LN levels in chest CT as well as the proof-of-principle-experiment for automatic N-staging, revealed promising results, warranting large-scale validation for clinical application.",2021,10.1016/j.ejrad.2021.109718,cross-sectional,diagnosis,CT,Lymph node
Automated prediction of emphysema visual score using homology-based quantification of low-attenuation lung region,"OBJECTIVE: The purpose of this study was to investigate the relationship between visual score of emphysema and homology-based emphysema quantification (HEQ) and evaluate whether visual score was accurately predicted by machine learning and HEQ. MATERIALS AND METHODS: A total of 115 anonymized computed tomography images from 39 patients were obtained from a public database. Emphysema quantification of these images was performed by measuring the percentage of low-attenuation lung area (LAA%). The following values related to HEQ were obtained: nb0 and nb1. LAA% and HEQ were calculated at various threshold levels ranging from -1000 HU to -700 HU. Spearman's correlation coefficients between emphysema quantification and visual score were calculated at the various threshold levels. Visual score was predicted by machine learning and emphysema quantification (LAA% or HEQ). Random Forest was used as a machine learning algorithm, and accuracy of prediction was evaluated by leave-one-patient-out cross validation. The difference in the accuracy was assessed using McNemar's test. RESULTS: The correlation coefficients between emphysema quantification and visual score were as follows: LAA% (-950 HU), 0.567; LAA% (-910 HU), 0.654; LAA% (-875 HU), 0.704; nb0 (-950 HU), 0.552; nb0 (-910 HU), 0.629; nb0 (-875 HU), 0.473; nb1 (-950 HU), 0.149; nb1 (-910 HU), 0.519; and nb1 (-875 HU), 0.716. The accuracy of prediction was as follows: LAA%, 55.7% and HEQ, 66.1%. The difference in accuracy was statistically significant (p = 0.0290). CONCLUSION: LAA% and HEQ at -875 HU showed a stronger correlation with visual score than those at -910 or -950 HU. HEQ was more useful than LAA% for predicting visual score.",2017,10.1371/journal.pone.0178217,,,,
Automated Processing and Phenotype Extraction of Ovine Medical Images Using a Combined Generative Adversarial Network and Computer Vision Pipeline,"The speed and accuracy of phenotype detection from medical images are some of the most important qualities needed for any informed and timely response such as early detection of cancer or detection of desirable phenotypes for animal breeding. To improve both these qualities, the world is leveraging artificial intelligence and machine learning against this challenge. Most recently, deep learning has successfully been applied to the medical field to improve detection accuracies and speed for conditions including cancer and COVID-19. In this study, we applied deep neural networks, in the form of a generative adversarial network (GAN), to perform image-to-image processing steps needed for ovine phenotype analysis from CT scans of sheep. Key phenotypes such as gigot geometry and tissue distribution were determined using a computer vision (CV) pipeline. The results of the image processing using a trained GAN are strikingly similar (a similarity index of 98%) when used on unseen test images. The combined GAN-CV pipeline was able to process and determine the phenotypes at a speed of 0.11 s per medical image compared to approximately 30 min for manual processing. We hope this pipeline represents the first step towards automated phenotype extraction for ovine genetic breeding programmes.",2021,10.3390/s21217268,,,,
Automated processing of social media content for radiologists: applied deep learning to radiological content on twitter during COVID-19 pandemic,"PURPOSE: The purpose of this study was to develop an automated process to analyze multimedia content on Twitter during the COVID-19 outbreak and classify content for radiological significance using deep learning (DL). MATERIALS AND METHODS: Using Twitter search features, all tweets containing keywords from both ""radiology"" and ""COVID-19"" were collected for the period January 01, 2020 up to April 24, 2020. The resulting dataset comprised of 8354 tweets. Images were classified as (i) images with text (ii) radiological content (e.g., CT scan snapshots, X-ray images), and (iii) non-medical content like personal images or memes. We trained our deep learning model using Convolutional Neural Networks (CNN) on training dataset of 1040 labeled images drawn from all three classes. We then trained another DL classifier for segmenting images into categories based on human anatomy. All software used is open-source and adapted for this research. The diagnostic performance of the algorithm was assessed by comparing results on a test set of 1885 images. RESULTS: Our analysis shows that in COVID-19 related tweets on radiology, nearly 32% had textual images, another 24% had radiological content, and 44% were not of radiological significance. Our results indicated a 92% accuracy in classifying images originally labeled as chest X-ray or chest CT and a nearly 99% accurate classification of images containing medically relevant text. With larger training dataset and algorithmic tweaks, the accuracy can be further improved. CONCLUSION: Applying DL on rich textual images and other metadata in tweets we can process and classify content for radiological significance in real time.",2021,10.1007/s10140-020-01885-z,,,,
Automated Pulmonary Nodule Classification in Computed Tomography Images Using a Deep Convolutional Neural Network Trained by Generative Adversarial Networks,"Lung cancer is a leading cause of death worldwide. Although computed tomography (CT) examinations are frequently used for lung cancer diagnosis, it can be difficult to distinguish between benign and malignant pulmonary nodules on the basis of CT images alone. Therefore, a bronchoscopic biopsy may be conducted if malignancy is suspected following CT examinations. However, biopsies are highly invasive, and patients with benign nodules may undergo many unnecessary biopsies. To prevent this, an imaging diagnosis with high classification accuracy is essential. In this study, we investigate the automated classification of pulmonary nodules in CT images using a deep convolutional neural network (DCNN). We use generative adversarial networks (GANs) to generate additional images when only small amounts of data are available, which is a common problem in medical research, and evaluate whether the classification accuracy is improved by generating a large amount of new pulmonary nodule images using the GAN. Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. The benign nodules assessed in this study are difficult for radiologists to differentiate because they cannot be rejected as being malignant. A volume of interest centered on the pulmonary nodule is extracted from the CT images, and further images are created using axial sections and augmented data. The DCNN is trained using nodule images generated by the GAN and then fine-tuned using the actual nodule images to allow the DCNN to distinguish between benign and malignant nodules. This pretraining and fine-tuning process makes it possible to distinguish 66.7% of benign nodules and 93.9% of malignant nodules. These results indicate that the proposed method improves the classification accuracy by approximately 20% in comparison with training using only the original images.",2019,10.1155/2019/6051939,cross-sectional,diagnosis,CT,Lung
Automated pulmonary nodule detection based on three-dimensional shape-based feature descriptor,"Computer-aided detection (CAD) can help radiologists to detect pulmonary nodules at an early stage. In pulmonary nodule CAD systems, feature extraction is very important for describing the characteristics of nodule candidates. In this paper, we propose a novel three-dimensional shape-based feature descriptor to detect pulmonary nodules in CT scans. After lung volume segmentation, nodule candidates are detected using multi-scale dot enhancement filtering in the segmented lung volume. Next, we extract feature descriptors from the detected nodule candidates, and these are refined using an iterative wall elimination method. Finally, a support vector machine-based classifier is trained to classify nodules and non-nodules. The performance of the proposed system is evaluated on Lung Image Database Consortium data. The proposed method significantly reduces the number of false positives in nodule candidates. This method achieves 97.5% sensitivity, with only 6.76 false positives per scan.",2014,10.1016/j.cmpb.2013.08.015,cross-sectional,diagnosis,CT,Lung
Automated pulmonary nodule detection in CT images using 3D deep squeeze-and-excitation networks,"PURPOSE: Pulmonary nodule detection has great significance for early treating lung cancer and increasing patient survival. This work presents a novel automated computer-aided detection scheme for pulmonary nodules based on deep convolutional neural networks (DCNNs). METHODS: The proposed approach employs 3D DCNNs based on squeeze-and-excitation network and residual network (SE-ResNet) for pulmonary nodule candidate detection and false-positive reduction. Specifically, a 3D region proposal network with a U-Net-like structure is designed for detecting pulmonary nodule candidates. For the subsequent false-positive reduction, a 3D SE-ResNet-based classifier is presented to accurately discriminate the true nodules from candidates. The 3D SE-ResNet modules boost the representational power of the network by adaptively recalibrating channel-wise residual feature responses. Both models utilize 3D SE-ResNet modules to learn nodule features effectively and improve nodule detection performance. RESULTS: On the public available lung nodule analysis 2016 dataset with 888 scans included, the proposed method reaches high detection sensitivities of 93.6% and 95.7% at one and four false positives per scan, respectively. Meanwhile, the competition performance metric score of 0.904 is achieved. The proposed method has the capability to detect multi-size nodules, especially the extremely small nodules. CONCLUSION: In this paper, a 3D DCNNs framework based on 3D SE-ResNet modules is proposed to detect pulmonary nodules in chest CT images accurately. Experimental results demonstrate superior effectiveness of the proposed approach in pulmonary nodule detection task.",2019,10.1007/s11548-019-01979-1,,,,
Automated quantification of COVID-19 severity and progression using chest CT images,"OBJECTIVE: To develop and test computer software to detect, quantify, and monitor progression of pneumonia associated with COVID-19 using chest CT scans. METHODS: One hundred twenty chest CT scans from subjects with lung infiltrates were used for training deep learning algorithms to segment lung regions and vessels. Seventy-two serial scans from 24 COVID-19 subjects were used to develop and test algorithms to detect and quantify the presence and progression of infiltrates associated with COVID-19. The algorithm included (1) automated lung boundary and vessel segmentation, (2) registration of the lung boundary between serial scans, (3) computerized identification of the pneumonitis regions, and (4) assessment of disease progression. Agreement between radiologist manually delineated regions and computer-detected regions was assessed using the Dice coefficient. Serial scans were registered and used to generate a heatmap visualizing the change between scans. Two radiologists, using a five-point Likert scale, subjectively rated heatmap accuracy in representing progression. RESULTS: There was strong agreement between computer detection and the manual delineation of pneumonic regions with a Dice coefficient of 81% (CI 76-86%). In detecting large pneumonia regions (> 200 mm(3)), the algorithm had a sensitivity of 95% (CI 94-97%) and specificity of 84% (CI 81-86%). Radiologists rated 95% (CI 72 to 99) of heatmaps at least ""acceptable"" for representing disease progression. CONCLUSION: The preliminary results suggested the feasibility of using computer software to detect and quantify pneumonic regions associated with COVID-19 and to generate heatmaps that can be used to visualize and assess progression. KEY POINTS: • Both computer vision and deep learning technology were used to develop computer software to quantify the presence and progression of pneumonia associated with COVID-19 depicted on CT images. • The computer software was tested using both quantitative experiments and subjective assessment. • The computer software has the potential to assist in the detection of the pneumonic regions, monitor disease progression, and assess treatment efficacy related to COVID-19.",2021,10.1007/s00330-020-07156-2,cross-sectional,prognosis,CT,Lung
Automated quantification of reference levels in liver and mediastinal blood pool for the Deauville therapy response classification using FDG-PET/CT in Hodgkin and non-Hodgkin lymphomas,"BACKGROUND: 18F-FDG-PET/CT has become a standard for assessing treatment response in patients with lymphoma. A subjective interpretation of the scan based on the Deauville 5-point scale has been widely adopted. However, inter-observer variability due to the subjectivity of the interpretation is a limitation. Our main goal is to develop an objective and automated method for evaluating response. The first step is to develop and validate an artificial intelligence (AI)-based method, for the automated quantification of reference levels in the liver and mediastinal blood pool in patients with lymphoma. METHODS: The AI-based method was trained to segment the liver and the mediastinal blood pool in CT images from 80 lymphoma patients, who had undergone 18F-FDG-PET/CT, and apply this to a validation group of six lymphoma patients. CT segmentations were transferred to the PET images to obtain automatic standardized uptake values (SUV). The AI-based analysis was compared to corresponding manual segmentations performed by two radiologists. RESULTS: The mean difference for the comparison between the AI-based liver SUV quantifications and those of the two radiologists in the validation group was 0·02 and 0·02, respectively, and 0·02 and 0·02 for mediastinal blood pool respectively. CONCLUSIONS: An AI-based method for the automated quantification of reference levels in the liver and mediastinal blood pool shows good agreement with results obtained by experienced radiologists who had manually segmented the CT images. This is a first, promising step towards objective treatment response evaluation in patients with lymphoma based on 18F-FDG-PET/CT.",2019,10.1111/cpf.12546,,,,
Automated system for lung nodules classification based on wavelet feature descriptor and support vector machine,"BACKGROUND: Lung cancer is a leading cause of death worldwide; it refers to the uncontrolled growth of abnormal cells in the lung. A computed tomography (CT) scan of the thorax is the most sensitive method for detecting cancerous lung nodules. A lung nodule is a round lesion which can be either non-cancerous or cancerous. In the CT, the lung cancer is observed as round white shadow nodules. The possibility to obtain a manually accurate interpretation from CT scans demands a big effort by the radiologist and might be a fatiguing process. Therefore, the design of a computer-aided diagnosis (CADx) system would be helpful as a second opinion tool. METHODS: The stages of the proposed CADx are: a supervised extraction of the region of interest to eliminate the shape differences among CT images. The Daubechies db1, db2, and db4 wavelet transforms are computed with one and two levels of decomposition. After that, 19 features are computed from each wavelet sub-band. Then, the sub-band and attribute selection is performed. As a result, 11 features are selected and combined in pairs as inputs to the support vector machine (SVM), which is used to distinguish CT images containing cancerous nodules from those not containing nodules. RESULTS: The clinical data set used for experiments consists of 45 CT scans from ELCAP and LIDC. For the training stage 61 CT images were used (36 with cancerous lung nodules and 25 without lung nodules). The system performance was tested with 45 CT scans (23 CT scans with lung nodules and 22 without nodules), different from that used for training. The results obtained show that the methodology successfully classifies cancerous nodules with a diameter from 2 mm to 30 mm. The total preciseness obtained was 82%; the sensitivity was 90.90%, whereas the specificity was 73.91%. CONCLUSIONS: The CADx system presented is competitive with other literature systems in terms of sensitivity. The system reduces the complexity of classification by not performing the typical segmentation stage of most CADx systems. Additionally, the novelty of the algorithm is the use of a wavelet feature descriptor.",2015,10.1186/s12938-015-0003-y,cross-sectional,diagnosis,CT,Lung
Automated tracking of emergency department abdominal CT findings during the COVID-19 pandemic using natural language processing,"PURPOSE: During the COVID-19 pandemic, emergency department (ED) volumes have fluctuated. We hypothesized that natural language processing (NLP) models could quantify changes in detection of acute abdominal pathology (acute appendicitis (AA), acute diverticulitis (AD), or bowel obstruction (BO)) on CT reports. METHODS: This retrospective study included 22,182 radiology reports from CT abdomen/pelvis studies performed at an urban ED between January 1, 2018 to August 14, 2020. Using a subset of 2448 manually annotated reports, we trained random forest NLP models to classify the presence of AA, AD, and BO in report impressions. Performance was assessed using 5-fold cross validation. The NLP classifiers were then applied to all reports. RESULTS: The NLP classifiers for AA, AD, and BO demonstrated cross-validation classification accuracies between 0.97 and 0.99 and F1-scores between 0.86 and 0.91. When applied to all CT reports, the estimated numbers of AA, AD, and BO cases decreased 43-57% in April 2020 (first regional peak of COVID-19 cases) compared to 2018-2019. However, the number of abdominal pathologies detected rebounded in May-July 2020, with increases above historical averages for AD. The proportions of CT studies with these pathologies did not significantly increase during the pandemic period. CONCLUSION: Dramatic decreases in numbers of acute abdominal pathologies detected by ED CT studies were observed early on during the COVID-19 pandemic, though these numbers rapidly rebounded. The proportions of CT cases with these pathologies did not increase, which suggests patients deferred care during the first pandemic peak. NLP can help automatically track findings in ED radiology reporting.",2021,10.1016/j.ajem.2021.05.057,,,,
Automated tumor analysis for molecular profiling in lung cancer,"The discovery and clinical application of molecular biomarkers in solid tumors, increasingly relies on nucleic acid extraction from FFPE tissue sections and subsequent molecular profiling. This in turn requires the pathological review of haematoxylin & eosin (H&E) stained slides, to ensure sample quality, tumor DNA sufficiency by visually estimating the percentage tumor nuclei and tumor annotation for manual macrodissection. In this study on NSCLC, we demonstrate considerable variation in tumor nuclei percentage between pathologists, potentially undermining the precision of NSCLC molecular evaluation and emphasising the need for quantitative tumor evaluation. We subsequently describe the development and validation of a system called TissueMark for automated tumor annotation and percentage tumor nuclei measurement in NSCLC using computerized image analysis. Evaluation of 245 NSCLC slides showed precise automated tumor annotation of cases using Tissuemark, strong concordance with manually drawn boundaries and identical EGFR mutational status, following manual macrodissection from the image analysis generated tumor boundaries. Automated analysis of cell counts for % tumor measurements by Tissuemark showed reduced variability and significant correlation (p < 0.001) with benchmark tumor cell counts. This study demonstrates a robust image analysis technology that can facilitate the automated quantitative analysis of tissue samples for molecular profiling in discovery and diagnostics.",2015,10.18632/oncotarget.4391,,,,
"Automated, multiparametric monitoring of respiratory biomarkers and vital signs in clinical and home settings for COVID-19 patients","Capabilities in continuous monitoring of key physiological parameters of disease have never been more important than in the context of the global COVID-19 pandemic. Soft, skin-mounted electronics that incorporate high-bandwidth, miniaturized motion sensors enable digital, wireless measurements of mechanoacoustic (MA) signatures of both core vital signs (heart rate, respiratory rate, and temperature) and underexplored biomarkers (coughing count) with high fidelity and immunity to ambient noises. This paper summarizes an effort that integrates such MA sensors with a cloud data infrastructure and a set of analytics approaches based on digital filtering and convolutional neural networks for monitoring of COVID-19 infections in sick and healthy individuals in the hospital and the home. Unique features are in quantitative measurements of coughing and other vocal events, as indicators of both disease and infectiousness. Systematic imaging studies demonstrate correlations between the time and intensity of coughing, speaking, and laughing and the total droplet production, as an approximate indicator of the probability for disease spread. The sensors, deployed on COVID-19 patients along with healthy controls in both inpatient and home settings, record coughing frequency and intensity continuously, along with a collection of other biometrics. The results indicate a decaying trend of coughing frequency and intensity through the course of disease recovery, but with wide variations across patient populations. The methodology creates opportunities to study patterns in biometrics across individuals and among different demographic groups.",2021,10.1073/pnas.2026610118,,,,
Automatic Calcium Scoring in Low-Dose Chest CT Using Deep Neural Networks With Dilated Convolutions,"Heavy smokers undergoing screening with low-dose chest CT are affected by cardiovascular disease as much as by lung cancer. Low-dose chest CT scans acquired in screening enable quantification of atherosclerotic calcifications and thus enable identification of subjects at increased cardiovascular risk. This paper presents a method for automatic detection of coronary artery, thoracic aorta, and cardiac valve calcifications in low-dose chest CT using two consecutive convolutional neural networks. The first network identifies and labels potential calcifications according to their anatomical location and the second network identifies true calcifications among the detected candidates. This method was trained and evaluated on a set of 1744 CT scans from the National Lung Screening Trial. To determine whether any reconstruction or only images reconstructed with soft tissue filters can be used for calcification detection, we evaluated the method on soft and medium/sharp filter reconstructions separately. On soft filter reconstructions, the method achieved F(1) scores of 0.89, 0.89, 0.67, and 0.55 for coronary artery, thoracic aorta, aortic valve, and mitral valve calcifications, respectively. On sharp filter reconstructions, the F(1) scores were 0.84, 0.81, 0.64, and 0.66, respectively. Linearly weighted kappa coefficients for risk category assignment based on per subject coronary artery calcium were 0.91 and 0.90 for soft and sharp filter reconstructions, respectively. These results demonstrate that the presented method enables reliable automatic cardiovascular risk assessment in all low-dose chest CT scans acquired for lung cancer screening.",2018,10.1109/tmi.2017.2769839,cross-sectional,diagnosis,CT,Coronary artery
"Automatic Categorization and Scoring of Solid, Part-Solid and Non-Solid Pulmonary Nodules in CT Images with Convolutional Neural Network","We present a computer-aided diagnosis system (CADx) for the automatic categorization of solid, part-solid and non-solid nodules in pulmonary computerized tomography images using a Convolutional Neural Network (CNN). Provided with only a two-dimensional region of interest (ROI) surrounding each nodule, our CNN automatically reasons from image context to discover informative computational features. As a result, no image segmentation processing is needed for further analysis of nodule attenuation, allowing our system to avoid potential errors caused by inaccurate image processing. We implemented two computerized texture analysis schemes, classification and regression, to automatically categorize solid, part-solid and non-solid nodules in CT scans, with hierarchical features in each case learned directly by the CNN model. To show the effectiveness of our CNN-based CADx, an established method based on histogram analysis (HIST) was implemented for comparison. The experimental results show significant performance improvement by the CNN model over HIST in both classification and regression tasks, yielding nodule classification and rating performance concordant with those of practicing radiologists. Adoption of CNN-based CADx systems may reduce the inter-observer variation among screening radiologists and provide a quantitative reference for further nodule analysis.",2017,10.1038/s41598-017-08040-8,cross-sectional,diagnosis,CT,Lung
"Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods","This study aimed to develop and validate computer-aided diagnosis (CXDx) system for classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray (CXR) images. From two public datasets, 1248 CXR images were obtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia patients, non-COVID-19 pneumonia patients, and the healthy samples, respectively. The proposed CADx system utilized VGG16 as a pre-trained model and combination of conventional method and mixup as data augmentation methods. Other types of pre-trained models were compared with the VGG16-based model. Single type or no data augmentation methods were also evaluated. Splitting of training/validation/test sets was used when building and evaluating the CADx system. Three-category accuracy was evaluated for test set with 125 CXR images. The three-category accuracy of the CAD system was 83.6% between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for COVID-19 pneumonia was more than 90%. The combination of conventional method and mixup was more useful than single type or no data augmentation method. In conclusion, this study was able to create an accurate CADx system for the 3-category classification. Source code of our CADx system is available as open source for COVID-19 research.",2020,10.1038/s41598-020-74539-2,cross-sectional,diagnosis,X-ray,Lung
Automatic Classification of Label-Free Cells from Small Cell Lung Cancer and Poorly Differentiated Lung Adenocarcinoma with 2D Light Scattering Static Cytometry and Machine Learning,"Small cell lung cancer (SCLC) needs to be classified from poorly differentiated lung adenocarcinoma (PDLAC) for appropriate treatment of lung cancer patients. Currently, the classification is achieved by experienced clinicians, radiologists and pathologists based on subjective and qualitative analysis of imaging, cytological and immunohistochemical (IHC) features. Label-free classification of lung cancer cell lines is developed here by using two-dimensional (2D) light scattering static cytometric technique. Measurements of scattered light at forward scattering (FSC) and side scattering (SSC) by using conventional cytometry show that SCLC cells are overlapped with PDLAC cells. However, our 2D light scattering static cytometer reveals remarkable differences between the 2D light scattering patterns of SCLC cell lines (H209 and H69) and PDLAC cell line (SK-LU-1). By adopting support vector machine (SVM) classifier with leave-one-out cross-validation (LOO-CV), SCLC and PDLAC cells are automatically classified with an accuracy of 99.87%. Our label-free 2D light scattering static cytometer may serve as a new, accurate, and easy-to-use method for the automatic classification of SCLC and PDLAC cells. © 2018 International Society for Advancement of Cytometry.",2019,10.1002/cyto.a.23671,,,,
Automatic classification of lung nodule candidates based on a novel 3D convolution network and knowledge transferred from a 2D network,"OBJECTIVE: In the automatic lung nodule detection system, the authenticity of a large number of nodule candidates needs to be judged, which is a classification task. However, the variable shapes and sizes of the lung nodules have posed a great challenge to the classification of candidates. To solve this problem, we propose a method for classifying nodule candidates through three-dimensional (3D) convolution neural network (ConvNet) model which is trained by transferring knowledge from a multiresolution two-dimensional (2D) ConvNet model. METHODS: In this scheme, a novel 3D ConvNet model is preweighted with the weights of the trained 2D ConvNet model, and then the 3D ConvNet model is trained with 3D image volumes. In this way, the knowledge transfer method can make 3D network easier to converge and make full use of the spatial information of nodules with different sizes and shapes to improve the classification accuracy. RESULTS: The experimental results on 551 065 pulmonary nodule candidates in the LUNA16 dataset show that our method gains a competitive average score in the false-positive reduction track in lung nodule detection, with the sensitivities of 0.619 and 0.642 at 0.125 and 0.25 FPs per scan, respectively. CONCLUSIONS: The proposed method can maintain satisfactory classification accuracy even when the false-positive rate is extremely small in the face of nodules of different sizes and shapes. Moreover, as a transfer learning idea, the method to transfer knowledge from 2D ConvNet to 3D ConvNet is the first attempt to carry out full migration of parameters of various layers including convolution layers, full connection layers, and classifier between different dimensional models, which is more conducive to utilizing the existing 2D ConvNet resources and generalizing transfer learning schemes.",2019,10.1002/mp.13867,cross-sectional,diagnosis,CT,Lung
Automatic classification of lung nodules on MDCT images with the temporal subtraction technique,"PURPOSE: A temporal subtraction (TS) image is obtained by subtracting a previous image, which is warped to match the structures of the previous image and the related current image. The TS technique removes normal structures and enhances interval changes such as new lesions and substitutes in existing abnormalities from a medical image. However, many artifacts remaining on the TS image can be detected as false positives. METHOD: This paper presents a novel automatic segmentation of lung nodules using the Watershed method, multiscale gradient vector flow snakes and a detection method using the extracted features and classifiers for small lung nodules (20 mm or less). RESULT: Using the proposed method, we conduct an experiment on 30 thoracic multiple-detector computed tomography cases including 31 small lung nodules. CONCLUSION: The experimental results indicate the efficiency of our segmentation method.",2017,10.1007/s11548-017-1598-1,,,,
Automatic classification of pulmonary peri-fissural nodules in computed tomography using an ensemble of 2D views and a convolutional neural network out-of-the-box,"In this paper, we tackle the problem of automatic classification of pulmonary peri-fissural nodules (PFNs). The classification problem is formulated as a machine learning approach, where detected nodule candidates are classified as PFNs or non-PFNs. Supervised learning is used, where a classifier is trained to label the detected nodule. The classification of the nodule in 3D is formulated as an ensemble of classifiers trained to recognize PFNs based on 2D views of the nodule. In order to describe nodule morphology in 2D views, we use the output of a pre-trained convolutional neural network known as OverFeat. We compare our approach with a recently presented descriptor of pulmonary nodule morphology, namely Bag of Frequencies, and illustrate the advantages offered by the two strategies, achieving performance of AUC = 0.868, which is close to the one of human experts.",2015,10.1016/j.media.2015.08.001,cross-sectional,diagnosis,CT ,Lung
Automatic classification of solitary pulmonary nodules in PET/CT imaging employing transfer learning techniques,"Early and automatic diagnosis of Solitary Pulmonary Nodules (SPN) in Computed Tomography (CT) chest scans can provide early treatment for patients with lung cancer, as well as doctor liberation from time-consuming procedures. The purpose of this study is the automatic and reliable characterization of SPNs in CT scans extracted from Positron Emission Tomography and Computer Tomography (PET/CT) system. To achieve the aforementioned task, Deep Learning with Convolutional Neural Networks (CNN) is applied. The strategy of training specific CNN architectures from scratch and the strategy of transfer learning, by utilizing state-of-the-art pre-trained CNNs, are compared and evaluated. To enhance the training sets, data augmentation is performed. The publicly available database of CT scans, named as Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), is also utilized to further expand the training set and is added to the PET/CT dataset. The results highlight the effectiveness of transfer learning and data augmentation for the classification task of small datasets. The best accuracy obtained on the PET/CT dataset reached 94%, utilizing a modification proposal of a state-of-the-art CNN, called VGG16, and enhancing the training set with LIDC-IDRI dataset. Besides, the proposed modification outperforms in terms of sensitivity several similar researches, which exploit the benefits of transfer learning. Overview of the experiment setup. The two datasets containing nodule representations are combined to evaluate the effectiveness of transfer learning over the traditional approach of training Convolutional Neural Networks from scratch.",2021,10.1007/s11517-021-02378-y,cross-sectional,diagnosis,CT and PET/CT,Lung
Automatic contouring of normal tissues with deep learning for preclinical radiation studies,"Objective.Delineation of relevant normal tissues is a bottleneck in image-guided precision radiotherapy workflows for small animals. A deep learning (DL) model for automatic contouring using standardized 3D micro cone-beam CT (μCBCT) volumes as input is proposed, to provide a fully automatic, generalizable method for normal tissue contouring in preclinical studies.Approach.A 3D U-net was trained to contour organs in the head (whole brain, left/right brain hemisphere, left/right eye) and thorax (complete lungs, left/right lung, heart, spinal cord, thorax bone) regions. As an important preprocessing step, Hounsfield units (HUs) were converted to mass density (MD) values, to remove the energy dependency of theμCBCT scanner and improve generalizability of the DL model. Model performance was evaluated quantitatively by Dice similarity coefficient (DSC), mean surface distance (MSD), 95th percentile Hausdorff distance (HD(95p)), and center of mass displacement (ΔCoM). For qualitative assessment, DL-generated contours (for 40 and 80 kV images) were scored (0: unacceptable, manual re-contouring needed - 5: no adjustments needed). An uncertainty analysis using Monte Carlo dropout uncertainty was performed for delineation of the heart.Main results.The proposed DL model and accompanying preprocessing method provide high quality contours, with in general median DSC > 0.85, MSD < 0.25 mm, HD(95p) < 1 mm and ΔCoM < 0.5 mm. The qualitative assessment showed very few contours needed manual adaptations (40 kV: 20/155 contours, 80 kV: 3/155 contours). The uncertainty of the DL model is small (within 2%).Significance.A DL-based model dedicated to preclinical studies has been developed for multi-organ segmentation in two body sites. For the first time, a method independent of image acquisition parameters has been quantitatively evaluated, resulting in sub-millimeter performance, while qualitative assessment demonstrated the high quality of the DL-generated contours. The uncertainty analysis additionally showed that inherent model variability is low.",2022,10.1088/1361-6560/ac4da3,,,,
Automatic coronavirus disease 2019 diagnosis based on chest radiography and deep learning - Success story or dataset bias?,"PURPOSE: Over the last 2 years, the artificial intelligence (AI) community has presented several automatic screening tools for coronavirus disease 2019 (COVID-19) based on chest radiography (CXR), with reported accuracies often well over 90%. However, it has been noted that many of these studies have likely suffered from dataset bias, leading to overly optimistic results. The purpose of this study was to thoroughly investigate to what extent biases have influenced the performance of a range of previously proposed and promising convolutional neural networks (CNNs), and to determine what performance can be expected with current CNNs on a realistic and unbiased dataset. METHODS: Five CNNs for COVID-19 positive/negative classification were implemented for evaluation, namely VGG19, ResNet50, InceptionV3, DenseNet201, and COVID-Net. To perform both internal and cross-dataset evaluations, four datasets were created. The first dataset Valencian Region Medical Image Bank (BIMCV) followed strict reverse transcriptase-polymerase chain reaction (RT-PCR) test criteria and was created from a single reliable open access databank, while the second dataset (COVIDxB8) was created through a combination of six online CXR repositories. The third and fourth datasets were created by combining the opposing classes from the BIMCV and COVIDxB8 datasets. To decrease inter-dataset variability, a pre-processing workflow of resizing, normalization, and histogram equalization were applied to all datasets. Classification performance was evaluated on unseen test sets using precision and recall. A qualitative sanity check was performed by evaluating saliency maps displaying the top 5%, 10%, and 20% most salient segments in the input CXRs, to evaluate whether the CNNs were using relevant information for decision making. In an additional experiment and to further investigate the origin of potential dataset bias, all pixel values outside the lungs were set to zero through automatic lung segmentation before training and testing. RESULTS: When trained and evaluated on the single online source dataset (BIMCV), the performance of all CNNs is relatively low (precision: 0.65-0.72, recall: 0.59-0.71), but remains relatively consistent during external evaluation (precision: 0.58-0.82, recall: 0.57-0.72). On the contrary, when trained and internally evaluated on the combinatory datasets, all CNNs performed well across all metrics (precision: 0.94-1.00, recall: 0.77-1.00). However, when subsequently evaluated cross-dataset, results dropped substantially (precision: 0.10-0.61, recall: 0.04-0.80). For all datasets, saliency maps revealed the CNNs rarely focus on areas inside the lungs for their decision-making. However, even when setting all pixel values outside the lungs to zero, classification performance does not change and dataset bias remains. CONCLUSIONS: Results in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limited sensitivity (<70%) for COVID-19 infection in CXR, questioning their use as a reliable automatic screening tool.",2022,10.1002/mp.15419,cross-sectional,diagnosis,X-ray,Lung
Automatic COVID-19 Detection Using Exemplar Hybrid Deep Features with X-ray Images,"COVID-19 and pneumonia detection using medical images is a topic of immense interest in medical and healthcare research. Various advanced medical imaging and machine learning techniques have been presented to detect these respiratory disorders accurately. In this work, we have proposed a novel COVID-19 detection system using an exemplar and hybrid fused deep feature generator with X-ray images. The proposed Exemplar COVID-19FclNet9 comprises three basic steps: exemplar deep feature generation, iterative feature selection and classification. The novelty of this work is the feature extraction using three pre-trained convolutional neural networks (CNNs) in the presented feature extraction phase. The common aspects of these pre-trained CNNs are that they have three fully connected layers, and these networks are AlexNet, VGG16 and VGG19. The fully connected layer of these networks is used to generate deep features using an exemplar structure, and a nine-feature generation method is obtained. The loss values of these feature extractors are computed, and the best three extractors are selected. The features of the top three fully connected features are merged. An iterative selector is used to select the most informative features. The chosen features are classified using a support vector machine (SVM) classifier. The proposed COVID-19FclNet9 applied nine deep feature extraction methods by using three deep networks together. The most appropriate deep feature generation model selection and iterative feature selection have been employed to utilise their advantages together. By using these techniques, the image classification ability of the used three deep networks has been improved. The presented model is developed using four X-ray image corpora (DB1, DB2, DB3 and DB4) with two, three and four classes. The proposed Exemplar COVID-19FclNet9 achieved a classification accuracy of 97.60%, 89.96%, 98.84% and 99.64% using the SVM classifier with 10-fold cross-validation for four datasets, respectively. Our developed Exemplar COVID-19FclNet9 model has achieved high classification accuracy for all four databases and may be deployed for clinical application.",2021,10.3390/ijerph18158052,cross-sectional,diagnosis,X-ray,Lung
Automatic creation of annotations for chest radiographs based on the positional information extracted from radiographic image reports,"BACKGROUND AND OBJECTIVE: In this study, we tried to create a machine-learning method that detects disease lesions from chest X-ray (CXR) images using a data set annotated with extracted CXR reports information. We set the nodule as the target disease lesion. Manually annotating nodules is costly in terms of time. Therefore, we used the report information to automatically produce training data for the object detection task. METHODS: First, we use semantic segmentation model PSP-Net to recognize lung fields described in the CXR reports. Next, a classification model ResNeSt-50 is used to discriminate the nodule in segmented right and left field. It also can provide attention map by Grad-Cam. If the attention region corresponds to the location of the nodule in the CXR reports, an attention bounding box is generated. Finally, object detection model Faster-RCNN was performed using generated attention bounding box. The bounding boxes predicted by Faster-RCNN were filtered to satisfy the location extracted from CXR reports. RESULTS: For lung field segmentation, a mean intersection of union of 0.889 was achieved in our best model. 15,156 chest radiographs are used for classification. The area under the receiver operating characteristics curve was 0.843 and 0.852 for the left and right lung, respectively. The detection precision of the generated attention bounding box was 0.341 to 0.531 depending on the binary setting for attention map. Through object detection process, the detection precisions of the bounding boxes were improved to 0.567 to 0.800. CONCLUSION: We successfully generated bounding boxes with nodule on CXR images based on the positional information of the diseases extracted from the CXR reports. Our method has the potential to provide bounding boxes for various lung lesions which can reduce the annotation burden for specialists. SHORT ABSTRACT: Machine learning for computer aided image diagnosis requires annotation of images, but manual annotation is time-consuming for medical doctor. In this study, we tried to create a machine-learning method that creates bounding boxes with disease lesions on chest X-ray (CXR) images using the positional information extracted from CXR reports. We set the nodule as the target lesion. First, we use PSP-Net to segment the lung field according to the CXR reports. Next, a classification model ResNeSt-50 was used to discriminate the nodule in segmented lung field. We also created an attention map using the Grad-Cam algorithm. If the area of attention matched the area annotated by the CXR report, the coordinate of the bounding box was considered as a possible nodule area. Finally, we used the attention information obtained from the nodule classification model and let the object detection model trained by all of the generated bounding boxes. Through object detection model, the precision of the bounding boxes to detect nodule is improved.",2021,10.1016/j.cmpb.2021.106331,cross-sectional,diagnosis,X-ray,Lung
Automatic deep learning-based pleural effusion classification in lung ultrasound images for respiratory pathology diagnosis,"Lung ultrasound (LUS) imaging as a point-of-care diagnostic tool for lung pathologies has been proven superior to X-ray and comparable to CT, enabling earlier and more accurate diagnosis in real-time at the patient's bedside. The main limitation to widespread use is its dependence on the operator training and experience. COVID-19 lung ultrasound findings predominantly reflect a pneumonitis pattern, with pleural effusion being infrequent. However, pleural effusion is easy to detect and to quantify, therefore it was selected as the subject of this study, which aims to develop an automated system for the interpretation of LUS of pleural effusion. A LUS dataset was collected at the Royal Melbourne Hospital which consisted of 623 videos containing 99,209 2D ultrasound images of 70 patients using a phased array transducer. A standardized protocol was followed that involved scanning six anatomical regions providing complete coverage of the lungs for diagnosis of respiratory pathology. This protocol combined with a deep learning algorithm using a Spatial Transformer Network provides a basis for automatic pathology classification on an image-based level. In this work, the deep learning model was trained using supervised and weakly supervised approaches which used frame- and video-based ground truth labels respectively. The reference was expert clinician image interpretation. Both approaches show comparable accuracy scores on the test set of 92.4% and 91.1%, respectively, not statistically significantly different. However, the video-based labelling approach requires significantly less effort from clinical experts for ground truth labelling.",2021,10.1016/j.ejmp.2021.02.023,cross-sectional,diagnosis,Ultrasound,Lung
Automatic detect lung node with deep learning in segmentation and imbalance data labeling,"In this study, a novel method with the U-Net-based network architecture, 2D U-Net, is employed to segment the position of lung nodules, which are an early symptom of lung cancer and have a high probability of becoming a carcinoma, especially when a lung nodule is bigger than 15 [Formula: see text]. A serious problem of considering deep learning for all medical images is imbalanced labeling between foreground and background. The lung nodule is the foreground which accounts for a lower percentage in a whole image. The evaluation function adopted in this study is dice coefficient loss, which is usually used in image segmentation tasks. The proposed pre-processing method in this study is to use complementary labeling as the input in U-Net. With this method, the labeling is swapped. The no-nodule position is labeled. And the position of the nodule becomes non-labeled. The result shows that the proposal in this study is efficient in a small quantity of data. This method, complementary labeling could be used in a small data quantity scenario. With the use of ROI segmentation model in the data pre-processing, the results of lung nodule detection can be improved a lot as shown in the experiments.",2021,10.1038/s41598-021-90599-4,cross-sectional,diagnosis,CT,Lung
Predictive Features of Thymic Carcinoma and High-Risk Thymomas Using Random Forest Analysis,"PURPOSE: To determine the predictive features of thymic carcinomas and high-risk thymomas using random forest algorithm. METHODS: A total of 137 patients with pathologically confirmed high-risk thymomas and thymic carcinomas were enrolled in this study. Three clinical features and 20 computed tomography features were reviewed. The association between computed tomography features and pathological patterns was analyzed by univariate analysis and random forest. The predictive efficiency of the random forest algorithm was evaluated by receiver operating characteristic curve analysis. RESULTS: There were 92 thymic carcinomas and 45 high-risk thymomas in this study. In univariate analysis, patient age, presence of myasthenia gravis, lesion shape, enhancement pattern, presence of necrosis or cystic change, mediastinal invasion, vessel invasion, lymphadenopathy, pericardial effusion, and distant organ metastasis were found to be statistically different between high-risk thymomas and thymic carcinomas (all P < 0.01). Random forest suggested that tumor shape, lymphadenopathy, and the presence of pericardial effusion were the key features in tumor differentiation. The predictive accuracy for the test data and whole data was 94.73% and 96.35%, respectively. Further receiver operating characteristic curve analysis showed the area under the curve was 0.957 (95% confidence interval, 0.986-0.929). CONCLUSIONS: The random forest model in the present study has high efficiency in predictive diagnosis of thymic carcinomas and high-risk thymomas. Tumor shape, lymphadenopathy, and pericardial effusion are the key features for tumor differentiation. Thymic tumors with irregular shape, the presence of lymphadenopathy, and pericardial effusion are highly indicative of thymic carcinomas.",2020,10.1097/rct.0000000000000953,cross-sectional,diagnosis,CT,Thymoma
Deep Learning-Based Quantification of Epicardial Adipose Tissue Volume and Attenuation Predicts Major Adverse Cardiovascular Events in Asymptomatic Subjects,"BACKGROUND: Epicardial adipose tissue (EAT) volume (cm(3)) and attenuation (Hounsfield units) may predict major adverse cardiovascular events (MACE). We aimed to evaluate the prognostic value of fully automated deep learning-based EAT volume and attenuation measurements quantified from noncontrast cardiac computed tomography. METHODS: Our study included 2068 asymptomatic subjects (56±9 years, 59% male) from the EISNER trial (Early Identification of Subclinical Atherosclerosis by Noninvasive Imaging Research) with long-term follow-up after coronary artery calcium measurement. EAT volume and mean attenuation were quantified using automated deep learning software from noncontrast cardiac computed tomography. MACE was defined as myocardial infarction, late (>180 days) revascularization, and cardiac death. EAT measures were compared to coronary artery calcium score and atherosclerotic cardiovascular disease risk score for MACE prediction. RESULTS: At 14±3 years, 223 subjects suffered MACE. Increased EAT volume and decreased EAT attenuation were both independently associated with MACE. Atherosclerotic cardiovascular disease risk score, coronary artery calcium, and EAT volume were associated with increased risk of MACE (hazard ratio [95%CI]: 1.03 [1.01-1.04]; 1.25 [1.19-1.30]; and 1.35 [1.07-1.68], P<0.01 for all) and EAT attenuation was inversely associated with MACE (hazard ratio, 0.83 [95% CI, 0.72-0.96]; P=0.01), with corresponding Harrell C statistic of 0.76. MACE risk progressively increased with EAT volume ≥113 cm(3) and coronary artery calcium ≥100 AU and was highest in subjects with both (P<0.02 for all). In 1317 subjects, EAT volume was correlated with inflammatory biomarkers C-reactive protein, myeloperoxidase, and adiponectin reduction; EAT attenuation was inversely related to these biomarkers. CONCLUSIONS: Fully automated EAT volume and attenuation quantification by deep learning from noncontrast cardiac computed tomography can provide prognostic value for the asymptomatic patient, without additional imaging or physician interaction.",2020,10.1161/circimaging.119.009829,,,,
Epicardial adipose tissue is associated with extent of pneumonia and adverse outcomes in patients with COVID-19,"AIM: We sought to examine the association of epicardial adipose tissue (EAT) quantified on chest computed tomography (CT) with the extent of pneumonia and adverse outcomes in patients with coronavirus disease 2019 (COVID-19). METHODS: We performed a post-hoc analysis of a prospective international registry comprising 109 consecutive patients (age 64 ± 16 years; 62% male) with laboratory-confirmed COVID-19 and noncontrast chest CT imaging. Using semi-automated software, we quantified the burden (%) of lung abnormalities associated with COVID-19 pneumonia. EAT volume (mL) and attenuation (Hounsfield units) were measured using deep learning software. The primary outcome was clinical deterioration (intensive care unit admission, invasive mechanical ventilation, or vasopressor therapy) or in-hospital death. RESULTS: In multivariable linear regression analysis adjusted for patient comorbidities, the total burden of COVID-19 pneumonia was associated with EAT volume (β = 10.6, p = 0.005) and EAT attenuation (β = 5.2, p = 0.004). EAT volume correlated with serum levels of lactate dehydrogenase (r = 0.361, p = 0.001) and C-reactive protein (r = 0.450, p < 0.001). Clinical deterioration or death occurred in 23 (21.1%) patients at a median of 3 days (IQR 1-13 days) following the chest CT. In multivariable logistic regression analysis, EAT volume (OR 5.1 [95% CI 1.8-14.1] per doubling p = 0.011) and EAT attenuation (OR 3.4 [95% CI 1.5-7.5] per 5 Hounsfield unit increase, p = 0.003) were independent predictors of clinical deterioration or death, as was total pneumonia burden (OR 2.5, 95% CI 1.4-4.6, p = 0.002), chronic lung disease (OR 1.3 [95% CI 1.1-1.7], p = 0.011), and history of heart failure (OR 3.5 [95% 1.1-8.2], p = 0.037). CONCLUSIONS: EAT measures quantified from chest CT are independently associated with extent of pneumonia and adverse outcomes in patients with COVID-19, lending support to their use in clinical risk stratification.",2021,10.1016/j.metabol.2020.154436,cross-sectional,prognosis,CT,Heart
Automatic quantification of myocardium and pericardial fat from coronary computed tomography angiography: a multicenter study,"OBJECTIVES: To develop a deep learning-based method for simultaneous myocardium and pericardial fat quantification from coronary computed tomography angiography (CCTA) for the diagnosis and treatment of cardiovascular disease (CVD). METHODS: We retrospectively identified CCTA data obtained between May 2008 and July 2018 in a multicenter (six centers) CVD study. The proposed method was evaluated on 422 patients' data by two studies. The first overall study involves training model on CVD patients and testing on non-CVD patients, as well as training on non-CVD patients and testing on CVD patients. The second study was performed using the leave-center-out approach. The method performance was evaluated using Dice similarity coefficient (DSC), Jaccard index (JAC), 95% Hausdorff distance (HD95), mean surface distance (MSD), residual mean square distance (RMSD), and the center of mass distance (CMD). The robustness of the proposed method was tested using the nonparametric Kruskal-Wallis test and post hoc test to assess the equality of distribution of DSC values among different tests. RESULTS: The automatic segmentation achieved a strong correlation with contour (ICC and R > 0.97, p value < 0.001 throughout all tests). The accuracy of the proposed method remained high through all the tests, with the median DSC higher than 0.88 for pericardial fat and 0.96 for myocardium. The proposed method also resulted in mean MSD, RMSD, HD95, and CMD of less than 1.36 mm for pericardial fat and 1.00 mm for myocardium. CONCLUSIONS: The proposed deep learning-based segmentation method enables accurate simultaneous quantification of myocardium and pericardial fat in a multicenter study. KEY POINTS: • Deep learning-based myocardium and pericardial fat segmentation method tested on 422 patients' coronary computed tomography angiography in a multicenter study. • The proposed method provides segmentations with high volumetric accuracy (ICC and R > 0.97, p value < 0.001) and similar shape as manual annotation by experienced radiologists (median Dice similarity coefficient ≥ 0.88 for pericardial fat and 0.96 for myocardium).",2021,10.1007/s00330-020-07482-5,cross-sectional,diagnosis,CT,Heart
Automatic Detection and Classification of Lung Nodules in CT Image Using Optimized Neuro Fuzzy Classifier with Cuckoo Search Algorithm,"The Lung nodules are very important to indicate the lung cancer, and its early detection enables timely treatment and increases the survival rate of patient. Even though lots of works are done in this area, still improvement in accuracy is required for improving the survival rate of the patient. The proposed method can classify the stages of lung cancer in addition to the detection of lung nodules. There are two parts in the proposed method, the first part is used for classifying normal/abnormal and second part is used for classifying stages of lung cancer. Totally 10 features from the lung region segmented image are considered for detection and classification. The first part of the proposed method classifies the input images with the aid of Naive Bayes classifier as normal or abnormal. The second part of the system classifies the four stages of lung cancer using Neuro Fuzzy classifier with Cuckoo Search algorithm. The results of proposed system show that the rate of accuracy of classification is improved and the results are compared with SVM, Neural Network and Neuro Fuzzy Classifiers.",2019,10.1007/s10916-019-1177-9,cross-sectional,diagnosis,CT,Lung
Automatic Detection and Classification of Rib Fractures on Thoracic CT Using Convolutional Neural Network: Accuracy and Feasibility,"OBJECTIVE: To evaluate the performance of a convolutional neural network (CNN) model that can automatically detect and classify rib fractures, and output structured reports from computed tomography (CT) images. MATERIALS AND METHODS: This study included 1079 patients (median age, 55 years; men, 718) from three hospitals, between January 2011 and January 2019, who were divided into a monocentric training set (n = 876; median age, 55 years; men, 582), five multicenter/multiparameter validation sets (n = 173; median age, 59 years; men, 118) with different slice thicknesses and image pixels, and a normal control set (n = 30; median age, 53 years; men, 18). Three classifications (fresh, healing, and old fracture) combined with fracture location (corresponding CT layers) were detected automatically and delivered in a structured report. Precision, recall, and F1-score were selected as metrics to measure the optimum CNN model. Detection/diagnosis time, precision, and sensitivity were employed to compare the diagnostic efficiency of the structured report and that of experienced radiologists. RESULTS: A total of 25054 annotations (fresh fracture, 10089; healing fracture, 10922; old fracture, 4043) were labelled for training (18584) and validation (6470). The detection efficiency was higher for fresh fractures and healing fractures than for old fractures (F1-scores, 0.849, 0.856, 0.770, respectively, p = 0.023 for each), and the robustness of the model was good in the five multicenter/multiparameter validation sets (all mean F1-scores > 0.8 except validation set 5 [512 × 512 pixels; F1-score = 0.757]). The precision of the five radiologists improved from 80.3% to 91.1%, and the sensitivity increased from 62.4% to 86.3% with artificial intelligence-assisted diagnosis. On average, the diagnosis time of the radiologists was reduced by 73.9 seconds. CONCLUSION: Our CNN model for automatic rib fracture detection could assist radiologists in improving diagnostic efficiency, reducing diagnosis time and radiologists' workload.",2020, ,,,,
Automatic detection of COVID-19 from chest radiographs using deep learning,"INTRODUCTION: The breakdown of a deadly infectious disease caused by a newly discovered coronavirus (named SARS n-CoV2) back in December 2019 has shown no respite to slow or stop in general. This contagious disease has spread across different lengths and breadths of the globe, taking a death toll to nearly 700 k by the start of August 2020. The number is well expected to rise even more significantly. In the absence of a thoroughly tested and approved vaccine, the onus primarily lies on obliging to standard operating procedures and timely detection and isolation of the infected persons. The detection of SARS n-CoV2 has been one of the core concerns during the fight against this pandemic. To keep up with the scale of the outbreak, testing needs to be scaled at par with it. With the conventional PCR testing, most of the countries have struggled to minimize the gap between the scale of outbreak and scale of testing. METHOD: One way of expediting the scale of testing is to shift to a rigorous computational model driven by deep neural networks, as proposed here in this paper. The proposed model is a non-contact process of determining whether a subject is infected or not and is achieved by using chest radiographs; one of the most widely used imaging technique for clinical diagnosis due to fast imaging and low cost. The dataset used in this work contains 1428 chest radiographs with confirmed COVID-19 positive, common bacterial pneumonia, and healthy cases (no infection). We explored the pre-trained VGG-16 model for classification tasks in this. Transfer learning with fine-tuning was used in this study to train the network on relatively small chest radiographs effectively. RESULTS: Initial experiments showed that the model achieved promising results and can be significantly used to expedite COVID-19 detection. The experimentation showed an accuracy of 96% and 92.5% in two and three output class cases, respectively. CONCLUSION: We believe that this study could be used as an initial screening, which can help healthcare professionals to treat the COVID patients by timely detecting better and screening the presence of disease. IMPLICATION FOR PRACTICE: Its simplicity drives the proposed deep neural network model, the capability to work on small image dataset, the non-contact method with acceptable accuracy is a potential alternative for rapid COVID-19 testing that can be adapted by the medical fraternity considering the criticality of the time along with the magnitudes of the outbreak.",2021,10.1016/j.radi.2020.10.018,cross-sectional,diagnosis,X-Ray,Lung
Automatic detection of COVID-19 in chest radiographs using serially concatenated deep and handcrafted features,"Since the infectious disease occurrence rate in the human community is gradually rising due to varied reasons, appropriate diagnosis and treatments are essential to control its spread. The recently discovered COVID-19 is one of the contagious diseases, which infected numerous people globally. This contagious disease is arrested by several diagnoses and handling actions. Medical image-supported diagnosis of COVID-19 infection is an approved clinical practice. This research aims to develop a new Deep Learning Method (DLM) to detect the COVID-19 infection using the chest X-ray. The proposed work implemented two methods namely, detection of COVID-19 infection using (i) a Firefly Algorithm (FA) optimized deep-features and (ii) the combined deep and machine features optimized with FA. In this work, a 5-fold cross-validation method is engaged to train and test detection methods. The performance of this system is analyzed individually resulting in the confirmation that the deep feature-based technique helps to achieve a detection accuracy of > 92% with SVM-RBF classifier and combining deep and machine features achieves > 96% accuracy with Fine KNN classifier. In the future, this technique may have potential to play a vital role in testing and validating the X-ray images collected from patients suffering from the infection diseases.",2022,10.3233/xst-211050,cross-sectional,diagnosis,X-Ray,Lung
Automatic Detection of Covid-19 with Bidirectional LSTM Network Using Deep Features Extracted from Chest X-ray Images,"Coronavirus disease, which comes up in China at the end of 2019 and showed different symptoms in people infected, affected millions of people. Computer-aided expert systems are needed due to the inadequacy of the reverse transcription-polymerase chain reaction kit, which is widely used in the diagnosis of this disease. Undoubtedly, expert systems that provide effective solutions to many problems will be very useful in the detection of Covid-19 disease, especially when unskilled personnel and financial deficiencies in underdeveloped countries are taken into consideration. In the literature, there are numerous machine learning approaches built with different classifiers in the detection of this disease. This paper proposes an approach based on deep learning which detects Covid-19 and no-finding cases using chest X-ray images. Here, the classification performance of the Bi-LSTM network on the deep features was compared with the Deep Neural Network within the frame of the fivefold cross-validation technique. Accuracy, sensitivity, specificity and precision metrics were used to evaluate the classification performance of the trained models. Bi-LSTM network presented better performance compare to DNN with 97.6% value of high accuracy despite the few numbers of Covid-19 images in the dataset. In addition, it is understood that concatenated deep features more meaningful than deep features obtained with pre-trained networks by one by, as well. Consequently, it is thought that the proposed study based on the Bi-LSTM network and concatenated deep features will be noteworthy in the design of highly sensitive automated Covid-19 monitoring systems.",2022,10.1007/s12539-021-00463-2,cross-sectional,diagnosis,X-Ray,Lung
Automatic detection of lung nodules in CT datasets based on stable 3D mass-spring models,"We propose a computer-aided detection (CAD) system which can detect small-sized (from 3mm) pulmonary nodules in spiral CT scans. A pulmonary nodule is a small lesion in the lungs, round-shaped (parenchymal nodule) or worm-shaped (juxtapleural nodule). Both kinds of lesions have a radio-density greater than lung parenchyma, thus appearing white on the images. Lung nodules might indicate a lung cancer and their early stage detection arguably improves the patient survival rate. CT is considered to be the most accurate imaging modality for nodule detection. However, the large amount of data per examination makes the full analysis difficult, leading to omission of nodules by the radiologist. We developed an advanced computerized method for the automatic detection of internal and juxtapleural nodules on low-dose and thin-slice lung CT scan. This method consists of an initial selection of nodule candidates list, the segmentation of each candidate nodule and the classification of the features computed for each segmented nodule candidate.The presented CAD system is aimed to reduce the number of omissions and to decrease the radiologist scan examination time. Our system locates with the same scheme both internal and juxtapleural nodules. For a correct volume segmentation of the lung parenchyma, the system uses a Region Growing (RG) algorithm and an opening process for including the juxtapleural nodules. The segmentation and the extraction of the suspected nodular lesions from CT images by a lung CAD system constitutes a hard task. In order to solve this key problem, we use a new Stable 3D Mass-Spring Model (MSM) combined with a spline curves reconstruction process. Our model represents concurrently the characteristic gray value range, the directed contour information as well as shape knowledge, which leads to a much more robust and efficient segmentation process. For distinguishing the real nodules among nodule candidates, an additional classification step is applied; furthermore, a neural network is applied to reduce the false positives (FPs) after a double-threshold cut. The system performance was tested on a set of 84 scans made available by the Lung Image Database Consortium (LIDC) annotated by four expert radiologists. The detection rate of the system is 97% with 6.1 FPs/CT. A reduction to 2.5 FPs/CT is achieved at 88% sensitivity. We presented a new 3D segmentation technique for lung nodules in CT datasets, using deformable MSMs. The result is a efficient segmentation process able to converge, identifying the shape of the generic ROI, after a few iterations. Our suitable results show that the use of the 3D AC model and the feature analysis based FPs reduction process constitutes an accurate approach to the segmentation and the classification of lung nodules.",2012,10.1016/j.compbiomed.2012.09.002,cross-sectional,diagnosis,CT,Lung
Automatic detection of multisize pulmonary nodules in CT images: Large-scale validation of the false-positive reduction step,"PURPOSE: Currently reported computer-aided detection (CAD) approaches face difficulties in identifying the diverse pulmonary nodules in thoracic computed tomography (CT) images, especially in heterogeneous datasets. We present a novel CAD system specifically designed to identify multisize nodule candidates in multiple heterogeneous datasets. METHODS: The proposed CAD scheme is divided into two phases: primary phase and final phase. The primary phase started with the lung segmentation algorithm and the segmented lungs were further refined using morphological closing process to include the pleural nodules. Next, we empirically formulated three subalgorithms modules to detect different sizes of nodule candidates (≥3 and <6 mm; ≥6 and <10 mm; and ≥10 mm). Each subalgorithm module included a multistage flow of rule-based thresholding and morphological processes. In the final phase, the nodule candidates were augmented to boost the performance of the classifier. The CAD system was trained using a total number of nodule candidates = 201,654 (after augmentation) and nonnodule candidates = 731,486. A rich set of 515 features based on cluster, texture, and voxel-based intensity features were utilized to train a neural network classifier. The proposed method was trained on 899 scans from the Lung Image Database Consortium/Image Database Resource Initiative (LIDC-IDRI). The CAD system was also independently tested on 153 CT scans taken from the AAPM-SPIE-LungX Dataset and two subsets from the Early Lung Cancer Action Project (ELCAP and PCF). RESULTS: For the LIDC-IDRI training set, the proposed CAD scheme yielded an overall sensitivity of 85.6% (1189/1390) and 83.5% (1161/1390) at 8 FP/scan and 1 FP/scan, respectively. For the three independent test sets, the CAD system achieved an average sensitivity of 68.4% at 8 FP/scan. CONCLUSION: The authors conclude that the proposed CAD system can identify dissimilar nodule candidates in the multiple heterogeneous datasets. It could be considered as a useful tool to support radiologists during screening trials.",2018,10.1002/mp.12746,cross-sectional,diagnosis,CT,Lung
Automatic detection of pneumonia in chest X-ray images using textural features,"Fast and accurate diagnosis is critical for the triage and management of pneumonia, particularly in the current scenario of a COVID-19 pandemic, where this pathology is a major symptom of the infection. With the objective of providing tools for that purpose, this study assesses the potential of three textural image characterisation methods: radiomics, fractal dimension and the recently developed superpixel-based histon, as biomarkers to be used for training Artificial Intelligence (AI) models in order to detect pneumonia in chest X-ray images. Models generated from three different AI algorithms have been studied: K-Nearest Neighbors, Support Vector Machine and Random Forest. Two open-access image datasets were used in this study. In the first one, a dataset composed of paediatric chest X-ray, the best performing generated models achieved an 83.3% accuracy with 89% sensitivity for radiomics, 89.9% accuracy with 93.6% sensitivity for fractal dimension and 91.3% accuracy with 90.5% sensitivity for superpixels based histon. Second, a dataset derived from an image repository developed primarily as a tool for studying COVID-19 was used. For this dataset, the best performing generated models resulted in a 95.3% accuracy with 99.2% sensitivity for radiomics, 99% accuracy with 100% sensitivity for fractal dimension and 99% accuracy with 98.6% sensitivity for superpixel-based histons. The results confirm the validity of the tested methods as reliable and easy-to-implement automatic diagnostic tools for pneumonia.",2022,10.1016/j.compbiomed.2022.105466,cross-sectional,diagnosis,X-Ray,Lung
Automatic detection of pulmonary nodules in CT images by incorporating 3D tensor filtering with local image feature analysis,"Computer-aided detection (CAD) technology has been developed and demonstrated its potential to assist radiologists in detecting pulmonary nodules especially at an early stage. In this paper, we present a novel scheme for automatic detection of pulmonary nodules in CT images based on a 3D tensor filtering algorithm and local image feature analysis. We first apply a series of preprocessing steps to segment the lung volume and generate the isotropic volumetric CT data. Next, a unique 3D tensor filtering approach and local image feature analysis are used to detect nodule candidates. A 3D level set segmentation method is used to correct and refine the boundaries of nodule candidates subsequently. Then, we extract the features of the detected candidates and select the optimal features by using a CFS (Correlation Feature Selection) subset evaluator attribute selection method. Finally, a random forest classifier is trained to classify the detected candidates. The performance of this CAD scheme is validated using two datasets namely, the LUNA16 (Lung Nodule Analysis 2016) database and the ANODE09 (Automatic Nodule Detection 2009) database. By applying a 10-fold cross-validation method, the CAD scheme yielded a sensitivity of 79.3% at an average of 4 false positive detections per scan (FP/Scan) for the former dataset, and a sensitivity of 84.62% and 2.8 FP/Scan for the latter dataset, respectively. Our detection results show that the use of 3D tensor filtering algorithm combined with local image feature analysis constitutes an effective approach to detect pulmonary nodules.",2018,10.1016/j.ejmp.2018.01.019,cross-sectional,diagnosis,CT,Lung
Automatic feature learning using multichannel ROI based on deep structured algorithms for computerized lung cancer diagnosis,"This study aimed to analyze the ability of extracting automatically generated features using deep structured algorithms in lung nodule CT image diagnosis, and compare its performance with traditional computer aided diagnosis (CADx) systems using hand-crafted features. All of the 1018 cases were acquired from Lung Image Database Consortium (LIDC) public lung cancer database. The nodules were segmented according to four radiologists' markings, and 13,668 samples were generated by rotating every slice of nodule images. Three multichannel ROI based deep structured algorithms were designed and implemented in this study: convolutional neural network (CNN), deep belief network (DBN), and stacked denoising autoencoder (SDAE). For the comparison purpose, we also implemented a CADx system using hand-crafted features including density features, texture features and morphological features. The performance of every scheme was evaluated by using a 10-fold cross-validation method and an assessment index of the area under the receiver operating characteristic curve (AUC). The observed highest area under the curve (AUC) was 0.899±0.018 achieved by CNN, which was significantly higher than traditional CADx with the AUC=0.848±0.026. The results from DBN was also slightly higher than CADx, while SDAE was slightly lower. By visualizing the automatic generated features, we found some meaningful detectors like curvy stroke detectors from deep structured schemes. The study results showed the deep structured algorithms with automatically generated features can achieve desirable performance in lung nodule diagnosis. With well-tuned parameters and large enough dataset, the deep learning algorithms can have better performance than current popular CADx. We believe the deep learning algorithms with similar data preprocessing procedure can be used in other medical image analysis areas as well.",2017,10.1016/j.compbiomed.2017.04.006,cross-sectional,diagnosis,CT,Lung
Automatic learning-based beam angle selection for thoracic IMRT,"PURPOSE: The treatment of thoracic cancer using external beam radiation requires an optimal selection of the radiation beam directions to ensure effective coverage of the target volume and to avoid unnecessary treatment of normal healthy tissues. Intensity modulated radiation therapy (IMRT) planning is a lengthy process, which requires the planner to iterate between choosing beam angles, specifying dose-volume objectives and executing IMRT optimization. In thorax treatment planning, where there are no class solutions for beam placement, beam angle selection is performed manually, based on the planner's clinical experience. The purpose of this work is to propose and study a computationally efficient framework that utilizes machine learning to automatically select treatment beam angles. Such a framework may be helpful for reducing the overall planning workload. METHODS: The authors introduce an automated beam selection method, based on learning the relationships between beam angles and anatomical features. Using a large set of clinically approved IMRT plans, a random forest regression algorithm is trained to map a multitude of anatomical features into an individual beam score. An optimization scheme is then built to select and adjust the beam angles, considering the learned interbeam dependencies. The validity and quality of the automatically selected beams evaluated using the manually selected beams from the corresponding clinical plans as the ground truth. RESULTS: The analysis included 149 clinically approved thoracic IMRT plans. For a randomly selected test subset of 27 plans, IMRT plans were generated using automatically selected beams and compared to the clinical plans. The comparison of the predicted and the clinical beam angles demonstrated a good average correspondence between the two (angular distance 16.8° ± 10°, correlation 0.75 ± 0.2). The dose distributions of the semiautomatic and clinical plans were equivalent in terms of primary target volume coverage and organ at risk sparing and were superior over plans produced with fixed sets of common beam angles. The great majority of the automatic plans (93%) were approved as clinically acceptable by three radiation therapy specialists. CONCLUSIONS: The results demonstrated the feasibility of utilizing a learning-based approach for automatic selection of beam angles in thoracic IMRT planning. The proposed method may assist in reducing the manual planning workload, while sustaining plan quality.",2015,10.1118/1.4908000,cross-sectional,treatment,IMRT plan,Thorax
Automatic Lung Nodule Detection Combined With Gaze Information Improves Radiologists' Screening Performance,"Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67±0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives.",2020,10.1109/jbhi.2020.2976150,cross-sectional,diagnosis,CT,Lung
Automatic lung nodule detection in thoracic CT scans using dilated slice-wise convolutions,"PURPOSE: Most state-of-the-art automated medical image analysis methods for volumetric data rely on adaptations of two-dimensional (2D) and three-dimensional (3D) convolutional neural networks (CNNs). In this paper, we develop a novel unified CNN-based model that combines the benefits of 2D and 3D networks for analyzing volumetric medical images. METHODS: In our proposed framework, multiscale contextual information is first extracted from 2D slices inside a volume of interest (VOI). This is followed by dilated 1D convolutions across slices to aggregate in-plane features in a slice-wise manner and encode the information in the entire volume. Moreover, we formalize a curriculum learning strategy for a two-stage system (i.e., a system that consists of screening and false positive reduction), where the training samples are presented to the network in a meaningful order to further improve the performance. RESULTS: We evaluated the proposed approach by developing a computer-aided detection (CADe) system for lung nodules. Our results on 888 CT exams demonstrate that the proposed approach can effectively analyze volumetric data by achieving a sensitivity of > 0.99 in the screening stage and a sensitivity of > 0.96 at eight false positives per case in the false positive reduction stage. CONCLUSION: Our experimental results show that the proposed method provides competitive results compared to state-of-the-art 3D frameworks. In addition, we illustrate the benefits of curriculum learning strategies in two-stage systems that are of common use in medical imaging applications.",2021,10.1002/mp.14915,cross-sectional,diagnosis,CT,Lung
Automatic lung nodule detection using a 3D deep convolutional neural network combined with a multi-scale prediction strategy in chest CTs,"OBJECTIVE: A novel computer-aided detection (CAD) scheme for lung nodule detection using a 3D deep convolutional neural network combined with a multi-scale prediction strategy is proposed to assist radiologists by providing a second opinion on accurate lung nodule detection, which is a crucial step in early diagnosis of lung cancer. METHOD: A 3D deep convolutional neural network (CNN) with multi-scale prediction was used to detect lung nodules after the lungs were segmented from chest CT scans, with a comprehensive method utilized. Compared with a 2D CNN, a 3D CNN can utilize richer spatial 3D contextual information and generate more discriminative features after being trained with 3D samples to fully represent lung nodules. Furthermore, a multi-scale lung nodule prediction strategy, including multi-scale cube prediction and cube clustering, is also proposed to detect extremely small nodules. RESULT: The proposed method was evaluated on 888 thin-slice scans with 1186 nodules in the LUNA16 database. All results were obtained via 10-fold cross-validation. Three options of the proposed scheme are provided for selection according to the actual needs. The sensitivity of the proposed scheme with the primary option reached 87.94% and 92.93% at one and four false positives per scan, respectively. Meanwhile, the competition performance metric (CPM) score is very satisfying (0.7967). CONCLUSION: The experimental results demonstrate the outstanding detection performance of the proposed nodule detection scheme. In addition, the proposed scheme can be extended to other medical image recognition fields.",2018,10.1016/j.compbiomed.2018.10.011,cross-sectional,diagnosis,CT,Lung
Automatic lung nodule detection using multi-scale dot nodule-enhancement filter and weighted support vector machines in chest computed tomography,"A novel CAD scheme for automated lung nodule detection is proposed to assist radiologists with the detection of lung cancer on CT scans. The proposed scheme is composed of four major steps: (1) lung volume segmentation, (2) nodule candidate extraction and grouping, (3) false positives reduction for the non-vessel tree group, and (4) classification for the vessel tree group. Lung segmentation is performed first. Then, 3D labeling technology is used to divide nodule candidates into two groups. For the non-vessel tree group, nodule candidates are classified as true nodules at the false positive reduction stage if the candidates survive the rule-based classifier and are not screened out by the dot filter. For the vessel tree group, nodule candidates are extracted using dot filter. Next, RSFS feature selection is used to select the most discriminating features for classification. Finally, WSVM with an undersampling approach is adopted to discriminate true nodules from vessel bifurcations in vessel tree group. The proposed method was evaluated on 154 thin-slice scans with 204 nodules in the LIDC database. The performance of the proposed CAD scheme yielded a high sensitivity (87.81%) while maintaining a low false rate (1.057 FPs/scan). The experimental results indicate the performance of our method may be better than the existing methods.",2019,10.1371/journal.pone.0210551,cross-sectional,diagnosis,CT,Lung
Automatic lung segmentation in COVID-19 patients: Impact on quantitative computed tomography analysis,"PURPOSE: To assess the impact of lung segmentation accuracy in an automatic pipeline for quantitative analysis of CT images. METHODS: Four different platforms for automatic lung segmentation based on convolutional neural network (CNN), region-growing technique and atlas-based algorithm were considered. The platforms were tested using CT images of 55 COVID-19 patients with severe lung impairment. Four radiologists assessed the segmentations using a 5-point qualitative score (QS). For each CT series, a manually revised reference segmentation (RS) was obtained. Histogram-based quantitative metrics (QM) were calculated from CT histogram using lung segmentationsfrom all platforms and RS. Dice index (DI) and differences of QMs (ΔQMs) were calculated between RS and other segmentations. RESULTS: Highest QS and lower ΔQMs values were associated to the CNN algorithm. However, only 45% CNN segmentations were judged to need no or only minimal corrections, and in only 17 cases (31%), automatic segmentations provided RS without manual corrections. Median values of the DI for the four algorithms ranged from 0.993 to 0.904. Significant differences for all QMs calculated between automatic segmentations and RS were found both when data were pooled together and stratified according to QS, indicating a relationship between qualitative and quantitative measurements. The most unstable QM was the histogram 90th percentile, with median ΔQMs values ranging from 10HU and 158HU between different algorithms. CONCLUSIONS: None of tested algorithms provided fully reliable segmentation. Segmentation accuracy impacts differently on different quantitative metrics, and each of them should be individually evaluated according to the purpose of subsequent analyses.",2021,10.1016/j.ejmp.2021.06.001,cross-sectional,informatics,CT,Lung
Automatic multi-organ segmentation in dual-energy CT (DECT) with dedicated 3D fully convolutional DECT networks,"PURPOSE: Dual-energy computed tomography (DECT) has shown great potential in many clinical applications. By incorporating the information from two different energy spectra, DECT provides higher contrast and reveals more material differences of tissues compared to conventional single-energy CT (SECT). Recent research shows that automatic multi-organ segmentation of DECT data can improve DECT clinical applications. However, most segmentation methods are designed for SECT, while DECT has been significantly less pronounced in research. Therefore, a novel approach is required that is able to take full advantage of the extra information provided by DECT. METHODS: In the scope of this work, we proposed four three-dimensional (3D) fully convolutional neural network algorithms for the automatic segmentation of DECT data. We incorporated the extra energy information differently and embedded the fusion of information in each of the network architectures. RESULTS: Quantitative evaluation using 45 thorax/abdomen DECT datasets acquired with a clinical dual-source CT system was investigated. The segmentation of six thoracic and abdominal organs (left and right lungs, liver, spleen, and left and right kidneys) were evaluated using a fivefold cross-validation strategy. In all of the tests, we achieved the best average Dice coefficients of 98% for the right lung, 98% for the left lung, 96% for the liver, 92% for the spleen, 95% for the right kidney, 93% for the left kidney, respectively. The network architectures exploit dual-energy spectra and outperform deep learning for SECT. CONCLUSIONS: The results of the cross-validation show that our methods are feasible and promising. Successful tests on special clinical cases reveal that our methods have high adaptability in the practical application.",2020,10.1002/mp.13950,cross-sectional,informatics,CT(DECT),"lung,liver,kidney"
Automatic opportunistic osteoporosis screening using low-dose chest computed tomography scans obtained for lung cancer screening,"OBJECTIVE: Osteoporosis is a prevalent and treatable condition, but it remains underdiagnosed. In this study, a deep learning-based system was developed to automatically measure bone mineral density (BMD) for opportunistic osteoporosis screening using low-dose chest computed tomography (LDCT) scans obtained for lung cancer screening. METHODS: First, a deep learning model was trained and tested with 200 annotated LDCT scans to segment and label all vertebral bodies (VBs). Then, the mean CT numbers of the trabecular area of target VBs were obtained based on the segmentation mask through geometric operations. Finally, a linear function was built to map the trabecular CT numbers of target VBs to their BMDs collected from approved software used for osteoporosis diagnosis. The diagnostic performance of the developed system was evaluated using an independent dataset of 374 LDCT scans with standard BMDs and osteoporosis diagnosis. RESULTS: Our deep learning model achieved a mean Dice coefficient of 86.6% for VB segmentation and 97.5% accuracy for VB labeling. Line regression and Bland-Altman analyses showed good agreement between the predicted BMD and the ground truth, with correlation coefficients of 0.964-0.968 and mean errors of 2.2-4.0 mg/cm(3). The area under the curve (AUC) was 0.927 for detecting osteoporosis and 0.942 for distinguishing low BMD. CONCLUSION: The proposed deep learning-based system demonstrated the potential to automatically perform opportunistic osteoporosis screening using LDCT scans obtained for lung cancer screening. KEY POINTS: • Osteoporosis is a prevalent but underdiagnosed condition that can increase the risk of fracture. • A deep learning-based system was developed to fully automate bone mineral density measurement in low-dose chest computed tomography scans. • The developed system achieved high accuracy for automatic opportunistic osteoporosis screening using low-dose chest computed tomography scans obtained for lung cancer screening.",2020,10.1007/s00330-020-06679-y,cross-sectional,diagnosis,CT,Vertebral body in chest ct
Automatic pulmonary ground-glass opacity nodules detection and classification based on 3D neural network,"PURPOSE: Pulmonary ground-glass opacity (GGO) nodules are more likely to be malignant compared with solid solitary nodules. Due to indistinct boundaries of GGO nodules, the detection and diagnosis are challenging for doctors. Therefore, designing an automatic GGO nodule detection and classification scheme is significantly essential. METHODS: In this paper, we proposed a two-stage 3D GGO nodule detection and classification framework. First, we used a pretrained 3D U-Net to extract lung parenchyma. Second, we adapted the architecture of Mask region-based convolutional neural networks (RCNN) to handle 3D medical images. The 3D model was then applied to detect the locations of GGO nodules and classify lesions (benign or malignant). The class-balanced loss function was also used to balance the number of benign and malignant lesions. Finally, we employed a novel false positive elimination scheme called the feature-based weighted clustering (FWC) to promote the detection accuracy further. RESULTS: The experiments were conducted based on fivefold cross-validation with the imbalanced data set. Experimental results showed that the mean average precision could keep a high level (0.5182) in the phase of detection. Meanwhile, the false positive rate was effectively controlled, and the competition performance metric (CPM) reached 0.817 benefited from the FWC algorithm. The comparative statistical analyses with other deep learning methods also proved the effectiveness of our proposed method. CONCLUSIONS: We put forward an automatic pulmonary GGO nodules detection and classification framework based on deep learning. The proposed method locate and classify nodules accurately, which could be an effective tool to help doctors in clinical diagnoses.",2022,10.1002/mp.15501,cross-sectional,diagnosis,CT,Lung
Automatic Pulmonary Nodule Detection in CT Scans Using Convolutional Neural Networks Based on Maximum Intensity Projection,"Accurate pulmonary nodule detection is a crucial step in lung cancer screening. Computer-aided detection (CAD) systems are not routinely used by radiologists for pulmonary nodule detection in clinical practice despite their potential benefits. Maximum intensity projection (MIP) images improve the detection of pulmonary nodules in radiological evaluation with computed tomography (CT) scans. Inspired by the clinical methodology of radiologists, we aim to explore the feasibility of applying MIP images to improve the effectiveness of automatic lung nodule detection using convolutional neural networks (CNNs). We propose a CNN-based approach that takes MIP images of different slab thicknesses (5 mm, 10 mm, 15 mm) and 1 mm axial section slices as input. Such an approach augments the two-dimensional (2-D) CT slice images with more representative spatial information that helps discriminate nodules from vessels through their morphologies. Our proposed method achieves sensitivity of 92.7% with 1 false positive per scan and sensitivity of 94.2% with 2 false positives per scan for lung nodule detection on 888 scans in the LIDC-IDRI dataset. The use of thick MIP images helps the detection of small pulmonary nodules (3 mm-10 mm) and results in fewer false positives. Experimental results show that utilizing MIP images can increase the sensitivity and lower the number of false positives, which demonstrates the effectiveness and significance of the proposed MIP-based CNNs framework for automatic pulmonary nodule detection in CT scans. The proposed method also shows the potential that CNNs could gain benefits for nodule detection by combining the clinical procedure.",2020,10.1109/tmi.2019.2935553,cross-sectional,diagnosis,CT(MIP),Lung
Automatic pulmonary vessel segmentation on noncontrast chest CT: deep learning algorithm developed using spatiotemporally matched virtual noncontrast images and low-keV contrast-enhanced vessel maps,"OBJECTIVES: To develop a deep learning-based pulmonary vessel segmentation algorithm (DLVS) from noncontrast chest CT and to investigate its clinical implications in assessing vascular remodeling of chronic obstructive lung disease (COPD) patients. METHODS: For development, 104 pulmonary CT angiography scans (49,054 slices) using a dual-source CT were collected, and spatiotemporally matched virtual noncontrast and 50-keV images were generated. Vessel maps were extracted from the 50-keV images. The 3-dimensional U-Net-based DLVS was trained to segment pulmonary vessels (with a vessel map as the output) from virtual noncontrast images (as the input). For external validation, vendor-independent noncontrast CT images (n = 14) and the VESSEL 12 challenge open dataset (n = 3) were used. For each case, 200 points were selected including 20 intra-lesional points, and the probability value for each point was extracted. For clinical validation, we included 281 COPD patients with low-dose noncontrast CTs. The DLVS-calculated volume of vessels with a cross-sectional area < 5 mm(2) (PVV5) and the PVV5 divided by total vessel volume (%PVV5) were measured. RESULTS: DLVS correctly segmented 99.1% of the intravascular points (1,387/1,400) and 93.1% of the extravascular points (1,309/1,400). The areas-under-the receiver-operating characteristic curve (AUROCs) were 0.977 and 0.969 for the two external validation datasets. For the COPD patients, both PPV5 and %PPV5 successfully differentiated severe patients whose FEV1 < 50 (AUROCs; 0.715 and 0.804) and were significantly correlated with the emphysema index (Ps < .05). CONCLUSIONS: DLVS successfully segmented pulmonary vessels on noncontrast chest CT by utilizing spatiotemporally matched 50-keV images from a dual-source CT scanner and showed promising clinical applicability in COPD. KEY POINTS: • We developed a deep learning pulmonary vessel segmentation algorithm using virtual noncontrast images and 50-keV enhanced images produced by a dual-source CT scanner. • Our algorithm successfully segmented vessels on diseased lungs. • Our algorithm showed promising results in assessing the loss of small vessel density in COPD patients.",2021,10.1007/s00330-021-08036-z,cross-sectional,diagnosis,CT,Pulmonary vessel
Automatic recognition of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNNs,"Ground-glass opacity (GGO) is a common CT imaging sign on high-resolution CT, which means the lesion is more likely to be malignant compared to common solid lung nodules. The automatic recognition of GGO CT imaging signs is of great importance for early diagnosis and possible cure of lung cancers. The present GGO recognition methods employ traditional low-level features and system performance improves slowly. Considering the high-performance of CNN model in computer vision field, we proposed an automatic recognition method of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNN models in this paper. Our hybrid resampling is performed on multi-views and multi-receptive fields, which reduces the risk of missing small or large GGOs by adopting representative sampling panels and processing GGOs with multiple scales simultaneously. The layer-wise fine-tuning strategy has the ability to obtain the optimal fine-tuning model. Multi-CNN models fusion strategy obtains better performance than any single trained model. We evaluated our method on the GGO nodule samples in publicly available LIDC-IDRI dataset of chest CT scans. The experimental results show that our method yields excellent results with 96.64% sensitivity, 71.43% specificity, and 0.83 F1 score. Our method is a promising approach to apply deep learning method to computer-aided analysis of specific CT imaging signs with insufficient labeled images. Graphical abstract We proposed an automatic recognition method of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNN models in this paper. Our hybrid resampling reduces the risk of missing small or large GGOs by adopting representative sampling panels and processing GGOs with multiple scales simultaneously. The layer-wise fine-tuning strategy has ability to obtain the optimal fine-tuning model. Our method is a promising approach to apply deep learning method to computer-aided analysis of specific CT imaging signs with insufficient labeled images.",2018,10.1007/s11517-018-1850-z,cross-sectional,diagnosis,CT,Lung
Automatic recognition of anatomical regions in three-dimensional medical images,"This paper presents a method that detects anatomy regions in three-dimensional medical images. The method labels each axial slice of the image according to the anatomy region it belongs to. The detected regions are the head (and neck), the chest, the abdomen, the pelvis, and the legs. The proposed method consists of two main parts. The core of the algorithm is based on a two-dimensional feature extraction that is followed by a random forest classification. This recognition process achieves an overall accuracy of 91.5% in slice classification, but it cannot always provide fully consistent labeling. The subsequent post-processing step incorporates the expected sequence and size of the human anatomy regions in order to improve the accuracy of the labeling. In this part of the algorithm the detected anatomy regions (represented by Gaussian distributions) are fitted to the region probabilities provided by the random forest classifier. The proposed method was evaluated on a set of whole-body MR images. The results demonstrate that the accuracy of the labeling can be increased to 94.1% using the presented post-processing. In order to demonstrate the robustness of the proposed method it was applied to partial MRI scans of different sizes (cut from the whole-body examinations). According to the results the proposed method works reliably (91.3%) for partial body scans (having as little length as 35cm) as well.",2016,10.1016/j.compbiomed.2016.06.018,,,,
Automatic Scoring of Multiple Semantic Attributes With Multi-Task Feature Leverage: A Study on Pulmonary Nodules in CT Images,"The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of ""spiculation"", ""texture"", ""margin"", etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.",2017,10.1109/tmi.2016.2629462,cross-sectional,diagnosis,CT,Lung
Automatic segmentation of airway tree based on local intensity filter and machine learning technique in 3D chest CT volume,"PURPOSE: Airway segmentation plays an important role in analyzing chest computed tomography (CT) volumes for computerized lung cancer detection, emphysema diagnosis and pre- and intra-operative bronchoscope navigation. However, obtaining a complete 3D airway tree structure from a CT volume is quite a challenging task. Several researchers have proposed automated airway segmentation algorithms basically based on region growing and machine learning techniques. However, these methods fail to detect the peripheral bronchial branches, which results in a large amount of leakage. This paper presents a novel approach for more accurate extraction of the complex airway tree. METHODS: This proposed segmentation method is composed of three steps. First, Hessian analysis is utilized to enhance the tube-like structure in CT volumes; then, an adaptive multiscale cavity enhancement filter is employed to detect the cavity-like structure with different radii. In the second step, support vector machine learning will be utilized to remove the false positive (FP) regions from the result obtained in the previous step. Finally, the graph-cut algorithm is used to refine the candidate voxels to form an integrated airway tree. RESULTS: A test dataset including 50 standard-dose chest CT volumes was used for evaluating our proposed method. The average extraction rate was about 79.1 % with the significantly decreased FP rate. CONCLUSION: A new method of airway segmentation based on local intensity structure and machine learning technique was developed. The method was shown to be feasible for airway segmentation in a computer-aided diagnosis system for a lung and bronchoscope guidance system.",2017,10.1007/s11548-016-1492-2,cross-sectional,diagnosis,CT,Lung
Automatic segmentation of lung nodules with growing neural gas and support vector machine,"Lung cancer is distinguished by presenting one of the highest incidences and one of the highest rates of mortality among all other types of cancer. Unfortunately, this disease is often diagnosed late, affecting the treatment outcome. In order to help specialists in the search and identification of lung nodules in tomographic images, many research centers have developed computer-aided detection systems (CAD systems) to automate procedures. This work seeks to develop a methodology for automatic detection of lung nodules. The proposed method consists of the acquisition of computerized tomography images of the lung, the reduction of the volume of interest through techniques for the extraction of the thorax, extraction of the lung, and reconstruction of the original shape of the parenchyma. After that, growing neural gas (GNG) is applied to constrain even more the structures that are denser than the pulmonary parenchyma (nodules, blood vessels, bronchi, etc.). The next stage is the separation of the structures resembling lung nodules from other structures, such as vessels and bronchi. Finally, the structures are classified as either nodule or non-nodule, through shape and texture measurements together with support vector machine. The methodology ensures that nodules of reasonable size be found with 86% sensitivity and 91% specificity. This results in a mean accuracy of 91% for 10 experiments of training and testing in a sample of 48 nodules occurring in 29 exams. The rate of false positives per exam was of 0.138, for the 29 exams analyzed.",2012,10.1016/j.compbiomed.2012.09.003,cross-sectional,diagnosis,CT,Lung
Automatic segmentation of lung tumors on CT images based on a 2D & 3D hybrid convolutional neural network,"OBJECTIVE: A stable and accurate automatic tumor delineation method has been developed to facilitate the intelligent design of lung cancer radiotherapy process. The purpose of this paper is to introduce an automatic tumor segmentation network for lung cancer on CT images based on deep learning. METHODS: In this paper, a hybrid convolution neural network (CNN) combining 2D CNN and 3D CNN was implemented for the automatic lung tumor delineation using CT images. 3D CNN used V-Net model for the extraction of tumor context information from CT sequence images. 2D CNN used an encoder-decoder structure based on dense connection scheme, which could expand information flow and promote feature propagation. Next, 2D features and 3D features were fused through a hybrid module. Meanwhile, the hybrid CNN was compared with the individual 3D CNN and 2D CNN, and three evaluation metrics, Dice, Jaccard and Hausdorff distance (HD), were used for quantitative evaluation. The relationship between the segmentation performance of hybrid network and the GTV volume size was also explored. RESULTS: The newly introduced hybrid CNN was trained and tested on a dataset of 260 cases, and could achieve a median value of 0.73, with mean and stand deviation of 0.72 ± 0.10 for the Dice metric, 0.58 ± 0.13 and 21.73 ± 13.30 mm for the Jaccard and HD metrics, respectively. The hybrid network significantly outperformed the individual 3D CNN and 2D CNN in the three examined evaluation metrics (p < 0.001). A larger GTV present a higher value for the Dice metric, but its delineation at the tumor boundary is unstable. CONCLUSIONS: The implemented hybrid CNN was able to achieve good lung tumor segmentation performance on CT images. ADVANCES IN KNOWLEDGE: The hybrid CNN has valuable prospect with the ability to segment lung tumor.",2021,10.1259/bjr.20210038,cross-sectional,diagnosis,CT,Lung
Automatic segmentation of organs at risk and tumors in CT images of lung cancer from partially labelled datasets with a semi-supervised conditional nnU-Net,"BACKGROUND AND OBJECTIVE: Accurately and reliably defining organs at risk (OARs) and tumors are the cornerstone of radiation therapy (RT) treatment planning for lung cancer. Almost all segmentation networks based on deep learning techniques rely on fully annotated data with strong supervision. However, existing public imaging datasets encountered in the RT domain frequently include singly labelled tumors or partially labelled organs because annotating full OARs and tumors in CT images is both rigorous and tedious. To utilize labelled data from different sources, we proposed a dual-path semi-supervised conditional nnU-Net for OARs and tumor segmentation that is trained on a union of partially labelled datasets. METHODS: The framework employs the nnU-Net as the base model and introduces a conditioning strategy by incorporating auxiliary information as an additional input layer into the decoder. The conditional nnU-Net efficiently leverages prior conditional information to classify the target class at the pixelwise level. Specifically, we employ the uncertainty-aware mean teacher (UA-MT) framework to assist in OARs segmentation, which can effectively leverage unlabelled data (images from a tumor labelled dataset) by encouraging consistent predictions of the same input under different perturbations. Furthermore, we individually design different combinations of loss functions to optimize the segmentation of OARs (Dice loss and cross-entropy loss) and tumors (Dice loss and focal loss) in a dual path. RESULTS: The proposed method is evaluated on two publicly available datasets of the spinal cord, left and right lung, heart, esophagus, and lung tumor, in which satisfactory segmentation performance has been achieved in term of both the region-based Dice similarity coefficient (DSC) and the boundary-based Hausdorff distance (HD). CONCLUSIONS: The proposed semi-supervised conditional nnU-Net breaks down the barriers between nonoverlapping labelled datasets and further alleviates the problem of ""data hunger"" and ""data waste"" in multi-class segmentation. The method has the potential to help radiologists with RT treatment planning in clinical practice.",2021,10.1016/j.cmpb.2021.106419,cross-sectional,treatment,CT,Lung
Automatic segmentation of organs-at-risks of nasopharynx cancer and lung cancer by cross-layer attention fusion network with TELD-Loss,"PURPOSE: Radiotherapy is one of the main treatments of nasopharyngeal cancer (NPC) and lung cancer. Accurate segmentation of organs at risks (OARs) in CT images is a key step in radiotherapy planning for NPC and lung cancer. However, the segmentation of OARs is influenced by the highly imbalanced size of organs, which often results in very poor segmentation results for small and difficult-to-segment organs. In addition, the complex morphological changes and fuzzy boundaries of OARs also pose great challenges to the segmentation task. In this paper, we propose a cross-layer attention fusion network (CLAF-CNN) to solve the problem of accurately segmenting OARs. METHODS: In CLAF-CNN, we integrate the spatial attention maps of the adjacent spatial attention modules to make the segmentation targets more accurately focused, so that the network can capture more target-related features. In this way, the spatial attention modules in the network can be learned and optimized together. In addition, we introduce a new Top-K exponential logarithmic Dice loss (TELD-Loss) to solve the imbalance problem in OAR segmentation. The TELD-Loss further introduces a Top-K optimization mechanism based on Dice loss and exponential logarithmic loss, which makes the network pay more attention to small organs and difficult-to-segment organs, so as to enhance the overall performance of the segmentation model. RESULTS: We validated our framework on the OAR segmentation datasets of the head and neck and lung CT images in the StructSeg 2019 challenge. Experiments show that the CLAF-CNN outperforms the state-of-the-art attention-based segmentation methods in the OAR segmentation task with average Dice coefficient of 79.65% for head and neck OARs and 88.39% for lung OARs. CONCLUSIONS: This work provides a new network named CLAF-CNN which contains cross-layer spatial attention map fusion architecture and TELD-Loss for OAR segmentation. Results demonstrated that the proposed method could obtain accurate segmentation results for OARs, which has a potential of improving the efficiency of radiotherapy planning for nasopharynx cancer and lung cancer.",2021,10.1002/mp.15260,cross-sectional,treatment,CT,Lung
Automatic weighing attribute to retrieve similar lung cancer nodules,"BACKGROUND: Cancer is a disease characterized as an uncontrolled growth of abnormal cells that invades neighboring tissues and destroys them. Lung cancer is the primary cause of cancer-related deaths in the world, and it diagnosis is a complex task for specialists and it presents some big challenges as medical image interpretation process, pulmonary nodule detection and classification. In order to aid specialists in the early diagnosis of lung cancer, computer assistance must be integrated in the imaging interpretation and pulmonary nodule classification processes. Methods of Content-Based Image Retrieval (CBIR) have been described as one promising technique to computer-aided diagnosis and is expected to aid radiologists on image interpretation with a second opinion. However, CBIR presents some limitations: image feature extraction process and appropriate similarity measure. The efficiency of CBIR systems depends on calculating image features that may be relevant to the case similarity analysis. When specialists classify a nodule, they are supported by information from exams, images, etc. But each information has more or less weight over decision making about nodule malignancy. Thus, finding a way to measure the weight allows improvement of image retrieval process through the assignment of higher weights to that attributes that best characterize the nodules. METHODS: In this context, the aim of this work is to present a method to automatically calculate attribute weights based on local learning to reflect the interpretation on image retrieval process. The process consists of two stages that are performed sequentially and cyclically: Evaluation Stage and Training Stage. At each iteration the weights are adjusted according to retrieved nodules. After some iterations, it is possible reach a set of attribute weights that optimize the recovery of similar nodes. RESULTS: The results achieved by updated weights were promising because was possible increase precision by 10% to 6% on average to retrieve of benign and malignant nodules, respectively, with recall of 25% compared with tests without weights associated to attributes in similarity metric. The best result, we reaching values over 100% of precision average until thirtieth lung cancer nodule retrieved. CONCLUSIONS: Based on the results, WED applied to the three vectors used attributes (3D TA, 3D MSA and InV), with weights adjusted by the process, always achieved better results than those found with ED. With the weights, the Precision was increased on average by 17.3% compared with using ED.",2016,10.1186/s12911-016-0313-4,cross-sectional,diagnosis,CT,Lung
Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017,"PURPOSE: This report presents the methods and results of the Thoracic Auto-Segmentation Challenge organized at the 2017 Annual Meeting of American Association of Physicists in Medicine. The purpose of the challenge was to provide a benchmark dataset and platform for evaluating performance of autosegmentation methods of organs at risk (OARs) in thoracic CT images. METHODS: Sixty thoracic CT scans provided by three different institutions were separated into 36 training, 12 offline testing, and 12 online testing scans. Eleven participants completed the offline challenge, and seven completed the online challenge. The OARs were left and right lungs, heart, esophagus, and spinal cord. Clinical contours used for treatment planning were quality checked and edited to adhere to the RTOG 1106 contouring guidelines. Algorithms were evaluated using the Dice coefficient, Hausdorff distance, and mean surface distance. A consolidated score was computed by normalizing the metrics against interrater variability and averaging over all patients and structures. RESULTS: The interrater study revealed highest variability in Dice for the esophagus and spinal cord, and in surface distances for lungs and heart. Five out of seven algorithms that participated in the online challenge employed deep-learning methods. Although the top three participants using deep learning produced the best segmentation for all structures, there was no significant difference in the performance among them. The fourth place participant used a multi-atlas-based approach. The highest Dice scores were produced for lungs, with averages ranging from 0.95 to 0.98, while the lowest Dice scores were produced for esophagus, with a range of 0.55-0.72. CONCLUSION: The results of the challenge showed that the lungs and heart can be segmented fairly accurately by various algorithms, while deep-learning methods performed better on the esophagus. Our dataset together with the manual contours for all training cases continues to be available publicly as an ongoing benchmarking resource.",2018,10.1002/mp.13141,,,,
Auxiliary Diagnosis for COVID-19 with Deep Transfer Learning,"To assist physicians identify COVID-19 and its manifestations through the automatic COVID-19 recognition and classification in chest CT images with deep transfer learning. In this retrospective study, the used chest CT image dataset covered 422 subjects, including 72 confirmed COVID-19 subjects (260 studies, 30,171 images), 252 other pneumonia subjects (252 studies, 26,534 images) that contained 158 viral pneumonia subjects and 94 pulmonary tuberculosis subjects, and 98 normal subjects (98 studies, 29,838 images). In the experiment, subjects were split into training (70%), validation (15%) and testing (15%) sets. We utilized the convolutional blocks of ResNets pretrained on the public social image collections and modified the top fully connected layer to suit our task (the COVID-19 recognition). In addition, we tested the proposed method on a finegrained classification task; that is, the images of COVID-19 were further split into 3 main manifestations (ground-glass opacity with 12,924 images, consolidation with 7418 images and fibrotic streaks with 7338 images). Similarly, the data partitioning strategy of 70%-15%-15% was adopted. The best performance obtained by the pretrained ResNet50 model is 94.87% sensitivity, 88.46% specificity, 91.21% accuracy for COVID-19 versus all other groups, and an overall accuracy of 89.01% for the three-category classification in the testing set. Consistent performance was observed from the COVID-19 manifestation classification task on images basis, where the best overall accuracy of 94.08% and AUC of 0.993 were obtained by the pretrained ResNet18 (P < 0.05). All the proposed models have achieved much satisfying performance and were thus very promising in both the practical application and statistics. Transfer learning is worth for exploring to be applied in recognition and classification of COVID-19 on CT images with limited training data. It not only achieved higher sensitivity (COVID-19 vs the rest) but also took far less time than radiologists, which is expected to give the auxiliary diagnosis and reduce the workload for the radiologists.",2021,10.1007/s10278-021-00431-8,cross-sectional,diagnosis,CT,Lung
Auxiliary Diagnosis of Lung Cancer with Magnetic Resonance Imaging Data under Deep Learning,"This study was aimed at two image segmentation methods of three-dimensional (3D) U-shaped network (U-Net) and multilevel boundary sensing residual U-shaped network (RUNet) and their application values on the auxiliary diagnosis of lung cancer. In this study, on the basis of the 3D U-Net segmentation method, the multilevel boundary sensing RUNet was worked out after optimization. 92 patients with lung cancer were selected, and their clinical data were counted; meanwhile, the lung nodule detection was performed to obtain the segmentation effect under 3D U-Net. The accuracy of 3D U-Net and multilevel boundary sensing RUNet was compared on lung magnetic resonance imaging (MRI) after lung nodule segmentation. Patients with benign lung tumors were taken as controls; the blood immune biochemical indicators progastrin-releasing peptide (pro-CRP), carcinoembryonic antigen (CEA), and neuron-specific enolase (NSE) in patients with malignant lung tumors were analyzed. It was found that the accuracy, sensitivity, and specificity were all greater than 90% under the algorithm-based MRI of benign and malignant tumor patients. Based on the imaging signs for the MRI image of lung nodules, the segmentation effect of the RUNet was clearer than that of the 3D U-Net. In addition, serum levels of pro-CRP, NSE, and CAE in patients with benign lung tumors were 28.9 pg/mL, 12.5 ng/mL, and 10.8 ng/mL, respectively, which were lower than 175.6 pg/mL, 33.6 ng/mL, and 31.9 ng/mL in patients with malignant lung tumors significantly (P < 0.05). Thus, the RUNet image segmentation method was better than the 3D U-Net. The pro-CRP, CEA, and NSE could be used as diagnostic indicators for malignant lung tumors.",2022,10.1155/2022/1994082,cross-sectional,diagnosis,MRI,Lung
Based on improved deep convolutional neural network model pneumonia image classification,"Pneumonia remains the leading infectious cause of death in children under the age of five, killing about 700,000 children each year and affecting 7% of the world's population. X-ray images of lung become the key to the diagnosis of this disease, skilled doctors in the diagnosis of a certain degree of subjectivity, if the use of computer-aided medical diagnosis to automatically detect lung abnormalities, will improve the accuracy of diagnosis. This research aims to introduce a deep learning technology based on the combination of Xception neural network and long-term short-term memory (LSTM), which can realize automatic diagnosis of patients with pneumonia in X-ray images. First, the model uses the Xception network to extract the deep features of the data, passes the extracted features to the LSTM, and then the LSTM detects the extracted features, and finally selects the most needed features. Secondly, in the training set samples, the traditional cross-entropy loss cannot more balance the mismatch between categories. Therefore, this research combines Pearson's feature selection ideas, fusion of the correlation between the two loss functions, and optimizes the problem. The experimental results show that the accuracy rate of this paper is 96%, the receiver operator characteristic curve accuracy rate is 99%, the precision rate is 98%, the recall rate is 91%, and the F1 score accuracy rate is 94%. Compared with the existing technical methods, the research has achieved expected results on the currently available datasets. And assist doctors to provide higher reliability in the classification task of childhood pneumonia.",2021,10.1371/journal.pone.0258804,cross-sectional,diagnosis,X-Ray,Lung
Bayesian-based optimized deep learning model to detect COVID-19 patients using chest X-ray image data,"Coronavirus Disease 2019 (COVID-19) is extremely infectious and rapidly spreading around the globe. As a result, rapid and precise identification of COVID-19 patients is critical. Deep Learning has shown promising performance in a variety of domains and emerged as a key technology in Artificial Intelligence. Recent advances in visual recognition are based on image classification and artefacts detection within these images. The purpose of this study is to classify chest X-ray images of COVID-19 artefacts in changed real-world situations. A novel Bayesian optimization-based convolutional neural network (CNN) model is proposed for the recognition of chest X-ray images. The proposed model has two main components. The first one utilizes CNN to extract and learn deep features. The second component is a Bayesian-based optimizer that is used to tune the CNN hyperparameters according to an objective function. The used large-scale and balanced dataset comprises 10,848 images (i.e., 3616 COVID-19, 3616 normal cases, and 3616 Pneumonia). In the first ablation investigation, we compared Bayesian optimization to three distinct ablation scenarios. We used convergence charts and accuracy to compare the three scenarios. We noticed that the Bayesian search-derived optimal architecture achieved 96% accuracy. To assist qualitative researchers, address their research questions in a methodologically sound manner, a comparison of research method and theme analysis methods was provided. The suggested model is shown to be more trustworthy and accurate in real world.",2022,10.1016/j.compbiomed.2022.105213,cross-sectional,diagnosis,X-Ray,Lung
BEMD-3DCNN-based method for COVID-19 detection,"The coronavirus outbreak continues to spread around the world and no one knows when it will stop. Therefore, from the first day of the identification of the virus in Wuhan, China, scientists have launched numerous research projects to understand the nature of the virus, how to detect it, and search for the most effective medicine to help and protect patients. Importantly, a rapid diagnostic and detection system is a priority and should be developed to stop COVID-19 from spreading. Medical imaging techniques have been used for this purpose. Current research is focused on exploiting different backbones like VGG, ResNet, DenseNet, or combining them to detect COVID-19. By using these backbones many aspects cannot be analyzed like the spatial and contextual information in the images, although this information can be useful for more robust detection performance. In this paper, we used 3D representation of the data as input for the proposed 3DCNN-based deep learning model. The process includes using the Bi-dimensional Empirical Mode Decomposition (BEMD) technique to decompose the original image into IMFs, and then building a video of these IMF images. The formed video is used as input for the 3DCNN model to classify and detect the COVID-19 virus. The 3DCNN model consists of a 3D VGG-16 backbone followed by a Context-aware attention (CAA) module, and then fully connected layers for classification. Each CAA module takes the feature maps of different blocks of the backbone, which allows learning from different feature maps. In our experiments, we used 6484 X-ray images, of which 1802 were COVID-19 positive cases, 1910 normal cases, and 2772 pneumonia cases. The experiment results showed that our proposed technique achieved the desired results on the selected dataset. Additionally, the use of the 3DCNN model with contextual information processing exploited CAA networks to achieve better performance.",2022,10.1016/j.compbiomed.2021.105188,cross-sectional,diagnosis,X-Ray,Lung
Benign-malignant pulmonary nodule classification in low-dose CT with convolutional features,"PURPOSE: Low-Dose Computed Tomography (LDCT) is the most common imaging modality for lung cancer diagnosis. The presence of nodules in the scans does not necessarily portend lung cancer, as there is an intricate relationship between nodule characteristics and lung cancer. Therefore, benign-malignant pulmonary nodule classification at early detection is a crucial step to improve diagnosis and prolong patient survival. The aim of this study is to propose a method for predicting nodule malignancy based on deep abstract features. METHODS: To efficiently capture both intra-nodule heterogeneities and contextual information of the pulmonary nodules, a dual pathway model was developed to integrate the intra-nodule characteristics with contextual attributes. The proposed approach was implemented with both supervised and unsupervised learning schemes. A random forest model was added as a second component on top of the networks to generate the classification results. The discrimination power of the model was evaluated by calculating the Area Under the Receiver Operating Characteristic Curve (AUROC) metric. RESULTS: Experiments on 1297 manually segmented nodules show that the integration of context and target supervised deep features have a great potential for accurate prediction, resulting in a discrimination power of 0.936 in terms of AUROC, which outperformed the classification performance of the Kaggle 2017 challenge winner. CONCLUSION: Empirical results demonstrate that integrating nodule target and context images into a unified network improves the discrimination power, outperforming the conventional single pathway convolutional neural networks.",2021,10.1016/j.ejmp.2021.03.013,cross-sectional,diagnosis,CT,Lung
Bone Suppression on Chest Radiographs for Pulmonary Nodule Detection: Comparison between a Generative Adversarial Network and Dual-Energy Subtraction,"OBJECTIVE: To compare the effects of bone suppression imaging using deep learning (BSp-DL) based on a generative adversarial network (GAN) and bone subtraction imaging using a dual energy technique (BSt-DE) on radiologists' performance for pulmonary nodule detection on chest radiographs (CXRs). MATERIALS AND METHODS: A total of 111 adults, including 49 patients with 83 pulmonary nodules, who underwent both CXR using the dual energy technique and chest CT, were enrolled. Using CT as a reference, two independent radiologists evaluated CXR images for the presence or absence of pulmonary nodules in three reading sessions (standard CXR, BSt-DE CXR, and BSp-DL CXR). Person-wise and nodule-wise performances were assessed using receiver-operating characteristic (ROC) and alternative free-response ROC (AFROC) curve analyses, respectively. Subgroup analyses based on nodule size, location, and the presence of overlapping bones were performed. RESULTS: BSt-DE with an area under the AFROC curve (AUAFROC) of 0.996 and 0.976 for readers 1 and 2, respectively, and BSp-DL with AUAFROC of 0.981 and 0.958, respectively, showed better nodule-wise performance than standard CXR (AUAFROC of 0.907 and 0.808, respectively; p ≤ 0.005). In the person-wise analysis, BSp-DL with an area under the ROC curve (AUROC) of 0.984 and 0.931 for readers 1 and 2, respectively, showed better performance than standard CXR (AUROC of 0.915 and 0.798, respectively; p ≤ 0.011) and comparable performance to BSt-DE (AUROC of 0.988 and 0.974; p ≥ 0.064). BSt-DE and BSp-DL were superior to standard CXR for detecting nodules overlapping with bones (p < 0.017) or in the upper/middle lung zone (p < 0.017). BSt-DE was superior (p < 0.017) to BSp-DL in detecting peripheral and sub-centimeter nodules. CONCLUSION: BSp-DL (GAN-based bone suppression) showed comparable performance to BSt-DE and can improve radiologists' performance in detecting pulmonary nodules on CXRs. Nevertheless, for better delineation of small and peripheral nodules, further technical improvements are required.",2022,10.3348/kjr.2021.0146,cross-sectional,diagnosis,CT&X-ray,Lung
Boundary Restored Network for Subpleural Pulmonary Lesion Segmentation on Ultrasound Images at Local and Global Scales,"To evaluate the application of machine learning for the detection of subpleural pulmonary lesions (SPLs) in ultrasound (US) scans, we propose a novel boundary-restored network (BRN) for automated SPL segmentation to avoid issues associated with manual SPL segmentation (subjectivity, manual segmentation errors, and high time consumption). In total, 1612 ultrasound slices from 255 patients in which SPLs were visually present were exported. The segmentation performance of the neural network based on the Dice similarity coefficient (DSC), Matthews correlation coefficient (MCC), Jaccard similarity metric (Jaccard), Average Symmetric Surface Distance (ASSD), and Maximum symmetric surface distance (MSSD) was assessed. Our dual-stage boundary-restored network (BRN) outperformed existing segmentation methods (U-Net and a fully convolutional network (FCN)) for the segmentation accuracy parameters including DSC (83.45 ± 16.60%), MCC (0.8330 ± 0.1626), Jaccard (0.7391 ± 0.1770), ASSD (5.68 ± 2.70 mm), and MSSD (15.61 ± 6.07 mm). It also outperformed the original BRN in terms of the DSC by almost 5%. Our results suggest that deep learning algorithms aid fully automated SPL segmentation in patients with SPLs. Further improvement of this technology might improve the specificity of lung cancer screening efforts and could lead to new applications of lung US imaging.",2020,10.1007/s10278-020-00356-8,cross-sectional,diagnosis,US,Lung
Breathlessness in COPD: linking symptom clusters with brain activity,"BACKGROUND: Current models of breathlessness often fail to explain disparities between patients' experiences of breathlessness and objective measures of lung function. While a mechanistic understanding of this discordance has thus far remained elusive, factors such as mood, attention and expectation have all been implicated as important modulators of breathlessness. Therefore, we have developed a model to better understand the relationships between these factors using unsupervised machine learning techniques. Subsequently we examined how expectation-related brain activity differed between these symptom-defined clusters of participants. METHODS: A cohort of 91 participants with mild-to-moderate chronic obstructive pulmonary disease (COPD) underwent functional brain imaging, self-report questionnaires and clinical measures of respiratory function. Unsupervised machine learning techniques of exploratory factor analysis and hierarchical cluster modelling were used to model brain-behaviour-breathlessness links. RESULTS: We successfully stratified participants across four key factors corresponding to mood, symptom burden and two capability measures. Two key groups resulted from this stratification, corresponding to high and low symptom burden. Compared with the high symptom burden group, the low symptom burden group demonstrated significantly greater brain activity within the anterior insula, a key region thought to be involved in monitoring internal bodily sensations (interoception). CONCLUSIONS: This is the largest functional neuroimaging study of COPD to date, and is the first to provide a clear model linking brain, behaviour and breathlessness expectation. Furthermore, it was possible to stratify participants into groups, which then revealed differences in brain activity patterns. Together, these findings highlight the value of multimodal models of breathlessness in identifying behavioural phenotypes and for advancing understanding of differences in breathlessness burden.",2021,10.1183/13993003.04099-2020,,,,
BS-Net: Learning COVID-19 pneumonia severity on a large chest X-ray dataset,"In this work we design an end-to-end deep learning architecture for predicting, on Chest X-rays images (CXR), a multi-regional score conveying the degree of lung compromise in COVID-19 patients. Such semi-quantitative scoring system, namely Brixia score, is applied in serial monitoring of such patients, showing significant prognostic value, in one of the hospitals that experienced one of the highest pandemic peaks in Italy. To solve such a challenging visual task, we adopt a weakly supervised learning strategy structured to handle different tasks (segmentation, spatial alignment, and score estimation) trained with a ""from-the-part-to-the-whole"" procedure involving different datasets. In particular, we exploit a clinical dataset of almost 5,000 CXR annotated images collected in the same hospital. Our BS-Net demonstrates self-attentive behavior and a high degree of accuracy in all processing stages. Through inter-rater agreement tests and a gold standard comparison, we show that our solution outperforms single human annotators in rating accuracy and consistency, thus supporting the possibility of using this tool in contexts of computer-assisted monitoring. Highly resolved (super-pixel level) explainability maps are also generated, with an original technique, to visually help the understanding of the network activity on the lung areas. We also consider other scores proposed in literature and provide a comparison with a recently proposed non-specific approach. We eventually test the performance robustness of our model on an assorted public COVID-19 dataset, for which we also provide Brixia score annotations, observing good direct generalization and fine-tuning capabilities that highlight the portability of BS-Net in other clinical settings. The CXR dataset along with the source code and the trained model are publicly released for research purposes.",2021,10.1016/j.media.2021.102046,cross-sectional,diagnosis,X-ray,Lung
CAD system for lung nodule detection using deep learning with CNN,"The early detection of pulmonary nodules using computer-aided diagnosis (CAD) systems is very essential in reducing mortality rates of lung cancer. In this paper, we propose a new deep learning approach to improve the classification accuracy of pulmonary nodules in computed tomography (CT) images. Our proposed CNN-5CL (convolutional neural network with 5 convolutional layers) approach uses an 11-layer convolutional neural network (with 5 convolutional layers) for automatic feature extraction and classification. The proposed method is evaluated using LIDC/IDRI images. The proposed method is implemented in the Python platform, and the performance is evaluated with metrics such as accuracy, sensitivity, specificity, and receiver operating characteristics (ROC). The results show that the proposed method achieves accuracy, sensitivity, specificity, and area under the roc curve (AUC) of 98.88%, 99.62%, 93.73%, and 0.928, respectively. The proposed approach outperforms various other methods such as Naïve Bayes, K-nearest neighbor, support vector machine, adaptive neuro fuzzy inference system methods, and also other deep learning-based approaches.",2022,10.1007/s11517-021-02462-3,cross-sectional,diagnosis,CT,Lung
CADxReport: Chest x-ray report generation using co-attention mechanism and reinforcement learning,"BACKGROUND: Automated generation of radiological reports for different imaging modalities is essentially required to smoothen the clinical workflow and alleviate radiologists' workload. It involves the careful amalgamation of image processing techniques for medical image interpretation and language generation techniques for report generation. This paper presents CADxReport, a coattention and reinforcement learning based technique for generating clinically accurate reports from chest x-ray (CXR) images. METHOD: CADxReport, uses VGG19 network pre-trained over ImageNet dataset and a multi-label classifier for extracting visual and semantic features from CXR images, respectively. The co-attention mechanism with both the features is used to generate a context vector, which is then passed to HLSTM for radiological report generation. The model is trained using reinforcement learning to maximize CIDEr rewards. OpenI dataset, having 7, 470 CXRs along with 3, 955 associated structured radiological reports, is used for training and testing. RESULTS: Our proposed model is able to generate clinically accurate reports from CXR images. The quantitative evaluations confirm satisfactory results by achieving the following performance scores: BLEU-1 = 0.577, BLEU-2 = 0.478, BLEU-3 = 0.403, BLEU-4 = 0.346, ROUGE = 0.618 and CIDEr = 0.380. CONCLUSIONS: The evaluation using BLEU, ROUGE, and CIDEr score metrics indicates that the proposed model generates sufficiently accurate CXR reports and outperforms most of the state-of-the-art methods for the given task.",2022,10.1016/j.compbiomed.2022.105498,cross-sectional,diagnosis,X-ray,Lung
Calculating the target exposure index using a deep convolutional neural network and a rule base,"PURPOSE: The objective of this study is to determine the quality of chest X-ray images using a deep convolutional neural network (DCNN) and a rule base without performing any visual assessment. A method is proposed for determining the minimum diagnosable exposure index (EI) and the target exposure index (EIt). METHODS: The proposed method involves transfer learning to assess the lung fields, mediastinum, and spine using GoogLeNet, which is a type of DCNN that has been trained using conventional images. Three detectors were created, and the image quality of local regions was rated. Subsequently, the results were used to determine the overall quality of chest X-ray images using a rule-based technique that was in turn based on expert assessment. The minimum EI required for diagnosis was calculated based on the distribution of the EI values, which were classified as either suitable or non-suitable and then used to ascertain the EIt. RESULTS: The accuracy rate using the DCNN and the rule base was 81%. The minimum EI required for diagnosis was 230, and the EIt was 288. CONCLUSION: The results indicated that the proposed method using the DCNN and the rule base could discriminate different image qualities without any visual assessment; moreover, it could determine both the minimum EI required for diagnosis and the EIt.",2020,10.1016/j.ejmp.2020.02.012,cross-sectional,others,X-ray,Lung
Can a Novel Deep Neural Network Improve the Computer-Aided Detection of Solid Pulmonary Nodules and the Rate of False-Positive Findings in Comparison to an Established Machine Learning Computer-Aided Detection?,"OBJECTIVE: The aim of this study was to compare the performance of 2 approved computer-aided detection (CAD) systems for detection of pulmonary solid nodules (PSNs) in an oncologic cohort. The first CAD system is based on a conventional machine learning approach (VD10F), and the other is based on a deep 3D convolutional neural network (CNN) CAD software (VD20A). METHODS AND MATERIALS: Nine hundred sixty-seven patients with a total of 2451 PSNs were retrospectively evaluated using the 2 different CAD systems. All patients had thin-slice chest computed tomography (0.6 mm) using 100 kV and 100 mAs and a high-resolution kernel (I50f). The CAD images generated by VD10F were transferred to the PACS for evaluation. The images generated by VD20A were evaluated using a Web browser-based viewer. Finally, a senior radiologist who was blinded for the CAD results examined the thin-slice images of every patient (ground truth). RESULTS: A total of 2451 PSNs were detected by the senior radiologist. CAD-VD10F detected 1401 true-positive, 143 false-negative, 565 false-positive (FP), and 342 true-negative PSNs, resulting in sensitivity of 90.7%, specificity of 37.7%, positive predictive value of 0.71, and negative predictive value of 0.70. CAD-VD20A detected 1381 true-positive, 163 false-negative, 337 FP, and 570 true-negative PSNs, resulting in sensitivity of 89.4%, specificity of 62.8%, positive predictive value of 0.80, and negative predictive value 0.77, respectively. The rate of FP per scan was 0.6 for CAD-VD10F and 0.3 for CAD-VD20A. CONCLUSIONS: The new deep learning-based CAD software (VD20A) shows similar sensitivity with the conventional CAD software (VD10F), but a significantly higher specificity.",2021,10.1097/rli.0000000000000713,cross-sectional,diagnosis,CT,Lung
Can artificial intelligence distinguish between malignant and benign mediastinal lymph nodes using sonographic features on EBUS images?,"AIMS: This study aimed to develop a new intelligent diagnostic approach using an artificial neural network (ANN). Moreover, we investigated whether the learning-method-guided quantitative analysis approach adequately described mediastinal lymphadenopathies on endobronchial ultrasound (EBUS) images. METHODS: In total, 345 lymph nodes (LNs) from 345 EBUS images were used as source input datasets for the application group. The group consisted of 300 and 45 textural patterns as input and output variables, respectively. The input and output datasets were processed using MATLAB. All these datasets were utilized for the training and testing of the ANN. RESULTS: The best diagnostic accuracy was 82% of that obtained from the textural patterns of the LNs pattern (89% sensitivity, 72% specificity, and 78.2% area under the curve). The negative predictive values were 81% compared to the corresponding positive predictive values of 83%. Due to the application group's pattern-based evaluation, the LN pattern was statistically significant (p = .002). CONCLUSIONS: The proposed intelligent approach could be useful in making diagnoses. Further development is required to improve the diagnostic accuracy of the visual interpretation.",2020,10.1080/03007995.2020.1837763,cross-sectional,diagnosis,EBUS,Mediastinal
Can Deep Learning-Based Volumetric Analysis Predict Oxygen Demand Increase in Patients with COVID-19 Pneumonia?,"Background and Objectives: This study aimed to investigate whether predictive indicators for the deterioration of respiratory status can be derived from the deep learning data analysis of initial chest computed tomography (CT) scans of patients with coronavirus disease 2019 (COVID-19). Materials and Methods: Out of 117 CT scans of 75 patients with COVID-19 admitted to our hospital between April and June 2020, we retrospectively analyzed 79 CT scans that had a definite time of onset and were performed prior to any medication intervention. Patients were grouped according to the presence or absence of increased oxygen demand after CT scan. Quantitative volume data of lung opacity were measured automatically using a deep learning-based image analysis system. The sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) of the opacity volume data were calculated to evaluate the accuracy of the system in predicting the deterioration of respiratory status. Results: All 79 CT scans were included (median age, 62 years (interquartile range, 46-77 years); 56 (70.9%) were male. The volume of opacity was significantly higher for the increased oxygen demand group than for the nonincreased oxygen demand group (585.3 vs. 132.8 mL, p < 0.001). The sensitivity, specificity, and AUC were 76.5%, 68.2%, and 0.737, respectively, in the prediction of increased oxygen demand. Conclusion: Deep learning-based quantitative analysis of the affected lung volume in the initial CT scans of patients with COVID-19 can predict the deterioration of respiratory status to improve treatment and resource management.",2021,10.3390/medicina57111148,cross-sectional,prognosis,CT,Infectious(covid)
Can Peritumoral Radiomics Improve the Prediction of Malignancy of Solid Pulmonary Nodule Smaller Than 2 cm?,"RATIONALE AND OBJECTIVES: To compare the ability of radiomics models including the perinodular parenchyma and standard nodular radiomics model in lung cancer diagnosis of solid pulmonary nodules smaller than 2 cm. MATERIALS AND METHODS: In this retrospective study, the computed tomography (CT) scans of 206 patients with a lung nodule from a single institution in 2012-2019 were collected. For each nodule, four volumes of interest were defined using the gross tumor volume (GTV) and peritumoral volumes (PTVs) of 5, 10, and 15 mm around the tumor. RESULTS: Radiomics models created from GTV, GTV plus 5 mm of PTV, GTV plus 10 mm of PTV, and GTV plus 15 mm of PTV achieved AUCs of 0.89, 0.81, 0.81, and 0.73, respectively, in the validation cohort for the diagnostic classification of benign and malignant pulmonary nodules. The performance of the models gradually decreased as the PTV increased. Wavelet features were the primary features identified in optimal radiomics signatures (2/3 in R, 4/5 in GTV plus 5 mm PTV, 3/4 in GTV plus 10 mm PTV, 2/3 in GTV plus 15 mm PTV). CONCLUSION: Our study indicated that the radiomics signatures of GTV had a good prediction ability in distinguishing benign and malignant solid pulmonary nodules smaller than 2 cm on CT. However, the radiomics feature of the surrounding parenchyma of the nodule did not enhance the effectiveness of the diagnostic model.",2022,10.1016/j.acra.2020.10.029,cross-sectional,diagnosis,CT,Lung
Can texture features improve the differentiation of infiltrative lung adenocarcinoma appearing as ground glass nodules in contrast-enhanced CT?,"OBJECTIVES: To investigate the validity and efficacy of comparing texture features from contrast-enhanced images with non-enhanced images in identifying infiltrative lung adenocarcinoma represented as ground glass nodules (GGN). MATERIALS AND METHODS: A retrospective cohort study was conducted with patients presenting with lung adenocarcinoma and treated at a single centre between January 2015 to December 2017. All patients underwent standard and contrast-enhanced thoracic CT scans with 0.5 mm collimation and 1 mm slice reconstruction thickness before surgery. A total of 34 lung adenocarcinoma patients (representing 34 lesions) were analysed; including 21 instances of invasive adenocarcinoma (IAC) lesions, 4 instances of adenocarcinoma in situ (AIS) lesions, and 9 minimally invasive adenocarcinoma (MIA) lesions. After radiologists manually segmented the lesions, texture features were quantitatively extracted using Artificial Intelligence Kit (AK) software. Then, multivariate logistic regression analysis based on standard and contrast-enhanced CT texture features was employed to analyse the invasiveness of lung adenocarcinoma lesions appearing as GGNs. A receiver operating characteristic (ROC) curve analysis was used to evaluate the performance of those models. RESULTS: A total of 21 quantitative texture features were extracted using the AK software. After dimensionality reduction, 5 and 3 features extracted from thin-section unenhanced and contrast-enhanced CT, respectively, were used to establish the model. The area under the ROC curve (AUC) values for unenhanced CT and enhanced CT features were 0.890 and 0.868, respectively. There was no significant difference (P = 0.190) in the AUC between models based on non-enhanced and contrast-enhanced CT texture features. CONCLUSION: Compared with unenhanced CT, texture features extracted from contrast-enhanced CT provided no benefit in improving the differential diagnosis of infiltrative lung adenocarcinoma from non-infiltrative malignancies appearing as GGNs.",2019,10.1016/j.ejrad.2019.06.010,cross-sectional,prognosis,CT,Lung
Capnovolumetry in combination with clinical history for the diagnosis of asthma and COPD,"Capnovolumetry performed during resting ventilation is an easily applicable diagnostic tool sensitive to airway obstruction. In the present analysis, we investigated in which way capnovolumetric parameters can be combined with basic anamnestic information to support the diagnosis of asthma and COPD. Among 1400 patients of a previous diagnostic study, we selected 1057 patients with a diagnosis of asthma (n = 433), COPD (n = 260), or without respiratory disease (n = 364). Besides performing capnovolumetry, patients answered questions on symptoms and smoking status. Logistic regression analysis, single decision trees (CHAID), and ensembles of trees (random forest) were used to identify diagnostic patterns of asthma and COPD. In the random forest approach, area/volume of phase 3, dyspnea upon strong exertion, s3/s2, and current smoking were identified as relevant parameters for COPD vs control. For asthma vs control, they were wheezing, volume of phase 2, current smoking, and dyspnea at strong exertion. For COPD vs asthma, s3/s2 was the primary criterion, followed by current smoking and smoking history. These parameters were also identified as relevant in single decision trees. Regarding the diagnosis of asthma vs control, COPD vs control, and COPD vs asthma, the area under the curve was 0.623, 0.875, and 0.880, respectively, in the random forest approach. Our results indicate that for the diagnosis of asthma and COPD capnovolumetry can be combined with basic anamnestic information in a simple, intuitive, and efficient manner. As capnovolumetry requires less cooperation from the patient than spirometry, this approach might be helpful for clinical practice.",2020,10.1038/s41533-020-00190-z,cross-sectional,diagnosis,capnovolumetry,Lung
Cardiac Rhythm Device Identification Using Neural Networks,"OBJECTIVES: This paper reports the development, validation, and public availability of a new neural network-based system which attempts to identify the manufacturer and even the model group of a pacemaker or defibrillator from a chest radiograph. BACKGROUND: Medical staff often need to determine the model of a pacemaker or defibrillator (cardiac rhythm device) quickly and accurately. Current approaches involve comparing a device's radiographic appearance with a manual flow chart. METHODS: In this study, radiographic images of 1,676 devices, comprising 45 models from 5 manufacturers were extracted. A convolutional neural network was developed to classify the images, using a training set of 1,451 images. The testing set contained an additional 225 images consisting of 5 examples of each model. The network's ability to identify the manufacturer of a device was compared with that of cardiologists, using a published flowchart. RESULTS: The neural network was 99.6% (95% confidence interval [CI]: 97.5% to 100.0%) accurate in identifying the manufacturer of a device from a radiograph and 96.4% (95% CI: 93.1% to 98.5%) accurate in identifying the model group. Among 5 cardiologists who used the flowchart, median identification of manufacturer accuracy was 72.0% (range 62.2% to 88.9%), and model group identification was not possible. The network's ability to identify the manufacturer of the devices was significantly superior to that of all the cardiologists (p < 0.0001 compared with the median human identification; p < 0.0001 compared with the best human identification). CONCLUSIONS: A neural network can accurately identify the manufacturer and even model group of a cardiac rhythm device from a radiograph and exceeds human performance. This system may speed up the diagnosis and treatment of patients with cardiac rhythm devices, and it is publicly accessible online.",2019,10.1016/j.jacep.2019.02.003,cross-sectional,diagnosis,X-ray,Heart
Cardiothoracic ratio measurement using artificial intelligence: observer and method validation studies,"BACKGROUND: Artificial Intelligence (AI) is a promising tool for cardiothoracic ratio (CTR) measurement that has been technically validated but not clinically evaluated on a large dataset. We observed and validated AI and manual methods for CTR measurement using a large dataset and investigated the clinical utility of the AI method. METHODS: Five thousand normal chest x-rays and 2,517 images with cardiomegaly and CTR values, were analyzed using manual, AI-assisted, and AI-only methods. AI-only methods obtained CTR values from a VGG-16 U-Net model. An in-house software was used to aid the manual and AI-assisted measurements and to record operating time. Intra and inter-observer experiments were performed on manual and AI-assisted methods and the averages were used in a method variation study. AI outcomes were graded in the AI-assisted method as excellent (accepted by both users independently), good (required adjustment), and poor (failed outcome). Bland-Altman plot with coefficient of variation (CV), and coefficient of determination (R-squared) were used to evaluate agreement and correlation between measurements. Finally, the performance of a cardiomegaly classification test was evaluated using a CTR cutoff at the standard (0.5), optimum, and maximum sensitivity. RESULTS: Manual CTR measurements on cardiomegaly data were comparable to previous radiologist reports (CV of 2.13% vs 2.04%). The observer and method variations from the AI-only method were about three times higher than from the manual method (CV of 5.78% vs 2.13%). AI assistance resulted in 40% excellent, 56% good, and 4% poor grading. AI assistance significantly improved agreement on inter-observer measurement compared to manual methods (CV; bias: 1.72%; - 0.61% vs 2.13%; - 1.62%) and was faster to perform (2.2 ± 2.4 secs vs 10.6 ± 1.5 secs). The R-squared and classification-test were not reliable indicators to verify that the AI-only method could replace manual operation. CONCLUSIONS: AI alone is not yet suitable to replace manual operations due to its high variation, but it is useful to assist the radiologist because it can reduce observer variation and operation time. Agreement of measurement should be used to compare AI and manual methods, rather than R-square or classification performance tests.",2021,10.1186/s12880-021-00625-0,cross-sectional,diagnosis,X-ray,Heart
Cardiovascular signatures of COVID-19 predict mortality and identify barrier stabilizing therapies,"BACKGROUND: Endothelial cell (EC) activation, endotheliitis, vascular permeability, and thrombosis have been observed in patients with severe coronavirus disease 2019 (COVID-19), indicating that the vasculature is affected during the acute stages of SARS-CoV-2 infection. It remains unknown whether circulating vascular markers are sufficient to predict clinical outcomes, are unique to COVID-19, and if vascular permeability can be therapeutically targeted. METHODS: Prospectively evaluating the prevalence of circulating inflammatory, cardiac, and EC activation markers as well as developing a microRNA atlas in 241 unvaccinated patients with suspected SARS-CoV-2 infection allowed for prognostic value assessment using a Random Forest model machine learning approach. Subsequent ex vivo experiments assessed EC permeability responses to patient plasma and were used to uncover modulated gene regulatory networks from which rational therapeutic design was inferred. FINDINGS: Multiple inflammatory and EC activation biomarkers were associated with mortality in COVID-19 patients and in severity-matched SARS-CoV-2-negative patients, while dysregulation of specific microRNAs at presentation was specific for poor COVID-19-related outcomes and revealed disease-relevant pathways. Integrating the datasets using a machine learning approach further enhanced clinical risk prediction for in-hospital mortality. Exposure of ECs to COVID-19 patient plasma resulted in severity-specific gene expression responses and EC barrier dysfunction, which was ameliorated using angiopoietin-1 mimetic or recombinant Slit2-N. INTERPRETATION: Integration of multi-omics data identified microRNA and vascular biomarkers prognostic of in-hospital mortality in COVID-19 patients and revealed that vascular stabilizing therapies should be explored as a treatment for endothelial dysfunction in COVID-19, and other severe diseases where endothelial dysfunction has a central role in pathogenesis. FUNDING: This work was directly supported by grant funding from the Ted Rogers Center for Heart Research, Toronto, Ontario, Canada and the Peter Munk Cardiac Center, Toronto, Ontario, Canada.",2022,10.1016/j.ebiom.2022.103982,prospective cohort,prognosis,Biomarkers,Cardiovascular
CARes-UNet: Content-aware residual UNet for lesion segmentation of COVID-19 from chest CT images,"PURPOSE: Coronavirus disease 2019 (COVID-19) has caused a serious global health crisis. It has been proven that the deep learning method has great potential to assist doctors in diagnosing COVID-19 by automatically segmenting the lesions in computed tomography (CT) slices. However, there are still several challenges restricting the application of these methods, including high variation in lesion characteristics and low contrast between lesion areas and healthy tissues. Moreover, the lack of high-quality labeled samples and large number of patients lead to the urgency to develop a high accuracy model, which performs well not only under supervision but also with semi-supervised methods. METHODS: We propose a content-aware lung infection segmentation deep residual network (content-aware residual UNet (CARes-UNet)) to segment the lesion areas of COVID-19 from the chest CT slices. In our CARes-UNet, the residual connection was used in the convolutional block, which alleviated the degradation problem during the training. Then, the content-aware upsampling modules were introduced to improve the performance of the model while reducing the computation cost. Moreover, to achieve faster convergence, an advanced optimizer named Ranger was utilized to update the model's parameters during training. Finally, we employed a semi-supervised segmentation framework to deal with the problem of lacking pixel-level labeled data. RESULTS: We evaluated our approach using three public datasets with multiple metrics and compared its performance to several models. Our method outperforms other models in multiple indicators, for instance in terms of Dice coefficient on COVID-SemiSeg Dataset, CARes-UNet got the score 0.731, and semi-CARes-UNet further boosted it to 0.776. More ablation studies were done and validated the effectiveness of each key component of our proposed model. CONCLUSIONS: Compared with the existing neural network methods applied to the COVID-19 lesion segmentation tasks, our CARes-UNet can gain more accurate segmentation results, and semi-CARes-UNet can further improve it using semi-supervised learning methods while presenting a possible way to solve the problem of lack of high-quality annotated samples. Our CARes-UNet and semi-CARes-UNet can be used in artificial intelligence-empowered computer-aided diagnosis system to improve diagnostic accuracy in this ongoing COVID-19 pandemic.",2021,10.1002/mp.15231,cross-sectional,diagnosis,CT,Lung
Cell Painting predicts impact of lung cancer variants,"Most variants in most genes across most organisms have an unknown impact on the function of the corresponding gene. This gap in knowledge is especially acute in cancer, where clinical sequencing of tumors now routinely reveals patient-specific variants whose functional impact on the corresponding genes is unknown, impeding clinical utility. Transcriptional profiling was able to systematically distinguish these variants of unknown significance as impactful vs. neutral in an approach called expression-based variant-impact phenotyping. We profiled a set of lung adenocarcinoma-associated somatic variants using Cell Painting, a morphological profiling assay that captures features of cells based on microscopy using six stains of cell and organelle components. Using deep-learning-extracted features from each cell's image, we found that cell morphological profiling (cmVIP) can predict variants' functional impact and, particularly at the single-cell level, reveals biological insights into variants that can be explored at our public online portal. Given its low cost, convenient implementation, and single-cell resolution, cmVIP profiling therefore seems promising as an avenue for using non-gene specific assays to systematically assess the impact of variants, including disease-associated alleles, on gene function.",2022,10.1091/mbc.E21-11-0538,cross-sectional,prognosis,Cell painting,Lung
Central focused convolutional neural networks: Developing a data-driven model for lung nodule segmentation,"Accurate lung nodule segmentation from computed tomography (CT) images is of great importance for image-driven lung cancer analysis. However, the heterogeneity of lung nodules and the presence of similar visual characteristics between nodules and their surroundings make it difficult for robust nodule segmentation. In this study, we propose a data-driven model, termed the Central Focused Convolutional Neural Networks (CF-CNN), to segment lung nodules from heterogeneous CT images. Our approach combines two key insights: 1) the proposed model captures a diverse set of nodule-sensitive features from both 3-D and 2-D CT images simultaneously; 2) when classifying an image voxel, the effects of its neighbor voxels can vary according to their spatial locations. We describe this phenomenon by proposing a novel central pooling layer retaining much information on voxel patch center, followed by a multi-scale patch learning strategy. Moreover, we design a weighted sampling to facilitate the model training, where training samples are selected according to their degree of segmentation difficulty. The proposed method has been extensively evaluated on the public LIDC dataset including 893 nodules and an independent dataset with 74 nodules from Guangdong General Hospital (GDGH). We showed that CF-CNN achieved superior segmentation performance with average dice scores of 82.15% and 80.02% for the two datasets respectively. Moreover, we compared our results with the inter-radiologists consistency on LIDC dataset, showing a difference in average dice score of only 1.98%.",2017,10.1016/j.media.2017.06.014,cross-sectional,diagnosis,CT,Lung
Cerebral regional and network characteristics in asthma patients: a resting-state fMRI study,"Asthma is a serious health problem that involves not only the respiratory system but also the central nervous system. Previous studies identified either regional or network alterations in patients with asthma, but inconsistent results were obtained. A key question remains unclear: are the regional and neural network deficits related or are they two independent characteristics in asthma? Answering this question is the aim of this study. By collecting resting-state functional magnetic resonance imaging from 39 patients with asthma and 40 matched health controls, brain functional measures including regional activity (amplitude of low-frequency fluctuations) and neural network function (degree centrality (DC) and functional connectivity) were calculated to systematically characterize the functional alterations. Patients exhibited regional abnormities in the left angular gyrus, right precuneus, and inferior temporal gyrus within the default mode network. Network abnormalities involved both the sensorimotor network and visual network with key regions including the superior frontal gyrus and occipital lobes. Altered DC in the lingual gyrus was correlated with the degree of airway obstruction. This study elucidated different patterns of regional and network changes, thereby suggesting that the two parameters reflect different brain characteristics of asthma. These findings provide evidence for further understanding the potential cerebral alterations in the pathophysiology of asthma.",2020,10.1007/s11684-020-0745-1,,,,
Changes in CT Radiomic Features Associated with Lymphocyte Distribution Predict Overall Survival and Response to Immunotherapy in Non-Small Cell Lung Cancer,"No predictive biomarkers can robustly identify patients with non-small cell lung cancer (NSCLC) who will benefit from immune checkpoint inhibitor (ICI) therapies. Here, in a machine learning setting, we compared changes (""delta"") in the radiomic texture (DelRADx) of CT patterns both within and outside tumor nodules before and after two to three cycles of ICI therapy. We found that DelRADx patterns could predict response to ICI therapy and overall survival (OS) for patients with NSCLC. We retrospectively analyzed data acquired from 139 patients with NSCLC at two institutions, who were divided into a discovery set (D(1) = 50) and two independent validation sets (D(2) = 62, D(3) = 27). Intranodular and perinodular texture descriptors were extracted, and the relative differences were computed. A linear discriminant analysis (LDA) classifier was trained with 8 DelRADx features to predict RECIST-derived response. Association of delta-radiomic risk score (DRS) with OS was determined. The association of DelRADx features with tumor-infiltrating lymphocyte (TIL) density on the diagnostic biopsies (n = 36) was also evaluated. The LDA classifier yielded an AUC of 0.88 ± 0.08 in distinguishing responders from nonresponders in D(1), and 0.85 and 0.81 in D(2) and D(3) DRS was associated with OS [HR: 1.64; 95% confidence interval (CI), 1.22-2.21; P = 0.0011; C-index = 0.72). Peritumoral Gabor features were associated with the density of TILs on diagnostic biopsy samples. Our results show that DelRADx could be used to identify early functional responses in patients with NSCLC.",2020,10.1158/2326-6066.Cir-19-0476,retrospective cohort,prognosis,CT,Lung
Chest computed tomography in the diagnosis of COVID-19 in patients with false negative RT-PCR,"OBJECTIVE: To evaluate the role of chest computed tomography in patients with COVID-19 who presented initial negative result in reverse transcriptase-polymerase chain reaction (RT-PCR). METHODS: A single-center, retrospective study that evaluated 39 patients with negative RT-PCR for COVID-19, who underwent chest computed tomography and had a final clinical or serological diagnosis of COVID-19. The visual tomographic classification was evaluated according to the Consensus of the Radiological Society of North America and software developed with artificial intelligence for automatic detection of findings and chance estimation of COVID-19. RESULTS: In the visual tomographic analysis, only one of them (3%) presented computed tomography classified as negative, 69% were classified as typical and 28% as indeterminate. In the evaluation using the software, only four (about 10%) had a probability of COVID-19 <25%. CONCLUSION: Computed tomography can play an important role in management of suspected cases of COVID-19 with initial negative results in RT-PCR, especially considering those patients outside the ideal window for sample collection for RT-PCR.",2021,10.31744/einstein_journal/2021AO6363,cross-sectional,diagnosis,CT,Lung
Chest CT Evaluation of 11 Persistent Asymptomatic Patients with SARS-CoV-2 Infection,"In total, 11 asymptomatic carriers who underwent nasal or oropharyngeal swab tests for SARS-CoV-2 after being in close contact with patients who developed symptomatic 2019 coronavirus disease (COVID-19) were enrolled in this study. The chest multidetector computed tomography (CT) images of the enrolled patients were qualitatively and quantitatively analyzed. The findings of the first chest CT were normal in 3 (27.3%) patients, 2 of whom were aged below 15 years. The lesions of 2 (18.2%) patients involved 1 lobe with unifocal presence. Subpleural lesions were observed in 7 (63.6%) patients. Ground glass opacity (GGO) was the most common sign observed in 7 (63.6%) patients. Crazy-paving pattern and consolidation were detected in 2 (18.2%) and 4 (36.4%) patients, respectively. Based on deep learning and quantitative analysis, the mean volume of intrapulmonary lesions in the first CT image was 85.73 ± 84.46 cm(3). In patients with positive findings on CT images, the average interval between positive real-time reverse transcriptase polymerase chain reaction assay and peak volume on CT images was 5.1 ± 3.1 days. In conclusion, typical CT findings can be detected in over 70% of asymptomatic SARS-CoV-2 carriers. The initial presentation is typically GGO along the subpleural regions and bronchi, which absorbs in approximately 5 days.",2021,10.7883/yoken.JJID.2020.264,cross-sectional,diagnosis,CT,Lung
Chest CT for triage during COVID-19 on the emergency department: myth or truth?,"PURPOSE: We aimed to investigate the diagnostic performance of chest CT compared with first RT-PCR results in adult patients suspected of COVID-19 infection in an ED setting. We also constructed a predictive machine learning model based on chest CT and additional data to improve the diagnostic accuracy of chest CT. METHODS: This study's cohort consisted of 319 patients who underwent chest CT and RT-PCR testing at the ED. Patient characteristics, demographics, symptoms, vital signs, laboratory tests, and chest CT results (CO-RADS) were collected. With first RT-PCR as reference standard, the diagnostic performance of chest CT using the CO-RADS score was assessed. Additionally, a predictive machine learning model was constructed using logistic regression. RESULTS: Chest CT, with first RT-PCR as a reference, had a sensitivity, specificity, PPV, and NPV of 90.2%, 88.2%, 84.5%, and 92.7%, respectively. The prediction model with CO-RADS, ferritin, leucocyte count, CK, days of complaints, and diarrhea as predictors had a sensitivity, specificity, PPV, and NPV of 89.3%, 93.4%, 90.8%, and 92.3%, respectively. CONCLUSION: Chest CT, using the CO-RADS scoring system, is a sensitive and specific method that can aid in the diagnosis of COVID-19, especially if RT-PCR tests are scarce during an outbreak. Combining a predictive machine learning model could further improve the accuracy of diagnostic chest CT for COVID-19. Further candidate predictors should be analyzed to improve our model. However, RT-PCR should remain the primary standard of testing as up to 9% of RT-PCR positive patients are not diagnosed by chest CT or our machine learning model.",2020,10.1007/s10140-020-01821-1,cross-sectional,diagnosis,CT,Lung
Chest Radiographs in Congestive Heart Failure: Visualizing Neural Network Learning,"Purpose To examine Generative Visual Rationales (GVRs) as a tool for visualizing neural network learning of chest radiograph features in congestive heart failure (CHF). Materials and Methods A total of 103 489 frontal chest radiographs in 46 712 patients acquired from January 1, 2007, to December 31, 2016, were divided into a labeled data set (with B-type natriuretic peptide [BNP] result as a marker of CHF) and unlabeled data set (without BNP result). A generative model was trained on the unlabeled data set, and a neural network was trained on the encoded representations of the labeled data set to estimate BNP. The model was used to visualize how a radiograph with high estimated BNP would look without disease (a ""healthy"" radiograph). An overfitted model was developed for comparison, and 100 GVRs were blindly assessed by two experts for features of CHF. Area under the receiver operating characteristic curve (AUC), κ coefficient, and mixed-effects logistic regression were used for statistical analyses. Results At a cutoff BNP of 100 ng/L as a marker of CHF, the correctly trained model achieved an AUC of 0.82. Assessment of GVRs revealed that the correctly trained model highlighted conventional radiographic features of CHF as reasons for an elevated BNP prediction more frequently than the overfitted model, including cardiomegaly (153 [76.5%] of 200 vs 64 [32%] of 200, respectively; P < .001) and pleural effusions (47 [23.5%] of 200 vs 16 [8%] of 200, respectively; P = .003). Conclusion Features of congestive heart failure on chest radiographs learned by neural networks can be identified using Generative Visual Rationales, enabling detection of bias and overfitted models. © RSNA, 2018 See also the editorial by Ngo in this issue.",2019,10.1148/radiol.2018180887,cross-sectional,combined,X-ray,Heart
Chest X-ray Classification for the Detection of COVID-19 Using Deep Learning Techniques,"Recent technological developments pave the path for deep learning-based techniques to be used in almost every domain of life. The precision of deep learning techniques make it possible for these to be used in the medical field for the classification and detection of various diseases. Recently, the coronavirus (COVID-19) pandemic has put a lot of pressure on the health system all around the world. The diagnosis of COVID-19 is possible by PCR testing and medical imagining. Since COVID-19 is highly contagious, diagnosis using chest X-ray is considered safe in various situations. In this study, a deep learning-based technique is proposed to classify COVID-19 infection from other non-COVID-19 infections. To classify COVID-19, three different pre-trained models named EfficientNetB1, NasNetMobile and MobileNetV2 are used. The augmented dataset is used for training deep learning models while two different training strategies have been used for classification. In this study, not only are the deep learning model fine-tuned but also the hyperparameters are fine-tuned, which significantly improves the performance of the fine-tuned deep learning models. Moreover, the classification head is regularized to improve the performance. For the evaluation of the proposed techniques, several performance parameters are used to gauge the performance. EfficientNetB1 with regularized classification head outperforms the other models. The proposed technique successfully classifies four classes that include COVID-19, viral pneumonia, lung opacity, and normal, with an accuracy of 96.13%. The proposed technique shows superiority in terms of accuracy when compared with recent techniques present in the literature.",2022,10.3390/s22031211,cross-sectional,diagnosis,X-ray,Lung
Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network,"PURPOSE: Recently, the outbreak of the novel coronavirus disease 2019 (COVID-19) pandemic has seriously endangered human health and life. In fighting against COVID-19, effective diagnosis of infected patient is critical for preventing the spread of diseases. Due to limited availability of test kits, the need for auxiliary diagnostic approach has increased. Recent research has shown radiography of COVID-19 patient, such as CT and X-ray, contains salient information about the COVID-19 virus and could be used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster imaging time, wide availability, low cost, and portability gains much attention and becomes very promising. In order to reduce intra- and inter-observer variability, during radiological assessment, computer-aided diagnostic tools have been used in order to supplement medical decision making and subsequent management. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologist in the interpretation of the collected data. METHOD: In this study, we design a novel multi-feature convolutional neural network (CNN) architecture for multi-class improved classification of COVID-19 from CXR images. CXR images are enhanced using a local phase-based image enhancement method. The enhanced images, together with the original CXR data, are used as an input to our proposed CNN architecture. Using ablation studies, we show the effectiveness of the enhanced images in improving the diagnostic accuracy. We provide quantitative evaluation on two datasets and qualitative results for visual inspection. Quantitative evaluation is performed on data consisting of 8851 normal (healthy), 6045 pneumonia, and 3323 COVID-19 CXR scans. RESULTS: In Dataset-1, our model achieves 95.57% average accuracy for a three classes classification, 99% precision, recall, and F1-scores for COVID-19 cases. For Dataset-2, we have obtained 94.44% average accuracy, and 95% precision, recall, and F1-scores for detection of COVID-19. CONCLUSIONS: Our proposed multi-feature-guided CNN achieves improved results compared to single-feature CNN proving the importance of the local phase-based CXR image enhancement. Future work will involve further evaluation of the proposed method on a larger-size COVID-19 dataset as they become available.",2021,10.1007/s11548-020-02305-w,cross-sectional,diagnosis,X-ray,Lung
Choquet Integral and Coalition Game-Based Ensemble of Deep Learning Models for COVID-19 Screening From Chest X-Ray Images,"Under the present circumstances, when we are still under the threat of different strains of coronavirus, and since the most widely used method for COVID-19 detection, RT-PCR is a tedious and time-consuming manual procedure with poor precision, the application of Artificial Intelligence (AI) and Computer-Aided Diagnosis (CAD) is inevitable. Though, some vaccines have now been authorized worldwide, it will take huge time to reach everyone, especially in developing countries. In this work, we have analyzed Chest X-ray (CXR) images for the detection of the coronavirus. The primary agenda of this proposed research study is to leverage the classification performance of the deep learning models using ensemble learning. Many papers have proposed different ensemble learning techniques in this field, some methods using aggregation functions like Weighted Arithmetic Mean (WAM) among others. However, none of these methods take into consideration the decisions that subsets of the classifiers take. In this paper, we have applied Choquet integral for ensemble and propose a novel method for the evaluation of fuzzy measures using coalition game theory, information theory, and Lambda fuzzy approximation. Three different sets of fuzzy measures are calculated using three different weighting schemes along with information theory and coalition game theory. Using these three sets of fuzzy measures, three Choquet integrals are calculated and their decisions are finally combined. Besides, we have created a database by combining several image repositories developed recently. Impressive results on the newly developed dataset and the challenging COVIDx dataset support the efficacy and robustness of the proposed method. Our experimental results outperform many recently proposed methods.",2021,10.1109/jbhi.2021.3111415,cross-sectional,diagnosis,X-ray,Lung
Chronic Obstructive Pulmonary Disease Quantification Using CT Texture Analysis and Densitometry: Results From the Danish Lung Cancer Screening Trial,"OBJECTIVE. The purpose of this study is to establish whether texture analysis and densitometry are complementary quantitative measures of chronic obstructive pulmonary disease (COPD) in a lung cancer screening setting. MATERIALS AND METHODS. This was a retrospective study of data collected prospectively (in 2004-2010) in the Danish Lung Cancer Screening Trial. The texture score, relative area of emphysema, and percentile density were computed for 1915 baseline low-dose lung CT scans and were evaluated, both individually and in combination, for associations with lung function (i.e., forced expiratory volume in 1 second as a percentage of predicted normal [FEV(1)% predicted]), diagnosis of mild to severe COPD, and prediction of a rapid decline in lung function. Multivariate linear regression models with lung function as the outcome were compared using the likelihood ratio test or the Vuong test, and AUC values for diagnostic and prognostic capabilities were compared using the DeLong test. RESULTS. Texture showed a significantly stronger association with lung function (p < 0.001 vs densitometric measures), a significantly higher diagnostic AUC value (for COPD, 0.696; for Global Initiative for Chronic Obstructive Lung Disease (GOLD) grade 1, 0.648; for GOLD grade 2, 0.768; and for GOLD grade 3, 0.944; p < 0.001 vs densitometric measures), and a higher but not significantly different association with lung function decline. In addition, only texture could predict a rapid decline in lung function (AUC value, 0.538; p < 0.05 vs random guessing). The combination of texture and both densitometric measures strengthened the association with lung function and decline in lung function (p < 0.001 and p < 0.05, respectively, vs texture) but did not improve diagnostic or prognostic performance. CONCLUSION. The present study highlights texture as a promising quantitative CT measure of COPD to use alongside, or even instead of, densitometric measures. Moreover, texture may allow early detection of COPD in subjects who undergo lung cancer screening.",2020,10.2214/ajr.19.22300,retrospective cohort,diagnosis,CT,Lung
Chronic Obstructive Pulmonary Disease: Thoracic CT Texture Analysis and Machine Learning to Predict Pulmonary Ventilation,"Background Fixed airflow limitation and ventilation heterogeneity are common in chronic obstructive pulmonary disease (COPD). Conventional noncontrast CT provides airway and parenchymal measurements but cannot be used to directly determine lung function. Purpose To develop, train, and test a CT texture analysis and machine-learning algorithm to predict lung ventilation heterogeneity in participants with COPD. Materials and Methods In this prospective study (ClinicalTrials.gov: NCT02723474; conducted from January 2010 to February 2017), participants were randomized to optimization (n = 1), training (n = 67), and testing (n = 27) data sets. Hyperpolarized (HP) helium 3 ((3)He) MRI ventilation maps were co-registered with thoracic CT to provide ground truth labels, and 87 quantitative imaging features were extracted and normalized to lung averages to generate 174 features. The volume-of-interest dimension and the training data sampling method were optimized to maximize the area under the receiver operating characteristic curve (AUC). Forward feature selection was performed to reduce the number of features; logistic regression, linear support vector machine, and quadratic support vector machine classifiers were trained through fivefold cross validation. The highest-performing classification model was applied to the test data set. Pearson coefficients were used to determine the relationships between the model, MRI, and pulmonary function measurements. Results The quadratic support vector machine performed best in training and was applied to the test data set. Model-predicted ventilation maps had an accuracy of 88% (95% confidence interval [CI]: 88%, 88%) and an AUC of 0.82 (95% CI: 0.82, 0.83) when the HP (3)He MRI ventilation maps were used as the reference standard. Model-predicted ventilation defect percentage (VDP) was correlated with VDP at HP (3)He MRI (r = 0.90, P < .001). Both model-predicted and HP (3)He MRI VDP were correlated with forced expiratory volume in 1 second (FEV(1)) (model: r = -0.65, P < .001; MRI: r = -0.70, P < .001), ratio of FEV(1) to forced vital capacity (model: r = -0.73, P < .001; MRI: r = -0.75, P < .001), diffusing capacity (model: r = -0.69, P < .001; MRI: r = -0.65, P < .001), and quality-of-life score (model: r = 0.59, P = .001; MRI: r = 0.65, P < .001). Conclusion Model-predicted ventilation maps generated by using CT textures and machine learning were correlated with MRI ventilation maps (r = 0.90, P < .001). © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Fain in this issue.",2019,10.1148/radiol.2019190450,prospective cohort,combined,CT,Lung
Classification and Quantification of Emphysema Using a Multi-Scale Residual Network,"Automated tissue classification is an essential step for quantitative analysis and treatment of emphysema. Although many studies have been conducted in this area, there still remain two major challenges. First, different emphysematous tissue appears in different scales, which we call ""inter-class variations."" Second, the intensities of CT images acquired from different patients, scanners or scanning protocols may vary, which we call ""intra-class variations"". In this paper, we present a novel multi-scale residual network with two channels of raw CT image and its differential excitation component. We incorporate multi-scale information into our networks to address the challenge of inter-class variations. In addition to the conventional raw CT image, we use its differential excitation component as a pair of inputs to handle intra-class variations. Experimental results show that our approach has superior performance over the state-of-the- art methods, achieving a classification accuracy of 93.74% on our original emphysema database. Based on the classification results, we also perform the quantitative analysis of emphysema in 50 subjects by correlating the quantitative results (the area percentage of each class) with pulmonary functions. We show that centrilobular emphysema (CLE) and panlobular emphysema (PLE) have strong correlation with the pulmonary functions and the sum of CLE and PLE can be used as a new and accurate measure of emphysema severity instead of the conventional measure (sum of all subtypes of emphysema). The correlations between the new measure and various pulmonary functions are up to |r| = 0.922 (r is correlation coefficient).",2019,10.1109/jbhi.2018.2890045,cross-sectional,diagnosis,CT,Lung
Classification and Segmentation Algorithm in Benign and Malignant Pulmonary Nodules under Different CT Reconstruction,"METHODS: The imaging data of 55 patients with chest CT plain scan in the Xuancheng People's Hospital were collected retrospectively. The data of each patient included lung window reconstruction, mediastinum reconstruction, and bone window reconstruction. The depth neural network and 3D convolution neural network were used to construct the model and train the classification and segmentation algorithm. The pathological results were the gold standard for benign and malignant pulmonary nodules. The classification and segmentation algorithms under three CT reconstruction algorithms were compared and analyzed by analysis of variance. RESULTS: Under the three CT reconstruction algorithms, the classification accuracy of pulmonary nodule density types was 98.2%, 96.4%, and 94.5%, respectively. The Dice coefficients of all nodule segmentation were 80.32% ± 5.91%, 79.83% ± 6.12%, and 80.17% ± 5.89%, respectively. The diagnostic accuracy between benign and malignant pulmonary nodules under different reconstruction algorithms was 98.2%, 96.4%, and 94.5%, respectively. There was no significant difference in the classification accuracy, Dice coefficients, and diagnostic accuracy of pulmonary nodules under three different reconstruction algorithms (all P > 0.05). CONCLUSION: The depth neural network algorithm combined with 3D convolution neural network has a good efficiency in identifying benign and malignant pulmonary nodules under different CT reconstruction classification and segmentation algorithms.",2022,10.1155/2022/3490463,cross-sectional,diagnosis,CT,Lung
Classification by a stacking model using CNN features for COVID-19 infection diagnosis,"Affecting millions of people all over the world, the COVID-19 pandemic has caused the death of hundreds of thousands of people since its beginning. Examinations also found that even if the COVID-19 patients initially survived the coronavirus, pneumonia left behind by the virus may still cause severe diseases resulting in organ failure and therefore death in the future. The aim of this study is to classify COVID-19, normal and viral pneumonia using the chest X-ray images with machine learning methods. A total of 3486 chest X-ray images from three classes were first classified by three single machine learning models including the support vector machine (SVM), logistics regression (LR), artificial neural network (ANN) models, and then by a stacking model that was created by combining these 3 single models. Several performance evaluation indices including recall, precision, F-1 score, and accuracy were computed to evaluate and compare classification performance of 3 single four models and the final stacking model used in the study. As a result of the evaluations, the models namely, SVM, ANN, LR, and stacking, achieved 90.2%, 96.2%, 96.7%, and 96.9%classification accuracy, respectively. The study results indicate that the proposed stacking model is a fast and inexpensive method for assisting COVID-19 diagnosis, which can have potential to assist physicians and nurses to better and more efficiently diagnose COVID-19 infection cases in the busy clinical environment.",2022,10.3233/xst-211031,cross-sectional,diagnosis,X-ray,Lung
Classification Model to Estimate MIB-1 (Ki 67) Proliferation Index in NSCLC Patients Evaluated With (18)F-FDG-PET/CT,"BACKGROUND/AIM: Proliferation biomarkers such as MIB-1 are strong predictors of clinical outcome and response to therapy in patients with non-small-cell lung cancer, but they require histological examination. In this work, we present a classification model to predict MIB-1 expression based on clinical parameters from positron emission tomography. PATIENTS AND METHODS: We retrospectively evaluated 78 patients with histology-proven non-small-cell lung cancer (NSCLC) who underwent (18)F-FDG-PET/CT for clinical examination. We stratified the population into a low and high proliferation group using MIB-1=25% as cut-off value. We built a predictive model based on binary classification trees to estimate the group label from the maximum standardized uptake value (SUV(max)) and lesion diameter. RESULTS: The proposed model showed ability to predict the correct proliferation group with overall accuracy >82% (78% and 86% for the low- and high-proliferation group, respectively). CONCLUSION: Our results indicate that radiotracer activity evaluated via SUV(max) and lesion diameter are correlated with tumour proliferation index MIB-1.",2020,10.21873/anticanres.14318,,,,
Classification of Benign and Malignant Lung Nodules Based on Deep Convolutional Network Feature Extraction,"With the rapid development of detection technology, CT imaging technology has been widely used in the early clinical diagnosis of lung nodules. However, accurate assessment of the nature of the nodule remains a challenging task due to the subjective nature of the radiologist. With the increasing amount of publicly available lung image data, it has become possible to use convolutional neural networks for benign and malignant classification of lung nodules. However, as the network depth increases, network training methods based on gradient descent usually lead to gradient dispersion. Therefore, we propose a novel deep convolutional network approach to classify the benignity and malignancy of lung nodules. Firstly, we segmented, extracted, and performed zero-phase component analysis whitening on images of lung nodules. Then, a multilayer perceptron was introduced into the structure to construct a deep convolutional network. Finally, the minibatch stochastic gradient descent method with a momentum coefficient is used to fine-tune the deep convolutional network to avoid the gradient dispersion. The 750 lung nodules in the lung image database are used for experimental verification. Classification accuracy of the proposed method can reach 96.0%. The experimental results show that the proposed method can provide an objective and efficient aid to solve the problem of classifying benign and malignant lung nodules in medical images.",2021,10.1155/2021/8769652,cross-sectional,diagnosis,CT,Lung
Classification of benign and malignant lung nodules from CT images based on hybrid features,"The classification of benign and malignant lung nodules has great significance for the early detection of lung cancer, since early diagnosis of nodules can greatly increase patient survival. In this paper, we propose a novel classification method for lung nodules based on hybrid features from computed tomography (CT) images. The method fused 3D deep dual path network (DPN) features, local binary pattern (LBP)-based texture features and histogram of oriented gradients (HOG)-based shape features to characterize lung nodules. DPN is a convolutional neural network which integrates the advantages of aggregated residual transformations (ResNeXt) for feature reuse and a densely convolutional network (DenseNet) for exploring new features. LBP is a prominent feature descriptor for texture classification, when combining with the HOG descriptor, it can improve the classification performance considerably. To differentiate malignant nodules from benign ones, a gradient boosting machine (GBM) algorithm is employed. We evaluated the proposed method on the publicly available LUng Nodule Analysis 2016 (LUNA16) dataset with 1004 nodules, achieving an area under the receiver operating characteristic curve (AUC) of 0.9687 and accuracy of 93.78%. The promising results demonstrate that our method has strong robustness on the classification of nodule patterns by virtue of the joint use of texture features, shape features and 3D deep DPN features. The method has the potential to help radiologists to interpret diagnostic data and make decisions in clinical practice.",2019,10.1088/1361-6560/ab2544,cross-sectional,diagnosis,CT,Lung
Classification of COVID-19 and Influenza Patients Using Deep Learning,"Coronavirus (COVID-19) is a deadly virus that initially starts with flu-like symptoms. COVID-19 emerged in China and quickly spread around the globe, resulting in the coronavirus epidemic of 2019-22. As this virus is very similar to influenza in its early stages, its accurate detection is challenging. Several techniques for detecting the virus in its early stages are being developed. Deep learning techniques are a handy tool for detecting various diseases. For the classification of COVID-19 and influenza, we proposed tailored deep learning models. A publicly available dataset of X-ray images was used to develop proposed models. According to test results, deep learning models can accurately diagnose normal, influenza, and COVID-19 cases. Our proposed long short-term memory (LSTM) technique outperformed the CNN model in the evaluation phase on chest X-ray images, achieving 98% accuracy.",2022,10.1155/2022/8549707,cross-sectional,diagnosis,CT,Lung
Classification of COVID-19 by Compressed Chest CT Image through Deep Learning on a Large Patients Cohort,"Corona Virus Disease (COVID-19) has spread globally quickly, and has resulted in a large number of causalities and medical resources insufficiency in many countries. Reverse-transcriptase polymerase chain reaction (RT-PCR) testing is adopted as biopsy tool for confirmation of virus infection. However, its accuracy is as low as 60-70%, which is inefficient to uncover the infected. In comparison, the chest CT has been considered as the prior choice in diagnosis and monitoring progress of COVID-19 infection. Although the COVID-19 diagnostic systems based on artificial intelligence have been developed for assisting doctors in diagnosis, the small sample size and the excessive time consumption limit their applications. To this end, this paper proposed a diagnosis prototype system for COVID-19 infection testing. The proposed deep learning model is trained and is tested on 2267 CT sequences from 1357 patients clinically confirmed with COVID-19 and 1235 CT sequences from non-infected people. The main highlights of the prototype system are: (1) no data augmentation is needed to accurately discriminate the COVID-19 from normal controls with the specificity of 0.92 and sensitivity of 0.93; (2) the raw DICOM image is not necessary in testing. Highly compressed image like Jpeg can be used to allow a quick diagnosis; and (3) it discriminates the virus infection within 6 seconds and thus allows an online test with light cost. We also applied our model on 48 asymptomatic patients diagnosed with COVID-19. We found that: (1) the positive rate of RT-PCR assay is 63.5% (687/1082). (2) 45.8% (22/48) of the RT-PCR assay is negative for asymptomatic patients, yet the accuracy of CT scans is 95.8%. The online detection system is available: http://212.64.70.65/covid .",2021,10.1007/s12539-020-00408-1,cross-sectional,diagnosis,CT,Lung
Classification of COVID-19 Chest CT Images Based on Ensemble Deep Learning,"Novel coronavirus pneumonia (NCP) has become a global pandemic disease, and computed tomography-based (CT) image analysis and recognition are one of the important tools for clinical diagnosis. In order to assist medical personnel to achieve an efficient and fast diagnosis of patients with new coronavirus pneumonia, this paper proposes an assisted diagnosis algorithm based on ensemble deep learning. The method combines the Stacked Generalization ensemble learning with the VGG16 deep learning to form a cascade classifier, and the information constituting the cascade classifier comes from multiple subsets of the training set, each of which is used to collect deviant information about the generalization behavior of the data set, such that this deviant information fills the cascade classifier. The algorithm was experimentally validated for classifying patients with novel coronavirus pneumonia, patients with common pneumonia (CP), and normal controls, and the algorithm achieved a prediction accuracy of 93.57%, sensitivity of 94.21%, specificity of 93.93%, precision of 89.40%, and F1-score of 91.74% for the three categories. The results show that the method proposed in this paper has good classification performance and can significantly improve the performance of deep neural networks for multicategory prediction tasks.",2021,10.1155/2021/5528441,cross-sectional,diagnosis,CT,Lung
Classification of COVID-19 chest X-Ray and CT images using a type of dynamic CNN modification method,"Understanding and classifying Chest X-Ray (CXR) and computerised tomography (CT) images are of great significance for COVID-19 diagnosis. The existing research on the classification for COVID-19 cases faces the challenges of data imbalance, insufficient generalisability, the lack of comparative study, etc. To address these problems, this paper proposes a type of modified MobileNet to classify COVID-19 CXR images and a modified ResNet architecture for CT image classification. In particular, a modification method of convolutional neural networks (CNN) is designed to solve the gradient vanishing problem and improve the classification performance through dynamically combining features in different layers of a CNN. The modified MobileNet is applied to the classification of COVID-19, Tuberculosis, viral pneumonia (with the exception of COVID-19), bacterial pneumonia and normal controls using CXR images. Also, the proposed modified ResNet is used for the classification of COVID-19, non-COVID-19 infections and normal controls using CT images. The results show that the proposed methods achieve 99.6% test accuracy on the five-category CXR image dataset and 99.3% test accuracy on the CT image dataset. Six advanced CNN architectures and two specific COVID-19 detection models, i.e., COVID-Net and COVIDNet-CT are used in comparative studies. Two benchmark datasets and a CXR image dataset which combines eight different CXR image sources are employed to evaluate the performance of the above models. The results show that the proposed methods outperform the comparative models in classification accuracy, sensitivity, and precision, which demonstrate their potential in computer-aided diagnosis for healthcare applications.",2021,10.1016/j.compbiomed.2021.104425,cross-sectional,diagnosis,CT & X-ray,Lung
Classification of COVID-19 patients from chest CT images using multi-objective differential evolution-based convolutional neural networks,"Early classification of 2019 novel coronavirus disease (COVID-19) is essential for disease cure and control. Compared with reverse-transcription polymerase chain reaction (RT-PCR), chest computed tomography (CT) imaging may be a significantly more trustworthy, useful, and rapid technique to classify and evaluate COVID-19, specifically in the epidemic region. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of COVID-19 patients. However, the chest CT-based COVID-19 classification involves a radiology expert and considerable time, which is valuable when COVID-19 infection is growing at rapid rate. Therefore, an automated analysis of chest CT images is desirable to save the medical professionals' precious time. In this paper, a convolutional neural networks (CNN) is used to classify the COVID-19-infected patients as infected (+ve) or not (-ve). Additionally, the initial parameters of CNN are tuned using multi-objective differential evolution (MODE). Extensive experiments are performed by considering the proposed and the competitive machine learning techniques on the chest CT images. Extensive analysis shows that the proposed model can classify the chest CT images at a good accuracy rate.",2020,10.1007/s10096-020-03901-z,cross-sectional,diagnosis,CT,Lung
Classification of early stage non-small cell lung cancers on computed tomographic images into histological types using radiomic features: interobserver delineation variability analysis,"Radiomics, which involves the extraction of large numbers of quantitative features from medical images, has attracted attention in cancer research. In radiomics analysis, tumor segmentation is a crucial step. In this study, we evaluated the potential application of radiomics for predicting the histology of early stage non-small cell lung cancer (NSCLC) by analyzing interobserver variability in tumor delineation. Forty patient datasets were included in this study, 21 involving adenocarcinomas and 19 involving squamous cell carcinomas. All patients underwent stereotactic body radiotherapy treatment. In total, 476 features were extracted from each dataset, representing treatment planning, computed tomography images, and gross tumor volume (GTV). The definition of GTV can significantly affect the histology prediction. Therefore, in the present study, the effect of interobserver tumor delineation variability on radiomic features was evaluated by preparing 4 volumes of interest (VOIs) for each patient, as follows: the original GTV (which was delineated at treatment planning); two GTVs delineated retrospectively by radiation oncologists; and a semi-automatic GTV contoured by a medical physicist. Radiomic features extracted from each VOI were then analyzed using a naïve Bayesian model. Area-under-the-curve (AUC) analysis showed that interobserver variability in delineation is a significant factor in radiomics performance. Nevertheless, with 8 selected features, AUC values averaged over the VOIs were high (0.725 ± 0.070). The present study indicated that radiomics has potential for predicting early stage NSCLC histology despite variability in delineation. The high prediction accuracy implies that noninvasive histology evaluation by radiomics is a promising clinical application.",2018,10.1007/s12194-017-0433-2,cross-sectional,diagnosis,CT,Lung
Classification of lung adenocarcinoma transcriptome subtypes from pathological images using deep convolutional networks,"PURPOSE: Convolutional neural networks have become rapidly popular for image recognition and image analysis because of its powerful potential. In this paper, we developed a method for classifying subtypes of lung adenocarcinoma from pathological images using neural network whose that can evaluate phenotypic features from wider area to consider cellular distributions. METHODS: In order to recognize the types of tumors, we need not only to detail features of cells, but also to incorporate statistical distribution of the different types of cells. Variants of autoencoders as building blocks of pre-trained convolutional layers of neural networks are implemented. A sparse deep autoencoder which minimizes local information entropy on the encoding layer is then proposed and applied to images of size [Formula: see text]. We applied this model for feature extraction from pathological images of lung adenocarcinoma, which is comprised of three transcriptome subtypes previously defined by the Cancer Genome Atlas network. Since the tumor tissue is composed of heterogeneous cell populations, recognition of tumor transcriptome subtypes requires more information than local pattern of cells. The parameters extracted using this approach will then be used in multiple reduction stages to perform classification on larger images. RESULTS: We were able to demonstrate that these networks successfully recognize morphological features of lung adenocarcinoma. We also performed classification and reconstruction experiments to compare the outputs of the variants. The results showed that the larger input image that covers a certain area of the tissue is required to recognize transcriptome subtypes. The sparse autoencoder network with [Formula: see text] input provides a 98.9% classification accuracy. CONCLUSION: This study shows the potential of autoencoders as a feature extraction paradigm and paves the way for a whole slide image analysis tool to predict molecular subtypes of tumors from pathological features.",2018,10.1007/s11548-018-1835-2,,,,
Classification of lung cancer subtypes based on autofluorescence bronchoscopic pattern recognition: A preliminary study,"BACKGROUND AND OBJECTIVES: Lung cancer is the leading cause of cancer deaths worldwide. With current use of autofluorescent bronchoscopic imaging to detect early lung cancer and limitations of pathologic examinations, a computer-aided diagnosis (CAD) system based on autofluorescent bronchoscopy was proposed to distinguish different pathological cancer types to achieve objective and consistent diagnoses. METHODS: The collected database consisted of 12 adenocarcinomas and 11 squamous cell carcinomas. The corresponding autofluorescent bronchoscopic images were first transformed to a hue (H), saturation (S), and value (V) color space to obtain better interpretation of the color information. Color textural features were respectively extracted from the H, S, and V channels and combined in a logistic regression classifier to classify malignant types by machine learning. RESULTS: After feature selection, the proposed CAD system achieved an accuracy of 83% (19/23), a sensitivity of 73% (8/11), a specificity of 92% (11/12), a positive predictive value of 89% (8/9), a negative predictive value of 79% (11/14), and an area under the receiver operating characteristic curve of 0.81 for distinguishing lung cancer types. CONCLUSIONS: The proposed CAD system based on color textures of autofluorescent bronchoscopic images provides a diagnostic method of malignant types in clinical use.",2018,10.1016/j.cmpb.2018.05.016,,,,
Classification of lung nodules based on CT images using squeeze-and-excitation network and aggregated residual transformations,"Lung cancer is pointed as a leading cause of cancer death worldwide. Early lung nodule diagnosis has great significance for treating lung cancer and increasing patient survival. In this paper, we present a novel method to classify the malignant from benign lung nodules based on CT images using squeeze-and-excitation network and aggregated residual transformations (SE-ResNeXt). The state-of-the-art SE-ResNeXt module, which integrates the advantages of SENet for feature recalibration and ResNeXt for feature reuse, has great ability in boosting feature discriminability on imaging pattern recognition. The method is evaluated on the public available LUng Nodule Analysis 2016 (LUNA16) database with 1004 (450 malignant and 554 benign) nodules, achieving an area under the receiver operating characteristic curve (AUC) of 0. 9563 and accuracy of 91.67%. The promising results demonstrate that our method has strong robustness in the classification of nodules. The method has the potential to help radiologists better interpret diagnostic data and differentiate the benign from malignant lung nodules on CT images in clinical practice. To our best knowledge, the effectiveness of SE-ResNeXt on lung nodule classification has not been extensively explored.",2020,10.1007/s11547-019-01130-9,cross-sectional,diagnosis,CT,Lung
Classification of Lung Nodules Based on Deep Residual Networks and Migration Learning,"The classification process of lung nodule detection in a traditional computer-aided detection (CAD) system is complex, and the classification result is heavily dependent on the performance of each step in lung nodule detection, causing low classification accuracy and high false positive rate. In order to alleviate these issues, a lung nodule classification method based on a deep residual network is proposed. Abandoning traditional image processing methods and taking the 50-layer ResNet network structure as the initial model, the deep residual network is constructed by combining residual learning and migration learning. The proposed approach is verified by conducting experiments on the lung computed tomography (CT) images from the publicly available LIDC-IDRI database. An average accuracy of 98.23% and a false positive rate of 1.65% are obtained based on the ten-fold cross-validation method. Compared with the conventional support vector machine (SVM)-based CAD system, the accuracy of our method improved by 9.96% and the false positive rate decreased by 6.95%, while the accuracy improved by 1.75% and 2.42%, respectively, and the false positive rate decreased by 2.07% and 2.22%, respectively, in contrast to the VGG19 model and InceptionV3 convolutional neural networks. The experimental results demonstrate the effectiveness of our proposed method in lung nodule classification for CT images.",2020,10.1155/2020/8975078,cross-sectional,diagnosis,CT,Lung
Classification of lung nodules in CT scans using three-dimensional deep convolutional neural networks with a checkpoint ensemble method,"BACKGROUND: Accurately detecting and examining lung nodules early is key in diagnosing lung cancers and thus one of the best ways to prevent lung cancer deaths. Radiologists spend countless hours detecting small spherical-shaped nodules in computed tomography (CT) images. In addition, even after detecting nodule candidates, a considerable amount of effort and time is required for them to determine whether they are real nodules. The aim of this paper is to introduce a high performance nodule classification method that uses three dimensional deep convolutional neural networks (DCNNs) and an ensemble method to distinguish nodules between non-nodules. METHODS: In this paper, we use a three dimensional deep convolutional neural network (3D DCNN) with shortcut connections and a 3D DCNN with dense connections for lung nodule classification. The shortcut connections and dense connections successfully alleviate the gradient vanishing problem by allowing the gradient to pass quickly and directly. Connections help deep structured networks to obtain general as well as distinctive features of lung nodules. Moreover, we increased the dimension of DCNNs from two to three to capture 3D features. Compared with shallow 3D CNNs used in previous studies, deep 3D CNNs more effectively capture the features of spherical-shaped nodules. In addition, we use an alternative ensemble method called the checkpoint ensemble method to boost performance. RESULTS: The performance of our nodule classification method is compared with that of the state-of-the-art methods which were used in the LUng Nodule Analysis 2016 Challenge. Our method achieves higher competition performance metric (CPM) scores than the state-of-the-art methods using deep learning. In the experimental setup ESB-ALL, the 3D DCNN with shortcut connections and the 3D DCNN with dense connections using the checkpoint ensemble method achieved the highest CPM score of 0.910. CONCLUSION: The result demonstrates that our method of using a 3D DCNN with shortcut connections, a 3D DCNN with dense connections, and the checkpoint ensemble method is effective for capturing 3D features of nodules and distinguishing nodules between non-nodules.",2018,10.1186/s12880-018-0286-0,cross-sectional,diagnosis,CT,Lung
Classification of malignant and benign lung nodules using taxonomic diversity index and phylogenetic distance,"Lung cancer presents the highest cause of death among patients around the world, in addition of being one of the smallest survival rates after diagnosis. Therefore, this study proposes a methodology for diagnosis of lung nodules in benign and malignant tumors based on image processing and pattern recognition techniques. Mean phylogenetic distance (MPD) and taxonomic diversity index (Δ) were used as texture descriptors. Finally, the genetic algorithm in conjunction with the support vector machine were applied to select the best training model. The proposed methodology was tested on computed tomography (CT) images from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), with the best sensitivity of 93.42%, specificity of 91.21%, accuracy of 91.81%, and area under the ROC curve of 0.94. The results demonstrate the promising performance of texture extraction techniques using mean phylogenetic distance and taxonomic diversity index combined with phylogenetic trees. Graphical Abstract Stages of the proposed methodology.",2018,10.1007/s11517-018-1841-0,cross-sectional,diagnosis,CT,Lung
Classification of malignant lung cancer using deep learning,"In the automatic detection of suspicious shaded regions on CT images derived from the LIDC-IDRI dataset, the diagnostic system plays a significant role. This paper introduces an automatic recognition method for lung nodules of the regions of concern (ROI). The lung regions are segmented from DICOM image size 512 × 512 by adding a median filter, Gaussian filter, Gabor filter and watershed algorithm. AlexNet uses 227 × 227 × 3 with ""fc7"" (fully connected) layers and GoogLeNet uses 224 × 224 × 3 with ""pool5-drop 7 × 7 s1"" layers. Here, the authors explain what is better about AlexNet and GoogLeNet through its performance analysis, feature extraction, classification, sensitivity, specificity, detection and false alarm rate with time complexity. A multi-class SVM classifier with 100% precision and specificity provided the best performance in deep learning neural networks.",2021,10.1080/03091902.2020.1853837,cross-sectional,diagnosis,CT,Lung
Classification of pulmonary nodules by using hybrid features,"Early detection of pulmonary nodules is extremely important for the diagnosis and treatment of lung cancer. In this study, a new classification approach for pulmonary nodules from CT imagery is presented by using hybrid features. Four different methods are introduced for the proposed system. The overall detection performance is evaluated using various classifiers. The results are compared to similar techniques in the literature by using standard measures. The proposed approach with the hybrid features results in 90.7% classification accuracy (89.6% sensitivity and 87.5% specificity).",2013,10.1155/2013/148363,cross-sectional,diagnosis,CT,Lung
Classification of Severe and Critical Covid-19 Using Deep Learning and Radiomics,"OBJECTIVE: The coronavirus disease 2019 (COVID-19) is rapidly spreading inside China and internationally. We aimed to construct a model integrating information from radiomics and deep learning (DL) features to discriminate critical cases from severe cases of COVID-19 using computed tomography (CT) images. METHODS: We retrospectively enrolled 217 patients from three centers in China, including 82 patients with severe disease and 135 with critical disease. Patients were randomly divided into a training cohort (n = 174) and a test cohort (n = 43). We extracted 102 3-dimensional radiomic features from automatically segmented lung volume and selected the significant features. We also developed a 3-dimensional DL network based on center-cropped slices. Using multivariable logistic regression, we then created a merged model based on significant radiomic features and DL scores. We employed the area under the receiver operating characteristic curve (AUC) to evaluate the model's performance. We then conducted cross validation, stratified analysis, survival analysis, and decision curve analysis to evaluate the robustness of our method. RESULTS: The merged model can distinguish critical patients with AUCs of 0.909 (95% confidence interval [CI]: 0.859-0.952) and 0.861 (95% CI: 0.753-0.968) in the training and test cohorts, respectively. Stratified analysis indicated that our model was not affected by sex, age, or chronic disease. Moreover, the results of the merged model showed a strong correlation with patient outcomes. SIGNIFICANCE: A model combining radiomic and DL features of the lung could help distinguish critical cases from severe cases of COVID-19.",2020,10.1109/jbhi.2020.3036722,cross-sectional,diagnosis,CT,Lung
Classification of subtypes including LCNEC in lung cancer biopsy slides using convolutional neural network from scratch,"Identifying the lung carcinoma subtype in small biopsy specimens is an important part of determining a suitable treatment plan but is often challenging without the help of special and/or immunohistochemical stains. Pathology image analysis that tackles this issue would be helpful for diagnoses and subtyping of lung carcinoma. In this study, we developed AI models to classify multinomial patterns of lung carcinoma; ADC, LCNEC, SCC, SCLC, and non-neoplastic lung tissue based on convolutional neural networks (CNN or ConvNet). Four CNNs that were pre-trained using transfer learning and one CNN built from scratch were used to classify patch images from pathology whole-slide images (WSIs). We first evaluated the diagnostic performance of each model in the test sets. The Xception model and the CNN built from scratch both achieved the highest performance with a macro average AUC of 0.90. The CNN built from scratch model obtained a macro average AUC of 0.97 on the dataset of four classes excluding LCNEC, and 0.95 on the dataset of three subtypes of lung carcinomas; NSCLC, SCLC, and non-tumor, respectively. Of particular note is that the relatively simple CNN built from scratch may be an approach for pathological image analysis.",2022,10.1038/s41598-022-05709-7,,,,
Classification of the COVID-19 infected patients using DenseNet201 based deep transfer learning,"Deep learning models are widely used in the automatic analysis of radiological images. These techniques can train the weights of networks on large datasets as well as fine tuning the weights of pre-trained networks on small datasets. Due to the small COVID-19 dataset available, the pre-trained neural networks can be used for diagnosis of coronavirus. However, these techniques applied on chest CT image is very limited till now. Hence, the main aim of this paper to use the pre-trained deep learning architectures as an automated tool to detection and diagnosis of COVID-19 in chest CT. A DenseNet201 based deep transfer learning (DTL) is proposed to classify the patients as COVID infected or not i.e. COVID-19 (+) or COVID (-). The proposed model is utilized to extract features by using its own learned weights on the ImageNet dataset along with a convolutional neural structure. Extensive experiments are performed to evaluate the performance of the propose DTL model on COVID-19 chest CT scan images. Comparative analyses reveal that the proposed DTL based COVID-19 classification model outperforms the competitive approaches.Communicated by Ramaswamy H. Sarma.",2021,10.1080/07391102.2020.1788642,cross-sectional,diagnosis,CT,Lung
Classification of Volumetric Images Using Multi-Instance Learning and Extreme Value Theorem,"Volumetric imaging is an essential diagnostic tool for medical practitioners. The use of popular techniques such as convolutional neural networks (CNN) for analysis of volumetric images is constrained by the availability of detailed (with local annotations) training data and GPU memory. In this paper, the volumetric image classification problem is posed as a multi-instance classification problem and a novel method is proposed to adaptively select positive instances from positive bags during the training phase. This method uses the extreme value theory to model the feature distribution of the images without a pathology and use it to identify positive instances of an imaged pathology. The experimental results, on three separate image classification tasks (i.e. classify retinal OCT images according to the presence or absence of fluid build-ups, emphysema detection in pulmonary 3D-CT images and detection of cancerous regions in 2D histopathology images) show that the proposed method produces classifiers that have similar performance to fully supervised methods and achieves the state of the art performance in all examined test cases.",2020,10.1109/tmi.2019.2936244,cross-sectional,diagnosis,CT,Lung
Classifying brain metastases by their primary site of origin using a radiomics approach based on texture analysis: a feasibility study,"OBJECTIVE: To examine the capability of MRI texture analysis to differentiate the primary site of origin of brain metastases following a radiomics approach. METHODS: Sixty-seven untreated brain metastases (BM) were found in 3D T1-weighted MRI of 38 patients with cancer: 27 from lung cancer, 23 from melanoma and 17 from breast cancer. These lesions were segmented in 2D and 3D to compare the discriminative power of 2D and 3D texture features. The images were quantized using different number of gray-levels to test the influence of quantization. Forty-three rotation-invariant texture features were examined. Feature selection and random forest classification were implemented within a nested cross-validation structure. Classification was evaluated with the area under receiver operating characteristic curve (AUC) considering two strategies: multiclass and one-versus-one. RESULTS: In the multiclass approach, 3D texture features were more discriminative than 2D features. The best results were achieved for images quantized with 32 gray-levels (AUC = 0.873 ± 0.064) using the top four features provided by the feature selection method based on the p-value. In the one-versus-one approach, high accuracy was obtained when differentiating lung cancer BM from breast cancer BM (four features, AUC = 0.963 ± 0.054) and melanoma BM (eight features, AUC = 0.936 ± 0.070) using the optimal dataset (3D features, 32 gray-levels). Classification of breast cancer and melanoma BM was unsatisfactory (AUC = 0.607 ± 0.180). CONCLUSION: Volumetric MRI texture features can be useful to differentiate brain metastases from different primary cancers after quantizing the images with the proper number of gray-levels. KEY POINTS: • Texture analysis is a promising source of biomarkers for classifying brain neoplasms. • MRI texture features of brain metastases could help identifying the primary cancer. • Volumetric texture features are more discriminative than traditional 2D texture features.",2018,10.1007/s00330-018-5463-6,,,,
Classifying chest CT images as COVID-19 positive/negative using a convolutional neural network ensemble model and uniform experimental design method,"BACKGROUND: To classify chest computed tomography (CT) images as positive or negative for coronavirus disease 2019 (COVID-19) quickly and accurately, researchers attempted to develop effective models by using medical images. RESULTS: A convolutional neural network (CNN) ensemble model was developed for classifying chest CT images as positive or negative for COVID-19. To classify chest CT images acquired from COVID-19 patients, the proposed COVID19-CNN ensemble model combines the use of multiple trained CNN models with a majority voting strategy. The CNN models were trained to classify chest CT images by transfer learning from well-known pre-trained CNN models and by applying their algorithm hyperparameters as appropriate. The combination of algorithm hyperparameters for a pre-trained CNN model was determined by uniform experimental design. The chest CT images (405 from COVID-19 patients and 397 from healthy patients) used for training and performance testing of the COVID19-CNN ensemble model were obtained from an earlier study by Hu in 2020. Experiments showed that, the COVID19-CNN ensemble model achieved 96.7% accuracy in classifying CT images as COVID-19 positive or negative, which was superior to the accuracies obtained by the individual trained CNN models. Other performance measures (i.e., precision, recall, specificity, and F(1)-score) obtained bythe COVID19-CNN ensemble model were higher than those obtained by individual trained CNN models. CONCLUSIONS: The COVID19-CNN ensemble model had superior accuracy and excellent capability in classifying chest CT images as COVID-19 positive or negative.",2021,10.1186/s12859-021-04083-x,cross-sectional,diagnosis,CT,Lung
Clinical and Laboratory Approach to Diagnose COVID-19 Using Machine Learning,"Coronavirus 2 (SARS-CoV-2), often known by the name COVID-19, is a type of acute respiratory syndrome that has had a significant influence on both economy and health infrastructure worldwide. This novel virus is diagnosed utilising a conventional method known as the RT-PCR (Reverse Transcription Polymerase Chain Reaction) test. This approach, however, produces a lot of false-negative and erroneous outcomes. According to recent studies, COVID-19 can also be diagnosed using X-rays, CT scans, blood tests and cough sounds. In this article, we use blood tests and machine learning to predict the diagnosis of this deadly virus. We also present an extensive review of various existing machine-learning applications that diagnose COVID-19 from clinical and laboratory markers. Four different classifiers along with a technique called Synthetic Minority Oversampling Technique (SMOTE) were used for classification. Shapley Additive Explanations (SHAP) method was utilized to calculate the gravity of each feature and it was found that eosinophils, monocytes, leukocytes and platelets were the most critical blood parameters that distinguished COVID-19 infection for our dataset. These classifiers can be utilized in conjunction with RT-PCR tests to improve sensitivity and in emergency situations such as a pandemic outbreak that might happen due to new strains of the virus. The positive results indicate the prospective use of an automated framework that could help clinicians and medical personnel diagnose and screen patients.",2022,10.1007/s12539-021-00499-4,,,,
"Clinical and laboratory data, radiological structured report findings and quantitative evaluation of lung involvement on baseline chest CT in COVID-19 patients to predict prognosis","OBJECTIVE: To evaluate by means of regression models the relationships between baseline clinical and laboratory data and lung involvement on baseline chest CT and to quantify the thoracic disease using an artificial intelligence tool and a visual scoring system to predict prognosis in patients with COVID-19 pneumonia. MATERIALS AND METHODS: This study included 103 (41 women and 62 men; 68.8 years of mean age-range, 29-93 years) with suspicious COVID-19 viral infection evaluated by reverse transcription real-time fluorescence polymerase chain reaction (RT-PCR) test. All patients underwent CT examinations at the time of admission in addition to clinical and laboratory findings recording. All chest CT examinations were reviewed using a structured report. Moreover, using an artificial intelligence tool we performed an automatic segmentation on CT images based on Hounsfield unit to calculate residual healthy lung parenchyma, ground-glass opacities (GGO), consolidations and emphysema volumes for both right and left lungs. Two expert radiologists, in consensus, attributed at the CT pulmonary disease involvement a severity score using a scale of 5 levels; the score was attributed for GGO and consolidation for each lung, and then, an overall radiological severity visual score was obtained summing the single score. Univariate and multivariate regression analysis was performed. RESULTS: Symptoms and comorbidities did not show differences statistically significant in terms of patient outcome. Instead, SpO2 was significantly lower in patients hospitalized in critical conditions or died while age, HS CRP, leukocyte count, neutrophils, LDH, d-dimer, troponin, creatinine and azotemia, ALT, AST and bilirubin values were significantly higher. GGO and consolidations were the main CT patterns (a variable combination of GGO and consolidations was found in 87.8% of patients). CT COVID-19 disease was prevalently bilateral (77.6%) with peripheral distribution (74.5%) and multiple lobes localizations (52.0%). Consolidation, emphysema and residual healthy lung parenchyma volumes showed statistically significant differences in the three groups of patients based on outcome (patients discharged at home, patients hospitalized in stable conditions and patient hospitalized in critical conditions or died) while GGO volume did not affect the patient's outcome. Moreover, the overall radiological severity visual score (cutoff ≥ 8) was a predictor of patient outcome. The highest value of R-squared (R(2) = 0.93) was obtained by the model that combines clinical/laboratory findings at CT volumes. The highest accuracy was obtained by clinical/laboratory and CT findings model with a sensitivity, specificity and accuracy, respectively, of 88%, 78% and 81% to predict discharged/stable patients versus critical/died patients. CONCLUSION: In conclusion, both CT visual score and computerized software-based quantification of the consolidation, emphysema and residual healthy lung parenchyma on chest CT images were independent predictors of outcome in patients with COVID-19 pneumonia.",2021,10.1007/s11547-020-01293-w,cross-sectional,prognosis,CT,Lung
Clinical evaluation of a deep-learning-based computer-aided detection system for the detection of pulmonary nodules in a large teaching hospital,"AIM: To evaluate a deep-learning-based computer-aided detection (DL-CAD) software system for pulmonary nodule detection on computed tomography (CT) images and assess its added value in the clinical practice of a large teaching hospital. MATERIALS AND METHODS: A retrospective analysis was performed of 145 chest CT examinations by comparing the output of the DL-CAD software with a reference standard based on the consensus reading of three radiologists. For every nodule in each scan, the location, composition, and maximum diameter in the axial plane were recorded. The subgroup of chest CT examinations (n = 97) without any nodules was used to determine the negative predictive value at the given clinical sensitivity threshold setting. RESULTS: The radiologists found 91 nodules and the CAD system 130 nodules of which 80 were true positive. The measured sensitivity was 88% and the mean false-positive rate was 1.04 false positives/scan. The negative predictive value was 95%. For 23 nodules, there was a size discrepancy of which 19 (83%) were measured smaller by the radiologist. The agreement of nodule composition between the CAD results and the reference standard was 95%. CONCLUSIONS: The present study found a sensitivity of 88% and a false-positive rate of 1.04 false positives/scan, which match the vendor specification. Together with the measured negative predictive value of 95% the system performs very well; however, these rates are still not good enough to replace the radiologist, even for the specific task of nodule detection. Furthermore, a surprisingly high rate of overestimation of nodule size was observed, which can lead to too many follow-up examinations.",2021,10.1016/j.crad.2021.07.012,cross-sectional,diagnosis,CT,Lung
Clinical evaluation of atlas and deep learning based automatic contouring for lung cancer,"BACKGROUND AND PURPOSE: Contouring of organs at risk (OARs) is an important but time consuming part of radiotherapy treatment planning. The aim of this study was to investigate whether using institutional created software-generated contouring will save time if used as a starting point for manual OAR contouring for lung cancer patients. MATERIAL AND METHODS: Twenty CT scans of stage I-III NSCLC patients were used to compare user adjusted contours after an atlas-based and deep learning contour, against manual delineation. The lungs, esophagus, spinal cord, heart and mediastinum were contoured for this study. The time to perform the manual tasks was recorded. RESULTS: With a median time of 20 min for manual contouring, the total median time saved was 7.8 min when using atlas-based contouring and 10 min for deep learning contouring. Both atlas based and deep learning adjustment times were significantly lower than manual contouring time for all OARs except for the left lung and esophagus of the atlas based contouring. CONCLUSIONS: User adjustment of software generated contours is a viable strategy to reduce contouring time of OARs for lung radiotherapy while conforming to local clinical standards. In addition, deep learning contouring shows promising results compared to existing solutions.",2018,10.1016/j.radonc.2017.11.012,cross-sectional,treatment,CT,Lung
Clinical Factors and Quantitative CT Parameters Associated With ICU Admission in Patients of COVID-19 Pneumonia: A Multicenter Study,"The clinical spectrum of COVID-19 pneumonia is varied. Thus, it is important to identify risk factors at an early stage for predicting deterioration that require transferring the patients to ICU. A retrospective multicenter study was conducted on COVID-19 patients admitted to designated hospitals in China from Jan 17, 2020, to Feb 17, 2020. Clinical presentation, laboratory data, and quantitative CT parameters were also collected. The result showed that increasing risks of ICU admission were associated with age > 60 years (odds ratio [OR], 12.72; 95% confidence interval [CI], 2.42-24.61; P = 0.032), coexisting conditions (OR, 5.55; 95% CI, 1.59-19.38; P = 0.007) and CT derived total opacity percentage (TOP) (OR, 8.0; 95% CI, 1.45-39.29; P = 0.016). In conclusion, older age, coexisting conditions, larger TOP at the time of hospital admission are associated with ICU admission in patients with COVID-19 pneumonia. Early monitoring the progression of the disease and implementing appropriate therapies are warranted.",2021,10.3389/fpubh.2021.648360,cross-sectional,prognosis,CT,Lung
Clinical impact of a deep learning system for automated detection of missed pulmonary nodules on routine body computed tomography including the chest region,"OBJECTIVES: To evaluate the clinical impact of a deep learning system (DLS) for automated detection of pulmonary nodules on computed tomography (CT) images as a second reader. METHODS: This single-centre retrospective study screened 21,150 consecutive body CT studies from September 2018 to February 2019. Pulmonary nodules detected by the DLS on axial CT images but not mentioned in initial radiology reports were flagged. Flagged images were scored by four board-certificated radiologists each with at least 5 years of experience. Nodules with scores of 2 (understandable miss) or 3 (should not be missed) were then categorised as unlikely to be clinically significant (2a or 3a) or likely to be clinically significant (2b or 3b) according to the 2017 Fleischner guidelines for pulmonary nodules. The miss rate was defined as the total number of studies receiving scores of 2 or 3 divided by total screened studies. RESULTS: Among 172 nodules flagged by the DLS, 60 (35%) missed nodules were confirmed by the radiologists. The nodules were further categorised as 2a, 2b, 3a, and 3b in 24, 14, 10, and 12 studies, respectively, with an overall positive predictive value of 35%. Missed pulmonary nodules were identified in 0.3% of all CT images, and one-third of these lesions were considered clinically significant. CONCLUSIONS: Use of DLS-assisted automated detection as a second reader can identify missed pulmonary nodules, some of which may be clinically significant. CLINICAL RELEVANCE/APPLICATION: Use of DLS to help radiologists detect pulmonary lesions may improve patient care. KEY POINTS: • DLS-assisted automated detection as a second reader is feasible in a large consecutive cohort. • Performance of combined radiologists and DLS was better than DLS or radiologists alone. • Pulmonary nodules were missed more frequently in abdomino-pelvis CT than the thoracic CT.",2022,10.1007/s00330-021-08412-9,cross-sectional,diagnosis,CT,Lung
Clinical impact of variability on CT radiomics and suggestions for suitable feature selection: a focus on lung cancer,"BACKGROUND: Radiomics suffers from feature reproducibility. We studied the variability of radiomics features and the relationship of radiomics features with tumor size and shape to determine guidelines for optimal radiomics study. METHODS: We dealt with 260 lung nodules (180 for training, 80 for testing) limited to 2 cm or less. We quantified how voxel geometry (isotropic/anisotropic) and the number of histogram bins, factors commonly adjusted in multi-center studies, affect reproducibility. First, features showing high reproducibility between the original and isotropic transformed voxel settings were identified. Second, features showing high reproducibility in various binning settings were identified. Two hundred fifty-two features were computed and features with high intra-correlation coefficient were selected. Features that explained nodule status (benign/malignant) were retained using the least absolute shrinkage selector operator. Common features among different settings were identified, and the final features showing high reproducibility correlated with nodule status were identified. The identified features were used for the random forest classifier to validate the effectiveness of the features. The properties of the uncalculated feature were inspected to suggest a tentative guideline for radiomics studies. RESULTS: Nine features showing high reproducibility for both the original and isotropic voxel settings were selected and used to classify nodule status (AUC 0.659-0.697). Five features showing high reproducibility among different binning settings were selected and used in classification (AUC 0.729-0.748). Some texture features are likely to be successfully computed if a nodule was larger than 1000 mm(3). CONCLUSIONS: Features showing high reproducibility among different settings correlated with nodule status were identified.",2019,10.1186/s40644-019-0239-z,cross-sectional,informatics,CT,Lung
Clinical suitability of deep learning based synthetic CTs for adaptive proton therapy of lung cancer,"PURPOSE: Adaptive proton therapy (APT) of lung cancer patients requires frequent volumetric imaging of diagnostic quality. Cone-beam CT (CBCT) can provide these daily images, but x-ray scattering limits CBCT-image quality and hampers dose calculation accuracy. The purpose of this study was to generate CBCT-based synthetic CTs using a deep convolutional neural network (DCNN) and investigate image quality and clinical suitability for proton dose calculations in lung cancer patients. METHODS: A dataset of 33 thoracic cancer patients, containing CBCTs, same-day repeat CTs (rCT), planning-CTs (pCTs), and clinical proton treatment plans, was used to train and evaluate a DCNN with and without a pCT-based correction method. Mean absolute error (MAE), mean error (ME), peak signal-to-noise ratio, and structural similarity were used to quantify image quality. The evaluation of clinical suitability was based on recalculation of clinical proton treatment plans. Gamma pass ratios, mean dose to target volumes and organs at risk, and normal tissue complication probabilities (NTCP) were calculated. Furthermore, proton radiography simulations were performed to assess the HU-accuracy of sCTs in terms of range errors. RESULTS: On average, sCTs without correction resulted in a MAE of 34 ± 6 HU and ME of 4 ± 8 HU. The correction reduced the MAE to 31 ± 4HU (ME to 2 ± 4HU). Average 3%/3 mm gamma pass ratios increased from 93.7% to 96.8%, when the correction was applied. The patient specific correction reduced mean proton range errors from 1.5 to 1.1 mm. Relative mean target dose differences between sCTs and rCT were below ± 0.5% for all patients and both synthetic CTs (with/without correction). NTCP values showed high agreement between sCTs and rCT (<2%). CONCLUSION: CBCT-based sCTs can enable accurate proton dose calculations for APT of lung cancer patients. The patient specific correction method increased the image quality and dosimetric accuracy but had only a limited influence on clinically relevant parameters.",2021,10.1002/mp.15333,cross-sectional,combined,CBCT,Lung
Cloud-Based Lung Tumor Detection and Stage Classification Using Deep Learning Techniques,"Artificial intelligence (AI), Internet of Things (IoT), and the cloud computing have recently become widely used in the healthcare sector, which aid in better decision-making for a radiologist. PET imaging or positron emission tomography is one of the most reliable approaches for a radiologist to diagnosing many cancers, including lung tumor. In this work, we proposed stage classification of lung tumor which is a more challenging task in computer-aided diagnosis. As a result, a modified computer-aided diagnosis is being considered as a way to reduce the heavy workloads and second opinion to radiologists. In this paper, we present a strategy for classifying and validating different stages of lung tumor progression, as well as a deep neural model and data collection using cloud system for categorizing phases of pulmonary illness. The proposed system presents a Cloud-based Lung Tumor Detector and Stage Classifier (Cloud-LTDSC) as a hybrid technique for PET/CT images. The proposed Cloud-LTDSC initially developed the active contour model as lung tumor segmentation, and multilayer convolutional neural network (M-CNN) for classifying different stages of lung cancer has been modelled and validated with standard benchmark images. The performance of the presented technique is evaluated using a benchmark image LIDC-IDRI dataset of 50 low doses and also utilized the lung CT DICOM images. Compared with existing techniques in the literature, our proposed method achieved good result for the performance metrics accuracy, recall, and precision evaluated. Under numerous aspects, our proposed approach produces superior outcomes on all of the applied dataset images. Furthermore, the experimental result achieves an average lung tumor stage classification accuracy of 97%-99.1% and an average of 98.6% which is significantly higher than the other existing techniques.",2022,10.1155/2022/4185835,cross-sectional,prognosis,PET/CT,Lung
CNN models discriminating between pulmonary micro-nodules and non-nodules from CT images,"BACKGROUND: Early and automatic detection of pulmonary nodules from CT lung screening is the prerequisite for precise management of lung cancer. However, a large number of false positives appear in order to increase the sensitivity, especially for detecting micro-nodules (diameter < 3 mm), which increases the radiologists' workload and causes unnecessary anxiety for the patients. To decrease the false positive rate, we propose to use CNN models to discriminate between pulmonary micro-nodules and non-nodules from CT image patches. METHODS: A total of 13,179 micro-nodules and 21,315 non-nodules marked by radiologists are extracted with three different patch sizes (16 × 16, 32 × 32 and 64 × 64) from LIDC/IDRI database and used in the experiments. Three CNN models with different depths (1, 2 or 4 convolutional layers) are designed; their performances are evaluated by the fivefold cross-validation in term of the accuracy, area under the curve (AUC), F-score and sensitivity. The network parameters are also optimized. RESULTS: It is found that the performance of the CNN models is greatly dependent on the patches size and the number of convolutional layers. The CNN model with two convolutional layers presented the best performance in case of 32 × 32 patches size, achieving an accuracy of 88.28%, an AUC of 0.87, a F-score of 83.45% and a sensitivity of 83.82%. CONCLUSIONS: The CNN models with appropriate depth and size of image patches can effectively discriminate between pulmonary micro-nodules and non-nodules, and reduce the false positives and help manage lung cancer precisely.",2018,10.1186/s12938-018-0529-x,cross-sectional,combined,CT,Lung
CNN-based lung CT registration with multiple anatomical constraints,"Deep-learning-based registration methods emerged as a fast alternative to conventional registration methods. However, these methods often still cannot achieve the same performance as conventional registration methods because they are either limited to small deformation or they fail to handle a superposition of large and small deformations without producing implausible deformation fields with foldings inside. In this paper, we identify important strategies of conventional registration methods for lung registration and successfully developed the deep-learning counterpart. We employ a Gaussian-pyramid-based multilevel framework that can solve the image registration optimization in a coarse-to-fine fashion. Furthermore, we prevent foldings of the deformation field and restrict the determinant of the Jacobian to physiologically meaningful values by combining a volume change penalty with a curvature regularizer in the loss function. Keypoint correspondences are integrated to focus on the alignment of smaller structures. We perform an extensive evaluation to assess the accuracy, the robustness, the plausibility of the estimated deformation fields, and the transferability of our registration approach. We show that it achieves state-of-the-art results on the COPDGene dataset compared to conventional registration method with much shorter execution time. In our experiments on the DIRLab exhale to inhale lung registration, we demonstrate substantial improvements (TRE below 1.2 mm) over other deep learning methods. Our algorithm is publicly available at https://grand-challenge.org/algorithms/deep-learning-based-ct-lung-registration/.",2021,10.1016/j.media.2021.102139,cross-sectional,informatics,CT,Lung
CO-IRv2: Optimized InceptionResNetV2 for COVID-19 detection from chest CT images,"This paper focuses on the application of deep learning (DL) in the diagnosis of coronavirus disease (COVID-19). The novelty of this work is in the introduction of optimized InceptionResNetV2 for COVID-19 (CO-IRv2) method. A part of the CO-IRv2 scheme is derived from the concepts of InceptionNet and ResNet with hyperparameter tuning, while the remaining part is a new architecture consisting of a global average pooling layer, batch normalization, dense layers, and dropout layers. The proposed CO-IRv2 is applied to a new dataset of 2481 computed tomography (CT) images formed by collecting two independent datasets. Data resizing and normalization are performed, and the evaluation is run up to 25 epochs. Various performance metrics, including precision, recall, accuracy, F1-score, area under the receiver operating characteristics (AUC) curve are used as performance metrics. The effectiveness of three optimizers known as Adam, Nadam and RMSProp are evaluated in classifying suspected COVID-19 patients and normal people. Results show that for CO-IRv2 and for CT images, the obtained accuracies of Adam, Nadam and RMSProp optimizers are 94.97%, 96.18% and 96.18%, respectively. Furthermore, it is shown here that for the case of CT images, CO-IRv2 with Nadam optimizer has better performance than existing DL algorithms in the diagnosis of COVID-19 patients. Finally, CO-IRv2 is applied to an X-ray dataset of 1662 images resulting in a classification accuracy of 99.40%.",2021,10.1371/journal.pone.0259179,cross-sectional,diagnosis,CT + xray,Lung
Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe): A new radiomics descriptor,"In this paper, we introduce a new radiomic descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe) for capturing subtle differences between benign and pathologic phenotypes which may be visually indistinguishable on routine anatomic imaging. CoLlAGe seeks to capture and exploit local anisotropic differences in voxel-level gradient orientations to distinguish similar appearing phenotypes. CoLlAGe involves assigning every image voxel an entropy value associated with the co-occurrence matrix of gradient orientations computed around every voxel. The hypothesis behind CoLlAGe is that benign and pathologic phenotypes even though they may appear similar on anatomic imaging, will differ in their local entropy patterns, in turn reflecting subtle local differences in tissue microarchitecture. We demonstrate CoLlAGe's utility in three clinically challenging classification problems: distinguishing (1) radiation necrosis, a benign yet confounding effect of radiation treatment, from recurrent tumors on T1-w MRI in 42 brain tumor patients, (2) different molecular sub-types of breast cancer on DCE-MRI in 65 studies and (3) non-small cell lung cancer (adenocarcinomas) from benign fungal infection (granulomas) on 120 non-contrast CT studies. For each of these classification problems, CoLlAGE in conjunction with a random forest classifier outperformed state of the art radiomic descriptors (Haralick, Gabor, Histogram of Gradient Orientations).",2016,10.1038/srep37241,cross-sectional,diagnosis,CT,Lung
Coarse-to-fine airway segmentation using multi information fusion network and CNN-based region growing,"BACKGROUND AND OBJECTIVES: Automatic airway segmentation from chest computed tomography (CT) scans plays an important role in pulmonary disease diagnosis and computer-assisted therapy. However, low contrast at peripheral branches and complex tree-like structures remain as two mainly challenges for airway segmentation. Recent research has illustrated that deep learning methods perform well in segmentation tasks. Motivated by these works, a coarse-to-fine segmentation framework is proposed to obtain a complete airway tree. METHODS: Our framework segments the overall airway and small branches via the multi-information fusion convolution neural network (Mif-CNN) and the CNN-based region growing, respectively. In Mif-CNN, atrous spatial pyramid pooling (ASPP) is integrated into a u-shaped network, and it can expend the receptive field and capture multi-scale information. Meanwhile, boundary and location information are incorporated into semantic information. These information are fused to help Mif-CNN utilize additional context knowledge and useful features. To improve the performance of the segmentation result, the CNN-based region growing method is designed to focus on obtaining small branches. A voxel classification network (VCN), which can entirely capture the rich information around each voxel, is applied to classify the voxels into airway and non-airway. In addition, a shape reconstruction method is used to refine the airway tree. RESULTS: We evaluate our method on a private dataset and a public dataset from EXACT09. Compared with the segmentation results from other methods, our method demonstrated promising accuracy in complete airway tree segmentation. In the private dataset, the Dice similarity coefficient (DSC), Intersection over Union (IoU), false positive rate (FPR), and sensitivity are 93.5%, 87.8%, 0.015%, and 90.8%, respectively. In the public dataset, the DSC, IoU, FPR, and sensitivity are 95.8%, 91.9%, 0.053% and 96.6%, respectively. CONCLUSION: The proposed Mif-CNN and CNN-based region growing method segment the airway tree accurately and efficiently in CT scans. Experimental results also demonstrate that the framework is ready for application in computer-aided diagnosis systems for lung disease and other related works.",2022,10.1016/j.cmpb.2021.106610,cross-sectional,informatics,CT,Lung
CoLe-CNN+: Context learning - Convolutional neural network for COVID-19-Ground-Glass-Opacities detection and segmentation,"BACKGROUND AND OBJECTIVE: The most common tool for population-wide COVID-19 identification is the Reverse Transcription-Polymerase Chain Reaction test that detects the presence of the virus in the throat (or sputum) in swab samples. This test has a sensitivity between 59% and 71%. However, this test does not provide precise information regarding the extension of the pulmonary infection. Moreover, it has been proven that through the reading of a computed tomography (CT) scan, a clinician can provide a more complete perspective of the severity of the disease. Therefore, we propose a comprehensive system for fully-automated COVID-19 detection and lesion segmentation from CT scans, powered by deep learning strategies to support decision-making process for the diagnosis of COVID-19. METHODS: In the workflow proposed, the input CT image initially goes through lung delineation, then COVID-19 detection and finally lesion segmentation. The chosen neural network has a U-shaped architecture using a newly introduced Multiple Convolutional Layers structure, that produces a lung segmentation mask within a novel pipeline for direct COVID-19 detection and segmentation. In addition, we propose a customized loss function that guarantees an optimal balance on average between sensitivity and precision. RESULTS: Lungs' segmentation results show a sensitivity near 99% and Dice-score of 97%. No false positives were observed in the detection network after 10 different runs with an average accuracy of 97.1%. The average accuracy for lesion segmentation was approximately 99%. Using UNet as a benchmark, we compared our results with several other techniques proposed in the literature, obtaining the largest improvement over the UNet outcomes. CONCLUSIONS: The method proposed in this paper outperformed the state-of-the-art methods for COVID-19 lesion segmentation from CT images, and improved by 38.2% the results for F1-score of UNet. The high accuracy observed in this work opens up a wide range of possible applications of our algorithm in other fields related to medical image segmentation.",2021,10.1016/j.compbiomed.2021.104689,cross-sectional,diagnosis,CT,Lung
Combination of computed tomography imaging-based radiomics and clinicopathological characteristics for predicting the clinical benefits of immune checkpoint inhibitors in lung cancer,"BACKGROUND: In this study, we tested whether a combination of radiomic features extracted from baseline pre-immunotherapy computed tomography (CT) images and clinicopathological characteristics could be used as novel noninvasive biomarkers for predicting the clinical benefits of non-small cell lung cancer (NSCLC) patients treated with immune checkpoint inhibitors (ICIs). METHODS: The data from 92 consecutive patients with lung cancer who had been treated with ICIs were retrospectively analyzed. In total, 88 radiomic features were selected from the pretreatment CT images for the construction of a random forest model. Radiomics model 1 was constructed based on the Rad-score. Using multivariate logistic regression analysis, the Rad-score and significant predictors were integrated into a single predictive model (radiomics nomogram model 1) to predict the durable clinical benefit (DCB) of ICIs. Radiomics model 2 was developed based on the same Rad-score as radiomics model 1.Using multivariate Cox proportional hazards regression analysis, the Rad-score, and independent risk factors, radiomics nomogram model 2 was constructed to predict the progression-free survival (PFS). RESULTS: The models successfully predicted the patients who would benefit from ICIs. For radiomics model 1, the area under the receiver operating characteristic curve values for the training and validation cohorts were 0.848 and 0.795, respectively, whereas for radiomics nomogram model 1, the values were 0.902 and 0.877, respectively. For the PFS prediction, the Harrell's concordance indexes for the training and validation cohorts were 0.717 and 0.760, respectively, using radiomics model 2, whereas they were 0.749 and 0.791, respectively, using radiomics nomogram model 2. CONCLUSIONS: CT-based radiomic features and clinicopathological factors can be used prior to the initiation of immunotherapy for identifying NSCLC patients who are the most likely to benefit from the therapy. This could guide the individualized treatment strategy for advanced NSCLC.",2021,10.1186/s12931-021-01780-2,cross-sectional,treatment,CT,Lung
Combination of computer-aided detection algorithms for automatic lung nodule identification,"PURPOSE: The aim of this work is to evaluate the potential of combining different computer-aided detection (CADe) methods to increase the actual support for radiologists of automated systems in the identification of pulmonary nodules in CT scans. METHODS: The outputs of three different CADe systems developed by researchers of the Italian MAGIC-5 collaboration were combined. The systems are: the CAMCADe (based on a Channeler-Ant-Model which segments vessel tree and nodule candidates and a neural classifier), the RGVPCADe (a Region-Growing- Volume-Plateau algorithm detects nodule candidates and a neural network reduces false positives); the VBNACADe (two dedicated procedures, based respectively on a 3D dot-enhancement algorithm and on intersections of pleura surface normals, identifies internal and juxtapleural nodules, and a Voxel-Based-Neural-Approach reduces false positives. A dedicated OsiriX plugin implemented with the Cocoa environments of MacOSX allows annotating nodules and visualizing singles and combined CADe findings. RESULTS: The combined CADe has been tested on thin slice (lower than 2 mm) CTs of the LIDC public research database and the results have been compared with those obtained by the single systems. The FROC (Free Receiver Operating Characteristic) curves show better results than the best of the single approaches. CONCLUSIONS: Has been demonstrated that the combination of different approaches offers better results than each single CADe system. A clinical validation of the combined CADe as second reader is being addressed by means of the dedicated OsiriX plugin.",2012,10.1007/s11548-011-0637-6,cross-sectional,diagnosis,CT,Lung
Combination of Deep Learning-Based Denoising and Iterative Reconstruction for Ultra-Low-Dose CT of the Chest: Image Quality and Lung-RADS Evaluation,"OBJECTIVE. The objective of our study was to assess the effect of the combination of deep learning-based denoising (DLD) and iterative reconstruction (IR) on image quality and Lung Imaging Reporting and Data System (Lung-RADS) evaluation on chest ultra-low-dose CT (ULDCT). MATERIALS AND METHODS. Forty-one patients with 252 nodules were evaluated retrospectively. All patients underwent ULDCT (mean ± SD, 0.19 ± 0.01 mSv) and standard-dose CT (SDCT) (6.46 ± 2.28 mSv). ULDCT images were reconstructed using hybrid iterative reconstruction (HIR) and model-based iterative reconstruction (MBIR), and they were postprocessed using DLD (i.e., HIR-DLD and MBIR-DLD). SDCT images were reconstructed using filtered back projection. Three independent radiologists subjectively evaluated HIR, HIR-DLD, MBIR, and MBIR-DLD images on a 5-point scale in terms of noise, streak artifact, nodule edge, clarity of small vessels, homogeneity of the normal lung parenchyma, and overall image quality. Two radiologists independently evaluated the nodules according to Lung-RADS using HIR, MBIR, HIR-DLD, and MBIR-DLD ULDCT images and SDCT images. The median scores for subjective analysis were analyzed using Wilcoxon signed rank test with Bonferroni correction. Intraobserver agreement for Lung-RADS category between ULDCT and SDCT was evaluated using the weighted kappa coefficient. RESULTS. In the subjective analysis, ULDCT with DLD showed significantly better scores than did ULDCT without DLD (p < 0.001), and MBIR-DLD showed the best scores among the ULDCT images (p < 0.001) for all items. In the Lung-RADS evaluation, HIR showed fair or moderate agreement (reader 1 and reader 2: κw = 0.46 and 0.32, respectively); MBIR, moderate or good agreement (κw = 0.68 and 0.57); HIR-DLD, moderate agreement (κw = 0.53 and 0.48); and MBIR-DLD, good agreement (κw = 0.70 and 0.72). CONCLUSION. DLD improved the image quality of both HIR and MBIR on ULDCT. MBIR-DLD was superior to HIR_DLD for image quality and for Lung-RADS evaluation.",2020,10.2214/ajr.19.22680,cross-sectional,combined,CT,Lung
Combination of radiological and gray level co-occurrence matrix textural features used to distinguish solitary pulmonary nodules by computed tomography,"The objective of this study was to investigate the method of the combination of radiological and textural features for the differentiation of malignant from benign solitary pulmonary nodules by computed tomography. Features including 13 gray level co-occurrence matrix textural features and 12 radiological features were extracted from 2,117 CT slices, which came from 202 (116 malignant and 86 benign) patients. Lasso-type regularization to a nonlinear regression model was applied to select predictive features and a BP artificial neural network was used to build the diagnostic model. Eight radiological and two textural features were obtained after the Lasso-type regularization procedure. Twelve radiological features alone could reach an area under the ROC curve (AUC) of 0.84 in differentiating between malignant and benign lesions. The 10 selected characters improved the AUC to 0.91. The evaluation results showed that the method of selecting radiological and textural features appears to yield more effective in the distinction of malignant from benign solitary pulmonary nodules by computed tomography.",2013,10.1007/s10278-012-9547-6,cross-sectional,diagnosis,CT,Lung
Combining chest X-rays and electronic health record (EHR) data using machine learning to diagnose acute respiratory failure,"OBJECTIVE: When patients develop acute respiratory failure (ARF), accurately identifying the underlying etiology is essential for determining the best treatment. However, differentiating between common medical diagnoses can be challenging in clinical practice. Machine learning models could improve medical diagnosis by aiding in the diagnostic evaluation of these patients. MATERIALS AND METHODS: Machine learning models were trained to predict the common causes of ARF (pneumonia, heart failure, and/or chronic obstructive pulmonary disease [COPD]). Models were trained using chest radiographs and clinical data from the electronic health record (EHR) and applied to an internal and external cohort. RESULTS: The internal cohort of 1618 patients included 508 (31%) with pneumonia, 363 (22%) with heart failure, and 137 (8%) with COPD based on physician chart review. A model combining chest radiographs and EHR data outperformed models based on each modality alone. Models had similar or better performance compared to a randomly selected physician reviewer. For pneumonia, the combined model area under the receiver operating characteristic curve (AUROC) was 0.79 (0.77-0.79), image model AUROC was 0.74 (0.72-0.75), and EHR model AUROC was 0.74 (0.70-0.76). For heart failure, combined: 0.83 (0.77-0.84), image: 0.80 (0.71-0.81), and EHR: 0.79 (0.75-0.82). For COPD, combined: AUROC = 0.88 (0.83-0.91), image: 0.83 (0.77-0.89), and EHR: 0.80 (0.76-0.84). In the external cohort, performance was consistent for heart failure and increased for COPD, but declined slightly for pneumonia. CONCLUSIONS: Machine learning models combining chest radiographs and EHR data can accurately differentiate between common causes of ARF. Further work is needed to determine how these models could act as a diagnostic aid to clinicians in clinical settings.",2022,10.1093/jamia/ocac030,cross-sectional,diagnosis,X-ray,Lung
Combining computed tomography and biologically effective dose in radiomics and deep learning improves prediction of tumor response to robotic lung stereotactic body radiation therapy,"PURPOSE: The aim of this study is to improve the performance of machine learning (ML) models in predicting response of non-small cell lung cancer (NSCLC) to stereotactic body radiation therapy (SBRT) by integrating image features from pre-treatment computed tomography (CT) with features from the biologically effective dose (BED) distribution. MATERIALS AND METHODS: Image features, consisting of crafted radiomic features or machine-learned features extracted using a convolutional neural network, were calculated from pre-treatment CT data and from dose distributions converted into BED for 80 NSCLC lesions over 76 patients treated with robotic guided SBRT. ML models using different combinations of features were trained to predict complete or partial response according to response criteria in solid tumors, including radiomics CT (Rad(CT) ), radiomics CT and BED (Rad(CT,BED) ), deep learning (DL) CT (DL(CT) ), and DL CT and BED (DL(CT,BED) ). Training of ML included feature selection by neighborhood component analysis followed by ensemble ML using robust boosting. A model was considered as acceptable when the sum of average sensitivity and specificity on test data in repeated cross validations was at least 1.5. RESULTS: Complete or partial response occurred in 58 out of 80 lesions. The best models to predict the tumor response were those using BED variables, achieving significantly better area under curve (AUC) and accuracy than those using only features from CT, including a Rad(CT,BED) model using three radiomic features from BED, which scored an accuracy of 0.799 (95% confidence intervals (0.75-0.85)) and AUC of 0.773 (0.688-0.846), and a DL(CT,BED) model also using three variables with an accuracy of 0.798 (0.649-0.829) and AUC of 0.812 (0.755-0.867). CONCLUSION: According to our results, the inclusion of BED features improves the response prediction of ML models for lung cancer patients undergoing SBRT, regardless of the use of radiomic or DL features.",2021,10.1002/mp.15178,cross-sectional,prognosis,CT,Lung
Combining deep learning and coherent anti-Stokes Raman scattering imaging for automated differential diagnosis of lung cancer,"Lung cancer is the most prevalent type of cancer and the leading cause of cancer-related deaths worldwide. Coherent anti-Stokes Raman scattering (CARS) is capable of providing cellular-level images and resolving pathologically related features on human lung tissues. However, conventional means of analyzing CARS images requires extensive image processing, feature engineering, and human intervention. This study demonstrates the feasibility of applying a deep learning algorithm to automatically differentiate normal and cancerous lung tissue images acquired by CARS. We leverage the features learned by pretrained deep neural networks and retrain the model using CARS images as the input. We achieve 89.2% accuracy in classifying normal, small-cell carcinoma, adenocarcinoma, and squamous cell carcinoma lung images. This computational method is a step toward on-the-spot diagnosis of lung cancer and can be further strengthened by the efforts aimed at miniaturizing the CARS technique for fiber-based microendoscopic imaging.",2017,10.1117/1.Jbo.22.10.106017,,,,
Combining Deep Learning and Knowledge-driven Reasoning for Chest X-Ray Findings Detection,"The application of deep learning algorithms in medical imaging analysis is a steadily growing research area. While deep learning methods are thriving in the medical domain, they seldom utilize the rich knowledge associated with connected radiology reports. The knowledge derived from these reports can be utilized to enhance the performance of deep learning models. In this work, we used a comprehensive chest X-ray findings vocabulary to automatically annotate an extensive collection of chest X-rays using associated radiology reports and a vocabulary-driven concept annotation algorithm. The annotated X-rays are used to train a deep neural network classifier for finding detection. Finally, we developed a knowledge-driven reasoning algorithm that leverages knowledge learned from X-ray reports to improve upon the deep learning module's performance on finding detection. Our results suggest that combining deep learning and knowledge from radiology reports in a hybrid framework can significantly enhance overall performance in the CXR finding detection.",2020,,cross-sectional,diagnosis,X-ray,Lung
Combining machine learning and texture analysis to differentiate mediastinal lymph nodes in lung cancer patients,"Evaluate whether texture analysis associated with machine learning approaches could differentiate between malignant and benign lymph nodes. A total 18 patients with lung cancer were selected, with 39 lymph nodes, being 15 malignant and 24 benign. Retrospective computed tomography scans were utilized both with and without contrast medium. The great differential of this work was the use of 15 textures from mediastinal lymph nodes, with five different physicians as operators. First and second order statistical textures such as gray level run length and co-occurrence matrix were extracted and applied to three different machine learning classifiers. The best machine learning classifier demonstrated a variability of less than 5% among operators. The support vector machine (SVM) classifier presented 95% of the area under the ROC curve (AUC) and 89% of sensitivity for sequences without contrast medium. SVM classifier presented 93% of AUC and 86% of sensitivity for sequences with contrast medium. Texture analysis and machine learning may be helpful in the differentiation between malign and benign lymph nodes. This study can aid the physician in diagnosis and staging of lymph nodes and potentially reduce the number of invasive analysis to histopathological confirmation.",2021,10.1007/s13246-021-00988-2,cross-sectional,diagnosis,CT,Lung
Combining MRF-based deformable registration and deep binary 3D-CNN descriptors for large lung motion estimation in COPD patients,"PURPOSE: Deep convolutional neural networks in their various forms are currently achieving or outperforming state-of-the-art results on several medical imaging tasks. We aim to make these developments available to the so far unsolved task of accurate correspondence finding-especially with regard to image registration. METHODS: We propose a two-step hybrid approach to make deep learned features accessible to a discrete optimization-based registration method. In a first step, in order to extract expressive binary local descriptors, we train a deep network architecture on a patch-based landmark retrieval problem as auxiliary task. As second step at runtime within a MRF-regularised dense displacement sampling, their binary nature enables highly efficient similarity computations, thus making them an ideal candidate to replace the so far used handcrafted local feature descriptors during the registration process. RESULTS: We evaluate our approach on finding correspondences between highly non-rigidly deformed lung CT scans from different breathing states. Although the CNN-based descriptors excell at an auxiliary learning task for finding keypoint correspondences, self-similarity-based descriptors yield more accurate registration results. However, a combination of both approaches turns out to generate the most robust features for registration. CONCLUSION: We present a three-dimensional framework for large lung motion estimation based on the combination of CNN-based and handcrafted descriptors efficiently employed in a discrete registration method. Achieving best results by combining learned and handcrafted features encourages further research in this direction.",2019,10.1007/s11548-018-1888-2,cross-sectional,informatics,CT,Lung
"Combining multi-scale feature fusion with multi-attribute grading, a CNN model for benign and malignant classification of pulmonary nodules","Lung cancer has the highest mortality rate of all cancers, and early detection can improve survival rates. In the recent years, low-dose CT has been widely used to detect lung cancer. However, the diagnosis is limited by the subjective experience of doctors. Therefore, the main purpose of this study is to use convolutional neural network to realize the benign and malignant classification of pulmonary nodules in CT images. We collected 1004 cases of pulmonary nodules from LIDC-IDRI dataset, among which 554 cases were benign and 450 cases were malignant. According to the doctors' annotates on the center coordinates of the nodules, two 3D CT image patches of pulmonary nodules with different scales were extracted. In this study, our work focuses on two aspects. Firstly, we constructed a multi-stream multi-task network (MSMT), which combined multi-scale feature with multi-attribute classification for the first time, and applied it to the classification of benign and malignant pulmonary nodules. Secondly, we proposed a new loss function to balance the relationship between different attributes. The final experimental results showed that our model was effective compared with the same type of study. The area under ROC curve, accuracy, sensitivity, and specificity were 0.979, 93.92%, 92.60%, and 96.25%, respectively.",2020,10.1007/s10278-020-00333-1,cross-sectional,diagnosis,CT,Lung
Commercial AI solutions in detecting COVID-19 pneumonia in chest CT: not yet ready for clinical implementation?,"OBJECTIVES: In response to the COVID-19 pandemic, many researchers have developed artificial intelligence (AI) tools to differentiate COVID-19 pneumonia from other conditions in chest CT. However, in many cases, performance has not been clinically validated. The aim of this study was to evaluate the performance of commercial AI solutions in differentiating COVID-19 pneumonia from other lung conditions. METHODS: Four commercial AI solutions were evaluated on a dual-center clinical dataset consisting of 500 CT studies; COVID-19 pneumonia was microbiologically proven in 50 of these. Sensitivity, specificity, positive and negative predictive values, and AUC were calculated. In a subgroup analysis, the performance of the AI solutions in differentiating COVID-19 pneumonia from other conditions was evaluated in CT studies with ground-glass opacities (GGOs). RESULTS: Sensitivity and specificity ranges were 62-96% and 31-80%, respectively. Negative and positive predictive values ranged between 82-99% and 19-25%, respectively. AUC was in the range 0.54-0.79. In CT studies with GGO, sensitivity remained unchanged. However, specificity was lower, and ranged between 15 and 53%. AUC for studies with GGO was in the range 0.54-0.69. CONCLUSIONS: This study highlights the variable specificity and low positive predictive value of AI solutions in diagnosing COVID-19 pneumonia in chest CT. However, one solution yielded acceptable values for sensitivity. Thus, with further improvement, commercial AI solutions currently under development have the potential to be integrated as alert tools in clinical routine workflow. Randomized trials are needed to assess the true benefits and also potential harms of the use of AI in image analysis. KEY POINTS: • Commercial AI solutions achieved a sensitivity and specificity ranging from 62 to 96% and from 31 to 80%, respectively, in identifying patients suspicious for COVID-19 in a clinical dataset. • Sensitivity remained within the same range, while specificity was even lower in subgroup analysis of CT studies with ground-glass opacities, and interrater agreement between the commercial AI solutions was minimal to nonexistent. • Thus, commercial AI solutions have the potential to be integrated as alert tools for the detection of patients with lung changes suspicious for COVID-19 pneumonia in a clinical routine workflow, if further improvement is made.",2022,10.1007/s00330-021-08409-4,cross-sectional,diagnosis,CT,Lung
Commissioning of a fluoroscopic-based real-time markerless tumor tracking system in a superconducting rotating gantry for carbon-ion pencil beam scanning treatment,"PURPOSE: To perform the final quality assurance of our fluoroscopic-based markerless tumor tracking for gated carbon-ion pencil beam scanning (C-PBS) radiotherapy using a rotating gantry system, we evaluated the geometrical accuracy and tumor tracking accuracy using a moving chest phantom with simulated respiration. METHODS: The positions of the dynamic flat panel detector (DFPD) and x-ray tube are subject to changes due to gantry sag. To compensate for this, we generated a geometrical calibration table (gantry flex map) in 15° gantry angle steps by the bundle adjustment method. We evaluated five metrics: (a) Geometrical calibration was evaluated by calculating chest phantom positional error using 2D/3D registration software for each 5° step of the gantry angle. (b) Moving phantom displacement accuracy was measured (±10 mm in 1-mm steps) with a laser sensor. (c) Tracking accuracy was evaluated with machine learning (ML) and multi-template matching (MTM) algorithms, which used fluoroscopic images and digitally reconstructed radiographic (DRR) images as training data. The chest phantom was continuously moved ±10 mm in a sinusoidal path with a moving cycle of 4 s and respiration was simulated with ±5 mm expansion/contraction with a cycle of 2 s. This was performed with the gantry angle set at 0°, 45°, 120°, and 240°. (d) Four types of interlock function were evaluated: tumor velocity, DFPD image brightness variation, tracking anomaly detection, and tracking positional inconsistency in between the two corresponding rays. (e) Gate on/off latency, gating control system latency, and beam irradiation latency were measured using a laser sensor and an oscilloscope. RESULTS: By applying the gantry flex map, phantom positional accuracy was improved from 1.03 mm/0.33° to <0.45 mm/0.27° for all gantry angles. The moving phantom displacement error was 0.1 mm. Due to long computation time, the tracking accuracy achieved with ML was <0.49 mm (=95% confidence interval [CI]) for imaging rates of 15 and 7.5 fps; those at 30 fps were decreased to 1.84 mm (95% CI: 1.79 mm-1.92 mm). The tracking positional accuracy with MTM was <0.52 mm (=95% CI) for all gantry angles and imaging frame rates. The tumor velocity interlock signal delay time was 44.7 ms (=1.3 frame). DFPD image brightness interlock latency was 34 ms (=1.0 frame). The tracking positional error was improved from 2.27 ± 2.67 mm to 0.25 ± 0.24 mm by the tracking anomaly detection interlock function. Tracking positional inconsistency interlock signal was output within 5.0 ms. The gate on/off latency was <82.7 ± 7.6 ms. The gating control system latency was <3.1 ± 1.0 ms. The beam irradiation latency was <8.7 ± 1.2 ms. CONCLUSIONS: Our markerless tracking system is now ready for clinical use. We hope to shorten the computation time needed by the ML algorithm at 30 fps in the future.",2019,10.1002/mp.13403,,informatics,CT,Lung
Comparative evaluation of autocontouring in clinical practice: A practical method using the Turing test,"PURPOSE: Automated techniques for estimating the contours of organs and structures in medical images have become more widespread and a variety of measures are available for assessing their quality. Quantitative measures of geometric agreement, for example, overlap with a gold-standard delineation, are popular but may not predict the level of clinical acceptance for the contouring method. Therefore, surrogate measures that relate more directly to the clinical judgment of contours, and to the way they are used in routine workflows, need to be developed. The purpose of this study is to propose a method (inspired by the Turing Test) for providing contour quality measures that directly draw upon practitioners' assessments of manual and automatic contours. This approach assumes that an inability to distinguish automatically produced contours from those of clinical experts would indicate that the contours are of sufficient quality for clinical use. In turn, it is anticipated that such contours would receive less manual editing prior to being accepted for clinical use. In this study, an initial assessment of this approach is performed with radiation oncologists and therapists. METHODS: Eight clinical observers were presented with thoracic organ-at-risk contours through a web interface and were asked to determine if they were automatically generated or manually delineated. The accuracy of the visual determination was assessed, and the proportion of contours for which the source was misclassified recorded. Contours of six different organs in a clinical workflow were for 20 patient cases. The time required to edit autocontours to a clinically acceptable standard was also measured, as a gold standard of clinical utility. Established quantitative measures of autocontouring performance, such as Dice similarity coefficient with respect to the original clinical contour and the misclassification rate accessed with the proposed framework, were evaluated as surrogates of the editing time measured. RESULTS: The misclassification rates for each organ were: esophagus 30.0%, heart 22.9%, left lung 51.2%, right lung 58.5%, mediastinum envelope 43.9%, and spinal cord 46.8%. The time savings resulting from editing the autocontours compared to the standard clinical workflow were 12%, 25%, 43%, 77%, 46%, and 50%, respectively, for these organs. The median Dice similarity coefficients between the clinical contours and the autocontours were 0.46, 0.90, 0.98, 0.98, 0.94, and 0.86, respectively, for these organs. CONCLUSIONS: A better correspondence with time saving was observed for the misclassification rate than the quantitative contour measures explored. From this, we conclude that the inability to accurately judge the source of a contour indicates a reduced need for editing and therefore a greater time saving overall. Hence, task-based assessments of contouring performance may be considered as an additional way of evaluating the clinical utility of autosegmentation methods.",2018,10.1002/mp.13200,cross-sectional,informatics,CT,Thorax
Comparative evaluation of support vector machines for computer aided diagnosis of lung cancer in CT based on a multi-dimensional data set,"Lung cancer is one of the most common forms of cancer resulting in over a million deaths per year worldwide. In this paper, the usage of support vector machine (SVM) classification for lung cancer is investigated, presenting a systematic quantitative evaluation against Boosting, Decision trees, k-nearest neighbor, LASSO regressions, neural networks and random forests. A large database of 5984 regions of interest (ROIs) and 488 input features (including textural features, patient characteristics, and morphological features) were used to train the classifiers and evaluate for their performance. The evaluation for classifiers' performance was based on a tenfold cross validation framework, receiver operating characteristic curve (ROC), and Matthews correlation coefficient. Area under curve (AUC) of SVM, Boosting, Decision trees, k-nearest neighbor, LASSO, neural networks, random forests were 0.94, 0.86, 0.73, 0.72, 0.91, 0.92, and 0.85, respectively. It was proved that SVM classification offered significantly increased classification performance compared to the reference methods. This scheme may be used as an auxiliary tool to differentiate between benign and malignant SPNs of CT images in future.",2013,10.1016/j.cmpb.2013.04.016,cross-sectional,diagnosis,CT,Lung
Comparing different deep learning architectures for classification of chest radiographs,"Chest radiographs are among the most frequently acquired images in radiology and are often the subject of computer vision research. However, most of the models used to classify chest radiographs are derived from openly available deep neural networks, trained on large image datasets. These datasets differ from chest radiographs in that they are mostly color images and have substantially more labels. Therefore, very deep convolutional neural networks (CNN) designed for ImageNet and often representing more complex relationships, might not be required for the comparably simpler task of classifying medical image data. Sixteen different architectures of CNN were compared regarding the classification performance on two openly available datasets, the CheXpert and COVID-19 Image Data Collection. Areas under the receiver operating characteristics curves (AUROC) between 0.83 and 0.89 could be achieved on the CheXpert dataset. On the COVID-19 Image Data Collection, all models showed an excellent ability to detect COVID-19 and non-COVID pneumonia with AUROC values between 0.983 and 0.998. It could be observed, that more shallow networks may achieve results comparable to their deeper and more complex counterparts with shorter training times, enabling classification performances on medical image data close to the state-of-the-art methods even when using limited hardware.",2020,10.1038/s41598-020-70479-z,cross-sectional,others,x-ray,Lung
Comparing different machine learning techniques for predicting COVID-19 severity,"BACKGROUND: Coronavirus disease 2019 (COVID-19) is still ongoing spreading globally, machine learning techniques were used in disease diagnosis and to predict treatment outcomes, which showed favorable performance. The present study aims to predict COVID-19 severity at admission by different machine learning techniques including random forest (RF), support vector machine (SVM), and logistic regression (LR). Feature importance to COVID-19 severity were further identified. METHODS: A retrospective design was adopted in the JinYinTan Hospital from January 26 to March 28, 2020, eighty-six demographic, clinical, and laboratory features were selected with LassoCV method, Spearman's rank correlation, experts' opinions, and literature evaluation. RF, SVM, and LR were performed to predict severe COVID-19, the performance of the models was compared by the area under curve (AUC). Additionally, feature importance to COVID-19 severity were analyzed by the best performance model. RESULTS: A total of 287 patients were enrolled with 36.6% severe cases and 63.4% non-severe cases. The median age was 60.0 years (interquartile range: 49.0-68.0 years). Three models were established using 23 features including 1 clinical, 1 chest computed tomography (CT) and 21 laboratory features. Among three models, RF yielded better overall performance with the highest AUC of 0.970 than SVM of 0.948 and LR of 0.928, RF also achieved a favorable sensitivity of 96.7%, specificity of 69.5%, and accuracy of 84.5%. SVM had sensitivity of 93.9%, specificity of 79.0%, and accuracy of 88.5%. LR also achieved a favorable sensitivity of 92.3%, specificity of 72.3%, and accuracy of 85.2%. Additionally, chest-CT had highest importance to illness severity, and the following features were neutrophil to lymphocyte ratio, lactate dehydrogenase, and D-dimer, respectively. CONCLUSIONS: Our results indicated that RF could be a useful predictive tool to identify patients with severe COVID-19, which may facilitate effective care and further optimize resources.",2022,10.1186/s40249-022-00946-4,cross-sectional,diagnosis,CT,Lung
Comparison of (18)F-FDG avidity at PET of benign and malignant pure ground-glass opacities: a paradox? Part II: artificial neural network integration of the PET/CT characteristics of ground-glass opacities to predict their likelihood of malignancy,"AIM: To assess the ability of artificial neural networks (ANNs) to predict the likelihood of malignancy of pure ground-glass opacities (GGOs), using observations from computed tomography (CT) and 2-[(18)F]-fluoro-2-deoxy-d-glucose (FDG) positron-emission tomography (PET) images and relevant clinical information. MATERIALS AND METHODS: One hundred and twenty-five cases of pure GGOs described in a previous article were used to train and evaluate the performance of an ANN to predict the likelihood of malignancy in each of the GGOs. Eighty-five cases selected randomly were used for training the network and the remaining 40 cases for testing. The ANN was constructed from the image data and basic clinical information. The predictions of the ANN were compared with blinded expert estimates of the likelihood of malignancy. RESULTS: The ANN showed excellent predictive value in estimating the likelihood of malignancy (AUC = 0.98±0.02). Employing the optimal cut-off point from the receiver operating characteristic (ROC) curve, the ANN correctly identified 11/11 malignant lesions (sensitivity 100%) and 27/29 benign lesions (specificity 93.1%). The expert readers found 23 lesions indeterminate and correctly identified 17 lesions as benign. CONCLUSION: ANNs have potential to improve diagnostic certainty in the classification of pure GGOs, based upon their CT appearance, intensity of FDG uptake, and relevant clinical information, and may therefore, be useful to help direct clinical and imaging follow-up.",2019,10.1016/j.crad.2019.04.024,cross-sectional,diagnosis,CT,Lung
Comparison of a radiomic biomarker with volumetric analysis for decoding tumour phenotypes of lung adenocarcinoma with different disease-specific survival,"OBJECTIVES: To compare a multi-feature-based radiomic biomarker with volumetric analysis in discriminating lung adenocarcinomas with different disease-specific survival on computed tomography (CT) scans. METHODS: This retrospective study obtained institutional review board approval and was Health Insurance Portability and Accountability Act (HIPAA) compliant. Pathologically confirmed lung adenocarcinoma (n = 431) manifested as subsolid nodules on CT were identified. Volume and percentage solid volume were measured by using a computer-assisted segmentation method. Radiomic features quantifying intensity, texture and wavelet were extracted from the segmented volume of interest (VOI). Twenty best features were chosen by using the Relief method and subsequently fed to a support vector machine (SVM) for discriminating adenocarcinoma in situ (AIS)/minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC). Performance of the radiomic signatures was compared with volumetric analysis via receiver-operating curve (ROC) analysis and logistic regression analysis. RESULTS: The accuracy of proposed radiomic signatures for predicting AIS/MIA from IAC achieved 80.5% with ROC analysis (Az value, 0.829; sensitivity, 72.1%; specificity, 80.9%), which showed significantly higher accuracy than volumetric analysis (69.5%, P = 0.049). Regression analysis showed that radiomic signatures had superior prognostic performance to volumetric analysis, with AIC values of 81.2% versus 70.8%, respectively. CONCLUSIONS: The radiomic tumour-phenotypes biomarker exhibited better diagnostic accuracy than traditional volumetric analysis in discriminating lung adenocarcinoma with different disease-specific survival. KEY POINTS: • Radiomic biomarker on CT was designed to identify phenotypes of lung adenocarcinoma • Built up radiomic signature for lung adenocarcinoma manifested as subsolid nodules • Retrospective study showed radiomic signature had greater diagnostic accuracy than volumetric analysis • Radiomics help to evaluate intratumour heterogeneity within lung adenocarcinoma • Medical decision can be given with more confidence.",2017,10.1007/s00330-017-4855-3,cross-sectional,combined,CT,Lung
Comparison of Artificial Intelligence-Based Fully Automatic Chest CT Emphysema Quantification to Pulmonary Function Testing,"OBJECTIVE. The purpose of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for fully automated quantification of emphysema on chest CT compared with pulmonary function testing (spirometry). MATERIALS AND METHODS. A total of 141 patients (72 women, mean age ± SD of 66.46 ± 9.7 years [range, 23-86 years]; 69 men, mean age of 66.72 ± 11.4 years [range, 27-91 years]) who underwent both chest CT acquisition and spirometry within 6 months were retrospectively included. The spirometry-based Tiffeneau index (TI; calculated as the ratio of forced expiratory volume in the first second to forced vital capacity) was used to measure emphysema severity; a value less than 0.7 was considered to indicate airway obstruction. Segmentation of the lung based on two different reconstruction methods was carried out by using a deep convolution image-to-image network. This multilayer convolutional neural network was combined with multilevel feature chaining and depth monitoring. To discriminate the output of the network from ground truth, an adversarial network was used during training. Emphysema was quantified using spatial filtering and attenuation-based thresholds. Emphysema quantification and TI were compared using the Spearman correlation coefficient. RESULTS. The mean TI for all patients was 0.57 ± 0.13. The mean percentages of emphysema using reconstruction methods 1 and 2 were 9.96% ± 11.87% and 8.04% ± 10.32%, respectively. AI-based emphysema quantification showed very strong correlation with TI (reconstruction method 1, ρ = -0.86; reconstruction method 2, ρ = -0.85; both p < 0.0001), indicating that AI-based emphysema quantification meaningfully reflects clinical pulmonary physiology. CONCLUSION. AI-based, fully automated emphysema quantification shows good correlation with TI, potentially contributing to an image-based diagnosis and quantification of emphysema severity.",2020,10.2214/ajr.19.21572,cross-sectional,diagnosis,CT,Lung
Comparison of Chest Radiograph Interpretations by Artificial Intelligence Algorithm vs Radiology Residents,"IMPORTANCE: Chest radiography is the most common diagnostic imaging examination performed in emergency departments (EDs). Augmenting clinicians with automated preliminary read assistants could help expedite their workflows, improve accuracy, and reduce the cost of care. OBJECTIVE: To assess the performance of artificial intelligence (AI) algorithms in realistic radiology workflows by performing an objective comparative evaluation of the preliminary reads of anteroposterior (AP) frontal chest radiographs performed by an AI algorithm and radiology residents. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included a set of 72 findings assembled by clinical experts to constitute a full-fledged preliminary read of AP frontal chest radiographs. A novel deep learning architecture was designed for an AI algorithm to estimate the findings per image. The AI algorithm was trained using a multihospital training data set of 342 126 frontal chest radiographs captured in ED and urgent care settings. The training data were labeled from their associated reports. Image-based F1 score was chosen to optimize the operating point on the receiver operating characteristics (ROC) curve so as to minimize the number of missed findings and overcalls per image read. The performance of the model was compared with that of 5 radiology residents recruited from multiple institutions in the US in an objective study in which a separate data set of 1998 AP frontal chest radiographs was drawn from a hospital source representative of realistic preliminary reads in inpatient and ED settings. A triple consensus with adjudication process was used to derive the ground truth labels for the study data set. The performance of AI algorithm and radiology residents was assessed by comparing their reads with ground truth findings. All studies were conducted through a web-based clinical study application system. The triple consensus data set was collected between February and October 2018. The comparison study was preformed between January and October 2019. Data were analyzed from October to February 2020. After the first round of reviews, further analysis of the data was performed from March to July 2020. MAIN OUTCOMES AND MEASURES: The learning performance of the AI algorithm was judged using the conventional ROC curve and the area under the curve (AUC) during training and field testing on the study data set. For the AI algorithm and radiology residents, the individual finding label performance was measured using the conventional measures of label-based sensitivity, specificity, and positive predictive value (PPV). In addition, the agreement with the ground truth on the assignment of findings to images was measured using the pooled κ statistic. The preliminary read performance was recorded for AI algorithm and radiology residents using new measures of mean image-based sensitivity, specificity, and PPV designed for recording the fraction of misses and overcalls on a per image basis. The 1-sided analysis of variance test was used to compare the means of each group (AI algorithm vs radiology residents) using the F distribution, and the null hypothesis was that the groups would have similar means. RESULTS: The trained AI algorithm achieved a mean AUC across labels of 0.807 (weighted mean AUC, 0.841) after training. On the study data set, which had a different prevalence distribution, the mean AUC achieved was 0.772 (weighted mean AUC, 0.865). The interrater agreement with ground truth finding labels for AI algorithm predictions had pooled κ value of 0.544, and the pooled κ for radiology residents was 0.585. For the preliminary read performance, the analysis of variance test was used to compare the distributions of AI algorithm and radiology residents' mean image-based sensitivity, PPV, and specificity. The mean image-based sensitivity for AI algorithm was 0.716 (95% CI, 0.704-0.729) and for radiology residents was 0.720 (95% CI, 0.709-0.732) (P = .66), while the PPV was 0.730 (95% CI, 0.718-0.742) for the AI algorithm and 0.682 (95% CI, 0.670-0.694) for the radiology residents (P < .001), and specificity was 0.980 (95% CI, 0.980-0.981) for the AI algorithm and 0.973 (95% CI, 0.971-0.974) for the radiology residents (P < .001). CONCLUSIONS AND RELEVANCE: These findings suggest that it is possible to build AI algorithms that reach and exceed the mean level of performance of third-year radiology residents for full-fledged preliminary read of AP frontal chest radiographs. This diagnostic study also found that while the more complex findings would still benefit from expert overreads, the performance of AI algorithms was associated with the amount of data available for training rather than the level of difficulty of interpretation of the finding. Integrating such AI systems in radiology workflows for preliminary interpretations has the potential to expedite existing radiology workflows and address resource scarcity while improving overall accuracy and reducing the cost of care.",2020,10.1001/jamanetworkopen.2020.22779,retrospective cohort,diagnosis,x-ray,chest
Comparison of CT and MRI images for the prediction of soft-tissue sarcoma grading and lung metastasis via a convolutional neural networks model,"AIM: To realise the automated prediction of soft-tissue sarcoma (STS) grading and lung metastasis based on computed tomography (CT), T1-weighted (T1W) magnetic resonance imaging (MRI), and fat-suppressed T2-weighted MRI (FST2W) via the convolutional neural networks (CNN) model. MATERIALS AND METHODS: MRI and CT images of 51 patients diagnosed with STS were analysed retrospectively. The patients could be divided into three groups based on disease grading: high-grade group (n=28), intermediate-grade group (n=15), low-grade group (n=8). Among these patients, 32 had lung metastasis, while the remaining 19 had no lung metastasis. The data were divided into the training, validation, and testing groups according to the ratio of 5:2:3. The receiver operating characteristic (ROC) curves and accuracy values were acquired using the testing dataset to evaluate the performance of the CNN model. RESULTS: For STS grading, the accuracy of the T1W, FST2W, CT, and the fusion of T1W and FST2W testing data were 0.86, 0.89, 0.86, and 0.85, respectively. In addition, Area Under Curve (AUC) were 0.96, 0.97, 0.97, and 0.94 respectively. For the prediction of lung metastasis, the accuracy of the T1W, FST2W, CT, and the fusion of T1W and FST2W test data were 0.92, 0.93, 0.88, and 0.91, respectively. The corresponding AUC values were 0.97, 0.96, 0.95, and 0.95, respectively. FST2W MRI performed best for predicting STS grading and lung metastasis. CONCLUSION: MRI and CT images combined with the CNN model can be useful for making predictions regarding STS grading and lung metastasis, thus providing help for patient diagnosis and treatment.",2020,10.1016/j.crad.2019.08.008,cross-sectional,diagnosis,CT + MRI,Lung
Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification,"The increased availability of labeled X-ray image archives (e.g. ChestX-ray14 dataset) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolution of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a concluding experiment, we also investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we compare the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. Overall, we observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-38, integrating non-image data yields the best overall results. Furthermore, class activation maps are used to understand the classification process, and a detailed analysis of the impact of non-image features is provided.",2019,10.1038/s41598-019-42294-8,cross-sectional,diagnosis,x-ray,Lung
"Comparison of deep learning, radiomics and subjective assessment of chest CT findings in SARS-CoV-2 pneumonia","PURPOSE: Comparison of deep learning algorithm, radiomics and subjective assessment of chest CT for predicting outcome (death or recovery) and intensive care unit (ICU) admission in patients with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. METHODS: The multicenter, ethical committee-approved, retrospective study included non-contrast-enhanced chest CT of 221 SARS-CoV-2 positive patients from Italy (n = 196 patients; mean age 64 ± 16 years) and Denmark (n = 25; mean age 69 ± 13 years). A thoracic radiologist graded presence, type and extent of pulmonary opacities and severity of motion artifacts in each lung lobe on all chest CTs. Thin-section CT images were processed with CT Pneumonia Analysis Prototype (Siemens Healthineers) which yielded segmentation masks from a deep learning (DL) algorithm to derive features of lung abnormalities such as opacity scores, mean HU, as well as volume and percentage of all-attenuation and high-attenuation (opacities >-200 HU) opacities. Separately, whole lung radiomics were obtained for all CT exams. Analysis of variance and multiple logistic regression were performed for data analysis. RESULTS: Moderate to severe respiratory motion artifacts affected nearly one-quarter of chest CTs in patients. Subjective severity assessment, DL-based features and radiomics predicted patient outcome (AUC 0.76 vs AUC 0.88 vs AUC 0.83) and need for ICU admission (AUC 0.77 vs AUC 0.0.80 vs 0.82). Excluding chest CT with motion artifacts, the performance of DL-based and radiomics features improve for predicting ICU admission. CONCLUSION: DL-based and radiomics features of pulmonary opacities from chest CT were superior to subjective assessment for differentiating patients with favorable and adverse outcomes.",2021,10.1016/j.clinimag.2021.06.036,cross-sectional,diagnosis,CT,Lung
"Comparison of detection of trauma-related injuries using combined ""all-in-one"" fused images and conventionally reconstructed images in acute trauma CT","OBJECTIVES: To compare the accuracy of lesion detection of trauma-related injuries using combined ""all-in-one"" fused (AIO) and conventionally reconstructed images (CR) in acute trauma CT. METHODS: In this retrospective study, trauma CT of 66 patients (median age 47 years, range 18-96 years; 20 female (30.3%)) were read using AIO and CR. Images were independently reviewed by 4 blinded radiologists (two residents and two consultants) for trauma-related injuries in 22 regions. Sub-analyses were performed to analyze the influence of experience (residents vs. consultants) and body region (chest, abdomen, skeletal structures) on lesion detection. Paired t-test was used to compare the accuracy of lesion detection. The effect size was calculated (Cohen's d). Linear mixed-effects model with patients as the fixed effect and random forest models were used to investigate the effect of experience, reconstruction/image processing, and body region on lesion detection. RESULTS: Reading time of residents was significantly faster using AIO (AIO: 266 ± 72 s, CR: 318 ± 113 s; p < 0.001; d = 0.46) while no significant difference was observed in the accuracy of lesion detection (AIO: 93.5 ± 6.0%, CR: 94.6 ± 6.0% p = 0.092; d = - 0.21). Reading time of consultants showed no significant difference (AIO: 283 ± 82 s, CR: 274 ± 95 s; p = 0.067; d = 0.16). Accuracy was significantly higher using CR; however, the difference and effect size were very small (AIO 95.1 ± 4.9%, CR: 97.3 ± 3.7%, p = 0.002; d = - 0.39). The linear mixed-effects model showed only minor effect of image processing/reconstruction for lesion detection. CONCLUSIONS: Residents at the emergency department might benefit from faster reading time without sacrificing lesion detection rate using AIO for trauma CT. KEY POINTS: • Image fusion techniques decrease the reading time of acute trauma CT without sacrificing diagnostic accuracy.",2022,10.1007/s00330-021-08473-w,,,,
Comparison of machine learning algorithms for the identification of acute exacerbations in chronic obstructive pulmonary disease,"OBJECTIVES: Identifying acute exacerbations in chronic obstructive pulmonary disease (AECOPDs) is of utmost importance for reducing the associated mortality and financial burden. In this research, the authors aimed to develop identification models for AECOPDs and to compare the relative performance of different modeling paradigms to find the best model for this task. METHODS: Data were extracted from electronic medical records (EMRs) of patients with chronic obstructive pulmonary disease who admitted to the China-Japan Friendship Hospital between February 2011 and March 2017. Five machine learning algorithms (random forest, support vector machine, logistic regression, K-nearest neighbor and naïve Bayes) were used to develop the AECOPDs identification models. Feature selection was performed to find an optimal feature subset. 10-folds cross-validation was used to find the best hyperparameters for each model. The following metrics: area under the receiver operating characteristic curve, sensitivity, specificity, positive predictive value, and negative predictive value were used to evaluate the performance of these models. RESULTS: A total of 303 EMRs (AECOPDs patients:135; None AECOPDs patients: 168) were included in the study. The SVM model obtained the best performance (sensitivity: 0.80, specificity: 0.83, positive predictive value:0.81, negative predictive value:0.85 and area under the receiver operating characteristic curve: 0.90) after performing feature selection. CONCLUSIONS: Our research confirms that the proposed model based on the support vector machine is a powerful tool to identify AECOPDs patients, and it is promising to provide decision support for clinicians when they are struggling to give a confirmed clinical diagnosis.",2020,10.1016/j.cmpb.2019.105267,,,,
Comparison of pirfenidone and corticosteroid treatments at the COVID-19 pneumonia with the guide of artificial intelligence supported thoracic computed tomography,"AIM: We aimed to investigate the effect of short-term pirfenidone treatment on prolonged COVID-19 pneumonia. METHOD: Hospital files of patients hospitalised with a diagnosis of critical COVID-19 pneumonia from November 2020 to March 2021 were retrospectively reviewed. Chest computed tomography images taken both before treatment and 2 months after treatment, demographic characteristics and laboratory parameters of patients receiving pirfenidone + methylprednisolone (n = 13) and only methylprednisolones (n = 9) were recorded. Pulmonary function tests were performed after the second month of the treatment. CT involvement rates were determined by machine learning. RESULTS: A total of 22 patients, 13 of whom (59.1%) were using methylprednisolone + pirfenidone and 9 of whom (40.9%) were using only methylprednisolone were included. When the blood gas parameters and pulmonary function tests of the patients were compared at the end of the second month, it was found that the FEV1, FEV1%, FVC and FVC% values were statistically significantly higher in the methylprednisolone + pirfenidone group compared with the methylprednisolone group (P = .025, P = .012, P = .026 and P = .017, respectively). When the rates of change in CT scans at diagnosis and second month of treatment were examined, it was found that the involvement rates in the methylprednisolone + pirfenidone group were statistically significantly decreased (P < .001). CONCLUSION: Antifibrotic agents can reduce fibrosis that may develop in the future. These can also help dose reduction and/or non-use strategy for methylprednisolone therapy, which has many side effects. Further large series and randomised controlled studies are needed on this subject.",2021,10.1111/ijcp.14961,retrospective cohort,treatment,CT,Lung
Comparison of the automatic segmentation of multiple organs at risk in CT images of lung cancer between deep convolutional neural network-based and atlas-based techniques,"BACKGROUND: In this study, a deep convolutional neural network (CNN)-based automatic segmentation technique was applied to multiple organs at risk (OARs) depicted in computed tomography (CT) images of lung cancer patients, and the results were compared with those generated through atlas-based automatic segmentation. MATERIALS AND METHODS: An encoder-decoder U-Net neural network was produced. The trained deep CNN performed the automatic segmentation of CT images for 36 cases of lung cancer. The Dice similarity coefficient (DSC), the mean surface distance (MSD) and the 95% Hausdorff distance (95% HD) were calculated, with manual segmentation results used as the standard, and were compared with the results obtained through atlas-based segmentation. RESULTS: For the heart, lungs and liver, both the deep CNN-based and atlas-based techniques performed satisfactorily (average values: 0.87 < DSC < 0.95, 1.8 mm < MSD < 3.8 mm, 7.9 mm < 95% HD <11 mm). For the spinal cord and the oesophagus, the two methods had statistically significant differences. For the atlas-based technique, the average values were 0.54 < DSC < 0.71, 2.6 mm < MSD < 3.1 mm and 9.4 mm < 95% HD <12 mm. For the deep CNN-based technique, the average values were 0.71 < DSC < 0.79, 1.2 mm < MSD <2.2 mm and 4.0 mm < 95% HD < 7.9 mm. CONCLUSION: Our results showed that automatic segmentation based on a deep convolutional neural network enabled us to complete automatic segmentation tasks rapidly. Deep convolutional neural networks can be satisfactorily adapted to segment OARs during radiation treatment planning for lung cancer patients.",2019,10.1080/0284186x.2018.1529421,,,,
Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome,"Pathology images capture tumor histomorphological details in high resolution. However, manual detection and characterization of tumor regions in pathology images is labor intensive and subjective. Using a deep convolutional neural network (CNN), we developed an automated tumor region recognition system for lung cancer pathology images. From the identified tumor regions, we extracted 22 well-defined shape and boundary features and found that 15 of them were significantly associated with patient survival outcome in lung adenocarcinoma patients from the National Lung Screening Trial. A tumor region shape-based prognostic model was developed and validated in an independent patient cohort (n = 389). The predicted high-risk group had significantly worse survival than the low-risk group (p value = 0.0029). Predicted risk group serves as an independent prognostic factor (high-risk vs. low-risk, hazard ratio = 2.25, 95% CI 1.34-3.77, p value = 0.0022) after adjusting for age, gender, smoking status, and stage. This study provides new insights into the relationship between tumor shape and patient prognosis.",2018,10.1038/s41598-018-27707-4,,,,
Computed Tomography Image Processing Analysis in COVID-19 Patient Follow-Up Assessment,"The rapid worldwide spread of the COVID-19 pandemic has infected patients around the world in a short space of time. Chest computed tomography (CT) images of patients who are infected with COVID-19 can offer early diagnosis and efficient forecast monitoring at a low cost. The diagnosis of COVID-19 on CT in an automated way can speed up many tasks and the application of medical treatments. This can help complement reverse transcription-polymerase chain reaction (RT-PCR) diagnosis. The aim of this work is to develop a system that automatically identifies ground-glass opacity (GGO) and pulmonary infiltrates (PIs) on CT images from patients with COVID-19. The purpose is to assess the disease progression during the patient's follow-up assessment and evaluation. We propose an efficient methodology that incorporates oversegmentation mean shift followed by superpixel-SLIC (simple linear iterative clustering) algorithm on CT images with COVID-19 for pulmonary parenchyma segmentation. To identify the pulmonary parenchyma, we described each superpixel cluster according to its position, grey intensity, second-order texture, and spatial-context-saliency features to classify by a tree random forest (TRF). Second, by applying the watershed segmentation to the mean-shift clusters, only pulmonary parenchyma segmentation-identified zones showed GGO and PI based on the description of each watershed cluster of its position, grey intensity, gradient entropy, second-order texture, Euclidean position to the border region of the PI zone, and global saliency features, after using TRF. Our classification results for pulmonary parenchyma identification on CT images with COVID-19 had a precision of over 92% and recall of over 92% on twofold cross validation. For GGO, the PI identification showed 96% precision and 96% recall on twofold cross validation.",2021,10.1155/2021/8869372,cross-sectional,combined,CT,Lung
Computed tomography semi-automated lung volume quantification in SARS-CoV-2-related pneumonia,"OBJECTIVES: To evaluate a semi-automated segmentation and ventilated lung quantification on chest computed tomography (CT) to assess lung involvement in patients affected by SARS-CoV-2. Results were compared with clinical and functional parameters and outcomes. METHODS: All images underwent quantitative analyses with a dedicated workstation using a semi-automatic lung segmentation software to compute ventilated lung volume (VLV), Ground-glass opacity (GGO) volume (GGO-V), and consolidation volume (CONS-V) as absolute volume and as a percentage of total lung volume (TLV). The ratio between CONS-V, GGO-V, and VLV (CONS-V/VLV and GGO-V/VLV, respectively), TLV (CONS-V/TLV, GGO-V/TLV, and GGO-V + CONS-V/TLV respectively), and the ratio between VLV and TLV (VLV/TLV) were calculated. RESULTS: A total of 108 patients were enrolled. GGO-V/TLV significantly correlated with WBC (r = 0.369), neutrophils (r = 0.446), platelets (r = 0.182), CRP (r = 0.190), PaCO(2) (r = 0.176), HCO(3)(-) (r = 0.284), and PaO2/FiO2 (P/F) values (r = - 0.344). CONS-V/TLV significantly correlated with WBC (r = 0.294), neutrophils (r = 0.300), lymphocytes (r = -0.225), CRP (r = 0.306), PaCO(2) (r = 0.227), pH (r = 0.162), HCO(3)(-) (r = 0.394), and P/F (r = - 0.419) values. Statistically significant differences between CONS-V, GGO-V, GGO-V/TLV, CONS-V/TLV, GGO-V/VLV, CONS-V/VLV, GGO-V + CONS-V/TLV, VLV/TLV, CT score, and invasive ventilation by ET were found (all p < 0.05). CONCLUSION: The use of quantitative semi-automated algorithm for lung CT elaboration effectively correlates the severity of SARS-CoV-2-related pneumonia with laboratory parameters and the need for invasive ventilation. KEY POINTS: • Pathological lung volumes, expressed both as GGO-V and as CONS-V, can be considered a useful tool in SARS-CoV-2-related pneumonia. • All lung volumes, expressed themselves and as ratio with TLV and VLV, correlate with laboratory data, in particular C-reactive protein and white blood cell count. • All lung volumes correlate with patient's outcome, in particular concerning invasive ventilation.",2021,10.1007/s00330-020-07271-0,cross-sectional,,CT,Lung
Computed Tomography-Based Radiomic Features for Diagnosis of Indeterminate Small Pulmonary Nodules,"OBJECTIVE: This study aimed to determine the potential of radiomic features extracted from preoperative computed tomography to discriminate malignant from benign indeterminate small (≤10 mm) pulmonary nodules. METHODS: A total of 197 patients with 210 nodules who underwent surgical resections between January 2011 and March 2017 were analyzed. Three hundred eighty-five radiomic features were extracted from the computed tomographic images. Feature selection and data dimension reduction were performed using the Kruskal-Wallis test, Spearman correlation analysis, and principal component analysis. The random forest was used for radiomic signature building. The receiver operating characteristic curve analysis was used to evaluate the model performance. RESULTS: Fifteen principal component features were selected for modeling. The area under the curve, sensitivity, specificity, and accuracy of the prediction model were 0.877 (95% confidence interval [CI], 0.795-0.959), 81.8% (95% CI, 72.0%-90.9%), 77.4% (95% CI, 63.9%-89.3%), and 80.0% (95% CI, 72.0%-86.7%) in the validation cohort, respectively. CONCLUSIONS: Computed tomography-based radiomic features showed good discriminative power for benign and malignant indeterminate small pulmonary nodules.",2020,10.1097/rct.0000000000000976,cross-sectional,diagnosis,CT,Lung
Computed Tomography-Based Radiomics Signature: A Potential Indicator of Epidermal Growth Factor Receptor Mutation in Pulmonary Adenocarcinoma Appearing as a Subsolid Nodule,"BACKGROUND: Lung adenocarcinoma (LADC) with epidermal growth factor receptor (EGFR) mutation is considered a subgroup of lung cancer sensitive to EGFR-targeted tyrosine kinase inhibitors. We aimed to develop and validate a computed tomography (CT)-based radiomics signature for prediction of EGFR mutation status in LADC appearing as a subsolid nodule. MATERIALS AND METHODS: A total of 467 eligible patients were divided into training and validation cohorts (n = 306 and 161, respectively). Radiomics features were extracted from unenhanced CT images by using Pyradiomics. A CT-based radiomics signature for distinguishing EGFR mutation status was constructed using the random forest (RF) method in the training cohort and then tested in the validation cohort. A combination of the radiomics signature with a clinical factors model was also constructed using the RF method. The performance of the model was evaluated using the area under the curve (AUC) of a receiver operating characteristic curve. RESULTS: In this study, 64.2% (300/467) of the patients showed EGFR mutations. L858R mutation of exon 21 was the most common mutation type (185/301). We identified a CT-based radiomics signature that successfully discriminated between EGFR positive and EGFR negative in the training cohort (AUC = 0.831) and the validation cohort (AUC = 0.789). The radiomics signature combined with the clinical factors model was not superior to the simple radiomics signature in the two cohorts (p > .05). CONCLUSION: As a noninvasive method, the CT-based radiomics signature can be used to predict the EGFR mutation status of LADC appearing as a subsolid nodule. IMPLICATIONS FOR PRACTICE: Lung adenocarcinoma (LADC) with epidermal growth factor receptor (EGFR) mutation is considered a subgroup of lung cancer that is sensitive to EGFR-targeted tyrosine kinase inhibitors. However, some patients with inoperable subsolid LADC are unable to undergo tissue sampling by biopsy for molecular analysis in clinical practice. A computed tomography-based radiomics signature may serve as a noninvasive biomarker to predict the EGFR mutation status of subsolid LADCs when mutational profiling is not available or possible.",2019,10.1634/theoncologist.2018-0706,cross-sectional,combined,CT,Lung
Computer aid screening of COVID-19 using X-ray and CT scan images: An inner comparison,"The objective of this study is to conduct a critical analysis to investigate and compare a group of computer aid screening methods of COVID-19 using chest X-ray images and computed tomography (CT) images. The computer aid screening method includes deep feature extraction, transfer learning, and machine learning image classification approach. The deep feature extraction and transfer learning method considered 13 pre-trained CNN models. The machine learning approach includes three sets of handcrafted features and three classifiers. The pre-trained CNN models include AlexNet, GoogleNet, VGG16, VGG19, Densenet201, Resnet18, Resnet50, Resnet101, Inceptionv3, Inceptionresnetv2, Xception, MobileNetv2 and ShuffleNet. The handcrafted features are GLCM, LBP & HOG, and machine learning based classifiers are KNN, SVM & Naive Bayes. In addition, the different paradigms of classifiers are also analyzed. Overall, the comparative analysis is carried out in 65 classification models, i.e., 13 in deep feature extraction, 13 in transfer learning, and 39 in the machine learning approaches. Finally, all classification models perform better when applying to the chest X-ray image set as comparing to the use of CT scan image set. Among 65 classification models, the VGG19 with SVM achieved the highest accuracy of 99.81%when applying to the chest X-ray images. In conclusion, the findings of this analysis study are beneficial for the researchers who are working towards designing computer aid tools for screening COVID-19 infection diseases.",2021,10.3233/xst-200784,cross-sectional,diagnosis,CT&x-ray,Lung
Computer Vision Tool and Technician as First Reader of Lung Cancer Screening CT Scans,"OBJECTIVES: To implement a cost-effective low-dose computed tomography (LDCT) lung cancer screening program at the population level, accurate and efficient interpretation of a large volume of LDCT scans is needed. The objective of this study was to evaluate a workflow strategy to identify abnormal LDCT scans in which a technician assisted by computer vision (CV) software acts as a first reader with the aim to improve speed, consistency, and quality of scan interpretation. METHODS: Without knowledge of the diagnosis, a technician reviewed 828 randomly batched scans (136 with lung cancers, 556 with benign nodules, and 136 without nodules) from the baseline Pan-Canadian Early Detection of Lung Cancer Study that had been annotated by the CV software CIRRUS Lung Screening (Diagnostic Image Analysis Group, Nijmegen, The Netherlands). The scans were classified as either normal (no nodules ≥1 mm or benign nodules) or abnormal (nodules or other abnormality). The results were compared with the diagnostic interpretation by Pan-Canadian Early Detection of Lung Cancer Study radiologists. RESULTS: The overall sensitivity and specificity of the technician in identifying an abnormal scan were 97.8% (95% confidence interval: 96.4-98.8) and 98.0% (95% confidence interval: 89.5-99.7), respectively. Of the 112 prevalent nodules that were found to be malignant in follow-up, 92.9% were correctly identified by the technician plus CV compared with 84.8% by the study radiologists. The average time taken by the technician to review a scan after CV processing was 208 ± 120 seconds. CONCLUSIONS: Prescreening CV software and a technician as first reader is a promising strategy for improving the consistency and quality of screening interpretation of LDCT scans.",2016,10.1016/j.jtho.2016.01.021,,diagnosis,CT,Lung
Computer-aided COVID-19 diagnosis and a comparison of deep learners using augmented CXRs,"BACKGROUND: Coronavirus Disease 2019 (COVID-19) is contagious, producing respiratory tract infection, caused by a newly discovered coronavirus. Its death toll is too high, and early diagnosis is the main problem nowadays. Infected people show a variety of symptoms such as fatigue, fever, tastelessness, dry cough, etc. Some other symptoms may also be manifested by radiographic visual identification. Therefore, Chest X-Rays (CXR) play a key role in the diagnosis of COVID-19. METHODS: In this study, we use Chest X-Rays images to develop a computer-aided diagnosis (CAD) of the disease. These images are used to train two deep networks, the Convolution Neural Network (CNN), and the Long Short-Term Memory Network (LSTM) which is an artificial Recurrent Neural Network (RNN). The proposed study involves three phases. First, the CNN model is trained on raw CXR images. Next, it is trained on pre-processed CXR images and finally enhanced CXR images are used for deep network CNN training. Geometric transformations, color transformations, image enhancement, and noise injection techniques are used for augmentation. From augmentation, we get 3,220 augmented CXRs as training datasets. In the final phase, CNN is used to extract the features of CXR imagery that are fed to the LSTM model. The performance of the four trained models is evaluated by the evaluation techniques of different models, including accuracy, specificity, sensitivity, false-positive rate, and receiver operating characteristic (ROC) curve. RESULTS: We compare our results with other benchmark CNN models. Our proposed CNN-LSTM model gives superior accuracy (99.02%) than the other state-of-the-art models. Our method to get improved input, helped the CNN model to produce a very high true positive rate (TPR 1) and no false-negative result whereas false negative was a major problem while using Raw CXR images. CONCLUSIONS: We conclude after performing different experiments that some image pre-processing and augmentation, remarkably improves the results of CNN-based models. It will help a better early detection of the disease that will eventually reduce the mortality rate of COVID.",2022,10.3233/xst-211047,cross-sectional,diagnosis,x-ray,Lung
Computer-aided detection (CADe) and diagnosis (CADx) system for lung cancer with likelihood of malignancy,"BACKGROUND: CADe and CADx systems for the detection and diagnosis of lung cancer have been important areas of research in recent decades. However, these areas are being worked on separately. CADe systems do not present the radiological characteristics of tumors, and CADx systems do not detect nodules and do not have good levels of automation. As a result, these systems are not yet widely used in clinical settings. METHODS: The purpose of this article is to develop a new system for detection and diagnosis of pulmonary nodules on CT images, grouping them into a single system for the identification and characterization of the nodules to improve the level of automation. The article also presents as contributions: the use of Watershed and Histogram of oriented Gradients (HOG) techniques for distinguishing the possible nodules from other structures and feature extraction for pulmonary nodules, respectively. For the diagnosis, it is based on the likelihood of malignancy allowing more aid in the decision making by the radiologists. A rule-based classifier and Support Vector Machine (SVM) have been used to eliminate false positives. RESULTS: The database used in this research consisted of 420 cases obtained randomly from LIDC-IDRI. The segmentation method achieved an accuracy of 97 % and the detection system showed a sensitivity of 94.4 % with 7.04 false positives per case. Different types of nodules (isolated, juxtapleural, juxtavascular and ground-glass) with diameters between 3 mm and 30 mm have been detected. For the diagnosis of malignancy our system presented ROC curves with areas of: 0.91 for nodules highly unlikely of being malignant, 0.80 for nodules moderately unlikely of being malignant, 0.72 for nodules with indeterminate malignancy, 0.67 for nodules moderately suspicious of being malignant and 0.83 for nodules highly suspicious of being malignant. CONCLUSIONS: From our preliminary results, we believe that our system is promising for clinical applications assisting radiologists in the detection and diagnosis of lung cancer.",2016,10.1186/s12938-015-0120-7,cross-sectional,diagnosis,CT,Lung
Computer-aided detection of lung nodules using outer surface features,"In this study, a computer-aided detection (CAD) system was developed for the detection of lung nodules in computed tomography images. The CAD system consists of four phases, including two-dimensional and three-dimensional preprocessing phases. In the feature extraction phase, four different groups of features are extracted from volume of interests: morphological features, statistical and histogram features, statistical and histogram features of outer surface, and texture features of outer surface. The support vector machine algorithm is optimized using particle swarm optimization for classification. The CAD system provides 97.37% sensitivity, 86.38% selectivity, 88.97% accuracy and 2.7 false positive per scan using three groups of classification features. After the inclusion of outer surface texture features, classification results of the CAD system reaches 98.03% sensitivity, 87.71% selectivity, 90.12% accuracy and 2.45 false positive per scan. Experimental results demonstrate that outer surface texture features of nodule candidates are useful to increase sensitivity and decrease the number of false positives in the detection of lung nodules in computed tomography images.",2015,10.3233/bme-151418,,,,
Computer-aided Detection of Subsolid Nodules at Chest CT: Improved Performance with Deep Learning-based CT Section Thickness Reduction,"Background Studies on the optimal CT section thickness for detecting subsolid nodules (SSNs) with computer-aided detection (CAD) are lacking. Purpose To assess the effect of CT section thickness on CAD performance in the detection of SSNs and to investigate whether deep learning-based super-resolution algorithms for reducing CT section thickness can improve performance. Materials and Methods CT images obtained with 1-, 3-, and 5-mm-thick sections were obtained in patients who underwent surgery between March 2018 and December 2018. Patients with resected synchronous SSNs and those without SSNs (negative controls) were retrospectively evaluated. The SSNs, which ranged from 6 to 30 mm, were labeled ground-truth lesions. A deep learning-based CAD system was applied to SSN detection on CT images of each section thickness and those converted from 3- and 5-mm section thickness into 1-mm section thickness by using the super-resolution algorithm. The CAD performance on each section thickness was evaluated and compared by using the jackknife alternative free response receiver operating characteristic figure of merit. Results A total of 308 patients (mean age ± standard deviation, 62 years ± 10; 183 women) with 424 SSNs (310 part-solid and 114 nonsolid nodules) and 182 patients without SSNs (mean age, 65 years ± 10; 97 men) were evaluated. The figures of merit differed across the three section thicknesses (0.92, 0.90, and 0.89 for 1, 3, and 5 mm, respectively; P = .04) and between 1- and 5-mm sections (P = .04). The figures of merit varied for nonsolid nodules (0.78, 0.72, and 0.66 for 1, 3, and 5 mm, respectively; P < .001) but not for part-solid nodules (range, 0.93-0.94; P = .76). The super-resolution algorithm improved CAD sensitivity on 3- and 5-mm-thick sections (P = .02 for 3 mm, P < .001 for 5 mm). Conclusion Computer-aided detection (CAD) of subsolid nodules performed better at 1-mm section thickness CT than at 3- and 5-mm section thickness CT, particularly with nonsolid nodules. Application of a super-resolution algorithm improved the sensitivity of CAD at 3- and 5-mm section thickness CT. © RSNA, 2021 Online supplemental material is available for this article. See also the editorial by Goo in this issue.",2021,10.1148/radiol.2021203387,cross-sectional,diagnosis,CT,Lung
Computer-Aided Detection System for the Classification of Non-Small Cell Lung Lesions using SVM,"INTRODUCTION: Lung carcinoma is the most commonly cancer causing deaths throughout the world that mainly occurs due to smoking. Small cell lung cancer and Non-small cell lung cancer (NSCLC) are the two different types of Lung cancer. For the detection and classification of lung cancer, there are different techniques in the literature. METHODS: This paper emphasis on the three class classification of the Adenocarcinomas, Squamous cell carcinomas, and large cell carcinomas of NSCLC. For precise and superior results, Computer Aided Detection (CADe) system has been designed so that the radiologist can diagnose carcinoma in the ultrasonic images conveniently. CADe analyses the quality of the images, selects the region of interest, preprocesses the data, extracts the features and classifies the cancer. RESULTS: After exhaustive literature survey, Laws' mask features and SVM classifier with Gaussian RBF kernels have been used in this paper. The experimentation was performed on 92 images using 50% - 50% training and testing criteria. CONCLUSION: Comparative study reveals that our system for separating three class lung cancer provides 95.65% average accuracy for Laws' mask 3 dimensions using the SVM classifier that is maximum among the existing methods reported in the literature using the same dataset.",2020,10.2174/1573409916666200102122021,cross-sectional,diagnosis,CT,Lung
Computer-Aided Diagnosis (CAD) of Pulmonary Nodule of Thoracic CT Image Using Transfer Learning,"Computer-aided diagnosis (CAD) has already been widely used in medical image processing. We recently make another trial to implement convolutional neural network (CNN) on the classification of pulmonary nodules of thoracic CT images. The biggest challenge in medical image classification with the help of CNN is the difficulty of acquiring enough samples, and overfitting is a common problem when there are not enough images for training. Transfer learning has been verified as reasonable in dealing with such problems with an acceptable loss value. We use the classic LeNet-5 model to classify pulmonary nodules of thoracic CT images, including benign and malignant pulmonary nodules, and different malignancies of the malignant nodules. The CT images are obtained from Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) where both pulmonary nodule scanning and nodule annotations are available. These images are labeled and stored in a medical images knowledge base (KB), which is designed and implemented in our previous work. We implement the 10-folder cross validation (CV) to testify the robustness of the classification model we trained. The result demonstrates that the transfer learning of the LeNet-5 is good for classifying pulmonary nodules of thoracic CT images, and the average values of Top-1 accuracy are 97.041% and 96.685% respectively. We believe that our work is beneficial and has potential for practical diagnosis of lung nodules.",2019,10.1007/s10278-019-00204-4,cross-sectional,diagnosis,CT,Lung
Computer-Aided Diagnosis of COVID-19 CT Scans Based on Spatiotemporal Information Fusion,"Coronavirus disease (COVID-19) is highly contagious and pathogenic. Currently, the diagnosis of COVID-19 is based on nucleic acid testing, but it has false negatives and hysteresis. The use of lung CT scans can help screen and effectively monitor diagnosed cases. The application of computer-aided diagnosis technology can reduce the burden on doctors, which is conducive to rapid and large-scale diagnostic screening. In this paper, we proposed an automatic detection method for COVID-19 based on spatiotemporal information fusion. Using the segmentation network in the deep learning method to segment the lung area and the lesion area, the spatiotemporal information features of multiple CT scans are extracted to perform auxiliary diagnosis analysis. The performance of this method was verified on the collected dataset. We achieved the classification of COVID-19 CT scans and non-COVID-19 CT scans and analyzed the development of the patients' condition through the CT scans. The average accuracy rate is 96.7%, sensitivity is 95.2%, and F1 score is 95.9%. Each scan takes about 30 seconds for detection.",2021,10.1155/2021/6649591,cross-sectional,diagnosis,CT,Lung
Computer-aided diagnosis of endobronchial ultrasound images using convolutional neural network,"BACKGROUND AND OBJECTIVE: In the United States, lung cancer is the leading cause of cancer death. The survival rate could increase by early detection. In recent years, the endobronchial ultrasonography (EBUS) images have been utilized to differentiate between benign and malignant lesions and guide transbronchial needle aspiration because it is real-time, radiation-free and has better performance. However, the diagnosis depends on the subjective judgment from doctors. In some previous studies, which using the grayscale image textures of the EBUS images to classify the lung lesions but it belonged to semi-automated system which still need the experts to select a part of the lesion first. Therefore, the main purpose of this study was to achieve full automation assistance by using convolution neural network. METHODS: First of all, the EBUS images resized to the input size of convolution neural network (CNN). And then, the training data were rotated and flipped. The parameters of the model trained with ImageNet previously were transferred to the CaffeNet used to classify the lung lesions. And then, the parameter of the CaffeNet was optimized by the EBUS training data. The features with 4096 dimension were extracted from the 7th fully connected layer and the support vector machine (SVM) was utilized to differentiate benign and malignant. This study was validated with 164 cases including 56 benign and 108 malignant. RESULTS: According to the experiment results, applying the classification by the features from the CNN with transfer learning had better performance than the conventional method with gray level co-occurrence matrix (GLCM) features. The accuracy, sensitivity, specificity, and the area under ROC achieved 85.4% (140/164), 87.0% (94/108), 82.1% (46/56), and 0.8705, respectively. CONCLUSIONS: From the experiment results, it has potential ability to diagnose EBUS images with CNN.",2019,10.1016/j.cmpb.2019.05.020,cross-sectional,diagnosis,EBUS,Lung
Computer-aided diagnosis of ground glass pulmonary nodule by fusing deep learning and radiomics features,"OBJECTIVES: This study aims to develop a computer-aided diagnosis (CADx) scheme to classify between benign and malignant ground glass nodules (GGNs), and fuse deep leaning and radiomics imaging features to improve the classification performance. METHODS: We first retrospectively collected 513 surgery histopathology confirmed GGNs from two centers. Among these GGNs, 100 were benign and 413 were malignant. All malignant tumors were stage I lung adenocarcinoma. To segment GGNs, we applied a deep convolutional neural network and residual architecture to train and build a 3D U-Net. Then, based on the pre-trained U-Net, we used a transfer learning approach to build a deep neural network (DNN) to classify between benign and malignant GGNs. With the GGN segmentation results generated by 3D U-Net, we also developed a CT radiomics model by adopting a series of image processing techniques, i.e. radiomics feature extraction, feature selection, synthetic minority over-sampling technique, and support vector machine classifier training/testing, etc. Finally, we applied an information fusion method to fuse the prediction scores generated by DNN based CADx model and CT-radiomics based model. To evaluate the proposed model performance, we conducted a comparison experiment by testing on an independent testing dataset. RESULTS: Comparing with DNN model and radiomics model, our fusion model yielded a significant higher area under a receiver operating characteristic curve (AUC) value of 0.73 ± 0.06 (P < 0.01). The fusion model generated an accuracy of 75.6%, F1 score of 84.6%, weighted average F1 score of 70.3%, and Matthews correlation coefficient of 43.6%, which were higher than the DNN model and radiomics model individually. CONCLUSIONS: Our experimental results demonstrated that (1) applying a CADx scheme was feasible to diagnosis of early-stage lung adenocarcinoma, (2) deep image features and radiomics features provided complementary information in classifying benign and malignant GGNs, and (3) it was an effective way to build DNN model with limited dataset by using transfer learning. Thus, to build a robust image analysis based CADx model, one can combine different types of image features to decode the imaging phenotypes of GGN.",2021,10.1088/1361-6560/abe735,cross-sectional,diagnosis,CT,Lung
Computer-aided diagnosis of ground-glass opacity pulmonary nodules using radiomic features analysis,"This study aims to develop a CT-based radiomic features analysis approach for diagnosis of ground-glass opacity (GGO) pulmonary nodules, and also assess whether computer-aided diagnosis (CADx) performance changes in classifying between benign and malignant nodules associated with histopathological subtypes namely, adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC), respectively. The study involves 182 histopathology-confirmed GGO nodules collected from two cancer centers. Among them, 59 are benign, 50 are AIS, 32 are MIA, and 41 are IAC nodules. Four training/testing data sets-(1) all nodules, (2) benign and AIS nodules, (3) benign and MIA nodules, (4) benign and IAC nodules-are assembled based on their histopathological subtypes. We first segment pulmonary nodules depicted in CT images by using a 3D region growing and geodesic active contour level set algorithm. Then, we computed and extracted 1117 quantitative imaging features based on the 3D segmented nodules. After conducting radiomic features normalization process, we apply a leave-one-out cross-validation (LOOCV) method to build models by embedding with a Relief feature selection, synthetic minority oversampling technique (SMOTE) and three machine-learning classifiers namely, support vector machine classifier, logistic regression classifier and Gaussian Naïve Bayes classifier. When separately using four data sets to train and test three classifiers, the average areas under receiver operating characteristic curves (AUC) are 0.75, 0.55, 0.77 and 0.93, respectively. When testing on an independent data set, our scheme yields higher accuracy than two radiologists (61.3% versus radiologist 1: 53.1% and radiologist 2: 56.3%). This study demonstrates that: (1) the feasibility of using CT-based radiomic features analysis approach to distinguish between benign and malignant GGO nodules, (2) higher performance of CADx scheme in diagnosing GGO nodules comparing with radiologist, and (3) a consistently positive trend between classification performance and invasive grade of GGO nodules. Thus, to improve the CADx performance in diagnosing of GGO nodules, one should assemble an optimal training data set dominated with more nodules associated with non-invasive lung adenocarcinoma (i.e. AIS and MIA).",2019,10.1088/1361-6560/ab2757,cross-sectional,diagnosis,CT,Lung
Computer-aided diagnosis of lung cancer: the effect of training data sets on classification accuracy of lung nodules,"This study aims to develop a computer-aided diagnosis (CADx) scheme for classification between malignant and benign lung nodules, and also assess whether CADx performance changes in detecting nodules associated with early and advanced stage lung cancer. The study involves 243 biopsy-confirmed pulmonary nodules. Among them, 76 are benign, 81 are stage I and 86 are stage III malignant nodules. The cases are separated into three data sets involving: (1) all nodules, (2) benign and stage I malignant nodules, and (3) benign and stage III malignant nodules. A CADx scheme is applied to segment lung nodules depicted on computed tomography images and we initially computed 66 3D image features. Then, three machine learning models namely, a support vector machine, naïve Bayes classifier and linear discriminant analysis, are separately trained and tested by using three data sets and a leave-one-case-out cross-validation method embedded with a Relief-F feature selection algorithm. When separately using three data sets to train and test three classifiers, the average areas under receiver operating characteristic curves (AUC) are 0.94, 0.90 and 0.99, respectively. When using the classifiers trained using data sets with all nodules, average AUC values are 0.88 and 0.99 for detecting early and advanced stage nodules, respectively. AUC values computed from three classifiers trained using the same data set are consistent without statistically significant difference (p > 0.05). This study demonstrates (1) the feasibility of applying a CADx scheme to accurately distinguish between benign and malignant lung nodules, and (2) a positive trend between CADx performance and cancer progression stage. Thus, in order to increase CADx performance in detecting subtle and early cancer, training data sets should include more diverse early stage cancer cases.",2018,10.1088/1361-6560/aaa610,cross-sectional,diagnosis,CT,Lung
"Computer-aided diagnosis of lung nodule classification between benign nodule, primary lung cancer, and metastatic lung cancer at different image size using deep convolutional neural network with transfer learning","We developed a computer-aided diagnosis (CADx) method for classification between benign nodule, primary lung cancer, and metastatic lung cancer and evaluated the following: (i) the usefulness of the deep convolutional neural network (DCNN) for CADx of the ternary classification, compared with a conventional method (hand-crafted imaging feature plus machine learning), (ii) the effectiveness of transfer learning, and (iii) the effect of image size as the DCNN input. Among 1240 patients of previously-built database, computed tomography images and clinical information of 1236 patients were included. For the conventional method, CADx was performed by using rotation-invariant uniform-pattern local binary pattern on three orthogonal planes with a support vector machine. For the DCNN method, CADx was evaluated using the VGG-16 convolutional neural network with and without transfer learning, and hyperparameter optimization of the DCNN method was performed by random search. The best averaged validation accuracies of CADx were 55.9%, 68.0%, and 62.4% for the conventional method, the DCNN method with transfer learning, and the DCNN method without transfer learning, respectively. For image size of 56, 112, and 224, the best averaged validation accuracy for the DCNN with transfer learning were 60.7%, 64.7%, and 68.0%, respectively. DCNN was better than the conventional method for CADx, and the accuracy of DCNN improved when using transfer learning. Also, we found that larger image sizes as inputs to DCNN improved the accuracy of lung nodule classification.",2018,10.1371/journal.pone.0200721,cross-sectional,diagnosis,CT,Lung
Computer-aided diagnosis of lung nodule using gradient tree boosting and Bayesian optimization,"We aimed to evaluate a computer-aided diagnosis (CADx) system for lung nodule classification focussing on (i) usefulness of the conventional CADx system (hand-crafted imaging feature + machine learning algorithm), (ii) comparison between support vector machine (SVM) and gradient tree boosting (XGBoost) as machine learning algorithms, and (iii) effectiveness of parameter optimization using Bayesian optimization and random search. Data on 99 lung nodules (62 lung cancers and 37 benign lung nodules) were included from public databases of CT images. A variant of the local binary pattern was used for calculating a feature vector. SVM or XGBoost was trained using the feature vector and its corresponding label. Tree Parzen Estimator (TPE) was used as Bayesian optimization for parameters of SVM and XGBoost. Random search was done for comparison with TPE. Leave-one-out cross-validation was used for optimizing and evaluating the performance of our CADx system. Performance was evaluated using area under the curve (AUC) of receiver operating characteristic analysis. AUC was calculated 10 times, and its average was obtained. The best averaged AUC of SVM and XGBoost was 0.850 and 0.896, respectively; both were obtained using TPE. XGBoost was generally superior to SVM. Optimal parameters for achieving high AUC were obtained with fewer numbers of trials when using TPE, compared with random search. Bayesian optimization of SVM and XGBoost parameters was more efficient than random search. Based on observer study, AUC values of two board-certified radiologists were 0.898 and 0.822. The results show that diagnostic accuracy of our CADx system was comparable to that of radiologists with respect to classifying lung nodules.",2018,10.1371/journal.pone.0195875,cross-sectional,diagnosis,CT,Lung
"Computer-Aided Diagnosis of Lung Nodules in Computed Tomography by Using Phylogenetic Diversity, Genetic Algorithm, and SVM","Lung cancer is pointed as the major cause of death among patients with cancer throughout the world. This work is intended to develop a methodology for diagnosis of lung nodules using images from the Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). The proposed methodology uses image processing and pattern recognition techniques. In order to differentiate between the patterns of malignant and benign nodules, we used phylogenetic diversity by means of particular indexes, that are: intensive quadratic entropy, extensive quadratic entropy, average taxonomic distinctness, total taxonomic distinctness, and pure diversity indexes. After that, we applied the genetic algorithm for selection of the best model. In the tests' stage, we applied the proposed methodology to 1405 (394 malignant and 1011 benign) nodules. The proposed work presents promising results at the classification into malignant and benign, achieving accuracy of 92.52%, sensitivity of 93.1% and specificity of 92.26%. The results demonstrated a good rate of correct detections using texture features. Since a precocious detection allows a faster therapeutic intervention, thus a more favorable prognostic to the patient, we propose herein a methodology that contributes to the area in this aspect.",2017,10.1007/s10278-017-9973-6,cross-sectional,diagnosis,CT,Lung
Computer-aided diagnosis of pectus excavatum using CT images and deep learning methods,"Pectus excavatum (PE) is one of the most common chest wall defects. Accurate assessment of PE deformities is critical for effective surgical intervention. Index-based evaluations have become the standard for objectively estimating PE, however, these indexes cannot represent the whole information of chest CT images and may associated with significant error due to the individual differences. To overcome these limitations, this paper developed a computer-aided diagnosis (CAD) system based on the convolutional neural network (CNN) to automatically learn discriminative features and classify PE images. We also adopted block-wise fine-tuning methods based on the transfer learning strategy to reduce the potential risk of overfitting caused by limited data and experimentally explored the best fine-tuning degree. Our method achieved a high level of classification accuracy with 94.76% for PE diagnosis. Furthermore, we proposed a majority rule-based voting method to provide a comprehensively diagnostic results for each patient, which integrated the classification results of the whole thorax. The promising results support the feasibility of our proposed CNN-based CAD system for automatic PE diagnosis, which paves a way for comprehensive assessments of PE in clinics.",2020,10.1038/s41598-020-77361-y,cross-sectional,diagnosis,CT,Chest wall
Computer-Aided Diagnosis Research of a Lung Tumor Based on a Deep Convolutional Neural Network and Global Features,"Based on the better generalization ability and the feature learning ability of the deep convolutional neural network, it is very significant to use the DCNN on the computer-aided diagnosis of a lung tumor. Firstly, a deep convolutional neural network was constructed according to the fuzzy characteristics and the complexity of lung CT images. Secondly, the relation between model parameters (iterations, different resolution) and recognition rate is discussed. Thirdly, the effects of different model structures for the identification of a lung tumor were analyzed by changing convolution kernel size, feature dimension, and depth of the network. Fourthly, the different optimization methods on how to influence the DCNN performance were discussed from three aspects containing pooling methods (maximum pooling and mean pooling), activation function (sigmoid and ReLU), and training algorithm (batch gradient descent and gradient descent with momentum). Finally, the experimental results verified the feasibility of DCNN used on computer-aided diagnosis of lung tumors, and it can achieve a good recognition rate when selecting the appropriate model parameters and model structure and using the method of gradient descent with momentum.",2021,10.1155/2021/5513746,cross-sectional,diagnosis,CT,Lung
Computer-Aided Diagnosis system for diagnosis of pulmonary emphysema using bio-inspired algorithms,"Pulmonary emphysema is a condition characterized by the destruction and permanent enlargement of the alveoli of the lungs. The destruction of gas-exchanging alveoli causes shortness of breath followed by a chronic cough and sputum production. A Computer-Aided Diagnosis (CAD) framework for diagnosing pulmonary emphysema from chest Computed Tomography (CT) slices has been designed and implemented in this study. The process of implementing the CAD framework includes segmenting the lung tissues and extracting the regions of interest (ROIs) using the Spatial Intuitionistic Fuzzy C-Means clustering algorithm. The ROIs that were considered in this work were emphysematous lesions - namely, centrilobular, paraseptal, and bullae that were labelled by an expert radiologist. The shape, texture, and run-length features were extracted from each ROI. A wrapper approach that employed four bio-inspired algorithms - namely, Moth-Flame Optimization (MFO), Firefly Optimization (FFO), Artificial Bee Colony Optimization, and Ant Colony Optimization - with the accuracy of the support vector machine classifier as the fitness function was used to select the optimal feature subset. The selected features of each bio-inspired algorithm were trained independently using the Extreme Learning Machine classifier based on the tenfold cross-validation technique. The framework was tested on real-time and public emphysema datasets to perform binary classification of lung CT slices of patients with and without the presence of emphysema. The framework that used MFO and FFO for feature selection produced superior results regarding accuracy, precision, recall, and specificity for the real-time dataset and the public dataset, respectively, when compared to the other bio-inspired algorithms.",2020,10.1016/j.compbiomed.2020.103940,cross-sectional,diagnosis,CT,Lung
Computer-aided lung nodule recognition by SVM classifier based on combination of random undersampling and SMOTE,"In lung cancer computer-aided detection/diagnosis (CAD) systems, classification of regions of interest (ROI) is often used to detect/diagnose lung nodule accurately. However, problems of unbalanced datasets often have detrimental effects on the performance of classification. In this paper, both minority and majority classes are resampled to increase the generalization ability. We propose a novel SVM classifier combined with random undersampling (RU) and SMOTE for lung nodule recognition. The combinations of the two resampling methods not only achieve a balanced training samples but also remove noise and duplicate information in the training sample and retain useful information to improve the effective data utilization, hence improving performance of SVM algorithm for pulmonary nodules classification under the unbalanced data. Eight features including 2D and 3D features are extracted for training and classification. Experimental results show that for different sizes of training datasets our RU-SMOTE-SVM classifier gets the highest classification accuracy among the four kinds of classifiers, and the average classification accuracy is more than 92.94%.",2015,10.1155/2015/368674,cross-sectional,diagnosis,CT,Lung
Computer-Aided System for the Detection of Multicategory Pulmonary Tuberculosis in Radiographs,"The early screening and diagnosis of tuberculosis plays an important role in the control and treatment of tuberculosis infections. In this paper, an integrated computer-aided system based on deep learning is proposed for the detection of multiple categories of tuberculosis lesions in chest radiographs. In this system, the fully convolutional neural network method is used to segment the lung area from the entire chest radiograph for pulmonary tuberculosis detection. Different from the previous analysis of the whole chest radiograph, we focus on the specific tuberculosis lesion areas for the analysis and propose the first multicategory tuberculosis lesion detection method. In it, a learning scalable pyramid structure is introduced into the Faster Region-based Convolutional Network (Faster RCNN), which effectively improves the detection of small-area lesions, mines indistinguishable samples during the training process, and uses reinforcement learning to reduce the detection of false-positive lesions. To compare our method with the current tuberculosis detection system, we propose a classification rule for whole chest X-rays using a multicategory tuberculosis lesion detection model and achieve good performance on two public datasets (Montgomery: AUC = 0.977 and accuracy = 0.926; Shenzhen: AUC = 0.941 and accuracy = 0.902). Our proposed computer-aided system is superior to current systems that can be used to assist radiologists in diagnoses and public health providers in screening for tuberculosis in areas where tuberculosis is endemic.",2020,10.1155/2020/9205082,cross-sectional,diagnosis,x-ray,Lung
Computer-Assisted Decision Support System in Pulmonary Cancer detection and stage classification on CT images,"Pulmonary cancer is considered as one of the major causes of death worldwide. For the detection of lung cancer, computer-assisted diagnosis (CADx) systems have been designed. Internet-of-Things (IoT) has enabled ubiquitous internet access to biomedical datasets and techniques; in result, the progress in CADx is significant. Unlike the conventional CADx, deep learning techniques have the basic advantage of an automatic exploitation feature as they have the ability to learn mid and high level image representations. We proposed a Computer-Assisted Decision Support System in Pulmonary Cancer by using the novel deep learning based model and metastasis information obtained from MBAN (Medical Body Area Network). The proposed model, DFCNet, is based on the deep fully convolutional neural network (FCNN) which is used for classification of each detected pulmonary nodule into four lung cancer stages. The performance of proposed work is evaluated on different datasets with varying scan conditions. Comparison of proposed classifier is done with the existing CNN techniques. Overall accuracy of CNN and DFCNet was 77.6% and 84.58%, respectively. Experimental results illustrate the effectiveness of proposed method for the detection and classification of lung cancer nodules. These results demonstrate the potential for the proposed technique in helping the radiologists in improving nodule detection accuracy with efficiency.",2018,10.1016/j.jbi.2018.01.005,cross-sectional,diagnosis,CT,Lung
Computer-assisted subtyping and prognosis for non-small cell lung cancer patients with unresectable tumor,"BACKGROUND: The histological classification or subtyping of non-small cell lung cancer is essential for systematic therapy decisions. Differentiating between the two main subtypes of pulmonary adenocarcinoma and squamous cell carcinoma highlights the considerable differences that exist in the prognosis of patient outcomes. Physicians rely on a pathological analysis to reveal these phenotypic variations that requires invasive methods, such as biopsy and resection sample, but almost 70% of tumors are unresectable at the point of diagnosis. METHOD: A computational method that fuses two frameworks of computerized subtyping and prognosis was proposed, and it was validated against publicly available dataset in The Cancer Imaging Archive that consisted of 82 curated patients with CT scans. The accuracy of the proposed method was compared with the gold standard of pathological analysis, as defined by theInternational Classification of Disease for Oncology (ICD-O). A series of survival outcome test cases were evaluated using the Kaplan-Meier estimator and log-rank test (p-value) between the computational method and ICD-O. RESULTS: The computational method demonstrated high accuracy in subtyping (96.2%) and good consistency in the statistical significance of overall survival prediction for adenocarcinoma and squamous cell carcinoma patients (p < 0.03) with respect to its counterpart pathological subtyping (p < 0.02). The degree of reproducibility between prognosis taken on computational and pathological subtyping was substantial with an averaged concordance correlation coefficient (CCC) of 0.9910. CONCLUSION: The findings in this study support the idea that quantitative analysis is capable of representing tissue characteristics, as offered by a qualitative analysis.",2018,10.1016/j.compmedimag.2018.04.003,cross-sectional,prognosis,CT,Lung
Computer-assisted three-dimensional quantitation of programmed death-ligand 1 in non-small cell lung cancer using tissue clearing technology,"Immune checkpoint blockade therapy has revolutionized non-small cell lung cancer treatment. However, not all patients respond to this therapy. Assessing the tumor expression of immune checkpoint molecules, including programmed death-ligand 1 (PD-L1), is the current standard in predicting treatment response. However, the correlation between PD-L1 expression and anti-PD-1/PD-L1 treatment response is not perfect. This is partly caused by tumor heterogeneity and the common practice of assessing PD-L1 expression based on limited biopsy material. To overcome this problem, we developed a novel method that can make formalin-fixed, paraffin-embedded tissue translucent, allowing three-dimensional (3D) imaging. Our protocol can process tissues up to 150 μm in thickness, allowing anti-PD-L1 staining of the entire tissue and producing high resolution 3D images. Compared to a traditional 4 μm section, our 3D image provides 30 times more coverage of the specimen, assessing PD-L1 expression of approximately 10 times more cells. We further developed a computer-assisted PD-L1 quantitation method to analyze these images, and we found marked variation of PD-L1 expression in 3D. In 5 of 33 needle-biopsy-sized specimens (15.2%), the PD-L1 tumor proportion score (TPS) varied by greater than 10% at different depth levels. In 14 cases (42.4%), the TPS at different depth levels fell into different categories (< 1%, 1-49%, or ≥ 50%), which can potentially influence treatment decisions. Importantly, our technology permits recovery of the processed tissue for subsequent analysis, including histology examination, immunohistochemistry, and mutation analysis. In conclusion, our novel method has the potential to increase the accuracy of tumor PD-L1 expression assessment and enable precise deployment of cancer immunotherapy.",2022,10.1186/s12967-022-03335-5,,,,
Computerized characterization of lung nodule subtlety using thoracic CT images,"The goal of this work is to design computerized image analysis techniques for automatically characterizing lung nodule subtlety in CT images. Automated subtlety estimation methods may help in computer-aided detection (CAD) assessment by quantifying dataset difficulty and facilitating comparisons among different CAD algorithms. A dataset containing 813 nodules from 499 patients was obtained from the Lung Image Database Consortium. Each nodule was evaluated by four radiologists regarding nodule subtlety using a 5-point rating scale (1: most subtle). We developed a 3D technique for segmenting lung nodules using a prespecified initial ROI. Texture and morphological features were automatically extracted from the segmented nodules and their margins. The dataset was partitioned into trainers and testers using a 1:1 ratio. An artificial neural network (ANN) was trained with average reader subtlety scores as the reference. Effective features for characterizing nodule subtlety were selected based on the training set using the ANN and a stepwise feature selection method. The performance of the classifier was evaluated using prediction probability (PK) as an agreement measure, which is considered a generalization of the area under the receiver operating characteristic curve when the reference standard is multi-level. Using an ANN classifier trained with a set of 2 features (selected from a total of 30 features), including compactness and average gray value, the test concordance between computer scores and the average reader scores was 0.789 ± 0.014. Our results show that the proposed method had strong agreement with the average of subtlety scores provided by radiologists.",2014,10.1088/0031-9155/59/4/897,cross-sectional,diagnosis,CT,Lung
"Computerized detection of lung nodules by means of ""virtual dual-energy"" radiography","Major challenges in current computer-aided detection (CADe) schemes for nodule detection in chest radiographs (CXRs) are to detect nodules that overlap with ribs and/or clavicles and to reduce the frequent false positives (FPs) caused by ribs. Detection of such nodules by a CADe scheme is very important, because radiologists are likely to miss such subtle nodules. Our purpose in this study was to develop a CADe scheme with improved sensitivity and specificity by use of ""virtual dual-energy"" (VDE) CXRs where ribs and clavicles are suppressed with massive-training artificial neural networks (MTANNs). To reduce rib-induced FPs and detect nodules overlapping with ribs, we incorporated the VDE technology in our CADe scheme. The VDE technology suppressed rib and clavicle opacities in CXRs while maintaining soft-tissue opacity by use of the MTANN technique that had been trained with real dual-energy imaging. Our scheme detected nodule candidates on VDE images by use of a morphologic filtering technique. Sixty morphologic and gray-level-based features were extracted from each candidate from both original and VDE CXRs. A nonlinear support vector classifier was employed for classification of the nodule candidates. A publicly available database containing 140 nodules in 140 CXRs and 93 normal CXRs was used for testing our CADe scheme. All nodules were confirmed by computed tomography examinations, and the average size of the nodules was 17.8 mm. Thirty percent (42/140) of the nodules were rated ""extremely subtle"" or ""very subtle"" by a radiologist. The original scheme without VDE technology achieved a sensitivity of 78.6% (110/140) with 5 (1165/233) FPs per image. By use of the VDE technology, more nodules overlapping with ribs or clavicles were detected and the sensitivity was improved substantially to 85.0% (119/140) at the same FP rate in a leave-one-out cross-validation test, whereas the FP rate was reduced to 2.5 (583/233) per image at the same sensitivity level as the original CADe scheme obtained (Difference between the specificities of the original and the VDE-based CADe schemes was statistically significant). In particular, the sensitivity of our VDE-based CADe scheme for subtle nodules (66.7% = 28/42) was statistically significantly higher than that of the original CADe scheme (57.1% = 24/42). Therefore, by use of VDE technology, the sensitivity and specificity of our CADe scheme for detection of nodules, especially subtle nodules, in CXRs were improved substantially.",2013,10.1109/tbme.2012.2226583,cross-sectional,diagnosis,x-ray,Lung
Computerized detection of lung nodules through radiomics,"PURPOSE: Lung cancer is a major cause of cancer deaths, and the 5-year survival rate of stage IV lung cancer patients is only 2%. However, the 5-year survival rate of stage I lung cancer patients significantly increases to 50%. As such, spiral computed tomography (CT) scans are necessary to diagnose high-risk lung cancer patients in early stages. In this study, a computer-aided detection (CAD) system with radiomics was proposed. This system could automatically detect pulmonary nodules and reduce radiologists' workloads and human errors. METHODS: In the proposed scheme, a nodular enhancement filter was used to segment nodule candidates and extract radiomic features. A synthetic minority over-sampling technique was also applied to balance the samples, and a random forest method was utilized to distinguish between real nodules and false positive detections. The radiomics approach quantified intratumor heterogeneity and multifrequency information, which are highly correlated with lung nodules. RESULTS: The proposed method was used to evaluate 1004 CT cases from the well-known Lung Image Database Consortium, and 88.9% sensitivity with four false positive detections per CT scan was obtained by randomly selecting 502 cases for training and 502 other cases for testing. CONCLUSIONS: The proposed scheme yielded a high performance on the LIDC database. Therefore, the proposed scheme is possibly effective for various CT configurations used in routine diagnosis and lung cancer screening.",2017,10.1002/mp.12331,cross-sectional,diagnosis,CT,Lung
Computerized texture analysis of persistent part-solid ground-glass nodules: differentiation of preinvasive lesions from invasive pulmonary adenocarcinomas,"PURPOSE: To retrospectively investigate the value of computerized three-dimensional texture analysis for differentiation of preinvasive lesions from invasive pulmonary adenocarcinomas (IPAs) that manifest as part-solid ground-glass nodules (GGNs). MATERIALS AND METHODS: The institutional review board approved this retrospective study with a waiver of patients' informed consent. The study consisted of 86 patients with 86 pathologic analysis-confirmed part-solid GGNs (mean size, 16 mm ± 5.4 [standard deviation]) who had undergone computed tomographic (CT) imaging between January 2005 and October 2011. Each part-solid GGN was manually segmented and its computerized texture features were quantitatively extracted by using an in-house software program. Multivariate logistic regression analysis was performed to investigate the differentiating factors of preinvasive lesions from IPAs. Three-layered artificial neural networks (ANNs) with a back-propagation algorithm and receiver operating characteristic curve analysis were used to build a discriminating model with texture features and to evaluate its discriminating performance. RESULTS: Pathologic analysis confirmed 58 IPAs (seven minimally invasive adenocarcinomas and 51 invasive adenocarcinomas) and 28 preinvasive lesions (four atypical adenomatous hyperplasias and 24 adenocarcinomas in situ). IPAs and preinvasive lesions exhibited significant differences in various histograms and volumetric parameters (P < .05). Multivariate analysis revealed that smaller mass (adjusted odds ratio, 0.092) and higher kurtosis (adjusted odds ratio, 3.319) are significant differentiators of preinvasive lesions from IPAs (P < .05). With mean attenuation, standard deviation of attenuation, mass, kurtosis, and entropy, the ANNs model showed excellent accuracy in differentiation of preinvasive lesions from IPAs (area under the curve, 0.981). CONCLUSION: In part-solid GGNs, higher kurtosis and smaller mass are significant differentiators of preinvasive lesions from IPAs, and preinvasive lesions can be accurately differentiated from IPAs by using computerized texture analysis. Online supplemental material is available for this article.",2014,10.1148/radiol.14132187,cross-sectional,diagnosis,CT,Lung
Computerized Tomography Image Feature under Convolutional Neural Network Algorithm Evaluated for Therapeutic Effect of Clarithromycin Combined with Salmeterol/Fluticasone on Chronic Obstructive Pulmonary Disease,"This study was to explore the use of convolutional neural network (CNN) for the classification and recognition of computerized tomography (CT) images of chronic obstructive pulmonary disease (COPD) and the therapeutic effect of clarithromycin combined with salmeterol/fluticasone. First, the clinical data of COPD patients treated in hospital from September 2018 to December 2020 were collected, and CT and X-ray images were also collected. CT-CNN and X ray-CNN single modal models were constructed based on the LeNet-5 model. The randomized fusion algorithm was introduced to construct a fused CNN model for the diagnosis of COPD patients, and the recognition effect of the model was verified. Subsequently, the three-dimensional reconstruction of the patient's bronchus was performed using the classified CT images, and the changes of CT quantitative parameters in COPD patients were compared and analyzed. Finally, COPD patients were treated with salmeterol/fluticasone (COPD-C) and combined with clarithromycin (COPD-T). In addition, the differences between patients' lung function indexes, blood gas indexes, St. George respiratory questionnaire (SGRQ) scores, and the number of acute exacerbations (AECOPD) before and after treatment were evaluated. The results showed that the randomized fusion model under different iteration times and batch sizes always had the highest recognition rate, sensitivity, and specificity compared to the two single modal CNN models, but it also had longer training time. After CT images were used to quantitatively evaluate the changes of the patient's bronchus, it was found that the area of the upper and lower lung lobes of the affected side of COPD patients and the ratio of the area of the tube wall to the bronchus were significantly changed. The lung function, blood gas index, and SGRQ score of COPD-T patients were significantly improved compared with the COPD-C group (P < 0.05), but there was no considerable difference in AECOPD (P > 0.05). In summary, the randomized fusion-based CNN model can improve the recognition rate of COPD, and salmeterol/fluticasone combined with clarithromycin therapy can significantly improve the clinical treatment effect of COPD patients.",2021,10.1155/2021/8563181,retrospective cohort,combined,CT + x-ray,Lung
Conditional GAN based augmentation for predictive modeling of respiratory signals,"Respiratory illness is the primary cause of mortality and impairment in the life span of an individual in the current COVID-19 pandemic scenario. The inability to inhale and exhale is one of the difficult conditions for a person suffering from respiratory disorders. Unfortunately, the diagnosis of respiratory disorders with the presently available imaging and auditory screening modalities are sub-optimal and the accuracy of diagnosis varies with different medical experts. At present, deep neural nets demand a massive amount of data suitable for precise models. In reality, the respiratory data set is quite limited, and therefore, data augmentation (DA) is employed to enlarge the data set. In this study, conditional generative adversarial networks (cGAN) based DA is utilized for synthetic generation of signals. The publicly available repository such as ICBHI 2017 challenge, RALE and Think Labs Lung Sounds Library are considered for classifying the respiratory signals. To assess the efficacy of the artificially created signals by the DA approach, similarity measures are calculated between original and augmented signals. After that, to quantify the performance of augmentation in classification, scalogram representation of generated signals are fed as input to different pre-trained deep learning architectures viz Alexnet, GoogLeNet and ResNet-50. The experimental results are computed and performance results are compared with existing classical approaches of augmentation. The research findings conclude that the proposed cGAN method of augmentation provides better accuracy of 92.50% and 92.68%, respectively for both the two data sets using ResNet 50 model.",2021,10.1016/j.compbiomed.2021.104930,,,,
Cone-beam CT image quality improvement using Cycle-Deblur consistent adversarial networks (Cycle-Deblur GAN) for chest CT imaging in breast cancer patients,"Cone-beam computed tomography (CBCT) integrated with a linear accelerator is widely used to increase the accuracy of radiotherapy and plays an important role in image-guided radiotherapy (IGRT). For comparison with fan-beam computed tomography (FBCT), the image quality of CBCT is indistinct due to X-ray scattering, noise, and artefacts. We proposed a deep learning model, ""Cycle-Deblur GAN"", combined with CycleGAN and Deblur-GAN models to improve the image quality of chest CBCT images. The 8706 CBCT and FBCT image pairs were used for training, and 1150 image pairs were used for testing in deep learning. The generated CBCT images from the Cycle-Deblur GAN model demonstrated closer CT values to FBCT in the lung, breast, mediastinum, and sternum compared to the CycleGAN and RED-CNN models. The quantitative evaluations of MAE, PSNR, and SSIM for CBCT generated from the Cycle-Deblur GAN model demonstrated better results than the CycleGAN and RED-CNN models. The Cycle-Deblur GAN model improved image quality and CT-value accuracy and preserved structural details for chest CBCT images.",2021,10.1038/s41598-020-80803-2,,,,
Consensus study on the health system and patient-related barriers for lung cancer management in South Africa,"BACKGROUND: Lung cancer is the highest incident cancer globally and is associated with significant morbidity and mortality particularly if identified at a late stage. Poor patient outcomes in low- and middle-income countries (LMIC's) might reflect contextual patient and health system constraints at multiple levels, that act as barriers to prevention, disease recognition, diagnosis, and treatment. Lung cancer screening, even for high-risk patients, is not available in the public health sector in South Africa (SA), where the current HIV and tuberculosis (TB) epidemics often take precedence. Yet, there has been no formal assessment of the individual and health-system related barriers that may delay patients with lung cancer from seeking and accessing help within the public health care system and receiving the appropriate and effective diagnosis and treatment. This study aimed to derive consensus from health-system stakeholders in the urban Gauteng Province of SA on the most important challenges faced by the health services and patients in achieving optimum lung cancer management and to identify potential solutions. METHODS: The study was undertaken among 27 participant stakeholders representing clinical managers, clinicians, opinion leaders from the public health sector and non-governmental organisation (NGO) representatives. The study compromised two components: consensus and engagement. For the consensus component, the Delphi Technique was employed with open-ended questions and item ranking from five rounds of consensus-seeking, to achieve collective agreement on the most important challenges faced by patients and the health services in achieving optimal lung cancer management. For the engagement component, the Nominal Group Technique was used to articulate ideas and reach an agreement on the group's recommendations for solution strategies and approaches. RESULTS: Public health sector stakeholders suggested that a lack of knowledge and awareness of lung cancer, and the apparent stigma associated with the disease and its risk factors, as well as symptoms and signs, are critical to treatment delay. Furthermore, delays in up-referral of patients with suspected lung cancer from district health care level were attributed to inadequate knowledge arising from a lack of in-service training of nurses and doctors regarding oncologic symptoms, risk factors, need for further investigation, interpretation of x-rays and available treatments. At a tertiary level, participants suggested that insufficient availability of specialised diagnostic resources (imaging, cytological and pathological services including biomolecular assessment of lung cancer), theatres, cardiothoracic surgeons, and appropriate therapeutic modalities (chemotherapeutic agents and radiation oncology) are the main barriers to the provision of optimal care. It was suggested that a primary prevention programme initiated by the government that involves private-public partnerships may improve lung cancer management nationally. CONCLUSIONS: Considerable barriers to the early identification and treatment of lung cancer exist. Finding solutions to overcome both individual and health-system level obstacles to lung cancer screening and management are vital to facilitate early identification and treatment, and to improve survival. Furthermore, research on inexpensive biomarkers for asymptomatic disease detection, the introduction of diagnostic imaging tools that utilise artificial intelligence to compensate for inadequate human resources and improving clinical integration across all levels of the healthcare system are essential.",2021,10.1371/journal.pone.0246716,,,,
Content-Based Image Retrieval of Chest CT with Convolutional Neural Network for Diffuse Interstitial Lung Disease: Performance Assessment in Three Major Idiopathic Interstitial Pneumonias,"OBJECTIVE: To assess the performance of content-based image retrieval (CBIR) of chest CT for diffuse interstitial lung disease (DILD). MATERIALS AND METHODS: The database was comprised by 246 pairs of chest CTs (initial and follow-up CTs within two years) from 246 patients with usual interstitial pneumonia (UIP, n = 100), nonspecific interstitial pneumonia (NSIP, n = 101), and cryptogenic organic pneumonia (COP, n = 45). Sixty cases (30-UIP, 20-NSIP, and 10-COP) were selected as the queries. The CBIR retrieved five similar CTs as a query from the database by comparing six image patterns (honeycombing, reticular opacity, emphysema, ground-glass opacity, consolidation and normal lung) of DILD, which were automatically quantified and classified by a convolutional neural network. We assessed the rates of retrieving the same pairs of query CTs, and the number of CTs with the same disease class as query CTs in top 1-5 retrievals. Chest radiologists evaluated the similarity between retrieved CTs and queries using a 5-scale grading system (5-almost identical; 4-same disease; 3-likelihood of same disease is half; 2-likely different; and 1-different disease). RESULTS: The rate of retrieving the same pairs of query CTs in top 1 retrieval was 61.7% (37/60) and in top 1-5 retrievals was 81.7% (49/60). The CBIR retrieved the same pairs of query CTs more in UIP compared to NSIP and COP (p = 0.008 and 0.002). On average, it retrieved 4.17 of five similar CTs from the same disease class. Radiologists rated 71.3% to 73.0% of the retrieved CTs with a similarity score of 4 or 5. CONCLUSION: The proposed CBIR system showed good performance for retrieving chest CTs showing similar patterns for DILD.",2021,10.3348/kjr.2020.0603,cross-sectional,diagnosis,CT,Lung
Contralaterally Enhanced Networks for Thoracic Disease Detection,"Identifying and locating diseases in chest X-rays are very challenging, due to the low visual contrast between normal and abnormal regions, and distortions caused by other overlapping tissues. An interesting phenomenon is that there exist many similar structures in the left and right parts of the chest, such as ribs, lung fields and bronchial tubes. This kind of similarities can be used to identify diseases in chest X-rays, according to the experience of broad-certificated radiologists. Aimed at improving the performance of existing detection methods, we propose a deep end-to-end module to exploit the contralateral context information for enhancing feature representations of disease proposals. First of all, under the guidance of the spine line, the spatial transformer network is employed to extract local contralateral patches, which can provide valuable context information for disease proposals. Then, we build up a specific module, based on both additive and subtractive operations, to fuse the features of the disease proposal and the contralateral patch. Our method can be integrated into both fully and weakly supervised disease detection frameworks. It achieves 33.17 AP50 on a carefully annotated private chest X-ray dataset which contains 31,000 images. Experiments on the NIH chest X-ray dataset indicate that our method achieves state-of-the-art performance in weakly-supervised disease localization.",2021,10.1109/tmi.2021.3077913,cross-sectional,diagnosis,x-ray,Thorax
Contrast-enhanced T1-weighted image radiomics of brain metastases may predict EGFR mutation status in primary lung cancer,"Identification of EGFR mutations is critical to the treatment of primary lung cancer and brain metastases (BMs). Here, we explored whether radiomic features of contrast-enhanced T1-weighted images (T1WIs) of BMs predict EGFR mutation status in primary lung cancer cases. In total, 1209 features were extracted from the contrast-enhanced T1WIs of 61 patients with 210 measurable BMs. Feature selection and classification were optimized using several machine learning algorithms. Ten-fold cross-validation was applied to the T1WI BM dataset (189 BMs for training and 21 BMs for the test set). Area under receiver operating characteristic curves (AUC), accuracy, sensitivity, and specificity were calculated. Subgroup analyses were also performed according to metastasis size. For all measurable BMs, random forest (RF) classification with RF selection demonstrated the highest diagnostic performance for identifying EGFR mutation (AUC: 86.81). Support vector machine and AdaBoost were comparable to RF classification. Subgroup analyses revealed that small BMs had the highest AUC (89.09). The diagnostic performance for large BMs was lower than that for small BMs (the highest AUC: 78.22). Contrast-enhanced T1-weighted image radiomics of brain metastases predicted the EGFR mutation status of lung cancer BMs with good diagnostic performance. However, further study is necessary to apply this algorithm more widely and to larger BMs.",2020,10.1038/s41598-020-65470-7,,,,
Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification,"The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performanceson both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.",2020,10.1109/jbhi.2020.3023246,cross-sectional,diagnosis,CT,Lung
Contribution of artificial intelligence applications developed with the deep learning method to the diagnosis of COVID-19 pneumonia on computed tomography,"INTRODUCTION: Computed tomography (CT) is an auxiliary modality in the diagnosis of the novel Coronavirus (COVID-19) disease and can guide physicians in the presence of lung involvement. In this study, we aimed to investigate the contribution of deep learning to diagnosis in patients with typical COVID-19 pneumonia findings on CT. MATERIALS AND METHODS: This study retrospectively evaluated 690 lesions obtained from 35 patients diagnosed with COVID-19 pneumonia based on typical findings on non-contrast high-resolution CT (HRCT) in our hospital. The diagnoses of the patients were also confirmed by other necessary tests. HRCT images were assessed in the parenchymal window. In the images obtained, COVID-19 lesions were detected. For the deep Convolutional Neural Network (CNN) algorithm, the Confusion matrix was used based on a Tensorflow Framework in Python. RESULT: A total of 596 labeled lesions obtained from 224 sections of the images were used for the training of the algorithm, 89 labeled lesions from 27 sections were used in validation, and 67 labeled lesions from 25 images in testing. Fifty-six of the 67 lesions used in the testing stage were accurately detected by the algorithm while the remaining 11 were not recognized. There was no false positive. The Recall, Precision and F1 score values in the test group were 83.58, 1, and 91.06, respectively. CONCLUSIONS: We successfully detected the COVID-19 pneumonia lesions on CT images using the algorithms created with artificial intelligence. The integration of deep learning into the diagnostic stage in medicine is an important step for the diagnosis of diseases that can cause lung involvement in possible future pandemics.",2021,10.5578/tt.20219606,cross-sectional,diagnosis,CT,Lung
Conventional Filtering Versus U-Net Based Models for Pulmonary Nodule Segmentation in CT Images,"Lung cancer is considered one of the deadliest diseases in the world. An early and accurate diagnosis aims to promote the detection and characterization of pulmonary nodules, which is of vital importance to increase the patients' survival rates. The mentioned characterization is done through a segmentation process, facing several challenges due to the diversity in nodular shape, size, and texture, as well as the presence of adjacent structures. This paper tackles pulmonary nodule segmentation in computed tomography scans proposing three distinct methodologies. First, a conventional approach which applies the Sliding Band Filter (SBF) to estimate the filter's support points, matching the border coordinates. The remaining approaches are Deep Learning based, using the U-Net and a novel network called SegU-Net to achieve the same goal. Their performance is compared, as this work aims to identify the most promising tool to improve nodule characterization. All methodologies used 2653 nodules from the LIDC database, achieving a Dice score of 0.663, 0.830, and 0.823 for the SBF, U-Net and SegU-Net respectively. This way, the U-Net based models yield more identical results to the ground truth reference annotated by specialists, thus being a more reliable approach for the proposed exercise. The novel network revealed similar scores to the U-Net, while at the same time reducing computational cost and improving memory efficiency. Consequently, such study may contribute to the possible implementation of this model in a decision support system, assisting the physicians in establishing a reliable diagnosis of lung pathologies based on this segmentation task.",2020,10.1007/s10916-020-1541-9,cross-sectional,diagnosis,CT,Lung
Convolution kernel and iterative reconstruction affect the diagnostic performance of radiomics and deep learning in lung adenocarcinoma pathological subtypes,"BACKGROUND: The aim of this study was to investigate the influence of convolution kernel and iterative reconstruction on the diagnostic performance of radiomics and deep learning (DL) in lung adenocarcinomas. METHODS: A total of 183 patients with 215 lung adenocarcinomas were included in this study. All CT imaging data was reconstructed with three reconstruction algorithms (ASiR at 0%, 30%, 60% strength), each with two convolution kernels (bone and standard). A total of 171 nodules were selected as the training-validation set, whereas 44 nodules were selected as the testing set. Logistic regression and a DL framework-DenseNets were selected to tackle the task. Three logical experiments were implemented to fully explore the influence of the studied parameters on the diagnostic performance. The receiver operating characteristic curve (ROC) was used to evaluate the performance of constructed models. RESULTS: In Experiments A and B, no statistically significant results were found in the radiomic method, whereas two and six pairs were statistically significant (P < 0.05) in the DL method. In Experiment_C, significant differences in one and four models were found in the radiomics and DL methods, respectively. Moreover, models constructed with standard convolution kernel data outperformed that constructed with bone convolution kernel data in all studied ASiR levels in the DL method. In the DL method, B0 and S60 performed best in bone and standard convolution kernel, respectively. CONCLUSION: The results demonstrated that DL was more susceptible to CT parameter variability than radiomics. Standard convolution kernel images seem to be more appropriate for imaging analysis. Further investigation with a larger sample size is needed.",2019,10.1111/1759-7714.13161,cross-sectional,combined,CT,Lung
Convolutional Neural Network Addresses the Confounding Impact of CT Reconstruction Kernels on Radiomics Studies,"Achieving high feature reproducibility while preserving biological information is one of the main challenges for the generalizability of current radiomics studies. Non-clinical imaging variables, such as reconstruction kernels, have shown to significantly impact radiomics features. In this study, we retrain an open-source convolutional neural network (CNN) to harmonize computerized tomography (CT) images with various reconstruction kernels to improve feature reproducibility and radiomic model performance using epidermal growth factor receptor (EGFR) mutation prediction in lung cancer as a paradigm. In the training phase, the CNN was retrained and tested on 32 lung cancer patients' CT images between two different groups of reconstruction kernels (smooth and sharp). In the validation phase, the retrained CNN was validated on an external cohort of 223 lung cancer patients' CT images acquired using different CT scanners and kernels. The results showed that the retrained CNN could be successfully applied to external datasets with different CT scanner parameters, and harmonization of reconstruction kernels from sharp to smooth could significantly improve the performance of radiomics model in predicting EGFR mutation status in lung cancer. In conclusion, the CNN based method showed great potential in improving feature reproducibility and generalizability by harmonizing medical images with heterogeneous reconstruction kernels.",2021,10.3390/tomography7040074,cross-sectional,combined,CT,Lung
Convolutional Neural Network ensembles for accurate lung nodule malignancy prediction 2 years in the future,"Convolutional Neural Networks (CNNs) have been utilized for to distinguish between benign lung nodules and those that will become malignant. The objective of this study was to use an ensemble of CNNs to predict which baseline nodules would be diagnosed as lung cancer in a second follow up screening after more than one year. Low-dose helical computed tomography images and data were utilized from the National Lung Screening Trial (NLST). The malignant nodules and nodule positive controls were divided into training and test cohorts. T0 nodules were used to predict lung cancer incidence at T1 or T2. To increase the sample size, image augmentation was performed using rotations, flipping, and elastic deformation. Three CNN architectures were designed for malignancy prediction, and each architecture was trained using seven different seeds to create the initial weights. This enabled variability in the CNN models which were combined to generate a robust, more accurate ensemble model. Augmenting images using only rotation and flipping and training with images from T0 yielded the best accuracy to predict lung cancer incidence at T2 from a separate test cohort (Accuracy = 90.29%; AUC = 0.96) based on an ensemble 21 models. Images augmented by rotation and flipping enabled effective learning by increasing the relatively small sample size. Ensemble learning with deep neural networks is a compelling approach that accurately predicted lung cancer incidence at the second screening after the baseline screen mostly 2 years later.",2020,10.1016/j.compbiomed.2020.103882,case control,prognosis,CT,Lung
Convolutional neural network with group theory and random selection particle swarm optimizer for enhancing cancer image classification,"As an epitome of deep learning, convolutional neural network (CNN) has shown its advantages in solving many real-world problems. Successful CNN applications on medical prognosis and diagnosis have been achieved in recent years. Their common goal is to recognize the insights from the subtle details from medical images by building a suitable CNN model with maximum accuracy and minimum error. The CNN performance is extremely sensitive to the parameter tuning for any given network structure. To approach this concern, a novel self-tuning CNN model is proposed with a significant characteristic of having a metaheuristic-based optimizer. The most optimal set of parameters is often found via our proposed method, namely group theory and random selection-based particle swarm optimization (GTRS-PSO). The insights of symmetric essentials of model structure and parameter correlation are extracted, followed by the hierarchical partitioning of parameter space, and four operators on those partitions are designed for moving neighborhoods and formulating the swarm topology accordingly. The parameters are updated by a random selection strategy at each interval of partitions during the search process. Preliminary experiments over two radiology image datasets: breast cancer and lung cancer, are conducted for a comprehensive comparison of GTRS-PSO versus other optimization algorithms. The results show that CNN with GTRS-PSO optimizer can achieve the best performance for cancer image classifications, especially when there are symmetric components inside the data properties and model structures.",2021,10.3934/mbe.2021281,,,,
Convolutional neural networks can accurately distinguish four histologic growth patterns of lung adenocarcinoma in digital slides,"During the diagnostic workup of lung adenocarcinomas (LAC), pathologists evaluate distinct histological tumor growth patterns. The percentage of each pattern on multiple slides bears prognostic significance. To assist with the quantification of growth patterns, we constructed a pipeline equipped with a convolutional neural network (CNN) and soft-voting as the decision function to recognize solid, micropapillary, acinar, and cribriform growth patterns, and non-tumor areas. Slides of primary LAC were obtained from Cedars-Sinai Medical Center (CSMC), the Military Institute of Medicine in Warsaw and the TCGA portal. Several CNN models trained with 19,924 image tiles extracted from 78 slides (MIMW and CSMC) were evaluated on 128 test slides from the three sites by F1-score and accuracy using manual tumor annotations by pathologist. The best CNN yielded F1-scores of 0.91 (solid), 0.76 (micropapillary), 0.74 (acinar), 0.6 (cribriform), and 0.96 (non-tumor) respectively. The overall accuracy of distinguishing the five tissue classes was 89.24%. Slide-based accuracy in the CSMC set (88.5%) was significantly better (p < 2.3E-4) than the accuracy in the MIMW (84.2%) and TCGA (84%) sets due to superior slide quality. Our model can work side-by-side with a pathologist to accurately quantify the percentages of growth patterns in tumors with mixed LAC patterns.",2019,10.1038/s41598-018-37638-9,,,,
Convolutional Neural Networks in Predicting Nodal and Distant Metastatic Potential of Newly Diagnosed Non-Small Cell Lung Cancer on FDG PET Images,"OBJECTIVE. The purpose of this study was to assess, by analyzing features of the primary tumor with (18)F-FDG PET, the utility of deep machine learning with a convolutional neural network (CNN) in predicting the potential of newly diagnosed non-small cell lung cancer (NSCLC) to metastasize to lymph nodes or distant sites. MATERIALS AND METHODS. Consecutively registered patients with newly diagnosed, untreated NSCLC were retrospectively included in a single-center study. PET images were segmented with local image features extraction software, and data were used for CNN training and validation after data augmentation strategies were used. The standard of reference for designation of N category was invasive lymph node sampling or 6-month follow-up imaging. Distant metastases developing during the study follow-up period were assessed by imaging (CT or PET/CT), in tissue obtained from new suspected sites of disease, and according to the treating oncologist's designation. RESULTS. A total of 264 patients with NSCLC participated in follow-up for a median of 25.2 months (range, 6-43 months). N category designations were available for 223 of 264 (84.5%) patients, and M category for all 264. The sensitivity, specificity, and accuracy of CNN for predicting node positivity were 0.74 ± 0.32, 0.84 ± 0.16, and 0.80 ± 0.17. The corresponding values for predicting distant metastases were 0.45 ± 0.08, 0.79 ± 0.06, and 0.63 ± 0.05. CONCLUSION. This study showed that using a CNN to analyze segmented PET images of patients with previously untreated NSCLC can yield moderately high accuracy for designation of N category, although this may be insufficient to preclude invasive lymph node sampling. The sensitivity of the CNN in predicting distant metastases is fairly poor, although specificity is moderately high.",2020,10.2214/ajr.19.22346,retrospective cohort,prognosis,PET,Lung
Convolutional Neural Networks Promising in Lung Cancer T-Parameter Assessment on Baseline FDG-PET/CT,"AIM: To develop an algorithm, based on convolutional neural network (CNN), for the classification of lung cancer lesions as T1-T2 or T3-T4 on staging fluorodeoxyglucose positron emission tomography (FDG-PET)/CT images. METHODS: We retrospectively selected a cohort of 472 patients (divided in the training, validation, and test sets) submitted to staging FDG-PET/CT within 60 days before biopsy or surgery. TNM system seventh edition was used as reference. Postprocessing was performed to generate an adequate dataset. The input of CNNs was a bounding box on both PET and CT images, cropped around the lesion centre. The results were classified as Correct (concordance between reference and prediction) and Incorrect (discordance between reference and prediction). Accuracy (Correct/[Correct + Incorrect]), recall (Correctly predicted T3-T4/[all T3-T4]), and specificity (Correctly predicted T1-T2/[all T1-T2]), as commonly defined in deep learning models, were used to evaluate CNN performance. The area under the curve (AUC) was calculated for the final model. RESULTS: The algorithm, composed of two networks (a ""feature extractor"" and a ""classifier""), developed and tested achieved an accuracy, recall, specificity, and AUC of 87%, 69%, 69%, and 0.83; 86%, 77%, 70%, and 0.73; and 90%, 47%, 67%, and 0.68 in the training, validation, and test sets, respectively. CONCLUSION: We obtained proof of concept that CNNs can be used as a tool to assist in the staging of patients affected by lung cancer.",2018,10.1155/2018/1382309,cross-sectional,diagnosis,PET/CT,Lung
ConvPath: A software tool for lung adenocarcinoma digital pathological image analysis aided by a convolutional neural network,"BACKGROUND: The spatial distributions of different types of cells could reveal a cancer cell's growth pattern, its relationships with the tumor microenvironment and the immune response of the body, all of which represent key ""hallmarks of cancer"". However, the process by which pathologists manually recognize and localize all the cells in pathology slides is extremely labor intensive and error prone. METHODS: In this study, we developed an automated cell type classification pipeline, ConvPath, which includes nuclei segmentation, convolutional neural network-based tumor cell, stromal cell, and lymphocyte classification, and extraction of tumor microenvironment-related features for lung cancer pathology images. To facilitate users in leveraging this pipeline for their research, all source scripts for ConvPath software are available at https://qbrc.swmed.edu/projects/cnn/. FINDINGS: The overall classification accuracy was 92.9% and 90.1% in training and independent testing datasets, respectively. By identifying cells and classifying cell types, this pipeline can convert a pathology image into a ""spatial map"" of tumor, stromal and lymphocyte cells. From this spatial map, we can extract features that characterize the tumor micro-environment. Based on these features, we developed an image feature-based prognostic model and validated the model in two independent cohorts. The predicted risk group serves as an independent prognostic factor, after adjusting for clinical variables that include age, gender, smoking status, and stage. INTERPRETATION: The analysis pipeline developed in this study could convert the pathology image into a ""spatial map"" of tumor cells, stromal cells and lymphocytes. This could greatly facilitate and empower comprehensive analysis of the spatial organization of cells, as well as their roles in tumor progression and metastasis.",2019,10.1016/j.ebiom.2019.10.033,,,,
COPD identification and grading based on deep learning of lung parenchyma and bronchial wall in chest CT images,"OBJECTIVE: Chest CT can display the main pathogenic factors of chronic obstructive pulmonary disease (COPD), emphysema and airway wall remodeling. This study aims to establish deep convolutional neural network (CNN) models using these two imaging markers to diagnose and grade COPD. METHODS: Subjects who underwent chest CT and pulmonary function test (PFT) from one hospital (n = 373) were retrospectively included as the training cohort, and subjects from another hospital (n = 226) were used as the external test cohort. According to the PFT results, all subjects were labeled as Global Initiative for Chronic Obstructive Lung Disease (GOLD) Grade 1, 2, 3, 4 or normal. Two DenseNet-201 CNNs were trained using CT images of lung parenchyma and bronchial wall to generate two corresponding confidence levels to indicate the possibility of COPD, then combined with logistic regression analysis. Quantitative CT was used for comparison. RESULTS: In the test cohort, CNN achieved an area under the curve of 0.899 (95%CI: 0.853-0.935) to determine the existence of COPD, and an accuracy of 81.7% (76.2-86.7%), which was significantly higher than the accuracy 68.1% (61.6%-74.2%) using quantitative CT method (p < 0.05). For three-way (normal, GOLD 1-2, and GOLD 3-4) and five-way (normal, GOLD 1, 2, 3, and 4) classifications, CNN reached accuracies of 77.4 and 67.9%, respectively. CONCLUSION: CNN can identify emphysema and airway wall remodeling on CT images to infer lung function and determine the existence and severity of COPD. It provides an alternative way to detect COPD using the extensively available chest CT. ADVANCES IN KNOWLEDGE: CNN can identify the main pathological changes of COPD (emphysema and airway wall remodeling) based on CT images, to infer lung function and determine the existence and severity of COPD. CNN reached an area under the curve of 0.853 to determine the existence of COPD in the external test cohort. The CNN approach provides an alternative and effective way for early detection of COPD using extensively used chest CT, as an important alternative to pulmonary function test.",2022,10.1259/bjr.20210637,retrospective cohort,diagnosis,CT,Lung
Coronavirus disease analysis using chest X-ray images and a novel deep convolutional neural network,"BACKGROUND: The recent emergence of a highly infectious and contagious respiratory viral disease known as COVID-19 has vastly impacted human lives and overloaded the health care system. Therefore, it is crucial to develop a fast and accurate diagnostic system for the timely identification of COVID-19 infected patients and thus to help control its spread. METHODS: This work proposes a new deep CNN based technique for COVID-19 classification in X-ray images. In this regard, two novel custom CNN architectures, namely COVID-RENet-1 and COVID-RENet-2, are developed for COVID-19 specific pneumonia analysis. The proposed technique systematically employs Region and Edge-based operations along with convolution operations. The advantage of the proposed idea is validated by performing series of experimentation and comparing results with two baseline CNNs that exploited either a single type of pooling operation or strided convolution down the architecture. Additionally, the discrimination capacity of the proposed technique is assessed by benchmarking it against the state-of-the-art CNNs on radiologist's authenticated chest X-ray dataset. Implementation is available at https://github.com/PRLAB21/Coronavirus-Disease-Analysis-using-Chest-X-Ray-Images. RESULTS: The proposed classification technique shows good generalization as compared to existing CNNs by achieving promising MCC (0.96), F-score (0.98) and Accuracy (98%). This suggests that the idea of synergistically using Region and Edge-based operations aid in better exploiting the region homogeneity, textural variations, and region boundary-related information in an image, which helps to capture the pneumonia specific pattern. CONCLUSIONS: The encouraging results of the proposed classification technique on the test set with high sensitivity (0.98) and precision (0.98) suggest the effectiveness of the proposed technique. Thus, it suggests the potential use of the proposed technique in other X-ray imagery-based infectious disease analysis.",2021,10.1016/j.pdpdt.2021.102473,cross-sectional,diagnosis,x-ray,Lung
CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images,"BACKGROUND AND OBJECTIVE: The novel Coronavirus also called COVID-19 originated in Wuhan, China in December 2019 and has now spread across the world. It has so far infected around 1.8 million people and claimed approximately 114,698 lives overall. As the number of cases are rapidly increasing, most of the countries are facing shortage of testing kits and resources. The limited quantity of testing kits and increasing number of daily cases encouraged us to come up with a Deep Learning model that can aid radiologists and clinicians in detecting COVID-19 cases using chest X-rays. METHODS: In this study, we propose CoroNet, a Deep Convolutional Neural Network model to automatically detect COVID-19 infection from chest X-ray images. The proposed model is based on Xception architecture pre-trained on ImageNet dataset and trained end-to-end on a dataset prepared by collecting COVID-19 and other chest pneumonia X-ray images from two different publically available databases. RESULTS: CoroNet has been trained and tested on the prepared dataset and the experimental results show that our proposed model achieved an overall accuracy of 89.6%, and more importantly the precision and recall rate for COVID-19 cases are 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia viral vs normal). For 3-class classification (COVID vs Pneumonia vs normal), the proposed model produced a classification accuracy of 95%. The preliminary results of this study look promising which can be further improved as more training data becomes available. CONCLUSION: CoroNet achieved promising results on a small prepared dataset which indicates that given more data, the proposed model can achieve better results with minimum pre-processing of data. Overall, the proposed model substantially advances the current radiology based methodology and during COVID-19 pandemic, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis, quantification and follow-up of COVID-19 cases.",2020,10.1016/j.cmpb.2020.105581,cross-sectional,diagnosis,x-ray,Lung
Correlative light-electron microscopy in liquid using an inverted SEM (ASEM),"In atmospheric scanning electron microscope (ASEM), the inverted scanning electron microscope (SEM) observes the wet sample from below, while an optical microscope observes it from above simultaneously. The ASEM sample holder has a disposable dish shape with a silicon nitride film window at the bottom. It can be coated variously for the primary-culture of substrate-sensitive cells; primary cells were cultured in a few milliliters of culture medium in a stable incubator environment. For the inverted SEM observation, cells and the excised tissue blocks were aldehyde-fixed, immersed in radical scavenger solution, and observed at minimum electron dose. Neural networking, axonal segmentation, proplatelet-formation and phagocytosis, and Fas expression in embryonic stem cells were captured by optical or fluorescence microscopy, and imaged at high resolution by gold-labeled immuno-ASEM with/without metal staining. By exploiting optical microscopy, the region of interest of organ can be found from the wide area, and the cells and organelle were successfully examined at high resolution by the following scanning electron microscopy. We successfully visualized islet of Langerhans, blood microvessels, neuronal endplate, and bacterial flora on stomach epidermal surfaces. Bacterial biofilms and the typical structural features including ""leg complex"" of mycoplasma were visualized by exploiting CLEM of ASEM. Based on these studies, ASEM correlative microscopy promises to allow the research of various mesoscopic-scale biological phenomena in the near future.",2017,10.1016/bs.mcb.2017.03.015,,,,
Could automated analysis of chest X-rays detect early bronchiectasis in children?,"Non-cystic fibrosis bronchiectasis is increasingly described in the paediatric population. While diagnosis is by high-resolution chest computed tomography (CT), chest X-rays (CXRs) remain a first-line investigation. CXRs are currently insensitive in their detection of bronchiectasis. We aim to determine if quantitative digital analysis allows CT features of bronchiectasis to be detected in contemporaneously taken CXRs. Regions of radiologically (A) normal, (B) severe bronchiectasis, (C) mild airway dilation and (D) other parenchymal abnormalities were identified in CT and mapped to corresponding CXR. An artificial neural network (ANN) algorithm was used to characterise regions of classes A, B, C and D. The algorithm was then tested in 13 subjects and compared to CT scan features. Structural changes in CT were reflected in CXR, including mild airway dilation. The areas under the receiver operator curve for ANN feature detection were 0.74 (class A), 0.71 (class B), 0.76 (class C) and 0.86 (class D). CXR analysis identified CT measures of abnormality with a better correlation than standard radiological scoring at the 99% confidence level.Conclusion: Regional abnormalities can be detected by digital analysis of CXR, which may provide a low-cost and readily available tool to indicate the need for diagnostic CT and for ongoing disease monitoring. What is Known: • Bronchiectasis is a severe chronic respiratory disorder increasingly recognised in paediatric populations. • Diagnostic computed tomography imaging is often requested only after several chest X-ray investigations. What is New: • We show that a digital analysis of chest X-ray could provide more accurate identification of bronchiectasis features.",2021,10.1007/s00431-021-04061-8,cross-sectional,diagnosis,CT& X-ray,Lung
COV-DLS: Prediction of COVID-19 from X-Rays Using Enhanced Deep Transfer Learning Techniques,"In this paper, modifications in neoteric architectures such as VGG16, VGG19, ResNet50, and InceptionV3 are proposed for the classification of COVID-19 using chest X-rays. The proposed architectures termed ""COV-DLS"" consist of two phases: heading model construction and classification. The heading model construction phase utilizes four modified deep learning architectures, namely Modified-VGG16, Modified-VGG19, Modified-ResNet50, and Modified-InceptionV3. An attempt is made to modify these neoteric architectures by incorporating the average pooling and dense layers. The dropout layer is also added to prevent the overfitting problem. Two dense layers with different activation functions are also added. Thereafter, the output of these modified models is applied during the classification phase, when COV-DLS are applied on a COVID-19 chest X-ray image data set. Classification accuracy of 98.61% is achieved by Modified-VGG16, 97.22% by Modified-VGG19, 95.13% by Modified-ResNet50, and 99.31% by Modified-InceptionV3. COV-DLS outperforms existing deep learning models in terms of accuracy and F1-score.",2022,10.1155/2022/6216273,cross-sectional,diagnosis,x-ray,Lung
COVID Detection From Chest X-Ray Images Using Multi-Scale Attention,"Deep learning based methods have shown great promise in achieving accurate automatic detection of Coronavirus Disease (covid) - 19 from Chest X-Ray (cxr) images.However, incorporating explainability in these solutions remains relatively less explored. We present a hierarchical classification approach for separating normal, non-covid pneumonia (ncp) and covid cases using cxr images. We demonstrate that the proposed method achieves clinically consistent explainations. We achieve this using a novel multi-scale attention architecture called Multi-scale Attention Residual Learning (marl) and a new loss function based on conicity for training the proposed architecture. The proposed classification strategy has two stages. The first stage uses a model derived from DenseNet to separate pneumonia cases from normal cases while the second stage uses the marl architecture to discriminate between covid and ncp cases. With a five-fold cross validation the proposed method achieves 93%, 96.28%, and 84.51% accuracy respectively over three large, public datasets for normal vs. ncp vs. covid classification. This is competitive to the state-of-the-art methods. We also provide explanations in the form of GradCAM attributions, which are well aligned with expert annotations. The attributions are also seen to clearly indicate that marl deems the peripheral regions of the lungs to be more important in the case of covid cases while central regions are seen as more important in ncp cases. This observation matches the criteria described by radiologists in clinical literature, thereby attesting to the utility of the derived explanations.",2022,10.1109/jbhi.2022.3151171,cross-sectional,diagnosis,x-ray,Lung
COVID-19 Automatic Diagnosis With Radiographic Imaging: Explainable Attention Transfer Deep Neural Networks,"Researchers seek help from deep learning methods to alleviate the enormous burden of reading radiological images by clinicians during the COVID-19 pandemic. However, clinicians are often reluctant to trust deep models due to their black-box characteristics. To automatically differentiate COVID-19 and community-acquired pneumonia from healthy lungs in radiographic imaging, we propose an explainable attention-transfer classification model based on the knowledge distillation network structure. The attention transfer direction always goes from the teacher network to the student network. Firstly, the teacher network extracts global features and concentrates on the infection regions to generate attention maps. It uses a deformable attention module to strengthen the response of infection regions and to suppress noise in irrelevant regions with an expanded reception field. Secondly, an image fusion module combines attention knowledge transferred from teacher network to student network with the essential information in original input. While the teacher network focuses on global features, the student branch focuses on irregularly shaped lesion regions to learn discriminative features. Lastly, we conduct extensive experiments on public chest X-ray and CT datasets to demonstrate the explainability of the proposed architecture in diagnosing COVID-19.",2021,10.1109/jbhi.2021.3074893,cross-sectional,diagnosis,CT& X-ray,Lung
"COVID-19 Case Recognition from Chest CT Images by Deep Learning, Entropy-Controlled Firefly Optimization, and Parallel Feature Fusion","In healthcare, a multitude of data is collected from medical sensors and devices, such as X-ray machines, magnetic resonance imaging, computed tomography (CT), and so on, that can be analyzed by artificial intelligence methods for early diagnosis of diseases. Recently, the outbreak of the COVID-19 disease caused many deaths. Computer vision researchers support medical doctors by employing deep learning techniques on medical images to diagnose COVID-19 patients. Various methods were proposed for COVID-19 case classification. A new automated technique is proposed using parallel fusion and optimization of deep learning models. The proposed technique starts with a contrast enhancement using a combination of top-hat and Wiener filters. Two pre-trained deep learning models (AlexNet and VGG16) are employed and fine-tuned according to target classes (COVID-19 and healthy). Features are extracted and fused using a parallel fusion approach-parallel positive correlation. Optimal features are selected using the entropy-controlled firefly optimization method. The selected features are classified using machine learning classifiers such as multiclass support vector machine (MC-SVM). Experiments were carried out using the Radiopaedia database and achieved an accuracy of 98%. Moreover, a detailed analysis is conducted and shows the improved performance of the proposed scheme.",2021,10.3390/s21217286,cross-sectional,diagnosis,CT,Lung
COVID-19 classification of X-ray images using deep neural networks,"OBJECTIVES: In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray (CXR) imaging is playing an important role in diagnosis and monitoring of patients with COVID-19. We propose a deep learning model for detection of COVID-19 from CXRs, as well as a tool for retrieving similar patients according to the model's results on their CXRs. For training and evaluating our model, we collected CXRs from inpatients hospitalized in four different hospitals. METHODS: In this retrospective study, 1384 frontal CXRs, of COVID-19 confirmed patients imaged between March and August 2020, and 1024 matching CXRs of non-COVID patients imaged before the pandemic, were collected and used to build a deep learning classifier for detecting patients positive for COVID-19. The classifier consists of an ensemble of pre-trained deep neural networks (DNNS), specifically, ReNet34, ReNet50¸ ReNet152, and vgg16, and is enhanced by data augmentation and lung segmentation. We further implemented a nearest-neighbors algorithm that uses DNN-based image embeddings to retrieve the images most similar to a given image. RESULTS: Our model achieved accuracy of 90.3%, (95% CI: 86.3-93.7%) specificity of 90% (95% CI: 84.3-94%), and sensitivity of 90.5% (95% CI: 85-94%) on a test dataset comprising 15% (350/2326) of the original images. The AUC of the ROC curve is 0.96 (95% CI: 0.93-0.97). CONCLUSION: We provide deep learning models, trained and evaluated on CXRs that can assist medical efforts and reduce medical staff workload in handling COVID-19. KEY POINTS: • A machine learning model was able to detect chest X-ray (CXR) images of patients tested positive for COVID-19 with accuracy and detection rate above 90%. • A tool was created for finding existing CXR images with imaging characteristics most similar to a given CXR, according to the model's image embeddings.",2021,10.1007/s00330-021-08050-1,cross-sectional,diagnosis,x-ray,Lung
COVID-19 classification using thermal images,"SIGNIFICANCE: There is a scarcity of published research on the potential role of thermal imaging in the remote detection of respiratory issues due to coronavirus disease-19 (COVID-19). This is a comprehensive study that explores the potential of this imaging technology resulting from its convenient aspects that make it highly accessible: it is contactless, noninvasive, and devoid of harmful radiation effects, and it does not require a complicated installation process. AIM: We aim to investigate the role of thermal imaging, specifically thermal video, for the identification of SARS-CoV-2-infected people using infrared technology and to explore the role of breathing patterns in different parts of the thorax for the identification of possible COVID-19 infection. APPROACH: We used signal moment, signal texture, and shape moment features extracted from five different body regions of interest (whole upper body, chest, face, back, and side) of images obtained from thermal video clips in which optical flow and super-resolution were used. These features were classified into positive and negative COVID-19 using machine learning strategies. RESULTS: COVID-19 detection for male models [receiver operating characteristic (ROC) area under the ROC curve (AUC) = 0.605 95% confidence intervals (CI) 0.58 to 0.64] is more reliable than for female models (ROC AUC = 0.577 95% CI 0.55 to 0.61). Overall, thermal imaging is not very sensitive nor specific in detecting COVID-19; the metrics were below 60% except for the chest view from males. CONCLUSIONS: We conclude that, although it may be possible to remotely identify some individuals affected by COVID-19, at this time, the diagnostic performance of current methods for body thermal imaging is not good enough to be used as a mass screening tool.",2022,10.1117/1.Jbo.27.5.056003,,,,
COVID-19 CT Image Synthesis With a Conditional Generative Adversarial Network,"Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has spread rapidly since December 2019. Real-time reverse transcription polymerase chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits of quick reporting, a low cost, and high sensitivity for the detection of pulmonary infection. Recently, deep-learning-based computer vision methods have demonstrated great promise for use in medical imaging applications, including X-rays, magnetic resonance imaging, and CT imaging. However, training a deep-learning model requires large volumes of data, and medical staff faces a high risk when collecting COVID-19 CT data due to the high infectivity of the disease. Another issue is the lack of experts available for data labeling. In order to meet the data requirements for COVID-19 CT imaging, we propose a CT image synthesis approach based on a conditional generative adversarial network that can effectively generate high-quality and realistic COVID-19 CT images for use in deep-learning-based medical imaging tasks. Experimental results show that the proposed method outperforms other state-of-the-art image synthesis methods with the generated COVID-19 CT images and indicates promising for various machine learning applications including semantic segmentation and classification.",2021,10.1109/jbhi.2020.3042523,cross-sectional,informatics,CT,Lung
COVID-19 deep classification network based on convolution and deconvolution local enhancement,"Computer Tomography (CT) detection can effectively overcome the problems of traditional detection of Corona Virus Disease 2019 (COVID-19), such as lagging detection results and wrong diagnosis results, which lead to the increase of disease infection rate and prevalence rate. The novel coronavirus pneumonia is a significant difference between the positive and negative patients with asymptomatic infections. To effectively improve the accuracy of doctors' manual judgment of positive and negative COVID-19, this paper proposes a deep classification network model of the novel coronavirus pneumonia based on convolution and deconvolution local enhancement. Through convolution and deconvolution operation, the contrast between the local lesion region and the abdominal cavity of COVID-19 is enhanced. Besides, the middle-level features that can effectively distinguish the image types are obtained. By transforming the novel coronavirus detection problem into the region of interest (ROI) feature classification problem, it can effectively determine whether the feature vector in each feature channel contains the image features of COVID-19. This paper uses an open-source COVID-CT dataset provided by Petuum researchers from the University of California, San Diego, which is collected from 143 novel coronavirus pneumonia patients and the corresponding features are preserved. The complete dataset (including original image and enhanced image) contains 1460 images. Among them, 1022 (70%) and 438 (30%) are used to train and test the performance of the proposed model, respectively. The proposed model verifies the classification precision in different convolution layers and learning rates. Besides, it is compared with most state-of-the-art models. It is found that the proposed algorithm has good classification performance. The corresponding sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and precision are 0.98, 0.96, 0.98, and 0.97, respectively.",2021,10.1016/j.compbiomed.2021.104588,cross-sectional,diagnosis,CT,Lung
COVID-19 detection from lung CT-Scans using a fuzzy integral-based CNN ensemble,"The COVID-19 pandemic has collapsed the public healthcare systems, along with severely damaging the economy of the world. The SARS-CoV-2 virus also known as the coronavirus, led to community spread, causing the death of more than a million people worldwide. The primary reason for the uncontrolled spread of the virus is the lack of provision for population-wise screening. The apparatus for RT-PCR based COVID-19 detection is scarce and the testing process takes 6-9 h. The test is also not satisfactorily sensitive (71% sensitive only). Hence, Computer-Aided Detection techniques based on deep learning methods can be used in such a scenario using other modalities like chest CT-scan images for more accurate and sensitive screening. In this paper, we propose a method that uses a Sugeno fuzzy integral ensemble of four pre-trained deep learning models, namely, VGG-11, GoogLeNet, SqueezeNet v1.1 and Wide ResNet-50-2, for classification of chest CT-scan images into COVID and Non-COVID categories. The proposed framework has been tested on a publicly available dataset for evaluation and it achieves 98.93% accuracy and 98.93% sensitivity on the same. The model outperforms state-of-the-art methods on the same dataset and proves to be a reliable COVID-19 detector. The relevant source codes for the proposed approach can be found at: https://github.com/Rohit-Kundu/Fuzzy-Integral-Covid-Detection.",2021,10.1016/j.compbiomed.2021.104895,cross-sectional,diagnosis,CT,Lung
COVID-19 detection in chest X-ray images using deep boosted hybrid learning,"The new emerging COVID-19, declared a pandemic disease, has affected millions of human lives and caused a massive burden on healthcare centers. Therefore, a quick, accurate, and low-cost computer-based tool is required to timely detect and treat COVID-19 patients. In this work, two new deep learning frameworks: Deep Hybrid Learning (DHL) and Deep Boosted Hybrid Learning (DBHL), is proposed for effective COVID-19 detection in X-ray dataset. In the proposed DHL framework, the representation learning ability of the two developed COVID-RENet-1 & 2 models is exploited individually through a machine learning (ML) classifier. In COVID-RENet models, Region and Edge-based operations are carefully applied to learn region homogeneity and extract boundaries features. While in the case of the proposed DBHL framework, COVID-RENet-1 & 2 are fine-tuned using transfer learning on the chest X-rays. Furthermore, deep feature spaces are generated from the penultimate layers of the two models and then concatenated to get a single enriched boosted feature space. A conventional ML classifier exploits the enriched feature space to achieve better COVID-19 detection performance. The proposed COVID-19 detection frameworks are evaluated on radiologist's authenticated chest X-ray data, and their performance is compared with the well-established CNNs. It is observed through experiments that the proposed DBHL framework, which merges the two-deep CNN feature spaces, yields good performance (accuracy: 98.53%, sensitivity: 0.99, F-score: 0.98, and precision: 0.98). Furthermore, a web-based interface is developed, which takes only 5-10s to detect COVID-19 in each unseen chest X-ray image. This web-predictor is expected to help early diagnosis, save precious lives, and thus positively impact society.",2021,10.1016/j.compbiomed.2021.104816,cross-sectional,diagnosis,x-ray,Lung
COVID-19 detection in CT and CXR images using deep learning models,"Infectious diseases pose a threat to human life and could affect the whole world in a very short time. Corona-2019 virus disease (COVID-19) is an example of such harmful diseases. COVID-19 is a pandemic of an emerging infectious disease, called coronavirus disease 2019 or COVID-19, caused by the coronavirus SARS-CoV-2, which first appeared in December 2019 in Wuhan, China, before spreading around the world on a very large scale. The continued rise in the number of positive COVID-19 cases has disrupted the health care system in many countries, creating a lot of stress for governing bodies around the world, hence the need for a rapid way to identify cases of this disease. Medical imaging is a widely accepted technique for early detection and diagnosis of the disease which includes different techniques such as Chest X-ray (CXR), Computed Tomography (CT) scan, etc. In this paper, we propose a methodology to investigate the potential of deep transfer learning in building a classifier to detect COVID-19 positive patients using CT scan and CXR images. Data augmentation technique is used to increase the size of the training dataset in order to solve overfitting and enhance generalization ability of the model. Our contribution consists of a comprehensive evaluation of a series of pre-trained deep neural networks: ResNet50, InceptionV3, VGGNet-19, and Xception, using data augmentation technique. The findings proved that deep learning is effective at detecting COVID-19 cases. From the results of the experiments it was found that by considering each modality separately, the VGGNet-19 model outperforms the other three models proposed by using the CT image dataset where it achieved 88.5% precision, 86% recall, 86.5% F1-score, and 87% accuracy while the refined Xception version gave the highest precision, recall, F1-score, and accuracy values which equal 98% using CXR images dataset. On the other hand, and by applying the average of the two modalities X-ray and CT, VGG-19 presents the best score which is 90.5% for the accuracy and the F1-score, 90.3% for the recall while the precision is 91.5%. These results enables to automatize the process of analyzing chest CT scans and X-ray images with high accuracy and can be used in cases where RT-PCR testing and materials are limited.",2022,10.1007/s10522-021-09946-7,cross-sectional,diagnosis,CT & x-ray,Lung
COVID-19 detection in radiological text reports integrating entity recognition,"COVID-19 diagnosis is usually based on PCR test using radiological images, mainly chest Computed Tomography (CT) for the assessment of lung involvement by COVID-19. However, textual radiological reports also contain relevant information for determining the likelihood of presenting radiological signs of COVID-19 involving lungs. The development of COVID-19 automatic detection systems based on Natural Language Processing (NLP) techniques could provide a great help in supporting clinicians and detecting COVID-19 related disorders within radiological reports. In this paper we propose a text classification system based on the integration of different information sources. The system can be used to automatically predict whether or not a patient has radiological findings consistent with COVID-19 on the basis of radiological reports of chest CT. To carry out our experiments we use 295 radiological reports from chest CT studies provided by the ''HT médica"" clinic. All of them are radiological requests with suspicions of chest involvement by COVID-19. In order to train our text classification system we apply Machine Learning approaches and Named Entity Recognition. The system takes two sources of information as input: the text of the radiological report and COVID-19 related disorders extracted from SNOMED-CT. The best system is trained using SVM and the baseline results achieve 85% accuracy predicting lung involvement by COVID-19, which already offers competitive values that are difficult to overcome. Moreover, we apply mutual information in order to integrate the best quality information extracted from SNOMED-CT. In this way, we achieve around 90% accuracy improving the baseline results by 5 points.",2020,10.1016/j.compbiomed.2020.104066,,,,
COVID-19 detection using chest X-ray images based on a developed deep neural network,"AIM: Currently, a new coronavirus called COVID-19 is the biggest challenge of the human at 21st century. Now, the spread of this virus is such that mortality has risen strongly in all cities of countries. Therefore, it is necessary to think of a solution to handle the disease by fast and timely diagnosis. This paper proposes a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes. The aim of this study is to propose a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes. METHODS: 6 different databases from chest X-ray imagery that have been widely used in recent studies have been gathered for this aim. A Convolutional Neural Network-Long Short Time Memory model is designed and developed to extract features from raw data hierarchically. In order to make more realistic assumptions and use the Proposed Method in the practical field, white Gaussian noise is added to the raw chest X-ray imagery. Additionally, the proposed network is tested and investigated not only on 6 expressed databases but also on two additional databases. RESULTS: On the test set, the proposed network achieved an accuracy of more than 90% for all Scenarios excluding Scenario V, i.e. Healthy against the COVID-19 against the Viral, and also achieved 99% accuracy for separating the COVID-19 from the Healthy group. The results showed that the proposed network is robust to noise up to 1 dB. It is worth noting that the proposed network for two additional databases, which were only used as test databases, also achieved more than 90% accuracy. In addition, in comparison to the state-of-the-art pneumonia detection approaches, the final results obtained from the proposed network is so promising. CONCLUSIONS: The proposed network is effective in detecting COVID-19 and other lung infectious diseases using chest X-ray imagery and can thus assist radiologists in making rapid and accurate detections.",2022,10.1016/j.slast.2021.10.011,cross-sectional,diagnosis,x-ray,Lung
COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches,"Coronavirus causes a wide variety of respiratory infections and it is an RNA-type virus that can infect both humans and animal species. It often causes pneumonia in humans. Artificial intelligence models have been helpful for successful analyses in the biomedical field. In this study, Coronavirus was detected using a deep learning model, which is a sub-branch of artificial intelligence. Our dataset consists of three classes namely: coronavirus, pneumonia, and normal X-ray imagery. In this study, the data classes were restructured using the Fuzzy Color technique as a preprocessing step and the images that were structured with the original images were stacked. In the next step, the stacked dataset was trained with deep learning models (MobileNetV2, SqueezeNet) and the feature sets obtained by the models were processed using the Social Mimic optimization method. Thereafter, efficient features were combined and classified using Support Vector Machines (SVM). The overall classification rate obtained with the proposed approach was 99.27%. With the proposed approach in this study, it is evident that the model can efficiently contribute to the detection of COVID-19 disease.",2020,10.1016/j.compbiomed.2020.103805,cross-sectional,diagnosis,x-ray,Lung
COVID-19 detection using federated machine learning,"The current COVID-19 pandemic threatens human life, health, and productivity. AI plays an essential role in COVID-19 case classification as we can apply machine learning models on COVID-19 case data to predict infectious cases and recovery rates using chest x-ray. Accessing patient's private data violates patient privacy and traditional machine learning model requires accessing or transferring whole data to train the model. In recent years, there has been increasing interest in federated machine learning, as it provides an effective solution for data privacy, centralized computation, and high computation power. In this paper, we studied the efficacy of federated learning versus traditional learning by developing two machine learning models (a federated learning model and a traditional machine learning model)using Keras and TensorFlow federated, we used a descriptive dataset and chest x-ray (CXR) images from COVID-19 patients. During the model training stage, we tried to identify which factors affect model prediction accuracy and loss like activation function, model optimizer, learning rate, number of rounds, and data Size, we kept recording and plotting the model loss and prediction accuracy per each training round, to identify which factors affect the model performance, and we found that softmax activation function and SGD optimizer give better prediction accuracy and loss, changing the number of rounds and learning rate has slightly effect on model prediction accuracy and prediction loss but increasing the data size did not have any effect on model prediction accuracy and prediction loss. finally, we build a comparison between the proposed models' loss, accuracy, and performance speed, the results demonstrate that the federated machine learning model has a better prediction accuracy and loss but higher performance time than the traditional machine learning model.",2021,10.1371/journal.pone.0252573,cross-sectional,diagnosis,x-ray,Lung
COVID-19 diagnosis and severity detection from CT-images using transfer learning and back propagation neural network,"BACKGROUND: COVID-19 diagnosis in symptomatic patients is an important factor for arranging the necessary lifesaving facilities like ICU care and ventilator support. For this purpose, we designed a computer-aided diagnosis and severity detection method by using transfer learning and a back propagation neural network. METHOD: To increase the learning capability, we used data augmentation. Most of the previously done works in this area concentrate on private datasets, but we used two publicly available datasets. The first section diagnose COVID-19 from the input CT image using the transfer learning of the pre-trained network ResNet-50. We used ResNet-50 and DenseNet-201 pre-trained networks for feature extraction and trained a back propagation neural network to classify it into High, Medium, and Low severity. RESULTS: The proposed method for COVID-19 diagnosis gave an accuracy of 98.5% compared with the state-of-the-art methods. The experimental evaluation shows that combining the ResNet-50 and DenseNet-201 features gave more accurate results with the test data. The proposed system for COVID-19 severity detection gave better average classification accuracy of 97.84% compared with the state-of-the-art methods. This enables medical practitioners to identify the resources and treatment plans correctly. CONCLUSIONS: This work is useful in the medical field as a first-line severity risk detection that is helpful for medical personnel to plan patient care and assess the need for ICU facilities and ventilator support. A computer-aided system that is helpful to make a care plan for the huge amount of patient inflow each day is sure to be an asset in these turbulent times.",2021,10.1016/j.jiph.2021.07.015,cross-sectional,combined,CT,Lung
COVID-19 diagnosis by routine blood tests using machine learning,"Physicians taking care of patients with COVID-19 have described different changes in routine blood parameters. However, these changes hinder them from performing COVID-19 diagnoses. We constructed a machine learning model for COVID-19 diagnosis that was based and cross-validated on the routine blood tests of 5333 patients with various bacterial and viral infections, and 160 COVID-19-positive patients. We selected the operational ROC point at a sensitivity of 81.9% and a specificity of 97.9%. The cross-validated AUC was 0.97. The five most useful routine blood parameters for COVID-19 diagnosis according to the feature importance scoring of the XGBoost algorithm were: MCHC, eosinophil count, albumin, INR, and prothrombin activity percentage. t-SNE visualization showed that the blood parameters of the patients with a severe COVID-19 course are more like the parameters of a bacterial than a viral infection. The reported diagnostic accuracy is at least comparable and probably complementary to RT-PCR and chest CT studies. Patients with fever, cough, myalgia, and other symptoms can now have initial routine blood tests assessed by our diagnostic tool. All patients with a positive COVID-19 prediction would then undergo standard RT-PCR studies to confirm the diagnosis. We believe that our results represent a significant contribution to improvements in COVID-19 diagnosis.",2021,10.1038/s41598-021-90265-9,,,,
COVID-19 diagnosis from chest X-ray images using transfer learning: Enhanced performance by debiasing dataloader,"BACKGROUND: Chest X-ray imaging has been proved as a powerful diagnostic method to detect and diagnose COVID-19 cases due to its easy accessibility, lower cost and rapid imaging time. OBJECTIVE: This study aims to improve efficacy of screening COVID-19 infected patients using chest X-ray images with the help of a developed deep convolutional neural network model (CNN) entitled nCoV-NET. METHODS: To train and to evaluate the performance of the developed model, three datasets were collected from resources of ""ChestX-ray14"", ""COVID-19 image data collection"", and ""Chest X-ray collection from Indiana University,"" respectively. Overall, 299 COVID-19 pneumonia cases and 1,522 non-COVID 19 cases are involved in this study. To overcome the probable bias due to the unbalanced cases in two classes of the datasets, ResNet, DenseNet, and VGG architectures were re-trained in the fine-tuning stage of the process to distinguish COVID-19 classes using a transfer learning method. Lastly, the optimized final nCoV-NET model was applied to the testing dataset to verify the performance of the proposed model. RESULTS: Although the performance parameters of all re-trained architectures were determined close to each other, the final nCOV-NET model optimized by using DenseNet-161 architecture in the transfer learning stage exhibits the highest performance for classification of COVID-19 cases with the accuracy of 97.1 %. The Activation Mapping method was used to create activation maps that highlights the crucial areas of the radiograph to improve causality and intelligibility. CONCLUSION: This study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.",2021,10.3233/xst-200757,cross-sectional,diagnosis,x-ray,Lung
COVID-19 Diagnosis from CT Images with Convolutional Neural Network Optimized by Marine Predator Optimization Algorithm,"In recent years, almost every country in the world has struggled against the spread of Coronavirus Disease 2019. If governments and public health systems do not take action against the spread of the disease, it will have a severe impact on human life. A noteworthy technique to stop this pandemic is diagnosing COVID-19 infected patients and isolating them instantly. The present study proposes a method for the diagnosis of COVID-19 from CT images. The method is a hybrid method based on convolutional neural network which is optimized by a newly introduced metaheuristic, called marine predator optimization algorithm. This optimization method is performed to improve the system accuracy. The method is then implemented on the chest CT scans with the COVID-19-related findings (MosMedData) dataset, and the results are compared with three other methods from the literature to indicate the method's performance. The final results indicate that the proposed method with 98.11% accuracy, 98.13% precision, 98.66% sensitivity, and 97.26% F1 score has the highest performance in all indicators than the compared methods which shows its higher accuracy and reliability.",2021,10.1155/2021/5122962,cross-sectional,diagnosis,CT,Lung
COVID-19 diagnosis from CT scans and chest X-ray images using low-cost Raspberry Pi,"The diagnosis of COVID-19 is of vital demand. Several studies have been conducted to decide whether the chest X-ray and computed tomography (CT) scans of patients indicate COVID-19. While these efforts resulted in successful classification systems, the design of a portable and cost-effective COVID-19 diagnosis system has not been addressed yet. The memory requirements of the current state-of-the-art COVID-19 diagnosis systems are not suitable for embedded systems due to the required large memory size of these systems (e.g., hundreds of megabytes). Thus, the current work is motivated to design a similar system with minimal memory requirements. In this paper, we propose a diagnosis system using a Raspberry Pi Linux embedded system. First, local features are extracted using local binary pattern (LBP) algorithm. Second, the global features are extracted from the chest X-ray or CT scans using multi-channel fractional-order Legendre-Fourier moments (MFrLFMs). Finally, the most significant features (local and global) are selected. The proposed system steps are integrated to fit the low computational and memory capacities of the embedded system. The proposed method has the smallest computational and memory resources,less than the state-of-the-art methods by two to three orders of magnitude, among existing state-of-the-art deep learning (DL)-based methods.",2021,10.1371/journal.pone.0250688,,,,
COVID-19 diagnosis on CT scan images using a generative adversarial network and concatenated feature pyramid network with an attention mechanism,"OBJECTIVE: Coronavirus disease 2019 (COVID-19) has caused hundreds of thousands of infections and deaths. Efficient diagnostic methods could help curb its global spread. The purpose of this study was to develop and evaluate a method for accurately diagnosing COVID-19 based on computed tomography (CT) scans in real time. METHODS: We propose an architecture named ""concatenated feature pyramid network"" (""Concat-FPN"") with an attention mechanism, by concatenating feature maps of multiple. The proposed architecture is then used to form two networks, which we call COVID-CT-GAN and COVID-CT-DenseNet, the former for data augmentation and the latter for data classification. RESULTS: The proposed method is evaluated on 3 different numbers of magnitude of COVID-19 CT datasets. Compared with the method without GANs for data augmentation or the original network auxiliary classifier generative adversarial network, COVID-CT-GAN increases the accuracy by 2% to 3%, the recall by 2% to 4%, the precision by 1% to 3%, the F1-score by 1% to 3%, and the area under the curve by 1% to 4%. Compared with the original network DenseNet-201, COVID-CT-DenseNet increases the accuracy by 1% to 3%, the recall by 4% to 9%, the precision by 1%, the F1-score by 1% to 3%, and the area under the curve by 2%. CONCLUSION: The experimental results show that our method improves the efficiency of diagnosing COVID-19 on CT images, and helps overcome the problem of limited training data when using deep learning methods to diagnose COVID-19. SIGNIFICANCE: Our method can help clinicians build deep learning models using their private datasets to achieve automatic diagnosis of COVID-19 with a high precision.",2021,10.1002/mp.15044,cross-sectional,diagnosis,CT,Lung
COVID-19 Diagnosis Using an Enhanced Inception-ResNetV2 Deep Learning Model in CXR Images,"The COVID-19 pandemic has a significant negative effect on people's health, as well as on the world's economy. Polymerase chain reaction (PCR) is one of the main tests used to detect COVID-19 infection. However, it is expensive, time-consuming, and lacks sufficient accuracy. In recent years, convolutional neural networks have grabbed many researchers' attention in the machine learning field, due to its high diagnosis accuracy, especially the medical image recognition. Many architectures such as Inception, ResNet, DenseNet, and VGG16 have been proposed and gained an excellent performance at a low computational cost. Moreover, in a way to accelerate the training of these traditional architectures, residual connections are combined with inception architecture. Therefore, many hybrid architectures such as Inception-ResNetV2 are further introduced. This paper proposes an enhanced Inception-ResNetV2 deep learning model that can diagnose chest X-ray (CXR) scans with high accuracy. Besides, a Grad-CAM algorithm is used to enhance the visualization of the infected regions of the lungs in CXR images. Compared with state-of-the-art methods, our proposed paper proves superiority in terms of accuracy, recall, precision, and F1-measure.",2021,10.1155/2021/6658058,cross-sectional,diagnosis,x-ray,Lung
COVID-19 disease diagnosis from paper-based ECG trace image data using a novel convolutional neural network model,"Clinical reports show that COVID-19 disease has impacts on the cardiovascular system in addition to the respiratory system. Available COVID-19 diagnostic methods have been shown to have limitations. In addition to current diagnostic methods such as low-sensitivity standard RT-PCR tests and expensive medical imaging devices, the development of alternative methods for the diagnosis of COVID-19 disease would be beneficial for control of the COVID-19 pandemic. Further, it is important to quickly and accurately detect abnormalities caused by COVID-19 on the cardiovascular system via ECG. In this study, the diagnosis of COVID-19 disease is proposed using a novel deep Convolutional Neural Network model by using only ECG trace images created from ECG signals of COVID-19 infected patients based on the abnormalities caused by the COVID-19 virus on the cardiovascular system. An overall classification accuracy of 98.57%, 93.20%, 96.74% and AUC value of 0.9966, 0.9771, 0.9905 is achieved for COVID-19 vs. Normal, COVID-19 vs. Abnormal Heartbeats, COVID-19 vs. Myocardial Infarction binary classification tasks, respectively. In addition, an overall classification accuracy of 86.55% and 83.05% is achieved for COVID-19 vs. Abnormal Heartbeats vs. Myocardial Infarction and Normal vs. COVID-19 vs. Abnormal Heartbeats vs. Myocardial Infarction multi-classification tasks. This study is believed to have great potential to speed up the diagnosis and treatment of COVID-19 patients, saving clinicians time and facilitating the control of the pandemic.",2022,10.1007/s13246-022-01102-w,,,,
COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios,"BACKGROUND AND OBJECTIVE: The COVID-19 can cause severe pneumonia and is estimated to have a high impact on the healthcare system. Early diagnosis is crucial for correct treatment in order to possibly reduce the stress in the healthcare system. The standard image diagnosis tests for pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. Although CT scan is the gold standard, CXR are still useful because it is cheaper, faster and more widespread. This study aims to identify pneumonia caused by COVID-19 from other types and also healthy lungs using only CXR images. METHODS: In order to achieve the objectives, we have proposed a classification schema considering the following perspectives: i) a multi-class classification; ii) hierarchical classification, since pneumonia can be structured as a hierarchy. Given the natural data imbalance in this domain, we also proposed the use of resampling algorithms in the schema in order to re-balance the classes distribution. We observed that, texture is one of the main visual attributes of CXR images, our classification schema extract features using some well-known texture descriptors and also using a pre-trained CNN model. We also explored early and late fusion techniques in the schema in order to leverage the strength of multiple texture descriptors and base classifiers at once. To evaluate the approach, we composed a database, named RYDLS-20, containing CXR images of pneumonia caused by different pathogens as well as CXR images of healthy lungs. The classes distribution follows a real-world scenario in which some pathogens are more common than others. RESULTS: The proposed approach tested in RYDLS-20 achieved a macro-avg F1-Score of 0.65 using a multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in the hierarchical classification scenario. CONCLUSIONS: As far as we know, the top identification rate obtained in this paper is the best nominal rate obtained for COVID-19 identification in an unbalanced environment with more than three classes. We must also highlight the novel proposed hierarchical classification approach for this task, which considers the types of pneumonia caused by the different pathogens and lead us to the best COVID-19 recognition rate obtained here.",2020,10.1016/j.cmpb.2020.105532,cross-sectional,diagnosis,x-ray,Lung
COVID-19 image classification using deep features and fractional-order marine predators algorithm,"Currently, we witness the severe spread of the pandemic of the new Corona virus, COVID-19, which causes dangerous symptoms to humans and animals, its complications may lead to death. Although convolutional neural networks (CNNs) is considered the current state-of-the-art image classification technique, it needs massive computational cost for deployment and training. In this paper, we propose an improved hybrid classification approach for COVID-19 images by combining the strengths of CNNs (using a powerful architecture called Inception) to extract features and a swarm-based feature selection algorithm (Marine Predators Algorithm) to select the most relevant features. A combination of fractional-order and marine predators algorithm (FO-MPA) is considered an integration among a robust tool in mathematics named fractional-order calculus (FO). The proposed approach was evaluated on two public COVID-19 X-ray datasets which achieves both high performance and reduction of computational complexity. The two datasets consist of X-ray COVID-19 images by international Cardiothoracic radiologist, researchers and others published on Kaggle. The proposed approach selected successfully 130 and 86 out of 51 K features extracted by inception from dataset 1 and dataset 2, while improving classification accuracy at the same time. The results are the best achieved on these datasets when compared to a set of recent feature selection algorithms. By achieving 98.7%, 98.2% and 99.6%, 99% of classification accuracy and F-Score for dataset 1 and dataset 2, respectively, the proposed approach outperforms several CNNs and all recent works on COVID-19 images.",2020,10.1038/s41598-020-71294-2,cross-sectional,diagnosis,x-ray,Lung
COVID-19 Image Segmentation Based on Deep Learning and Ensemble Learning,"Medical imaging offers great potential for COVID-19 diagnosis and monitoring. Our work introduces an automated pipeline to segment areas of COVID-19 infection in CT scans using deep convolutional neural networks. Furthermore, we evaluate the performance impact of ensemble learning techniques (Bagging and Augmenting). Our models showed highly accurate segmentation results, in which Bagging achieved the highest dice similarity coefficient.",2021,10.3233/shti210223,cross-sectional,diagnosis,CT,Lung
COVID-19 in CXR: From Detection and Severity Scoring to Patient Disease Monitoring,"This work estimates the severity of pneumonia in COVID-19 patients and reports the findings of a longitudinal study of disease progression. It presents a deep learning model for simultaneous detection and localization of pneumonia in chest Xray (CXR) images, which is shown to generalize to COVID-19 pneumonia. The localization maps are utilized to calculate a ""Pneumonia Ratio"" which indicates disease severity. The assessment of disease severity serves to build a temporal disease extent profile for hospitalized patients. To validate the model's applicability to the patient monitoring task, we developed a validation strategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs - synthetic Xray) from serial CT scans; we then compared the disease progression profiles that were generated from the DRRs to those that were generated from CT volumes.",2021,10.1109/jbhi.2021.3069169,cross-sectional,diagnosis,CT + x-ray,Lung
COVID-19 infection localization and severity grading from chest X-ray images,"The immense spread of coronavirus disease 2019 (COVID-19) has left healthcare systems incapable to diagnose and test patients at the required rate. Given the effects of COVID-19 on pulmonary tissues, chest radiographic imaging has become a necessity for screening and monitoring the disease. Numerous studies have proposed Deep Learning approaches for the automatic diagnosis of COVID-19. Although these methods achieved outstanding performance in detection, they have used limited chest X-ray (CXR) repositories for evaluation, usually with a few hundred COVID-19 CXR images only. Thus, such data scarcity prevents reliable evaluation of Deep Learning models with the potential of overfitting. In addition, most studies showed no or limited capability in infection localization and severity grading of COVID-19 pneumonia. In this study, we address this urgent need by proposing a systematic and unified approach for lung segmentation and COVID-19 localization with infection quantification from CXR images. To accomplish this, we have constructed the largest benchmark dataset with 33,920 CXR images, including 11,956 COVID-19 samples, where the annotation of ground-truth lung segmentation masks is performed on CXRs by an elegant human-machine collaborative approach. An extensive set of experiments was performed using the state-of-the-art segmentation networks, U-Net, U-Net++, and Feature Pyramid Networks (FPN). The developed network, after an iterative process, reached a superior performance for lung region segmentation with Intersection over Union (IoU) of 96.11% and Dice Similarity Coefficient (DSC) of 97.99%. Furthermore, COVID-19 infections of various shapes and types were reliably localized with 83.05% IoU and 88.21% DSC. Finally, the proposed approach has achieved an outstanding COVID-19 detection performance with both sensitivity and specificity values above 99%.",2021,10.1016/j.compbiomed.2021.105002,cross-sectional,diagnosis,x-ray,Lung
COVID-19 lung infection segmentation with a novel two-stage cross-domain transfer learning framework,"With the global outbreak of COVID-19 in early 2020, rapid diagnosis of COVID-19 has become the urgent need to control the spread of the epidemic. In clinical settings, lung infection segmentation from computed tomography (CT) images can provide vital information for the quantification and diagnosis of COVID-19. However, accurate infection segmentation is a challenging task due to (i) the low boundary contrast between infections and the surroundings, (ii) large variations of infection regions, and, most importantly, (iii) the shortage of large-scale annotated data. To address these issues, we propose a novel two-stage cross-domain transfer learning framework for the accurate segmentation of COVID-19 lung infections from CT images. Our framework consists of two major technical innovations, including an effective infection segmentation deep learning model, called nCoVSegNet, and a novel two-stage transfer learning strategy. Specifically, our nCoVSegNet conducts effective infection segmentation by taking advantage of attention-aware feature fusion and large receptive fields, aiming to resolve the issues related to low boundary contrast and large infection variations. To alleviate the shortage of the data, the nCoVSegNet is pre-trained using a two-stage cross-domain transfer learning strategy, which makes full use of the knowledge from natural images (i.e., ImageNet) and medical images (i.e., LIDC-IDRI) to boost the final training on CT images with COVID-19 infections. Extensive experiments demonstrate that our framework achieves superior segmentation accuracy and outperforms the cutting-edge models, both quantitatively and qualitatively.",2021,10.1016/j.media.2021.102205,cross-sectional,diagnosis,CT,Lung
COVID-19 on Chest Radiographs: A Multireader Evaluation of an Artificial Intelligence System,"Background Chest radiography may play an important role in triage for coronavirus disease 2019 (COVID-19), particularly in low-resource settings. Purpose To evaluate the performance of an artificial intelligence (AI) system for detection of COVID-19 pneumonia on chest radiographs. Materials and Methods An AI system (CAD4COVID-XRay) was trained on 24 678 chest radiographs, including 1540 used only for validation while training. The test set consisted of a set of continuously acquired chest radiographs (n = 454) obtained in patients suspected of having COVID-19 pneumonia between March 4 and April 6, 2020, at one center (223 patients with positive reverse transcription polymerase chain reaction [RT-PCR] results, 231 with negative RT-PCR results). Radiographs were independently analyzed by six readers and by the AI system. Diagnostic performance was analyzed with the receiver operating characteristic curve. Results For the test set, the mean age of patients was 67 years ± 14.4 (standard deviation) (56% male). With RT-PCR test results as the reference standard, the AI system correctly classified chest radiographs as COVID-19 pneumonia with an area under the receiver operating characteristic curve of 0.81. The system significantly outperformed each reader (P < .001 using the McNemar test) at their highest possible sensitivities. At their lowest sensitivities, only one reader significantly outperformed the AI system (P = .04). Conclusion The performance of an artificial intelligence system in the detection of coronavirus disease 2019 on chest radiographs was comparable with that of six independent readers. © RSNA, 2020.",2020,10.1148/radiol.2020201874,cross-sectional,diagnosis,x-ray,Lung
COVID-19 pneumonia and the pulmonary vasculature: a marriage made in hell,Quantitative CT of the pulmonary vasculature is potentially important in COVID-19 associated pneumonia https://bit.ly/3vUTTRM,2021,10.1183/13993003.00811-2021,cross-sectional,prognosis,CT,thoracic vessels
COVID-19 Pneumonia Diagnosis Using a Simple 2D Deep Learning Framework With a Single Chest CT Image: Model Development and Validation,"BACKGROUND: Coronavirus disease (COVID-19) has spread explosively worldwide since the beginning of 2020. According to a multinational consensus statement from the Fleischner Society, computed tomography (CT) is a relevant screening tool due to its higher sensitivity for detecting early pneumonic changes. However, physicians are extremely occupied fighting COVID-19 in this era of worldwide crisis. Thus, it is crucial to accelerate the development of an artificial intelligence (AI) diagnostic tool to support physicians. OBJECTIVE: We aimed to rapidly develop an AI technique to diagnose COVID-19 pneumonia in CT images and differentiate it from non-COVID-19 pneumonia and nonpneumonia diseases. METHODS: A simple 2D deep learning framework, named the fast-track COVID-19 classification network (FCONet), was developed to diagnose COVID-19 pneumonia based on a single chest CT image. FCONet was developed by transfer learning using one of four state-of-the-art pretrained deep learning models (VGG16, ResNet-50, Inception-v3, or Xception) as a backbone. For training and testing of FCONet, we collected 3993 chest CT images of patients with COVID-19 pneumonia, other pneumonia, and nonpneumonia diseases from Wonkwang University Hospital, Chonnam National University Hospital, and the Italian Society of Medical and Interventional Radiology public database. These CT images were split into a training set and a testing set at a ratio of 8:2. For the testing data set, the diagnostic performance of the four pretrained FCONet models to diagnose COVID-19 pneumonia was compared. In addition, we tested the FCONet models on an external testing data set extracted from embedded low-quality chest CT images of COVID-19 pneumonia in recently published papers. RESULTS: Among the four pretrained models of FCONet, ResNet-50 showed excellent diagnostic performance (sensitivity 99.58%, specificity 100.00%, and accuracy 99.87%) and outperformed the other three pretrained models in the testing data set. In the additional external testing data set using low-quality CT images, the detection accuracy of the ResNet-50 model was the highest (96.97%), followed by Xception, Inception-v3, and VGG16 (90.71%, 89.38%, and 87.12%, respectively). CONCLUSIONS: FCONet, a simple 2D deep learning framework based on a single chest CT image, provides excellent diagnostic performance in detecting COVID-19 pneumonia. Based on our testing data set, the FCONet model based on ResNet-50 appears to be the best model, as it outperformed other FCONet models based on VGG16, Xception, and Inception-v3.",2020,10.2196/19569,cross-sectional,diagnosis,CT,Lung
"COVID-19 prognostic modeling using CT radiomic features and machine learning algorithms: Analysis of a multi-institutional dataset of 14,339 patients","BACKGROUND: We aimed to analyze the prognostic power of CT-based radiomics models using data of 14,339 COVID-19 patients. METHODS: Whole lung segmentations were performed automatically using a deep learning-based model to extract 107 intensity and texture radiomics features. We used four feature selection algorithms and seven classifiers. We evaluated the models using ten different splitting and cross-validation strategies, including non-harmonized and ComBat-harmonized datasets. The sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were reported. RESULTS: In the test dataset (4,301) consisting of CT and/or RT-PCR positive cases, AUC, sensitivity, and specificity of 0.83 ± 0.01 (CI95%: 0.81-0.85), 0.81, and 0.72, respectively, were obtained by ANOVA feature selector + Random Forest (RF) classifier. Similar results were achieved in RT-PCR-only positive test sets (3,644). In ComBat harmonized dataset, Relief feature selector + RF classifier resulted in the highest performance of AUC, reaching 0.83 ± 0.01 (CI95%: 0.81-0.85), with a sensitivity and specificity of 0.77 and 0.74, respectively. ComBat harmonization did not depict statistically significant improvement compared to a non-harmonized dataset. In leave-one-center-out, the combination of ANOVA feature selector and RF classifier resulted in the highest performance. CONCLUSION: Lung CT radiomics features can be used for robust prognostic modeling of COVID-19. The predictive power of the proposed CT radiomics model is more reliable when using a large multicentric heterogeneous dataset, and may be used prospectively in clinical setting to manage COVID-19 patients.",2022,10.1016/j.compbiomed.2022.105467,cross-sectional,prognosis,CT,Lung
COVID-19 Screening in Chest X-Ray Images Using Lung Region Priors,"Early screening of COVID-19 is essential for pandemic control, and thus to relieve stress on the health care system. Lung segmentation from chest X-ray (CXR) is a promising method for early diagnoses of pulmonary diseases. Recently, deep learning has achieved great success in supervised lung segmentation. However, how to effectively utilize the lung region in screening COVID-19 still remains a challenge due to domain shift and lack of manual pixel-level annotations. We hereby propose a multi-appearance COVID-19 screening framework by using lung region priors derived from CXR images. Firstly, we propose a multi-scale adversarial domain adaptation network (MS-AdaNet) to boost the cross-domain lung segmentation task as the prior knowledge to the classification network. Then, we construct a multi-appearance network (MA-Net), which is composed of three sub-networks to realize multi-appearance feature extraction and fusion using lung region priors. At last, we can obtain prediction results from normal, viral pneumonia, and COVID-19 using the proposed MA-Net. We extend the proposed MS-AdaNet for lung segmentation task on three different public CXR datasets. The results suggest that the MS-AdaNet outperforms contrastive methods in cross-domain lung segmentation. Moreover, experiments reveal that the proposed MA-Net achieves accuracy of 98.83 % and F1-score of 98.71 % on COVID-19 screening. The results indicate that the proposed MA-Net can obtain significant performance on COVID-19 screening.",2021,10.1109/jbhi.2021.3104629,cross-sectional,diagnosis,x-ray,Lung
Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks,"In this study, a dataset of X-ray images from patients with common bacterial pneumonia, confirmed Covid-19 disease, and normal incidents, was utilized for the automatic detection of the Coronavirus disease. The aim of the study is to evaluate the performance of state-of-the-art convolutional neural network architectures proposed over the recent years for medical image classification. Specifically, the procedure called Transfer Learning was adopted. With transfer learning, the detection of various abnormalities in small medical image datasets is an achievable target, often yielding remarkable results. The datasets utilized in this experiment are two. Firstly, a collection of 1427 X-ray images including 224 images with confirmed Covid-19 disease, 700 images with confirmed common bacterial pneumonia, and 504 images of normal conditions. Secondly, a dataset including 224 images with confirmed Covid-19 disease, 714 images with confirmed bacterial and viral pneumonia, and 504 images of normal conditions. The data was collected from the available X-ray images on public medical repositories. The results suggest that Deep Learning with X-ray imaging may extract significant biomarkers related to the Covid-19 disease, while the best accuracy, sensitivity, and specificity obtained is 96.78%, 98.66%, and 96.46% respectively. Since by now, all diagnostic tests show failure rates such as to raise concerns, the probability of incorporating X-rays into the diagnosis of the disease could be assessed by the medical community, based on the findings, while more research to evaluate the X-ray approach from different aspects may be conducted.",2020,10.1007/s13246-020-00865-4,cross-sectional,diagnosis,x-ray,Lung
COVID-19: Challenges and its Technological Solutions using IoT,"COVID-19 is a global pandemic that has affected many countries in a short span of time. People worldwide are susceptible to this deadly disease. To control the prevailing havoc of coronavirus, researchers are adopting techniques like plasma therapy, proning, medicines, etc. To stop the rapid spread of COVID-19, contact tracing is one of the important ways to check the infected people. This paper explains the various challenges people and health practitioners are facing due to COVID-19. In this paper, various ways with which the impact of COVID-19 can be controlled using IoT technology have been discussed. A six-layer architecture of IoT solutions for containing the deadly COVID-19 has been proposed. In addition to this, the role of machine learning techniques for diagnosing COVID-19 has been discussed in this paper, and a quick explanation of the unmanned aerial vehicle (UAVs) applications for contact tracing has also been specified. From the study conducted, it is evident that IoT solutions can be used in various ways for restricting the impact of COVID-19. Furthermore, IoT can be used in the healthcare sector to assure people's safety and good health with minimal costs.",2022,10.2174/1573405617666210215143503,,,,
"COVID-19: Role of Robotics, Artificial Intelligence and Machine Learning During the Pandemic","The outbreak of COVID-19 has led to a global health emergency. Emerging from China, it has now been declared as a pandemic. Owing to the fast pace at which it spreads, its control and prevention have now become the greatest challenge. The inner structural analysis of the virus is an important area of research for the invention of the potential drug. The countries are following different strategies and policies to fight against COVID-19; various schemes have also been employed to cope up with the economic crisis. While the government is struggling to balance between the public health sector and the economic collapse, the researchers and medicine practitioners are inclined towards obtaining treatment and early detection of the deadly disease. Further, the impact of COVID-19 on Dentistry is alarming and posing severe threats to the professionals as well. Now, the technology is helping the countries fight against the disease. ML and AI based applications are substantially aiding the process of detection and diagnosis of novel coronavirus. Science of Robotics is another approach followed with an aim to improve patient care.",2022,10.2174/1573405617666210224115722,,,,
COVID-AL: The diagnosis of COVID-19 with deep active learning,"The efficient diagnosis of COVID-19 plays a key role in preventing the spread of this disease. The computer-aided diagnosis with deep learning methods can perform automatic detection of COVID-19 using CT scans. However, large scale annotation of CT scans is impossible because of limited time and heavy burden on the healthcare system. To meet the challenge, we propose a weakly-supervised deep active learning framework called COVID-AL to diagnose COVID-19 with CT scans and patient-level labels. The COVID-AL consists of the lung region segmentation with a 2D U-Net and the diagnosis of COVID-19 with a novel hybrid active learning strategy, which simultaneously considers sample diversity and predicted loss. With a tailor-designed 3D residual network, the proposed COVID-AL can diagnose COVID-19 efficiently and it is validated on a large CT scan dataset collected from the CC-CCII. The experimental results demonstrate that the proposed COVID-AL outperforms the state-of-the-art active learning approaches in the diagnosis of COVID-19. With only 30% of the labeled data, the COVID-AL achieves over 95% accuracy of the deep learning method using the whole dataset. The qualitative and quantitative analysis proves the effectiveness and efficiency of the proposed COVID-AL framework.",2021,10.1016/j.media.2020.101913,cross-sectional,diagnosis,CT,Lung
COVID-CCD-Net: COVID-19 and colon cancer diagnosis system with optimized CNN hyperparameters using gradient-based optimizer,"Coronavirus disease-2019 (COVID-19) is a new types of coronavirus which have turned into a pandemic within a short time. Reverse transcription-polymerase chain reaction (RT-PCR) test is used for the diagnosis of COVID-19 in national healthcare centers. Because the number of PCR test kits is often limited, it is sometimes difficult to diagnose the disease at an early stage. However, X-ray technology is accessible nearly all over the world, and it succeeds in detecting symptoms of COVID-19 more successfully. Another disease which affects people's lives to a great extent is colorectal cancer. Tissue microarray (TMA) is a technological method which is widely used for its high performance in the analysis of colorectal cancer. Computer-assisted approaches which can classify colorectal cancer in TMA images are also needed. In this respect, the present study proposes a convolutional neural network (CNN) classification approach with optimized parameters using gradient-based optimizer (GBO) algorithm. Thanks to the proposed approach, COVID-19, normal, and viral pneumonia in various chest X-ray images can be classified accurately. Additionally, other types such as epithelial and stromal regions in epidermal growth factor receptor (EFGR) colon in TMAs can also be classified. The proposed approach was called COVID-CCD-Net. AlexNet, DarkNet-19, Inception-v3, MobileNet, ResNet-18, and ShuffleNet architectures were used in COVID-CCD-Net, and the hyperparameters of this architecture was optimized for the proposed approach. Two different medical image classification datasets, namely, COVID-19 and Epistroma, were used in the present study. The experimental findings demonstrated that proposed approach increased the classification performance of the non-optimized CNN architectures significantly and displayed a very high classification performance even in very low value of epoch.",2022,10.1007/s11517-022-02553-9,cross-sectional,diagnosis,x-ray,Lung
COVID-Classifier: an automated machine learning model to assist in the diagnosis of COVID-19 infection in chest X-ray images,"Chest-X ray (CXR) radiography can be used as a first-line triage process for non-COVID-19 patients with pneumonia. However, the similarity between features of CXR images of COVID-19 and pneumonia caused by other infections makes the differential diagnosis by radiologists challenging. We hypothesized that machine learning-based classifiers can reliably distinguish the CXR images of COVID-19 patients from other forms of pneumonia. We used a dimensionality reduction method to generate a set of optimal features of CXR images to build an efficient machine learning classifier that can distinguish COVID-19 cases from non-COVID-19 cases with high accuracy and sensitivity. By using global features of the whole CXR images, we successfully implemented our classifier using a relatively small dataset of CXR images. We propose that our COVID-Classifier can be used in conjunction with other tests for optimal allocation of hospital resources by rapid triage of non-COVID-19 cases.",2021,10.1038/s41598-021-88807-2,cross-sectional,diagnosis,x-ray,Lung
COVID-Transformer: Interpretable COVID-19 Detection Using Vision Transformer for Healthcare,"In the recent pandemic, accurate and rapid testing of patients remained a critical task in the diagnosis and control of COVID-19 disease spread in the healthcare industry. Because of the sudden increase in cases, most countries have faced scarcity and a low rate of testing. Chest X-rays have been shown in the literature to be a potential source of testing for COVID-19 patients, but manually checking X-ray reports is time-consuming and error-prone. Considering these limitations and the advancements in data science, we proposed a Vision Transformer-based deep learning pipeline for COVID-19 detection from chest X-ray-based imaging. Due to the lack of large data sets, we collected data from three open-source data sets of chest X-ray images and aggregated them to form a 30 K image data set, which is the largest publicly available collection of chest X-ray images in this domain to our knowledge. Our proposed transformer model effectively differentiates COVID-19 from normal chest X-rays with an accuracy of 98% along with an AUC score of 99% in the binary classification task. It distinguishes COVID-19, normal, and pneumonia patient's X-rays with an accuracy of 92% and AUC score of 98% in the Multi-class classification task. For evaluation on our data set, we fine-tuned some of the widely used models in literature, namely, EfficientNetB0, InceptionV3, Resnet50, MobileNetV3, Xception, and DenseNet-121, as baselines. Our proposed transformer model outperformed them in terms of all metrics. In addition, a Grad-CAM based visualization is created which makes our approach interpretable by radiologists and can be used to monitor the progression of the disease in the affected lungs, assisting healthcare.",2021,10.3390/ijerph182111086,cross-sectional,diagnosis,x-ray,Lung
COVID-view: Diagnosis of COVID-19 using Chest CT,"Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.",2022,10.1109/tvcg.2021.3114851,cross-sectional,diagnosis,CT,Lung
COVID19-CT-dataset: an open-access chest CT image repository of 1000+ patients with confirmed COVID-19 diagnosis,"OBJECTIVES: The ongoing Coronavirus disease 2019 (COVID-19) pandemic has drastically impacted the global health and economy. Computed tomography (CT) is the prime imaging modality for diagnosis of lung infections in COVID-19 patients. Data-driven and Artificial intelligence (AI)-powered solutions for automatic processing of CT images predominantly rely on large-scale, heterogeneous datasets. Owing to privacy and data availability issues, open-access and publicly available COVID-19 CT datasets are difficult to obtain, thus limiting the development of AI-enabled automatic diagnostic solutions. To tackle this problem, large CT image datasets encompassing diverse patterns of lung infections are in high demand. DATA DESCRIPTION: In the present study, we provide an open-source repository containing 1000+ CT images of COVID-19 lung infections established by a team of board-certified radiologists. CT images were acquired from two main general university hospitals in Mashhad, Iran from March 2020 until January 2021. COVID-19 infections were ratified with matching tests including Reverse transcription polymerase chain reaction (RT-PCR) and accompanying clinical symptoms. All data are 16-bit grayscale images composed of 512 × 512 pixels and are stored in DICOM standard. Patient privacy is preserved by removing all patient-specific information from image headers. Subsequently, all images corresponding to each patient are compressed and stored in RAR format.",2021,10.1186/s13104-021-05592-x,,,,
COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images,"The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a major pandemic outbreak recently. Various diagnostic technologies have been under active development. The novel coronavirus disease (COVID-19) may induce pulmonary failures, and chest X-ray imaging becomes one of the major confirmed diagnostic technologies. The very limited number of publicly available samples has rendered the training of the deep neural networks unstable and inaccurate. This study proposed a two-step transfer learning pipeline and a deep residual network framework COVID19XrayNet for the COVID-19 detection problem based on chest X-ray images. COVID19XrayNet firstly tunes the transferred model on a large dataset of chest X-ray images, which is further tuned using a small dataset of annotated chest X-ray images. The final model achieved 0.9108 accuracy. The experimental data also suggested that the model may be improved with more training samples being released. COVID19XrayNet, a two-step transfer learning framework designed for biomedical images.",2020,10.1007/s12539-020-00393-5,cross-sectional,diagnosis,x-ray,Lung
COVIDGR Dataset and COVID-SDNet Methodology for Predicting COVID-19 Based on Chest X-Ray Images,"Currently, Coronavirus disease (COVID-19), one of the most infectious diseases in the 21st century, is diagnosed using RT-PCR testing, CT scans and/or Chest X-Ray (CXR) images. CT (Computed Tomography) scanners and RT-PCR testing are not available in most medical centers and hence in many cases CXR images become the most time/cost effective tool for assisting clinicians in making decisions. Deep learning neural networks have a great potential for building COVID-19 triage systems and detecting COVID-19 patients, especially patients with low severity. Unfortunately, current databases do not allow building such systems as they are highly heterogeneous and biased towards severe cases. This article is three-fold: (i) we demystify the high sensitivities achieved by most recent COVID-19 classification models, (ii) under a close collaboration with Hospital Universitario Clínico San Cecilio, Granada, Spain, we built COVIDGR-1.0, a homogeneous and balanced database that includes all levels of severity, from normal with Positive RT-PCR, Mild, Moderate to Severe. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior) CXR views and (iii) we propose COVID Smart Data based Network (COVID-SDNet) methodology for improving the generalization capacity of COVID-classification models. Our approach reaches good and stable results with an accuracy of [Formula: see text], [Formula: see text], [Formula: see text] in severe, moderate and mild COVID-19 severity levels. Our approach could help in the early detection of COVID-19. COVIDGR-1.0 along with the severity level labels are available to the scientific community through this link https://dasci.es/es/transferencia/open-data/covidgr/.",2020,10.1109/jbhi.2020.3037127,cross-sectional,diagnosis,x-ray,Lung
COVIDiag: a clinical CAD system to diagnose COVID-19 pneumonia based on CT findings,"OBJECTIVES: CT findings of COVID-19 look similar to other atypical and viral (non-COVID-19) pneumonia diseases. This study proposes a clinical computer-aided diagnosis (CAD) system using CT features to automatically discriminate COVID-19 from non-COVID-19 pneumonia patients. METHODS: Overall, 612 patients (306 COVID-19 and 306 non-COVID-19 pneumonia) were recruited. Twenty radiological features were extracted from CT images to evaluate the pattern, location, and distribution of lesions of patients in both groups. All significant CT features were fed in five classifiers namely decision tree, K-nearest neighbor, naïve Bayes, support vector machine, and ensemble to evaluate the best performing CAD system in classifying COVID-19 and non-COVID-19 cases. RESULTS: Location and distribution pattern of involvement, number of the lesion, ground-glass opacity (GGO) and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features to classify COVID-19 from non-COVID-19 groups. Our proposed CAD system obtained the sensitivity, specificity, and accuracy of 0.965, 93.54%, 90.32%, and 91.94%, respectively, using ensemble (COVIDiag) classifier. CONCLUSIONS: This study proposed a COVIDiag model obtained promising results using CT radiological routine features. It can be considered an adjunct tool by the radiologists during the current COVID-19 pandemic to make an accurate diagnosis. KEY POINTS: • Location and distribution of involvement, number of lesions, GGO and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features between COVID-19 from non-COVID-19 groups. • The proposed CAD system, COVIDiag, could diagnose COVID-19 pneumonia cases with an AUC of 0.965 (sensitivity = 93.54%; specificity = 90.32%; and accuracy = 91.94%). • The AUC, sensitivity, specificity, and accuracy obtained by radiologist diagnosis are 0.879, 87.10%, 88.71%, and 87.90%, respectively.",2021,10.1007/s00330-020-07087-y,cross-sectional,diagnosis,CT,Lung
COVIDiagnosis-Net: Deep Bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from X-ray images,"The Coronavirus Disease 2019 (COVID-19) outbreak has a tremendous impact on global health and the daily life of people still living in more than two hundred countries. The crucial action to gain the force in the fight of COVID-19 is to have powerful monitoring of the site forming infected patients. Most of the initial tests rely on detecting the genetic material of the coronavirus, and they have a poor detection rate with the time-consuming operation. In the ongoing process, radiological imaging is also preferred where chest X-rays are highlighted in the diagnosis. Early studies express the patients with an abnormality in chest X-rays pointing to the presence of the COVID-19. On this motivation, there are several studies cover the deep learning-based solutions to detect the COVID-19 using chest X-rays. A part of the existing studies use non-public datasets, others perform on complicated Artificial Intelligent (AI) structures. In our study, we demonstrate an AI-based structure to outperform the existing studies. The SqueezeNet that comes forward with its light network design is tuned for the COVID-19 diagnosis with Bayesian optimization additive. Fine-tuned hyperparameters and augmented dataset make the proposed network perform much better than existing network designs and to obtain a higher COVID-19 diagnosis accuracy.",2020,10.1016/j.mehy.2020.109761,cross-sectional,diagnosis,x-ray,Lung
CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR,"To mitigate the spread of the current coronavirus disease 2019 (COVID-19) pandemic, it is crucial to have an effective screening of infected patients to be isolated and treated. Chest X-Ray (CXR) radiological imaging coupled with Artificial Intelligence (AI) applications, in particular Convolutional Neural Network (CNN), can speed the COVID-19 diagnostic process. In this paper, we optimize the data augmentation and the CNN hyperparameters for detecting COVID-19 from CXRs in terms of validation accuracy. This optimization increases the accuracy of the popular CNN architectures such as the Visual Geometry Group network (VGG-19) and the Residual Neural Network (ResNet-50), by 11.93% and 4.97%, respectively. We then proposed CovidXrayNet model that is based on EfficientNet-B0 and our optimization results. We evaluated CovidXrayNet on two datasets, including our generated balanced COVIDcxr dataset (960 CXRs) and the benchmark COVIDx dataset (15,496 CXRs). With only 30 epochs of training, CovidXrayNet achieves state-of-the-art accuracy of 95.82% on the COVIDx dataset in the three-class classification task (COVID-19, normal or pneumonia). The CovidXRayNet model, the COVIDcxr dataset, and several optimization experiments are publicly available at https://github.com/MaramMonshi/CovidXrayNet.",2021,10.1016/j.compbiomed.2021.104375,cross-sectional,diagnosis,x-ray,Lung
CovXNet: A multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization,"With the recent outbreak of COVID-19, fast diagnostic testing has become one of the major challenges due to the critical shortage of test kit. Pneumonia, a major effect of COVID-19, needs to be urgently diagnosed along with its underlying reasons. In this paper, deep learning aided automated COVID-19 and other pneumonia detection schemes are proposed utilizing a small amount of COVID-19 chest X-rays. A deep convolutional neural network (CNN) based architecture, named as CovXNet, is proposed that utilizes depthwise convolution with varying dilation rates for efficiently extracting diversified features from chest X-rays. Since the chest X-ray images corresponding to COVID-19 caused pneumonia and other traditional pneumonias have significant similarities, at first, a large number of chest X-rays corresponding to normal and (viral/bacterial) pneumonia patients are used to train the proposed CovXNet. Learning of this initial training phase is transferred with some additional fine-tuning layers that are further trained with a smaller number of chest X-rays corresponding to COVID-19 and other pneumonia patients. In the proposed method, different forms of CovXNets are designed and trained with X-ray images of various resolutions and for further optimization of their predictions, a stacking algorithm is employed. Finally, a gradient-based discriminative localization is integrated to distinguish the abnormal regions of X-ray images referring to different types of pneumonia. Extensive experimentations using two different datasets provide very satisfactory detection performance with accuracy of 97.4% for COVID/Normal, 96.9% for COVID/Viral pneumonia, 94.7% for COVID/Bacterial pneumonia, and 90.2% for multiclass COVID/normal/Viral/Bacterial pneumonias. Hence, the proposed schemes can serve as an efficient tool in the current state of COVID-19 pandemic. All the architectures are made publicly available at: https://github.com/Perceptron21/CovXNet.",2020,10.1016/j.compbiomed.2020.103869,cross-sectional,diagnosis,x-ray,Lung
Creating a training set for artificial intelligence from initial segmentations of airways,"Airways segmentation is important for research about pulmonary disease but require a large amount of time by trained specialists. We used an openly available software to improve airways segmentations obtained from an artificial intelligence (AI) tool and retrained the tool to get a better performance. Fifteen initial airway segmentations from low-dose chest computed tomography scans were obtained with a 3D-Unet AI tool previously trained on Danish Lung Cancer Screening Trial and Erasmus-MC Sophia datasets. Segmentations were manually corrected in 3D Slicer. The corrected airway segmentations were used to retrain the 3D-Unet. Airway measurements were automatically obtained and included count, airway length and luminal diameter per generation from the segmentations. Correcting segmentations required 2-4 h per scan. Manually corrected segmentations had more branches (p < 0.001), longer airways (p < 0.001) and smaller luminal diameters (p = 0.004) than initial segmentations. Segmentations from retrained 3D-Unets trended towards more branches and longer airways compared to the initial segmentations. The largest changes were seen in airways from 6th generation onwards. Manual correction results in significantly improved segmentations and is potentially a useful and time-efficient method to improve the AI tool performance on a specific hospital or research dataset.",2021,10.1186/s41747-021-00247-9,cross-sectional,diagnosis,CT,Lung
Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development,"We developed a rich dataset of Chest X-Ray (CXR) images to assist investigators in artificial intelligence. The data were collected using an eye-tracking system while a radiologist reviewed and reported on 1,083 CXR images. The dataset contains the following aligned data: CXR image, transcribed radiology report text, radiologist's dictation audio and eye gaze coordinates data. We hope this dataset can contribute to various areas of research particularly towards explainable and multimodal deep learning/machine learning methods. Furthermore, investigators in disease classification and localization, automated radiology report generation, and human-machine interaction can benefit from these data. We report deep learning experiments that utilize the attention maps produced by the eye gaze dataset to show the potential utility of this dataset.",2021,10.1038/s41597-021-00863-5,,,,
Cross-modality (CT-MRI) prior augmented deep learning for robust lung tumor segmentation from small MR datasets,"PURPOSE: Accurate tumor segmentation is a requirement for magnetic resonance (MR)-based radiotherapy. Lack of large expert annotated MR datasets makes training deep learning models difficult. Therefore, a cross-modality (MR-CT) deep learning segmentation approach that augments training data using pseudo MR images produced by transforming expert-segmented CT images was developed. METHODS: Eighty-one T2-weighted MRI scans from 28 patients with non-small cell lung cancers (nine with pretreatment and weekly MRI and the remainder with pre-treatment MRI scans) were analyzed. Cross-modality model encoding the transformation of CT to pseudo MR images resembling T2w MRI was learned as a generative adversarial deep learning network. This model was used to translate 377 expert segmented non-small cell lung cancer CT scans from the Cancer Imaging Archive into pseudo MRI that served as additional training set. This method was benchmarked against shallow learning using random forest, standard data augmentation, and three state-of-the art adversarial learning-based cross-modality data (pseudo MR) augmentation methods. Segmentation accuracy was computed using Dice similarity coefficient (DSC), Hausdorff distance metrics, and volume ratio. RESULTS: The proposed approach produced the lowest statistical variability in the intensity distribution between pseudo and T2w MR images measured as Kullback-Leibler divergence of 0.069. This method produced the highest segmentation accuracy with a DSC of (0.75 ± 0.12) and the lowest Hausdorff distance of (9.36 mm ± 6.00 mm) on the test dataset using a U-Net structure. This approach produced highly similar estimations of tumor growth as an expert (P = 0.37). CONCLUSIONS: A novel deep learning MR segmentation was developed that overcomes the limitation of learning robust models from small datasets by leveraging learned cross-modality information using a model that explicitly incorporates knowledge of tumors in modality translation to augment segmentation training. The results show the feasibility of the approach and the corresponding improvement over the state-of-the-art methods.",2019,10.1002/mp.13695,cross-sectional,treatment,CT,Lung
Crystallographic molecular replacement using an in silico-generated search model of SARS-CoV-2 ORF8,"The majority of crystal structures are determined by the method of molecular replacement (MR). The range of application of MR is limited mainly by the need for an accurate search model. In most cases, pre-existing experimentally determined structures are used as search models. In favorable cases, ab initio predicted structures have yielded search models adequate for MR. The ORF8 protein of SARS-CoV-2 represents a challenging case for MR using an ab initio prediction because ORF8 has an all β-sheet fold and few orthologs. We previously determined experimentally the structure of ORF8 using the single anomalous dispersion (SAD) phasing method, having been unable to find an MR solution to the crystallographic phase problem. Following a report of an accurate prediction of the ORF8 structure, we assessed whether the predicted model would have succeeded as an MR search model. A phase problem solution was found, and the resulting structure was refined, yielding structural parameters equivalent to the original experimental solution.",2021,10.1002/pro.4050,,,,
CT and clinical assessment in asymptomatic and pre-symptomatic patients with early SARS-CoV-2 in outbreak settings,"OBJECTIVES: The early infection dynamics of patients with SARS-CoV-2 are not well understood. We aimed to investigate and characterize associations between clinical, laboratory, and imaging features of asymptomatic and pre-symptomatic patients with SARS-CoV-2. METHODS: Seventy-four patients with RT-PCR-proven SARS-CoV-2 infection were asymptomatic at presentation. All were retrospectively identified from 825 patients with chest CT scans and positive RT-PCR following exposure or travel risks in outbreak settings in Japan and China. CTs were obtained for every patient within a day of admission and were reviewed for infiltrate subtypes and percent with assistance from a deep learning tool. Correlations of clinical, laboratory, and imaging features were analyzed and comparisons were performed using univariate and multivariate logistic regression. RESULTS: Forty-eight of 74 (65%) initially asymptomatic patients had CT infiltrates that pre-dated symptom onset by 3.8 days. The most common CT infiltrates were ground glass opacities (45/48; 94%) and consolidation (22/48; 46%). Patient body temperature (p < 0.01), CRP (p < 0.01), and KL-6 (p = 0.02) were associated with the presence of CT infiltrates. Infiltrate volume (p = 0.01), percent lung involvement (p = 0.01), and consolidation (p = 0.043) were associated with subsequent development of symptoms. CONCLUSIONS: COVID-19 CT infiltrates pre-dated symptoms in two-thirds of patients. Body temperature elevation and laboratory evaluations may identify asymptomatic patients with SARS-CoV-2 CT infiltrates at presentation, and the characteristics of CT infiltrates could help identify asymptomatic SARS-CoV-2 patients who subsequently develop symptoms. The role of chest CT in COVID-19 may be illuminated by a better understanding of CT infiltrates in patients with early disease or SARS-CoV-2 exposure. KEY POINTS: • Forty-eight of 74 (65%) pre-selected asymptomatic patients with SARS-CoV-2 had abnormal chest CT findings. • CT infiltrates pre-dated symptom onset by 3.8 days (range 1-5). • KL-6, CRP, and elevated body temperature identified patients with CT infiltrates. Higher infiltrate volume, percent lung involvement, and pulmonary consolidation identified patients who developed symptoms.",2021,10.1007/s00330-020-07401-8,cross-sectional,diagnosis,CT,Lung
CT Image Analysis and Clinical Diagnosis of New Coronary Pneumonia Based on Improved Convolutional Neural Network,"In this paper, based on the improved convolutional neural network, in-depth analysis of the CT image of the new coronary pneumonia, using the U-Net series of deep neural networks to semantically segment the CT image of the new coronary pneumonia, to obtain the new coronary pneumonia area as the foreground and the remaining areas as the background of the binary image, provides a basis for subsequent image diagnosis. Secondly, the target-detection framework Faster RCNN extracts features from the CT image of the new coronary pneumonia tumor, obtains a higher-level abstract representation of the data, determines the lesion location of the new coronary pneumonia tumor, and gives its bounding box in the image. By generating an adversarial network to diagnose the lesion area of the CT image of the new coronary pneumonia tumor, obtaining a complete image of the new coronary pneumonia, achieving the effect of the CT image diagnosis of the new coronary pneumonia tumor, and three-dimensionally reconstructing the complete new coronary pneumonia model, filling the current the gap in this aspect, provide a basis to produce new coronary pneumonia prosthesis and improve the accuracy of diagnosis.",2021,10.1155/2021/7259414,cross-sectional,diagnosis,CT,Lung
CT Image Conversion among Different Reconstruction Kernels without a Sinogram by Using a Convolutional Neural Network,"OBJECTIVE: The aim of our study was to develop and validate a convolutional neural network (CNN) architecture to convert CT images reconstructed with one kernel to images with different reconstruction kernels without using a sinogram. MATERIALS AND METHODS: This retrospective study was approved by the Institutional Review Board. Ten chest CT scans were performed and reconstructed with the B10f, B30f, B50f, and B70f kernels. The dataset was divided into six, two, and two examinations for training, validation, and testing, respectively. We constructed a CNN architecture consisting of six convolutional layers, each with a 3 × 3 kernel with 64 filter banks. Quantitative performance was evaluated using root mean square error (RMSE) values. To validate clinical use, image conversion was conducted on 30 additional chest CT scans reconstructed with the B30f and B50f kernels. The influence of image conversion on emphysema quantification was assessed with Bland-Altman plots. RESULTS: Our scheme rapidly generated conversion results at the rate of 0.065 s/slice. Substantial reduction in RMSE was observed in the converted images in comparison with the original images with different kernels (mean reduction, 65.7%; range, 29.5-82.2%). The mean emphysema indices for B30f, B50f, converted B30f, and converted B50f were 5.4 ± 7.2%, 15.3 ± 7.2%, 5.9 ± 7.3%, and 16.8 ± 7.5%, respectively. The 95% limits of agreement between B30f and other kernels (B50f and converted B30f) ranged from -14.1% to -2.6% (mean, -8.3%) and -2.3% to 0.7% (mean, -0.8%), respectively. CONCLUSION: CNN-based CT kernel conversion shows adequate performance with high accuracy and speed, indicating its potential clinical use.",2019,10.3348/kjr.2018.0249,cross-sectional,informatics,CT,Lung
CT image segmentation for inflamed and fibrotic lungs using a multi-resolution convolutional neural network,"The purpose of this study was to develop a fully-automated segmentation algorithm, robust to various density enhancing lung abnormalities, to facilitate rapid quantitative analysis of computed tomography images. A polymorphic training approach is proposed, in which both specifically labeled left and right lungs of humans with COPD, and nonspecifically labeled lungs of animals with acute lung injury, were incorporated into training a single neural network. The resulting network is intended for predicting left and right lung regions in humans with or without diffuse opacification and consolidation. Performance of the proposed lung segmentation algorithm was extensively evaluated on CT scans of subjects with COPD, confirmed COVID-19, lung cancer, and IPF, despite no labeled training data of the latter three diseases. Lobar segmentations were obtained using the left and right lung segmentation as input to the LobeNet algorithm. Regional lobar analysis was performed using hierarchical clustering to identify radiographic subtypes of COVID-19. The proposed lung segmentation algorithm was quantitatively evaluated using semi-automated and manually-corrected segmentations in 87 COVID-19 CT images, achieving an average symmetric surface distance of [Formula: see text] mm and Dice coefficient of [Formula: see text]. Hierarchical clustering identified four radiographical phenotypes of COVID-19 based on lobar fractions of consolidated and poorly aerated tissue. Lower left and lower right lobes were consistently more afflicted with poor aeration and consolidation. However, the most severe cases demonstrated involvement of all lobes. The polymorphic training approach was able to accurately segment COVID-19 cases with diffuse consolidation without requiring COVID-19 cases for training.",2021,10.1038/s41598-020-80936-4,,,,
CT Manifestations of Coronavirus Disease (COVID-19) Pneumonia and Influenza Virus Pneumonia: A Comparative Study,"OBJECTIVE. The purpose of this study was to investigate differences in CT manifestations of coronavirus disease (COVID-19) pneumonia and those of influenza virus pneumonia. MATERIALS AND METHODS. We conducted a retrospective study of 52 patients with COVID-19 pneumonia and 45 patients with influenza virus pneumonia. All patients had positive results for the respective viruses from nucleic acid testing and had complete clinical data and CT images. CT findings of pulmonary inflammation, CT score, and length of largest lesion were evaluated in all patients. Mean density, volume, and mass of lesions were further calculated using artificial intelligence software. CT findings and clinical data were evaluated. RESULTS. Between the group of patients with COVID-19 pneumonia and the group of patients with influenza virus pneumonia, the largest lesion close to the pleura (i.e., no pulmonary parenchyma between the lesion and the pleura), mucoid impaction, presence of pleural effusion, and axial distribution showed statistical difference (p < 0.05). The properties of the largest lesion, presence of ground-glass opacity, presence of consolidation, mosaic attenuation, bronchial wall thickening, centrilobular nodules, interlobular septal thickening, crazy paving pattern, air bronchogram, unilateral or bilateral distribution, and longitudinal distribution did not show significant differences (p > 0.05). In addition, no significant difference was seen in CT score, length of the largest lesion, mean density, volume, or mass of the lesions between the two groups (p > 0.05). CONCLUSION. Most lesions in patients with COVID-19 pneumonia were located in the peripheral zone and close to the pleura, whereas influenza virus pneumonia was more prone to show mucoid impaction and pleural effusion. However, differentiating between COVID-19 pneumonia and influenza virus pneumonia in clinical practice remains difficult.",2021,10.2214/ajr.20.23304,cross-sectional,diagnosis,CT,Lung
CT Quantification and Machine-learning Models for Assessment of Disease Severity and Prognosis of COVID-19 Patients,"OBJECTIVE: This study was to investigate the CT quantification of COVID-19 pneumonia and its impacts on the assessment of disease severity and the prediction of clinical outcomes in the management of COVID-19 patients. MATERIALS METHODS: Ninety-nine COVID-19 patients who were confirmed by positive nucleic acid test (NAT) of RT-PCR and hospitalized from January 19, 2020 to February 19, 2020 were collected for this retrospective study. All patients underwent arterial blood gas test, routine blood test, chest CT examination, and physical examination on admission. In addition, follow-up clinical data including the disease severity, clinical treatment, and clinical outcomes were collected for each patient. Lung volume, lesion volume, nonlesion lung volume (NLLV) (lung volume - lesion volume), and fraction of nonlesion lung volume (%NLLV) (nonlesion lung volume / lung volume) were quantified in CT images by using two U-Net models trained for segmentation of lung and COVID-19 lesions in CT images. Furthermore, we calculated 20 histogram textures for lesions volume and NLLV, respectively. To investigate the validity of CT quantification in the management of COVID-19, we built random forest (RF) models for the purpose of classification and regression to assess the disease severity (Moderate, Severe, and Critical) and to predict the need and length of ICU stay, the duration of oxygen inhalation, hospitalization, sputum NAT-positive, and patient prognosis. The performance of RF classifiers was evaluated using the area under the receiver operating characteristic curves (AUC) and that of RF regressors using the root-mean-square error. RESULTS: Patients were classified into three groups of disease severity: moderate (n = 25), severe (n = 47) and critical (n = 27), according to the clinical staging. Of which, a total of 32 patients, 1 (1/25) moderate, 6 (6/47) severe, and 25 critical (25/27), respectively, were admitted to ICU. The median values of ICU stay were 0, 0, and 12 days, the duration of oxygen inhalation 10, 15, and 28 days, the hospitalization 12, 16, and 28 days, and the sputum NAT-positive 8, 9, and 13 days, in three severity groups, respectively. The clinical outcomes were complete recovery (n = 3), partial recovery with residual pulmonary damage (n = 80), prolonged recovery (n = 15), and death (n = 1). The %NLLV in three severity groups were 92.18 ± 9.89%, 82.94 ± 16.49%, and 66.19 ± 24.15% with p value <0.05 among each two groups. The AUCs of RF classifiers using hybrid models were 0.927 and 0.929 in classification of moderate vs (severe + critical), and severe vs critical, respectively, which were significantly higher than either radiomics models or clinical models (p < 0.05). The root-mean-square errors of RF regressors were 0.88 weeks for prediction of duration of hospitalization (mean: 2.60 ± 1.01 weeks), 0.92 weeks for duration of oxygen inhalation (mean: 2.44 ± 1.08 weeks), 0.90 weeks for duration of sputum NAT-positive (mean: 1.59 ± 0.98 weeks), and 0.69 weeks for stay of ICU (mean: 1.32 ± 0.67 weeks), respectively. The AUCs for prediction of ICU treatment and prognosis (partial recovery vs prolonged recovery) were 0.945 and 0.960, respectively. CONCLUSION: CT quantification and machine-learning models show great potentials for assisting decision-making in the management of COVID-19 patients by assessing disease severity and predicting clinical outcomes.",2020,10.1016/j.acra.2020.09.004,cross-sectional,prognosis,CT,Lung
CT quantification of pneumonia lesions in early days predicts progression to severe illness in a cohort of COVID-19 patients,"Rationale: Some patients with coronavirus disease 2019 (COVID-19) rapidly develop respiratory failure or even die, underscoring the need for early identification of patients at elevated risk of severe illness. This study aims to quantify pneumonia lesions by computed tomography (CT) in the early days to predict progression to severe illness in a cohort of COVID-19 patients. Methods: This retrospective cohort study included confirmed COVID-19 patients. Three quantitative CT features of pneumonia lesions were automatically calculated using artificial intelligence algorithms, representing the percentages of ground-glass opacity volume (PGV), semi-consolidation volume (PSV), and consolidation volume (PCV) in both lungs. CT features, acute physiology and chronic health evaluation II (APACHE-II) score, neutrophil-to-lymphocyte ratio (NLR), and d-dimer, on day 0 (hospital admission) and day 4, were collected to predict the occurrence of severe illness within a 28-day follow-up using both logistic regression and Cox proportional hazard models. Results: We included 134 patients, of whom 19 (14.2%) developed any severe illness. CT features on day 0 and day 4, as well as their changes from day 0 to day 4, showed predictive capability. Changes in CT features from day 0 to day 4 performed the best in the prediction (area under the receiver operating characteristic curve = 0.93, 95% confidence interval [CI] 0.87~0.99; C-index=0.88, 95% CI 0.81~0.95). The hazard ratios of PGV and PCV were 1.39 (95% CI 1.05~1.84, P=0.023) and 1.67 (95% CI 1.17~2.38, P=0.005), respectively. CT features, adjusted for age and gender, on day 4 and in terms of changes from day 0 to day 4 outperformed APACHE-II, NLR, and d-dimer. Conclusions: CT quantification of pneumonia lesions can early and non-invasively predict the progression to severe illness, providing a promising prognostic indicator for clinical management of COVID-19.",2020,10.7150/thno.45985,retrospective cohort,prognosis,CT,Lung
CT radiomics facilitates more accurate diagnosis of COVID-19 pneumonia: compared with CO-RADS,"BACKGROUND: Limited data was available for rapid and accurate detection of COVID-19 using CT-based machine learning model. This study aimed to investigate the value of chest CT radiomics for diagnosing COVID-19 pneumonia compared with clinical model and COVID-19 reporting and data system (CO-RADS), and develop an open-source diagnostic tool with the constructed radiomics model. METHODS: This study enrolled 115 laboratory-confirmed COVID-19 and 435 non-COVID-19 pneumonia patients (training dataset, n = 379; validation dataset, n = 131; testing dataset, n = 40). Key radiomics features extracted from chest CT images were selected to build a radiomics signature using least absolute shrinkage and selection operator (LASSO) regression. Clinical and clinico-radiomics combined models were constructed. The combined model was further validated in the viral pneumonia cohort, and compared with performance of two radiologists using CO-RADS. The diagnostic performance was assessed by receiver operating characteristics curve (ROC) analysis, calibration curve, and decision curve analysis (DCA). RESULTS: Eight radiomics features and 5 clinical variables were selected to construct the combined radiomics model, which outperformed the clinical model in diagnosing COVID-19 pneumonia with an area under the ROC (AUC) of 0.98 and good calibration in the validation cohort. The combined model also performed better in distinguishing COVID-19 from other viral pneumonia with an AUC of 0.93 compared with 0.75 (P = 0.03) for clinical model, and 0.69 (P = 0.008) or 0.82 (P = 0.15) for two trained radiologists using CO-RADS. The sensitivity and specificity of the combined model can be achieved to 0.85 and 0.90. The DCA confirmed the clinical utility of the combined model. An easy-to-use open-source diagnostic tool was developed using the combined model. CONCLUSIONS: The combined radiomics model outperformed clinical model and CO-RADS for diagnosing COVID-19 pneumonia, which can facilitate more rapid and accurate detection.",2021,10.1186/s12967-020-02692-3,cross-sectional,diagnosis,CT,Lung
CT Slice Thickness and Convolution Kernel Affect Performance of a Radiomic Model for Predicting EGFR Status in Non-Small Cell Lung Cancer: A Preliminary Study,"We evaluated whether the optimal selection of CT reconstruction settings enables the construction of a radiomics model to predict epidermal growth factor receptor (EGFR) mutation status in primary lung adenocarcinoma (LAC) using standard of care CT images. Fifty-one patients (EGFR:wildtype = 23:28) with LACs of clinical stage I/II/IIIA were included in the analysis. The LACs were segmented in four conditions, two slice thicknesses (Thin: 1 mm; Thick: 5 mm) and two convolution kernels (Sharp: B70f/B70s; Smooth: B30f/B31f/B31s), which constituted four groups: (1) Thin-Sharp, (2) Thin-Smooth, (3) Thick-Sharp, and (4) Thick-Smooth. Machine learning algorithms selected and combined 1,695 quantitative image features to build prediction models. The performance of prediction models was assessed by calculating the area under the curve (AUC). The best prediction model yielded AUC (95%CI) = 0.83 (0.68, 0.92) using the Thin-Smooth reconstruction setting. The AUC of models using thick slices was significantly lower than that of thin slices (P < 10(-3)), whereas the impact of reconstruction kernel was not significant. Our study showed that the optimal prediction of EGFR mutational status in early stage LACs was achieved by using thin CT-scan slices, independently of convolution kernels. Results from the prediction model suggest that tumor heterogeneity is associated with EGFR mutation.",2018,10.1038/s41598-018-36421-0,cross-sectional,informatics,CT,Lung
CT texture analysis as predictive factor in metastatic lung adenocarcinoma treated with tyrosine kinase inhibitors (TKIs),"PURPOSE: To assess the predictive and prognostic value of pre-treatment CT texture features in lung adenocarcinoma treated with tyrosine kinase inhibitors (TKI). MATERIALS AND METHODS: Texture analysis was performed using commercially available software (TexRAD Ltd, Cambridge, UK) on pre-treatment contrast-enhanced CT studies from 50 patients with metastatic lung adenocarcinoma treated by TKI. Texture features were quantified on a 5-mm-thick central slice of the primary tumor and were correlated with progression-free and overall survival (PFS and OS) using an internally cross-validated machine learning approach then validated on a bootstrapped sample. RESULTS: Median PFS and OS were 10.5 and 20.7 months, respectively. A noninvasive signature based on five texture parameters predicted 6-month progression with Area Under the Curve (AUC) of 0.8 (95% CI) and 1-year progression with AUC of 0.76. A high-risk group had hazard ratios for progression of 4.63 and 5.78 when divided by median and best cut-off points, respectively. Texture signature did not correlate with OS. Available clinical variables did not correlate with PFS or with OS. CONCLUSION: Texture features seem to be associated with PFS in lung adenocarcinoma treated with TKI.",2018,10.1016/j.ejrad.2018.10.016,cross-sectional,combined,CT,Lung
CT texture analysis-based nomogram for the preoperative prediction of visceral pleural invasion in cT1N0M0 lung adenocarcinoma: an external validation cohort study,"AIM: To develop a nomogram based on computed tomography (CT) texture analysis for the preoperative prediction of visceral pleural invasion in patients with cT1N0M0 lung adenocarcinoma. MATERIALS AND METHODS: A dataset of chest CT containing lung nodules was collected from two institutions, and all surgically resected nodules were classified pathologically based on the presence of visceral pleural invasion. Each nodule on the CT image was segmented automatically by artificial-intelligence software and its CT texture features were extracted. The dataset was divided into training and external validation cohorts according to the institution, and a nomogram for predicting visceral pleural invasion was developed and validated. RESULTS: Of a total of 313 patients enrolled from two independent institutions, 63 were diagnosed with visceral pleural invasion. Three-dimensional (3D) CT long diameter, skewness, and sphericity, and chronic obstructive pulmonary disease were identified as independent predictors for visceral pleural invasion by multivariable logistic regression. The nomogram based on multivariable logistic regression showed great discriminative ability, as indicated by a C-index of 0.890 (95% confidence interval [CI]: 0.867-0.914) and 0.864 (95% CI: 0.817-0.911) for the training and external validation cohorts, respectively. Additionally, calibration of the nomogram revealed good predictive ability, as indicated by the Brier score (0.108 and 0.100 for the training and external validation cohorts, respectively). CONCLUSIONS: A nomogram was developed that could compute the probability of visceral pleural invasion in patients with cT1N0M0 lung adenocarcinoma with good calibration and discrimination. The nomogram has potential as a reliable tool for clinical evaluation and decision-making.",2022,10.1016/j.crad.2021.11.008,cross-sectional,diagnosis,CT,Lung
CT-Based COVID-19 triage: Deep multitask learning improves joint identification and severity quantification,"The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight patients with severe COVID-19, thus direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods could provide reasonable quality only for one of these setups. We employ a multitask approach to consolidate both triage approaches and propose a convolutional neural network to leverage all available labels within a single model. In contrast with the related multitask approaches, we show the benefit from applying the classification layers to the most spatially detailed feature map at the upper part of U-Net instead of the less detailed latent representation at the bottom. We train our model on approximately 1500 publicly available CT studies and test it on the holdout dataset that consists of 123 chest CT studies of patients drawn from the same healthcare system, specifically 32 COVID-19 and 30 bacterial pneumonia cases, 30 cases with cancerous nodules, and 31 healthy controls. The proposed multitask model outperforms the other approaches and achieves ROC AUC scores of 0.87±0.01 vs. bacterial pneumonia, 0.93±0.01 vs. cancerous nodules, and 0.97±0.01 vs. healthy controls in Identification of COVID-19, and achieves 0.97±0.01 Spearman Correlation in Severity quantification. We have released our code and shared the annotated lesions masks for 32 CT images of patients with COVID-19 from the test dataset.",2021,10.1016/j.media.2021.102054,cross-sectional,diagnosis,CT,Lung
CT-based deep learning model to differentiate invasive pulmonary adenocarcinomas appearing as subsolid nodules among surgical candidates: comparison of the diagnostic performance with a size-based logistic model and radiologists,"OBJECTIVES: To evaluate the deep learning models for differentiating invasive pulmonary adenocarcinomas (IACs) among subsolid nodules (SSNs) considered for resection in a retrospective diagnostic cohort in comparison with a size-based logistic model and expert radiologists. METHODS: This study included 525 patients (309 women; median, 62 years) to develop models, and an independent cohort of 101 patients (57 women; median, 66 years) was used for validation. A size-based logistic model and deep learning models using 2.5-dimension (2.5D) and three-dimension (3D) CT images were developed to discriminate IAC from less invasive pathologies. Overall performance, discrimination, and calibration were assessed. Diagnostic performances of the three thoracic radiologists were compared with those of the deep learning model. RESULTS: The overall performances of the deep learning models (Brier score, 0.122 for the 2.5D DenseNet and 0.121 for the 3D DenseNet) were superior to those of the size-based logistic model (Brier score, 0.198). The area under the receiver operating characteristic curve (AUC) of the 2.5D DenseNet (0.921) was significantly higher than that of the 3D DenseNet (0.835; p = 0.037) and the size-based logistic model (0.836; p = 0.009). At equally high sensitivities of 90%, the 2.5D DenseNet showed significantly higher specificity (88.2%; all p < 0.05) and positive predictive value (97.4%; all p < 0.05) than other models. Model calibration was poor for all models (all p < 0.05). The 2.5D DenseNet had a comparable performance with the radiologists (AUC, 0.848-0.910). CONCLUSION: The 2.5D DenseNet model could be used as a highly sensitive and specific diagnostic tool to differentiate IACs among SSNs for surgical candidates. KEY POINTS: • The deep learning model developed using 2.5D DenseNet showed higher overall performance and discrimination than the size-based logistic model for the differentiation of invasive adenocarcinomas among subsolid nodules for surgical candidates. • The 2.5D DenseNet demonstrated a thoracic radiologist-level diagnostic performance and had higher specificity (88.2%) at equal sensitivities (90%) than the size-based logistic model (specificity, 52.9%). • The 2.5D DenseNet could be used to reduce potential overtreatment for the indolent subsolid nodules or to select candidates for sublobar resection instead of the standard lobectomy.",2020,10.1007/s00330-019-06628-4,cross-sectional,diagnosis,CT,Lung
CT-Based Hand-crafted Radiomic Signatures Can Predict PD-L1 Expression Levels in Non-small Cell Lung Cancer: a Two-Center Study,"Here, we used pre-treatment CT images to develop and evaluate a radiomic signature that can predict the expression of programmed death ligand 1 (PD-L1) in non-small cell lung cancer (NSCLC). We then verified its predictive performance by cross-referencing its results with clinical characteristics. This two-center retrospective analysis included 125 patients with histologically confirmed NSCLC. A total of 1287 hand-crafted radiomic features were observed from manually determined tumor regions. Valuable features were then selected with a ridge regression-based recursive feature elimination approach. Machine learning-based prediction models were then built from this and compared each other. The final radiomic signature was built using logistic regression in the primary cohort, and then tested in a validation cohort. Finally, we compared the efficacy of the radiomic signature to the clinical model and the radiomic-clinical nomogram. Among the 125 patients, 89 were classified as having PD-L1 positive expression. However, there was no significant difference in PD-L1 expression levels determined by clinical characteristics (P = 0.109-0.955). Upon selecting 9 radiomic features, we found that the logistic regression-based prediction model performed the best (AUC = 0.96, P < 0.001). In the external cohort, our radiomic signature showed an AUC of 0.85, which outperformed both the clinical model (AUC = 0.38, P < 0.001) and the radiomics-nomogram model (AUC = 0.61, P < 0.001). Our CT-based hand-crafted radiomic signature model can effectively predict PD-L1 expression levels, providing a noninvasive means of better understanding PD-L1 expression in patients with NSCLC.",2021,10.1007/s10278-021-00484-9,cross-sectional,diagnosis,CT,Lung
CT-based radiomics and machine learning to predict spread through air space in lung adenocarcinoma,"PURPOSE: Spread through air space (STAS) is a novel invasive pattern of lung adenocarcinoma and is also a risk factor for recurrence and worse prognosis of lung adenocarcinoma. The aims of this study are to develop and validate a computed tomography (CT)‑based radiomics model for preoperative prediction of STAS in lung adenocarcinoma. METHODS AND MATERIALS: This retrospective study was approved by an institutional review board and included 462 (mean age, 58.06 years) patients with pathologically confirmed lung adenocarcinoma. STAS was identified in 90 patients (19.5%). Two experienced radiologists segmented and extracted radiomics features on preoperative thin-slice CT images with radiomics extension independently. Intraclass correlation coefficients (ICC) and Pearson's correlation were used to rule out those low reliable (ICC < 0.75) and redundant (r > 0.9) features. Univariate logistic regression was applied to select radiomics features which were associated with STAS. A radiomics-based machine learning predictive model using a random forest (RF) was developed and calibrated with fivefold cross-validation. The diagnostic performance of the model was measured by the area under the curve (AUC) of receiver operating characteristic (ROC). RESULTS: With univariate analysis, 12 radiomics features and age were found to be associated with STAS significantly. The RF model achieved an AUC of 0.754 (a sensitivity of 0.880 and a specificity of 0.588) for predicting STAS. CONCLUSION: CT-based radiomics model can preoperatively predict STAS in lung adenocarcinoma with good diagnosis performance. KEY POINTS: • CT-based radiomics and machine learning model can predict spread through air space (STAS) in lung adenocarcinoma with high accuracy. • The random forest (RF) model achieved an AUC of 0.754 (a sensitivity of 0.880 and a specificity of 0.588) for predicting STAS.",2020,10.1007/s00330-020-06694-z,cross-sectional,diagnosis,CT,Lung
CT-based radiomics for predicting the rapid progression of coronavirus disease 2019 (COVID-19) pneumonia lesions,"OBJECTIVES: To develop and validate a radiomic model to predict the rapid progression (defined as volume growth of pneumonia lesions > 50% within seven days) in patients with coronavirus disease 2019 (COVID-19). METHODS: Patients with laboratory-confirmed COVID-19 who underwent longitudinal chest CT between January 01 and February 18, 2020 were included. A total of 1316 radiomic features were extracted from the lung parenchyma window for each CT. The least absolute shrinkage and selection operator (LASSO), Relief, Las Vegas Wrapper (LVW), L1-norm-Support Vector Machine (L1-norm-SVM), and recursive feature elimination (RFE) were applied to select the features that associated with rapid progression. Four machine learning classifiers were used for modeling, including Support Vector Machine (SVM), Random Forest (RF), Logistic Regression (LR), and Decision Tree (DT). Accordingly, 20 radiomic models were developed on the basis of 296 CT scans and validated in 74 CT scans. Model performance was determined by the receiver operating characteristic curve. RESULTS: A total of 107 patients (median age, 49.0 years, interquartile range, 35-54) were evaluated. The patients underwent a total of 370 chest CT scans with a median interval of 4 days (interquartile range, 3-5 days). The combination methods of L1-norm SVM and SVM with 17 radiomic features yielded the highest performance in predicting the likelihood of rapid progression of pneumonia lesions on next CT scan, with an AUC of 0.857 (95% CI: 0.766-0.947), sensitivity of 87.5%, and specificity of 70.7%. CONCLUSIONS: Our radiomic model based on longitudinal chest CT data could predict the rapid progression of pneumonia lesions, which may facilitate the CT follow-up intervals and reduce the radiation. ADVANCES IN KNOWLEDGE: Radiomic features extracted from the current chest CT have potential in predicting the likelihood of rapid progression of pneumonia lesions on the next chest CT, which would improve clinical decision-making regarding timely treatment.",2021,10.1259/bjr.20201007,cross-sectional,prognosis,CT,Lung
CT-based radiomics for prediction of histologic subtype and metastatic disease in primary malignant lung neoplasms,"PURPOSE: As some of the most important factors for treatment decision of lung cancer (which is the deadliest neoplasm) are staging and histology, this work aimed to associate quantitative contrast-enhanced computed tomography (CT) features from malignant lung tumors with distant and nodal metastases (according to clinical TNM staging) and histopathology (according to biopsy and surgical resection) using radiomics assessment. METHODS: A local cohort of 85 patients were retrospectively (2010-2017) analyzed after approval by the institutional research review board. CT images acquired with the same protocol were semiautomatically segmented by a volumetric segmentation method. Tumors were characterized by quantitative CT features of shape, first-order, second-order, and higher-order textures. Statistical and machine learning analyses assessed the features individually and combined with clinical data. RESULTS: Univariate and multivariate analyses identified 40, 2003, and 45 quantitative features associated with distant metastasis, nodal metastasis, and histopathology (adenocarcinoma and squamous cell carcinoma), respectively. A machine learning model yielded the highest areas under the receiver operating characteristic curves of 0.92, 0.84, and 0.88 to predict the same previous patterns. CONCLUSION: Several radiomic features (including wavelet energies, information measures of correlation and maximum probability from co-occurrence matrix, busyness from neighborhood intensity-difference matrix, directionalities from Tamura's texture, and fractal dimension estimation) significantly associated with distant metastasis, nodal metastasis, and histology were discovered in this work, presenting great potential as imaging biomarkers for pathological diagnosis and target therapy decision.",2020,10.1007/s11548-019-02093-y,cross-sectional,diagnosis,CT,Lung
CT-based radiomics signatures can predict the tumor response of non-small cell lung cancer patients treated with first-line chemotherapy and targeted therapy,"OBJECTIVES: The goal of this study was to evaluate the effectiveness of radiomics signatures on pre-treatment computed tomography (CT) images of lungs to predict the tumor responses of non-small cell lung cancer (NSCLC) patients treated with first-line chemotherapy, targeted therapy, or a combination of both. MATERIALS AND METHODS: This retrospective study included 322 NSCLC patients who were treated with first-line chemotherapy, targeted therapy, or a combination of both. Of these patients, 224 were randomly assigned to a cohort to help develop the radiomics signature. A total of 1946 radiomics features were obtained from each patient's CT scan. The top-ranked features were selected by the Minimum Redundancy Maximum Relevance (MRMR) feature-ranking method and used to build a lightweight radiomics signature with the Random Forest (RF) classifier. The independent predictive (IP) features (AUC > 0.6, p value < 0.05) were further identified from the top-ranked features and used to build a refined radiomics signature by the RF classifier. Its prediction performance was tested on the validation cohort, which consisted of the remaining 98 patients. RESULTS: The initial lightweight radiomics signature constructed from 15 top-ranked features had an AUC of 0.721 (95% CI, 0.619-0.823). After six IP features were further identified and a refined radiomics signature was built, it had an AUC of 0.746 (95% CI, 0.646-0.846). CONCLUSIONS: Radiomics signatures based on pre-treatment CT scans can accurately predict tumor response in NSCLC patients after first-line chemotherapy or targeted therapy treatments. Radiomics features could be used as promising prognostic imaging biomarkers in the future. KEY POINTS: The radiomics signature extracted from baseline CT images in patients with NSCLC can predict response to first-line chemotherapy, targeted therapy, or both treatments with an AUC = 0.746 (95% CI, 0.646-0.846). The radiomics signature could be used as a new biomarker for quantitative analysis in radiology, which might provide value in decision-making and to define personalized treatments for cancer patients.",2022,10.1007/s00330-021-08277-y,cross-sectional,treatment,CT,Lung
CTumorGAN: a unified framework for automatic computed tomography tumor segmentation,"PURPOSE: Unlike the normal organ segmentation task, automatic tumor segmentation is a more challenging task because of the existence of similar visual characteristics between tumors and their surroundings, especially on computed tomography (CT) images with severe low contrast resolution, as well as the diversity and individual characteristics of data acquisition procedures and devices. Consequently, most of the recently proposed methods have become increasingly difficult to be applied on a different tumor dataset with good results, and moreover, some tumor segmentors usually fail to generalize beyond those datasets and modalities used in their original evaluation experiments. METHODS: In order to alleviate some of the problems with the recently proposed methods, we propose a novel unified and end-to-end adversarial learning framework for automatic segmentation of any kinds of tumors from CT scans, called CTumorGAN, consisting of a Generator network and a Discriminator network. Specifically, the Generator attempts to generate segmentation results that are close to their corresponding golden standards, while the Discriminator aims to distinguish between generated samples and real tumor ground truths. More importantly, we deliberately design different modules to take into account the well-known obstacles, e.g., severe class imbalance, small tumor localization, and the label noise problem with poor expert annotation quality, and then use these modules to guide the CTumorGAN training process by utilizing multi-level supervision more effectively. RESULTS: We conduct a comprehensive evaluation on diverse loss functions for tumor segmentation and find that mean square error is more suitable for the CT tumor segmentation task. Furthermore, extensive experiments with multiple evaluation criteria on three well-established datasets, including lung tumor, kidney tumor, and liver tumor databases, also demonstrate that our CTumorGAN achieves stable and competitive performance compared with the state-of-the-art approaches for CT tumor segmentation. CONCLUSION: In order to overcome those key challenges arising from CT datasets and solve some of the main problems existing in the current deep learning-based methods, we propose a novel unified CTumorGAN framework, which can be effectively generalized to address any kinds of tumor datasets with superior performance.",2020,10.1007/s00259-020-04781-3,cross-sectional,diagnosis,CT,Lung
CXR-RefineDet: Single-Shot Refinement Neural Network for Chest X-Ray Radiograph Based on Multiple Lesions Detection,"The workload of radiologists has dramatically increased in the context of the COVID-19 pandemic, causing misdiagnosis and missed diagnosis of diseases. The use of artificial intelligence technology can assist doctors in locating and identifying lesions in medical images. In order to improve the accuracy of disease diagnosis in medical imaging, we propose a lung disease detection neural network that is superior to the current mainstream object detection model in this paper. By combining the advantages of RepVGG block and Resblock in information fusion and information extraction, we design a backbone RRNet with few parameters and strong feature extraction capabilities. After that, we propose a structure called Information Reuse, which can solve the problem of low utilization of the original network output features by connecting the normalized features back to the network. Combining the network of RRNet and the improved RefineDet, we propose the overall network which was called CXR-RefineDet. Through a large number of experiments on the largest public lung chest radiograph detection dataset VinDr-CXR, it is found that the detection accuracy and inference speed of CXR-RefineDet have reached 0.1686 mAP and 6.8 fps, respectively, which is better than the two-stage object detection algorithm using a strong backbone like ResNet-50 and ResNet-101. In addition, the fast reasoning speed of CXR-RefineDet also provides the possibility for the actual implementation of the computer-aided diagnosis system.",2022,10.1155/2022/4182191,cross-sectional,diagnosis,x-ray,Lung
Cytotoxic T lymphocytes targeting a conserved SARS-CoV-2 spike epitope are efficient serial killers,"Understanding immune response to infections and vaccines lags understanding humoral responses. While neutralizing antibody responses wane over time, T cells are instrumental in long-term immunity. We apply machine learning and time-lapse imaging microscopy in nanowell grids (TIMING) to study thousands of videos of T cells with specificity for SARS-CoV-2 eliminating targets bearing spike protein as a surrogate for viral infection. The data on effector functions, including cytokine secretion and cytotoxicity, provide the first direct evidence that cytotoxic T lymphocytes from a convalescent patient targeting an epitope conserved across all known variants of concern are serial killers capable of eliminating multiple infected target cells. These data have implications for vaccine development and for the recovery and monitoring of infected individuals.",2022,10.2144/btn-2022-0016,,,,
D2-CovidNet: A Deep Learning Model for COVID-19 Detection in Chest X-Ray Images,"Since the outbreak of Coronavirus disease 2019 (COVID-19), it has been spreading rapidly worldwide and has not yet been effectively controlled. Many researchers are studying novel Coronavirus pneumonia from chest X-ray images. In order to improve the detection accuracy, two modules sensitive to feature information, dual-path multiscale feature fusion module and dense depthwise separable convolution module, are proposed. Based on these two modules, a lightweight convolutional neural network model, D2-CovidNet, is designed to assist experts in diagnosing COVID-19 by identifying chest X-ray images. D2-CovidNet is tested on two public data sets, and its classification accuracy, precision, sensitivity, specificity, and F1-score are 94.56%, 95.14%, 94.02%, 96.61%, and 95.30%, respectively. Specifically, the precision, sensitivity, and specificity of the network for COVID-19 are 98.97%, 94.12%, and 99.84%, respectively. D2-CovidNet has fewer computation number and parameter number. Compared with other methods, D2-CovidNet can help diagnose COVID-19 more quickly and accurately.",2021,10.1155/2021/9952109,cross-sectional,diagnosis,x-ray,Lung
Data science in unveiling COVID-19 pathogenesis and diagnosis: evolutionary origin to drug repurposing,"MOTIVATION: The outbreak of novel severe acute respiratory syndrome coronavirus (SARS-CoV-2, also known as COVID-19) in Wuhan has attracted worldwide attention. SARS-CoV-2 causes severe inflammation, which can be fatal. Consequently, there has been a massive and rapid growth in research aimed at throwing light on the mechanisms of infection and the progression of the disease. With regard to this data science is playing a pivotal role in in silico analysis to gain insights into SARS-CoV-2 and the outbreak of COVID-19 in order to forecast, diagnose and come up with a drug to tackle the virus. The availability of large multiomics, radiological, bio-molecular and medical datasets requires the development of novel exploratory and predictive models, or the customisation of existing ones in order to fit the current problem. The high number of approaches generates the need for surveys to guide data scientists and medical practitioners in selecting the right tools to manage their clinical data. RESULTS: Focusing on data science methodologies, we conduct a detailed study on the state-of-the-art of works tackling the current pandemic scenario. We consider various current COVID-19 data analytic domains such as phylogenetic analysis, SARS-CoV-2 genome identification, protein structure prediction, host-viral protein interactomics, clinical imaging, epidemiological research and drug discovery. We highlight data types and instances, their generation pipelines and the data science models currently in use. The current study should give a detailed sketch of the road map towards handling COVID-19 like situations by leveraging data science experts in choosing the right tools. We also summarise our review focusing on prime challenges and possible future research directions. CONTACT: hguzzi@unicz.it, sroy01@cus.ac.in.",2021,10.1093/bib/bbaa420,,,,
Data-efficient and weakly supervised computational pathology on whole-slide images,"Deep-learning methods for computational pathology require either manual annotation of gigapixel whole-slide images (WSIs) or large datasets of WSIs with slide-level labels and typically suffer from poor domain adaptation and interpretability. Here we report an interpretable weakly supervised deep-learning method for data-efficient WSI processing and learning that only requires slide-level labels. The method, which we named clustering-constrained-attention multiple-instance learning (CLAM), uses attention-based learning to identify subregions of high diagnostic value to accurately classify whole slides and instance-level clustering over the identified representative regions to constrain and refine the feature space. By applying CLAM to the subtyping of renal cell carcinoma and non-small-cell lung cancer as well as the detection of lymph node metastasis, we show that it can be used to localize well-known morphological features on WSIs without the need for spatial labels, that it overperforms standard weakly supervised classification algorithms and that it is adaptable to independent test cohorts, smartphone microscopy and varying tissue content.",2021,10.1038/s41551-020-00682-w,,,,
DCT-MIL: Deep CNN transferred multiple instance learning for COPD identification using CT images,"While many pre-defined computed tomographic (CT) measures have been utilized to characterize chronic obstructive pulmonary disease (COPD), it is still challenging to represent pathological alternations of multiple dimensions and highly spatial heterogeneity. Deep CNN transferred multiple instance learning (DCT-MIL) is proposed to identify COPD via CT images. After the lung is divided into eight sections along the axial direction, one random axial CT image is taken out from each section as one instance. With one instance as the input, the activations of neural layers of AlexNet trained by natural images are extracted as features. After dimension reduction through principle component analysis, features of all instances are input into three MIL methods: Citation k-Nearest-Neighbor (Citation-KNN), multiple instance support vector machine, and expectation-maximization diverse density. Moreover, the performance dependence of the resulted models on the depth of the neural layer where activations are extracted and the number of features is investigated. The proposed DCT-MIL achieves an exceptional performance with an accuracy of 99.29% and area under curve of 0.9826 while using 100 principle components of features extracted from the fourth convolutional layer and Citation-KNN. It outperforms not only DCT-MIL models using other settings and the pre-trained AlexNet with fine-tuning by montages of eight lung CT images, but also other state-of-art methods. Deep CNN transferred multiple instance learning is suited for identification of COPD using CT images. It can help finding subgroups with high risk of COPD from large populations through CT scans ordered doing lung cancer screening.",2020,10.1088/1361-6560/ab857d,cross-sectional,diagnosis,CT,Lung
De Novo Drug Design of Targeted Chemical Libraries Based on Artificial Intelligence and Pair-Based Multiobjective Optimization,"Artificial intelligence and multiobjective optimization represent promising solutions to bridge chemical and biological landscapes by addressing the automated de novo design of compounds as a result of a humanlike creative process. In the present study, we conceived a novel pair-based multiobjective approach implemented in an adapted SMILES generative algorithm based on recurrent neural networks for the automated de novo design of new molecules whose overall features are optimized by finding the best trade-offs among relevant physicochemical properties (MW, logP, HBA, HBD) and additional similarity-based constraints biasing specific biological targets. In this respect, we carried out the de novo design of chemical libraries targeting neuraminidase, acetylcholinesterase, and the main protease of severe acute respiratory syndrome coronavirus 2. Several quality metrics were employed to assess drug-likeness, chemical feasibility, diversity content, and validity. Molecular docking was finally carried out to better evaluate the scoring and posing of the de novo generated molecules with respect to X-ray cognate ligands of the corresponding molecular counterparts. Our results indicate that artificial intelligence and multiobjective optimization allow us to capture the latent links joining chemical and biological aspects, thus providing easy-to-use options for customizable design strategies, which are especially effective for both lead generation and lead optimization. The algorithm is freely downloadable at https://github.com/alberdom88/moo-denovo and all of the data are available as Supporting Information.",2020,10.1021/acs.jcim.0c00517,,,,
Deciphering unclassified tumors of non-small-cell lung cancer through radiomics,"BACKGROUND: Tumors are highly heterogeneous at the phenotypic, physiologic, and genomic levels. They are categorized in terms of a differentiated appearance under a microscope. Non-small-cell lung cancer tumors are categorized into three main subgroups: adenocarcinoma, squamous cell carcinoma, and large cell carcinoma. In approximately 20% of pathology reports, they are returned unclassified or classified as not-otherwise-specified (NOS) owing to scant materials or poor tumor differentiation. METHOD: We present a radiomic interrogation of molecular spatial variations to decode unclassified NOS tumor architecture quantitatively. Twelve spatial descriptors with various displacements and directions were extracted and profiled with respect to the subgroups. The profiled descriptors were used to decipher the NOS tumor morphologic clues from the imaging phenotype perspective. This profiler was built as an extended version of a conventional support-vector-machine classifier, wherein a genetic algorithm and correlation analysis were embedded to define the molecular signatures of poorly differentiated tumors using well-differentiated-tumor information. RESULTS: Sixteen multi-class classifier models with an 81% average accuracy and descriptor subset size ranging from 12 to 144 were reported. The average area under the curve was 86.3% at a 95% confidence interval and a 0.03-0.08 standard error. Correlation analysis returned an unclassified NOS membership matrix with respect to the cell-architecture similarity score for the subgroups. The best model demonstrated 53% NOS reduction. CONCLUSION: The membership matrix is expected to assist pathologists and oncologists in cases of unresectable tumors or scant biopsy materials for histological subtyping and cancer therapy.",2017,10.1016/j.compbiomed.2017.10.029,cross-sectional,diagnosis,CT,Lung
Decision based on big data research for non-small cell lung cancer in medical artificial system in developing country,"Non-small cell lung cancer (NSCLC) is a high risk cancer and is usually scanned by PET-CT for testing, predicting and then give the treatment methods. However, in the actual hospital system, at least 640 images must be generated for each patient through PET-CT scanning. Especially in developing countries, a huge number of patients in NSCLC are attended by doctors. Artificial system can predict and make decision rapidly. According to explore and research artificial medical system, the selection of artificial observations also can result in low work efficiency for doctors. In this study, data information of 2,789,675 patients in three hospitals in China are collected, compiled, and used as the research basis; these data are obtained through image acquisition and diagnostic parameter machine decision-making method on the basis of the machine diagnosis and medical system design model of adjuvant therapy. By combining image and diagnostic parameters, the machine decision diagnosis auxiliary algorithm is established. Experimental result shows that the accuracy has reached 77% in NSCLC.",2018,10.1016/j.cmpb.2018.03.004,cross-sectional,others,PET/CT,Lung
Decoding COVID-19 pneumonia: comparison of deep learning and radiomics CT image signatures,"PURPOSE: High-dimensional image features that underlie COVID-19 pneumonia remain opaque. We aim to compare feature engineering and deep learning methods to gain insights into the image features that drive CT-based for COVID-19 pneumonia prediction, and uncover CT image features significant for COVID-19 pneumonia from deep learning and radiomics framework. METHODS: A total of 266 patients with COVID-19 and other viral pneumonia with clinical symptoms and CT signs similar to that of COVID-19 during the outbreak were retrospectively collected from three hospitals in China and the USA. All the pneumonia lesions on CT images were manually delineated by four radiologists. One hundred eighty-four patients (n = 93 COVID-19 positive; n = 91 COVID-19 negative; 24,216 pneumonia lesions from 12,001 CT image slices) from two hospitals from China served as discovery cohort for model development. Thirty-two patients (17 COVID-19 positive, 15 COVID-19 negative; 7883 pneumonia lesions from 3799 CT image slices) from a US hospital served as external validation cohort. A bi-directional adversarial network-based framework and PyRadiomics package were used to extract deep learning and radiomics features, respectively. Linear and Lasso classifiers were used to develop models predictive of COVID-19 versus non-COVID-19 viral pneumonia. RESULTS: 120-dimensional deep learning image features and 120-dimensional radiomics features were extracted. Linear and Lasso classifiers identified 32 high-dimensional deep learning image features and 4 radiomics features associated with COVID-19 pneumonia diagnosis (P < 0.0001). Both models achieved sensitivity > 73% and specificity > 75% on external validation cohort with slight superior performance for radiomics Lasso classifier. Human expert diagnostic performance improved (increase by 16.5% and 11.6% in sensitivity and specificity, respectively) when using a combined deep learning-radiomics model. CONCLUSIONS: We uncover specific deep learning and radiomics features to add insight into interpretability of machine learning algorithms and compare deep learning and radiomics models for COVID-19 pneumonia that might serve to augment human diagnostic performance.",2021,10.1007/s00259-020-05075-4,cross-sectional,diagnosis,CT,Lung
Deep Bidirectional Classification Model for COVID-19 Disease Infected Patients,"In December of 2019, a novel coronavirus (COVID-19) appeared in Wuhan city, China and has been reported in many countries with millions of people infected within only four months. Chest computed Tomography (CT) has proven to be a useful supplement to reverse transcription polymerase chain reaction (RT-PCR) and has been shown to have high sensitivity to diagnose this condition. Therefore, radiological examinations are becoming crucial in early examination of COVID-19 infection. Currently, CT findings have already been suggested as an important evidence for scientific examination of COVID-19 in Hubei, China. However, classification of patient from chest CT images is not an easy task. Therefore, in this paper, a deep bidirectional long short-term memory network with mixture density network (DBM) model is proposed. To tune the hyperparameters of the DBM model, a Memetic Adaptive Differential Evolution (MADE) algorithm is used. Extensive experiments are drawn by considering the benchmark chest-Computed Tomography (chest-CT) images datasets. Comparative analysis reveals that the proposed MADE-DBM model outperforms the competitive COVID-19 classification approaches in terms of various performance metrics. Therefore, the proposed MADE-DBM model can be used in real-time COVID-19 classification systems.",2021,10.1109/tcbb.2020.3009859,cross-sectional,diagnosis,CT,Lung
"Deep CNN models for pulmonary nodule classification: Model modification, model integration, and transfer learning","BACKGROUND: Deep learning has made spectacular achievements in analysing natural images, but it faces challenges for medical applications partly due to inadequate images. OBJECTIVE: Aiming to classify malignant and benign pulmonary nodules using CT images, we explore different strategies to utilize the state-of-the-art deep convolutional neural networks (CNN). METHODS: Experiments are conducted using the Lung Image Database Consortium image collection (LIDC-IDRI), which is a public database containing 1018 cases. Three strategies are implemented including to 1) modify some state-of-the-art CNN architectures, 2) integrate different CNNs and 3) adopt transfer learning. Totally, 11 deep CNN models are compared using the same dataset. RESULTS: Study demonstrates that, for the model modification scheme, a concise CifarNet performs better than the other modified CNNs with more complex architectures, achieving an area under ROC curve of AUC = 0.90. Integrated CNN models do not significantly improve the classification performance, but the model complexity is reduced. Transfer learning outperforms the other two schemes and ResNet with fine-tuning leads to the best performance with an AUC = 0.94, as well as the sensitivity of 91% and an overall accuracy of 88%. CONCLUSIONS: Model modification, model integration, and transfer learning can play important roles to identify and generate optimal deep CNN models in classifying pulmonary nodules based on CT images efficiently. Transfer learning is preferred when applying deep learning to medical imaging applications.",2019,10.3233/xst-180490,cross-sectional,diagnosis,CT,Lung
Deep Convolutional Hashing for Low-Dimensional Binary Embedding of Histopathological Images,"Compact binary representations of histopa-thology images using hashing methods provide efficient approximate nearest neighbor search for direct visual query in large-scale databases. They can be utilized to measure the probability of the abnormality of the query image based on the retrieved similar cases, thereby providing support for medical diagnosis. They also allow for efficient managing of large-scale image databases because of a low storage requirement. However, the effectiveness of binary representations heavily relies on the visual descriptors that represent the semantic information in the histopathological images. Traditional approaches with hand-crafted visual descriptors might fail due to significant variations in image appearance. Recently, deep learning architectures provide promising solutions to address this problem using effective semantic representations. In this paper, we propose a deep convolutional hashing method that can be trained ""point-wise"" to simultaneously learn both semantic and binary representations of histopathological images. Specifically, we propose a convolutional neural network that introduces a latent binary encoding (LBE) layer for low-dimensional feature embedding to learn binary codes. We design a joint optimization objective function that encourages the network to learn discriminative representations from the label information, and reduce the gap between the real-valued low-dimensional embedded features and desired binary values. The binary encoding for new images can be obtained by forward propagating through the network and quantizing the output of the LBE layer. Experimental results on a large-scale histopathological image dataset demonstrate the effectiveness of the proposed method.",2019,10.1109/jbhi.2018.2827703,,,,
Deep convolutional neural network for segmentation of thoracic organs-at-risk using cropped 3D images,"PURPOSE: Automatic segmentation of organs-at-risk (OARs) is a key step in radiation treatment planning to reduce human efforts and bias. Deep convolutional neural networks (DCNN) have shown great success in many medical image segmentation applications but there are still challenges in dealing with large 3D images for optimal results. The purpose of this study is to develop a novel DCNN method for thoracic OARs segmentation using cropped 3D images. METHODS: To segment the five organs (left and right lungs, heart, esophagus and spinal cord) from the thoracic CT scans, preprocessing to unify the voxel spacing and intensity was first performed, a 3D U-Net was then trained on resampled thoracic images to localize each organ, then the original images were cropped to only contain one organ and served as the input to each individual organ segmentation network. The segmentation maps were then merged to get the final results. The network structures were optimized for each step, as well as the training and testing strategies. A novel testing augmentation with multiple iterations of image cropping was used. The networks were trained on 36 thoracic CT scans with expert annotations provided by the organizers of the 2017 AAPM Thoracic Auto-segmentation Challenge and tested on the challenge testing dataset as well as a private dataset. RESULTS: The proposed method earned second place in the live phase of the challenge and first place in the subsequent ongoing phase using a newly developed testing augmentation approach. It showed superior-than-human performance on average in terms of Dice scores (spinal cord: 0.893 ± 0.044, right lung: 0.972 ± 0.021, left lung: 0.979 ± 0.008, heart: 0.925 ± 0.015, esophagus: 0.726 ± 0.094), mean surface distance (spinal cord: 0.662 ± 0.248 mm, right lung: 0.933 ± 0.574 mm, left lung: 0.586 ± 0.285 mm, heart: 2.297 ± 0.492 mm, esophagus: 2.341 ± 2.380 mm) and 95% Hausdorff distance (spinal cord: 1.893 ± 0.627 mm, right lung: 3.958 ± 2.845 mm, left lung: 2.103 ± 0.938 mm, heart: 6.570 ± 1.501 mm, esophagus: 8.714 ± 10.588 mm). It also achieved good performance in the private dataset and reduced the editing time to 7.5 min per patient following automatic segmentation. CONCLUSIONS: The proposed DCNN method demonstrated good performance in automatic OAR segmentation from thoracic CT scans. It has the potential for eventual clinical adoption of deep learning in radiation treatment planning due to improved accuracy and reduced cost for OAR segmentation.",2019,10.1002/mp.13466,cross-sectional,treatment,CT,thorax
Deep Convolutional Neural Network-Based Computer-Aided Detection System for COVID-19 Using Multiple Lung Scans: Design and Implementation Study,"BACKGROUND: Owing to the COVID-19 pandemic and the imminent collapse of health care systems following the exhaustion of financial, hospital, and medicinal resources, the World Health Organization changed the alert level of the COVID-19 pandemic from high to very high. Meanwhile, more cost-effective and precise COVID-19 detection methods are being preferred worldwide. OBJECTIVE: Machine vision-based COVID-19 detection methods, especially deep learning as a diagnostic method in the early stages of the pandemic, have been assigned great importance during the pandemic. This study aimed to design a highly efficient computer-aided detection (CAD) system for COVID-19 by using a neural search architecture network (NASNet)-based algorithm. METHODS: NASNet, a state-of-the-art pretrained convolutional neural network for image feature extraction, was adopted to identify patients with COVID-19 in their early stages of the disease. A local data set, comprising 10,153 computed tomography scans of 190 patients with and 59 without COVID-19 was used. RESULTS: After fitting on the training data set, hyperparameter tuning, and topological alterations of the classifier block, the proposed NASNet-based model was evaluated on the test data set and yielded remarkable results. The proposed model's performance achieved a detection sensitivity, specificity, and accuracy of 0.999, 0.986, and 0.996, respectively. CONCLUSIONS: The proposed model achieved acceptable results in the categorization of 2 data classes. Therefore, a CAD system was designed on the basis of this model for COVID-19 detection using multiple lung computed tomography scans. The system differentiated all COVID-19 cases from non-COVID-19 ones without any error in the application phase. Overall, the proposed deep learning-based CAD system can greatly help radiologists detect COVID-19 in its early stages. During the COVID-19 pandemic, the use of a CAD system as a screening tool would accelerate disease detection and prevent the loss of health care resources.",2021,10.2196/27468,cross-sectional,diagnosis,CT,Lung
Deep Convolutional Neural Network-based Software Improves Radiologist Detection of Malignant Lung Nodules on Chest Radiographs,"Background Multicenter studies are required to validate the added benefit of using deep convolutional neural network (DCNN) software for detecting malignant pulmonary nodules on chest radiographs. Purpose To compare the performance of radiologists in detecting malignant pulmonary nodules on chest radiographs when assisted by deep learning-based DCNN software with that of radiologists or DCNN software alone in a multicenter setting. Materials and Methods Investigators at four medical centers retrospectively identified 600 lung cancer-containing chest radiographs and 200 normal chest radiographs. Each radiograph with a lung cancer had at least one malignant nodule confirmed by CT and pathologic examination. Twelve radiologists from the four centers independently analyzed the chest radiographs and marked regions of interest. Commercially available deep learning-based computer-aided detection software separately trained, tested, and validated with 19 330 radiographs was used to find suspicious nodules. The radiologists then reviewed the images with the assistance of DCNN software. The sensitivity and number of false-positive findings per image of DCNN software, radiologists alone, and radiologists with the use of DCNN software were analyzed by using logistic regression and Poisson regression. Results The average sensitivity of radiologists improved (from 65.1% [1375 of 2112; 95% confidence interval {CI}: 62.0%, 68.1%] to 70.3% [1484 of 2112; 95% CI: 67.2%, 73.1%], P < .001) and the number of false-positive findings per radiograph declined (from 0.2 [488 of 2400; 95% CI: 0.18, 0.22] to 0.18 [422 of 2400; 95% CI: 0.16, 0.2], P < .001) when the radiologists re-reviewed radiographs with the DCNN software. For the 12 radiologists in this study, 104 of 2400 radiographs were positively changed (from false-negative to true-positive or from false-positive to true-negative) using the DCNN, while 56 of 2400 radiographs were changed negatively. Conclusion Radiologists had better performance with deep convolutional network software for the detection of malignant pulmonary nodules on chest radiographs than without. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Jacobson in this issue.",2020,10.1148/radiol.2019182465,cross-sectional,diagnosis,radiograph,Lung
Deep convolutional neural networks for COVID-19 automatic diagnosis,"This article is mainly concerned with COVID-19 diagnosis from X-ray images. The number of cases infected with COVID-19 is increasing daily, and there is a limitation in the number of test kits needed in hospitals. Therefore, there is an imperative need to implement an efficient automatic diagnosis system to alleviate COVID-19 spreading among people. This article presents a discussion of the utilization of convolutional neural network (CNN) models with different learning strategies for automatic COVID-19 diagnosis. First, we consider the CNN-based transfer learning approach for automatic diagnosis of COVID-19 from X-ray images with different training and testing ratios. Different pre-trained deep learning models in addition to a transfer learning model are considered and compared for the task of COVID-19 detection from X-ray images. Confusion matrices of these studied models are presented and analyzed. Considering the performance results obtained, ResNet models (ResNet18, ResNet50, and ResNet101) provide the highest classification accuracy on the two considered datasets with different training and testing ratios, namely 80/20, 70/30, 60/40, and 50/50. The accuracies obtained using the first dataset with 70/30 training and testing ratio are 97.67%, 98.81%, and 100% for ResNet18, ResNet50, and ResNet101, respectively. For the second dataset, the reported accuracies are 99%, 99.12%, and 99.29% for ResNet18, ResNet50, and ResNet101, respectively. The second approach is the training of a proposed CNN model from scratch. The results confirm that training of the CNN from scratch can lead to the identification of the signs of COVID-19 disease.",2021,10.1002/jemt.23713,cross-sectional,diagnosis,x-ray,Lung
Deep convolutional neural networks for multiplanar lung nodule detection: Improvement in small nodule identification,"PURPOSE: Early detection of lung cancer is of importance since it can increase patients' chances of survival. To detect nodules accurately during screening, radiologists would commonly take the axial, coronal, and sagittal planes into account, rather than solely the axial plane in clinical evaluation. Inspired by clinical work, the paper aims to develop an accurate deep learning framework for nodule detection by a combination of multiple planes. METHODS: The nodule detection system is designed in two stages, multiplanar nodule candidate detection, multiscale false positive (FP) reduction. At the first stage, a deeply supervised encoder-decoder network is trained by axial, coronal, and sagittal slices for the candidate detection task. All possible nodule candidates from the three different planes are merged. To further refine results, a three-dimensional multiscale dense convolutional neural network that extracts multiscale contextual information is applied to remove non-nodules. In the public LIDC-IDRI dataset, 888 computed tomography scans with 1186 nodules accepted by at least three of four radiologists are selected to train and evaluate our proposed system via a tenfold cross-validation scheme. The free-response receiver operating characteristic curve is used for performance assessment. RESULTS: The proposed system achieves a sensitivity of 94.2% with 1.0 FP/scan and a sensitivity of 96.0% with 2.0 FPs/scan. Although it is difficult to detect small nodules (i.e., <6 mm), our designed CAD system reaches a sensitivity of 93.4% (95.0%) of these small nodules at an overall FP rate of 1.0 (2.0) FPs/scan. At the nodule candidate detection stage, results show that the system with a multiplanar method is capable to detect more nodules compared to using a single plane. CONCLUSION: Our approach achieves good performance not only for small nodules but also for large lesions on this dataset. This demonstrates the effectiveness of our developed CAD system for lung nodule detection.",2021,10.1002/mp.14648,cross-sectional,diagnosis,CT,Lung
Deep convolutional neural networks with multiplane consensus labeling for lung function quantification using UTE proton MRI,"BACKGROUND: Ultrashort echo time (UTE) proton MRI has gained popularity for assessing lung structure and function in pulmonary imaging; however, the development of rapid biomarker extraction and regional quantification has lagged behind due to labor-intensive lung segmentation. PURPOSE: To evaluate a deep learning (DL) approach for automated lung segmentation to extract image-based biomarkers from functional lung imaging using 3D radial UTE oxygen-enhanced (OE) MRI. STUDY TYPE: Retrospective study aimed to evaluate a technical development. POPULATION: Forty-five human subjects, including 16 healthy volunteers, 5 asthma, and 24 patients with cystic fibrosis. FIELD STRENGTH/SEQUENCE: 1.5T MRI, 3D radial UTE (TE = 0.08 msec) sequence. ASSESSMENT: Two 3D radial UTE volumes were acquired sequentially under normoxic (21% O(2) ) and hyperoxic (100% O(2) ) conditions. Automated segmentation of the lungs using 2D convolutional encoder-decoder based DL method, and the subsequent functional quantification via adaptive K-means were compared with the results obtained from the reference method, supervised region growing. STATISTICAL TESTS: Relative to the reference method, the performance of DL on volumetric quantification was assessed using Dice coefficient with 95% confidence interval (CI) for accuracy, two-sided Wilcoxon signed-rank test for computation time, and Bland-Altman analysis on the functional measure derived from the OE images. RESULTS: The DL method produced strong agreement with supervised region growing for the right (Dice: 0.97; 95% CI = [0.96, 0.97]; P < 0.001) and left lungs (Dice: 0.96; 95% CI = [0.96, 0.97]; P < 0.001). The DL method averaged 46 seconds to generate the automatic segmentations in contrast to 1.93 hours using the reference method (P < 0.001). Bland-Altman analysis showed nonsignificant intermethod differences of volumetric (P ≥ 0.12) and functional measurements (P ≥ 0.34) in the left and right lungs. DATA CONCLUSION: DL provides rapid, automated, and robust lung segmentation for quantification of regional lung function using UTE proton MRI. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2019;50:1169-1181.",2019,10.1002/jmri.26734,cross-sectional,diagnosis,MRI,Lung
Deep cross-modality (MR-CT) educed distillation learning for cone beam CT lung tumor segmentation,"PURPOSE: Despite the widespread availability of in-treatment room cone beam computed tomography (CBCT) imaging, due to the lack of reliable segmentation methods, CBCT is only used for gross set up corrections in lung radiotherapies. Accurate and reliable auto-segmentation tools could potentiate volumetric response assessment and geometry-guided adaptive radiation therapies. Therefore, we developed a new deep learning CBCT lung tumor segmentation method. METHODS: The key idea of our approach called cross-modality educed distillation (CMEDL) is to use magnetic resonance imaging (MRI) to guide a CBCT segmentation network training to extract more informative features during training. We accomplish this by training an end-to-end network comprised of unpaired domain adaptation (UDA) and cross-domain segmentation distillation networks (SDNs) using unpaired CBCT and MRI datasets. UDA approach uses CBCT and MRI that are not aligned and may arise from different sets of patients. The UDA network synthesizes pseudo MRI from CBCT images. The SDN consists of teacher MRI and student CBCT segmentation networks. Feature distillation regularizes the student network to extract CBCT features that match the statistical distribution of MRI features extracted by the teacher network and obtain better differentiation of tumor from background. The UDA network was implemented with a cycleGAN improved with contextual losses separately on Unet and dense fully convolutional segmentation networks (DenseFCN). Performance comparisons were done against CBCT only using 2D and 3D networks. We also compared against an alternative framework that used UDA with MR segmentation network, whereby segmentation was done on the synthesized pseudo MRI representation. All networks were trained with 216 weekly CBCTs and 82 T2-weighted turbo spin echo MRI acquired from different patient cohorts. Validation was done on 20 weekly CBCTs from patients not used in training. Independent testing was done on 38 weekly CBCTs from patients not used in training or validation. Segmentation accuracy was measured using surface Dice similarity coefficient (SDSC) and Hausdroff distance at 95th percentile (HD95) metrics. RESULTS: The CMEDL approach significantly improved (p < 0.001) the accuracy of both Unet (SDSC of 0.83 ± 0.08; HD95 of 7.69 ± 7.86 mm) and DenseFCN (SDSC of 0.75 ± 0.13; HD95 of 11.42 ± 9.87 mm) over CBCT only 2DUnet (SDSC of 0.69 ± 0.11; HD95 of 21.70 ± 16.34 mm), 3D Unet (SDSC of 0.72 ± 0.20; HD95 15.01 ± 12.98 mm), and DenseFCN (SDSC of 0.66 ± 0.15; HD95 of 22.15 ± 17.19 mm) networks. The alternate framework using UDA with the MRI network was also more accurate than the CBCT only methods but less accurate the CMEDL approach. CONCLUSIONS: Our results demonstrate feasibility of the introduced CMEDL approach to produce reasonably accurate lung cancer segmentation from CBCT images. Further validation on larger datasets is necessary for clinical translation.",2021,10.1002/mp.14902,cross-sectional,treatment,CT +MRI,Lung
Deep Ensemble Model for Classification of Novel Coronavirus in Chest X-Ray Images,"The novel coronavirus, SARS-CoV-2, can be deadly to people, causing COVID-19. The ease of its propagation, coupled with its high capacity for illness and death in infected individuals, makes it a hazard to the community. Chest X-rays are one of the most common but most difficult to interpret radiographic examination for early diagnosis of coronavirus-related infections. They carry a considerable amount of anatomical and physiological information, but it is sometimes difficult even for the expert radiologist to derive the related information they contain. Automatic classification using deep learning models can help in better assessing these infections swiftly. Deep CNN models, namely, MobileNet, ResNet50, and InceptionV3, were applied with different variations, including training the model from the start, fine-tuning along with adjusting learned weights of all layers, and fine-tuning with learned weights along with augmentation. Fine-tuning with augmentation produced the best results in pretrained models. Out of these, two best-performing models (MobileNet and InceptionV3) selected for ensemble learning produced accuracy and FScore of 95.18% and 90.34%, and 95.75% and 91.47%, respectively. The proposed hybrid ensemble model generated with the merger of these deep models produced a classification accuracy and FScore of 96.49% and 92.97%. For test dataset, which was separately kept, the model generated accuracy and FScore of 94.19% and 88.64%. Automatic classification using deep ensemble learning can help radiologists in the correct identification of coronavirus-related infections in chest X-rays. Consequently, this swift and computer-aided diagnosis can help in saving precious human lives and minimizing the social and economic impact on society.",2021,10.1155/2021/8890226,cross-sectional,diagnosis,x-ray,Lung
"Deep Feature Stability Analysis Using CT Images of a Physical Phantom Across Scanner Manufacturers, Cartridges, Pixel Sizes, and Slice Thickness","Image acquisition parameters for computed tomography scans such as slice thickness and field of view may vary depending on tumor size and site. Recent studies have shown that some radiomics features were dependent on voxel size (= pixel size × slice thickness), and with proper normalization, this voxel size dependency could be reduced. Deep features from a convolutional neural network (CNN) have shown great promise in characterizing cancers. However, how do these deep features vary with changes in imaging acquisition parameters? To analyze the variability of deep features, a physical radiomics phantom with 10 different material cartridges was scanned on 8 different scanners. We assessed scans from 3 different cartridges (rubber, dense cork, and normal cork). Deep features from the penultimate layer of the CNN before (pre-rectified linear unit) and after (post-rectified linear unit) applying the rectified linear unit activation function were extracted from a pre-trained CNN using transfer learning. We studied both the interscanner and intrascanner dependency of deep features and also the deep features' dependency over the 3 cartridges. We found some deep features were dependent on pixel size and that, with appropriate normalization, this dependency could be reduced. False discovery rate was applied for multiple comparisons, to mitigate potentially optimistic results. We also used stable deep features for prognostic analysis on 1 non-small cell lung cancer data set.",2020,10.18383/j.tom.2020.00003,,,,
Deep Learning Algorithm for COVID-19 Classification Using Chest X-Ray Images,"Early diagnosis of the harmful severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), along with clinical expertise, allows governments to break the transition chain and flatten the epidemic curve. Although reverse transcription-polymerase chain reaction (RT-PCR) offers quick results, chest X-ray (CXR) imaging is a more reliable method for disease classification and assessment. The rapid spread of the coronavirus disease 2019 (COVID-19) has triggered extensive research towards developing a COVID-19 detection toolkit. Recent studies have confirmed that the deep learning-based approach, such as convolutional neural networks (CNNs), provides an optimized solution for COVID-19 classification; however, they require substantial training data for learning features. Gathering this training data in a short period has been challenging during the pandemic. Therefore, this study proposes a new model of CNN and deep convolutional generative adversarial networks (DCGANs) that classify CXR images into normal, pneumonia, and COVID-19. The proposed model contains eight convolutional layers, four max-pooling layers, and two fully connected layers, which provide better results than the existing pretrained methods (AlexNet and GoogLeNet). DCGAN performs two tasks: (1) generating synthetic/fake images to overcome the challenges of an imbalanced dataset and (2) extracting deep features of all images in the dataset. In addition, it enlarges the dataset and represents the characteristics of diversity to provide a good generalization effect. In the experimental analysis, we used four distinct publicly accessible datasets of chest X-ray images (COVID-19 X-ray, COVID Chest X-ray, COVID-19 Radiography, and CoronaHack-Chest X-Ray) to train and test the proposed CNN and the existing pretrained methods. Thereafter, the proposed CNN method was trained with the four datasets based on the DCGAN synthetic images, resulting in higher accuracy (94.8%, 96.6%, 98.5%, and 98.6%) than the existing pretrained models. The overall results suggest that the proposed DCGAN-CNN approach is a promising solution for efficient COVID-19 diagnosis.",2021,10.1155/2021/9269173,cross-sectional,diagnosis,x-ray,Lung
Deep Learning Algorithm for Reducing CT Slice Thickness: Effect on Reproducibility of Radiomic Features in Lung Cancer,"OBJECTIVE: To retrospectively assess the effect of CT slice thickness on the reproducibility of radiomic features (RFs) of lung cancer, and to investigate whether convolutional neural network (CNN)-based super-resolution (SR) algorithms can improve the reproducibility of RFs obtained from images with different slice thicknesses. MATERIALS AND METHODS: CT images with 1-, 3-, and 5-mm slice thicknesses obtained from 100 pathologically proven lung cancers between July 2017 and December 2017 were evaluated. CNN-based SR algorithms using residual learning were developed to convert thick-slice images into 1-mm slices. Lung cancers were semi-automatically segmented and a total of 702 RFs (tumor intensity, texture, and wavelet features) were extracted from 1-, 3-, and 5-mm slices, as well as the 1-mm slices generated from the 3- and 5-mm images. The stabilities of the RFs were evaluated using concordance correlation coefficients (CCCs). RESULTS: The mean CCCs for the comparisons of original 1 mm vs. 3 mm, 1 mm vs. 5 mm, and 3 mm vs. 5 mm images were 0.41, 0.27, and 0.65, respectively (p < 0.001 for all comparisons). Tumor intensity features showed the best reproducibility while wavelets showed the lowest reproducibility. The majority of RFs failed to achieve reproducibility (CCC ≥ 0.85; 3.6%, 1.0%, and 21.5%, respectively). After applying the CNN-based SR algorithms, the reproducibility significantly improved in all three pairings (mean CCCs: 0.58, 0.45, and 0.72; p < 0.001 for all comparisons). The reproducible RFs also increased (36.3%, 17.4%, and 36.9%, respectively). CONCLUSION: The reproducibility of RFs in lung cancer is significantly influenced by CT slice thickness, which can be improved by the CNN-based SR algorithms.",2019,10.3348/kjr.2019.0212,cross-sectional,informatics,CT,Lung
Deep Learning Algorithms-Based CT Images in Glucocorticoid Therapy in Asthma Children with Small Airway Obstruction,"CT image information data under deep learning algorithms was adopted to evaluate small airway function and analyze the clinical efficacy of different glucocorticoid administration ways in asthmatic children with small airway obstruction. The Res-NET in the deep learning algorithm was used to perform feature extraction, summary classification, and other reconstruction of CT images. A deep learning network model Mask-R-CNN was constructed to enhance the ability of image reconstruction. A total of 118 children hospitalized with acute exacerbation of asthma in the hospital were recruited. After acute exacerbation treatment, 96 children with asthma were screened out for small airway obstruction, which were divided into glucocorticoid aerosol inhalation group (group A, 32 cases), glucocorticoid combined with bronchodilator aerosol inhalation group (group B, 32 cases), and oral hormone therapy group (group C, 32 cases). Asthmatic children with small airway obstruction were screened after acute exacerbation treatment and were rolled into glucocorticoid aerosol inhalation group (group A), glucocorticoid combined with bronchodilators aerosol inhalation group (group B), and oral hormone therapy group (group C). Lung function indicators (maximal mid-expiratory flow (MMEF75 and 25), 50% forced expiratory flow (FEF50), and 75% forced expiratory flow (FEF75)), FeNO level, and airway inflammation indicators (IL-6, IL-35, and eosinophilic (EOS)) were compared before and one month after treatment. The ratio of airway wall thickness to outer diameter (T/D) and the percentage of airway wall area to total airway area (WA%) were measured by e-Health high-resolution CT (HRCT). The constructed network model was used to measure the patient's coronary artery plaque and blood vessel volume, and the image was reconstructed on the Res-Net network. It was found that the MSE value of the Res-Net network was the lowest, and the efficiency was very high during the training process. T/D and WA (%) of asthmatic children with small airway obstruction after treatment were significantly lower than those before treatment (P < 0.01). After treatment, MMEF75/25 and FEF75 were significantly higher than those before treatment (P < 0.05). Lung function-related indicator FEF50 was significantly higher than that before treatment (P < 0.01). FeNO level after treatment was remarkably lower than that before treatment (P < 0.01). In addition, lung function-related indicators, airway inflammation indicators, and FeNO level improved the most in group C, followed by group B, and those improvements in group A were the least obvious, with great differences among groups (P < 0.05). In summary, the Res-Net model proposed was of certain feasibility and effectiveness for CT image segmentation and can effectively improve the clinical evaluation of patient CT image information. Glucocorticoids could improve small airway function and airway inflammation in asthmatic children with small airway obstruction, and oral corticosteroids were more effective than aerosol inhalation therapy.",2021,10.1155/2021/5317403,cross-sectional,combined,CT,Lung
Deep Learning Analysis in Prediction of COVID-19 Infection Status Using Chest CT Scan Features,"Background and aims Non-contrast chest computed tomography (CT) scanning is one of the important tools for evaluating of lung lesions. The aim of this study was to use a deep learning approach for predicting the outcome of patients with COVID-19 into two groups of critical and non-critical according to their CT features. Methods This was carried out as a retrospective study from March to April 2020 in Baqiyatallah Hospital, Tehran, Iran. From total of 1078 patients with COVID-19 pneumonia who underwent chest CT, 169 were critical cases and 909 were non-critical. Deep learning neural networks were used to classify samples into critical or non-critical ones according to the chest CT results. Results The best accuracy of prediction was seen by the presence of diffuse opacities and lesion distribution (both=0.91, 95% CI: 0.83-0.99). The largest sensitivity was achieved using lesion distribution (0.74, 95% CI: 0.55-0.93), and the largest specificity was for presence of diffuse opacities (0.95, 95% CI: 0.9-1). The total model showed an accuracy of 0.89 (95% CI: 0.79-0.99), and the corresponding sensitivity and specificity were 0.71 (95% CI: 0.51-0.91) and 0.93 (95% CI: 0.87-0.96), respectively. Conclusions The results showed that CT scan can accurately classify and predict critical and non-critical COVID-19 cases.",2021,10.1007/978-3-030-71697-4_11,cross-sectional,diagnosis,CT,Lung
Deep learning and lung ultrasound for Covid-19 pneumonia detection and severity classification,"The Covid-19 European outbreak in February 2020 has challenged the world's health systems, eliciting an urgent need for effective and highly reliable diagnostic instruments to help medical personnel. Deep learning (DL) has been demonstrated to be useful for diagnosis using both computed tomography (CT) scans and chest X-rays (CXR), whereby the former typically yields more accurate results. However, the pivoting function of a CT scan during the pandemic presents several drawbacks, including high cost and cross-contamination problems. Radiation-free lung ultrasound (LUS) imaging, which requires high expertise and is thus being underutilised, has demonstrated a strong correlation with CT scan results and a high reliability in pneumonia detection even in the early stages. In this study, we developed a system based on modern DL methodologies in close collaboration with Fondazione IRCCS Policlinico San Matteo's Emergency Department (ED) of Pavia. Using a reliable dataset comprising ultrasound clips originating from linear and convex probes in 2908 frames from 450 hospitalised patients, we conducted an investigation into detecting Covid-19 patterns and ranking them considering two severity scales. This study differs from other research projects by its novel approach involving four and seven classes. Patients admitted to the ED underwent 12 LUS examinations in different chest parts, each evaluated according to standardised severity scales. We adopted residual convolutional neural networks (CNNs), transfer learning, and data augmentation techniques. Hence, employing methodological hyperparameter tuning, we produced state-of-the-art results meeting F1 score levels, averaged over the number of classes considered, exceeding 98%, and thereby manifesting stable measurements over precision and recall.",2021,10.1016/j.compbiomed.2021.104742,cross-sectional,diagnosis,US,Lung
Deep learning applied to lung ultrasound videos for scoring COVID-19 patients: A multicenter study,"In the current pandemic, lung ultrasound (LUS) played a useful role in evaluating patients affected by COVID-19. However, LUS remains limited to the visual inspection of ultrasound data, thus negatively affecting the reliability and reproducibility of the findings. Moreover, many different imaging protocols have been proposed, most of which lacked proper clinical validation. To address these problems, we were the first to propose a standardized imaging protocol and scoring system. Next, we developed the first deep learning (DL) algorithms capable of evaluating LUS videos providing, for each video-frame, the score as well as semantic segmentation. Moreover, we have analyzed the impact of different imaging protocols and demonstrated the prognostic value of our approach. In this work, we report on the level of agreement between the DL and LUS experts, when evaluating LUS data. The results show a percentage of agreement between DL and LUS experts of 85.96% in the stratification between patients at high risk of clinical worsening and patients at low risk. These encouraging results demonstrate the potential of DL models for the automatic scoring of LUS data, when applied to high quality data acquired accordingly to a standardized imaging protocol.",2021,10.1121/10.0004855,,,,
Deep learning approach based on superpixel segmentation assisted labeling for automatic pressure ulcer diagnosis,"A pressure ulcer is an injury of the skin and underlying tissues adjacent to a bony eminence. Patients who suffer from this disease may have difficulty accessing medical care. Recently, the COVID-19 pandemic has exacerbated this situation. Automatic diagnosis based on machine learning (ML) brings promising solutions. Traditional ML requires complicated preprocessing steps for feature extraction. Its clinical applications are thus limited to particular datasets. Deep learning (DL), which extracts features from convolution layers, can embrace larger datasets that might be deliberately excluded in traditional algorithms. However, DL requires large sets of domain specific labeled data for training. Labeling various tissues of pressure ulcers is a challenge even for experienced plastic surgeons. We propose a superpixel-assisted, region-based method of labeling images for tissue classification. The boundary-based method is applied to create a dataset for wound and re-epithelialization (re-ep) segmentation. Five popular DL models (U-Net, DeeplabV3, PsPNet, FPN, and Mask R-CNN) with encoder (ResNet-101) were trained on the two datasets. A total of 2836 images of pressure ulcers were labeled for tissue classification, while 2893 images were labeled for wound and re-ep segmentation. All five models had satisfactory results. DeeplabV3 had the best performance on both tasks with a precision of 0.9915, recall of 0.9915 and accuracy of 0.9957 on the tissue classification; and a precision of 0.9888, recall of 0.9887 and accuracy of 0.9925 on the wound and re-ep segmentation task. Combining segmentation results with clinical data, our algorithm can detect the signs of wound healing, monitor the progress of healing, estimate the wound size, and suggest the need for surgical debridement.",2022,10.1371/journal.pone.0264139,,,,
Deep learning approach to classification of lung cytological images: Two-step training using actual and synthesized images by progressive growing of generative adversarial networks,"Cytology is the first pathological examination performed in the diagnosis of lung cancer. In our previous study, we introduced a deep convolutional neural network (DCNN) to automatically classify cytological images as images with benign or malignant features and achieved an accuracy of 81.0%. To further improve the DCNN's performance, it is necessary to train the network using more images. However, it is difficult to acquire cell images which contain a various cytological features with the use of many manual operations with a microscope. Therefore, in this study, we aim to improve the classification accuracy of a DCNN with the use of actual and synthesized cytological images with a generative adversarial network (GAN). Based on the proposed method, patch images were obtained from a microscopy image. Accordingly, these generated many additional similar images using a GAN. In this study, we introduce progressive growing of GANs (PGGAN), which enables the generation of high-resolution images. The use of these images allowed us to pretrain a DCNN. The DCNN was then fine-tuned using actual patch images. To confirm the effectiveness of the proposed method, we first evaluated the quality of the images which were generated by PGGAN and by a conventional deep convolutional GAN. We then evaluated the classification performance of benign and malignant cells, and confirmed that the generated images had characteristics similar to those of the actual images. Accordingly, we determined that the overall classification accuracy of lung cells was 85.3% which was improved by approximately 4.3% compared to a previously conducted study without pretraining using GAN-generated images. Based on these results, we confirmed that our proposed method will be effective for the classification of cytological images in cases at which only limited data are acquired.",2020,10.1371/journal.pone.0229951,,,,
Deep learning based automatic segmentation of metastasis hotspots in thorax bone SPECT images,"SPECT imaging has been identified as an effective medical modality for diagnosis, treatment, evaluation and prevention of a range of serious diseases and medical conditions. Bone SPECT scan has the potential to provide more accurate assessment of disease stage and severity. Segmenting hotspot in bone SPECT images plays a crucial role to calculate metrics like tumor uptake and metabolic tumor burden. Deep learning techniques especially the convolutional neural networks have been widely exploited for reliable segmentation of hotspots or lesions, organs and tissues in the traditional structural medical images (i.e., CT and MRI) due to their ability of automatically learning the features from images in an optimal way. In order to segment hotspots in bone SPECT images for automatic assessment of metastasis, in this work, we develop several deep learning based segmentation models. Specifically, each original whole-body bone SPECT image is processed to extract the thorax area, followed by image mirror, translation and rotation operations, which augments the original dataset. We then build segmentation models based on two commonly-used famous deep networks including U-Net and Mask R-CNN by fine-tuning their structures. Experimental evaluation conducted on a group of real-world bone SEPCT images reveals that the built segmentation models are workable on identifying and segmenting hotspots of metastasis in bone SEPCT images, achieving a value of 0.9920, 0.7721, 0.6788 and 0.6103 for PA (accuracy), CPA (precision), Rec (recall) and IoU, respectively. Finally, we conclude that the deep learning technology have the huge potential to identify and segment hotspots in bone SPECT images.",2020,10.1371/journal.pone.0243253,,,,
Deep Learning Based Early Detection Framework for Preliminary Diagnosis of COVID-19 via Onboard Smartphone Sensors,"The COVID-19 pandemic has affected almost every country causing devastating economic and social disruption and stretching healthcare systems to the limit. Furthermore, while being the current gold standard, existing test methods including NAAT (Nucleic Acid Amplification Tests), clinical analysis of chest CT (Computer Tomography) scan images, and blood test results, require in-person visits to a hospital which is not an adequate way to control such a highly contagious pandemic. Therefore, top priority must be given, among other things, to enlisting recent and adequate technologies to reduce the adverse impact of this pandemic. Modern smartphones possess a rich variety of embedded MEMS (Micro-Electro-Mechanical-Systems) sensors capable of recording movements, temperature, audio, and video of their carriers. This study leverages the smartphone sensors for the preliminary diagnosis of COVID-19. Deep learning, an important breakthrough in the domain of artificial intelligence in the past decade, has huge potential for extracting apt and appropriate features in healthcare. Motivated from these facts, this paper presents a new framework that leverages advanced machine learning and data analytics techniques for the early detection of coronavirus disease using smartphone embedded sensors. The proposal provides a simple to use and quickly deployable screening tool that can be easily configured with a smartphone. Experimental results indicate that the model can detect positive cases with an overall accuracy of 79% using only the data from the smartphone sensors. This means that the patient can either be isolated or treated immediately to prevent further spread, thereby saving more lives. The proposed approach does not involve any medical tests and is a cost-effective solution that provides robust results.",2021,10.3390/s21206853,,,,
Deep learning classification of lung cancer histology using CT images,"Tumor histology is an important predictor of therapeutic response and outcomes in lung cancer. Tissue sampling for pathologist review is the most reliable method for histology classification, however, recent advances in deep learning for medical image analysis allude to the utility of radiologic data in further describing disease characteristics and for risk stratification. In this study, we propose a radiomics approach to predicting non-small cell lung cancer (NSCLC) tumor histology from non-invasive standard-of-care computed tomography (CT) data. We trained and validated convolutional neural networks (CNNs) on a dataset comprising 311 early-stage NSCLC patients receiving surgical treatment at Massachusetts General Hospital (MGH), with a focus on the two most common histological types: adenocarcinoma (ADC) and Squamous Cell Carcinoma (SCC). The CNNs were able to predict tumor histology with an AUC of 0.71(p = 0.018). We also found that using machine learning classifiers such as k-nearest neighbors (kNN) and support vector machine (SVM) on CNN-derived quantitative radiomics features yielded comparable discriminative performance, with AUC of up to 0.71 (p = 0.017). Our best performing CNN functioned as a robust probabilistic classifier in heterogeneous test sets, with qualitatively interpretable visual explanations to its predictions. Deep learning based radiomics can identify histological phenotypes in lung cancer. It has the potential to augment existing approaches and serve as a corrective aid for diagnosticians.",2021,10.1038/s41598-021-84630-x,cross-sectional,diagnosis,CT,Lung
Deep learning combined with radiomics may optimize the prediction in differentiating high-grade lung adenocarcinomas in ground glass opacity lesions on CT scans,"PURPOSE: Adenocarcinoma (ADC) is the most common histological subtype of lung cancers in non-small cell lung cancer (NSCLC) in which ground glass opacifications (GGOs) found on computed tomography (CT) scans are the most common lesions. However, the presence of a micropapillary or a solid component is identified as an independent predictor of prognosis, suggesting a more extensive resection. The purpose of our study is to explore imaging phenotyping using a method combining radiomics with deep learning (RDL) to predict high-grade patterns within lung ADC. METHODS: Included in this study were 111 patients differentiated as having GGOs and pathologically confirmed ADC. Four different groups of methods were compared to classify the GGOs for the prediction of the pathological subtypes of high-grade lung ADCs in definitive hematoxylin and eosin stain, including radiomics with gray-level features, radiomics with textural features, deep learning method, and the RDL. RESULTS: We evaluated the performance of different models on 111 NSCLC patients using 4-fold cross-validation. The proposed RDL has achieved an overall accuracy of 0.913, which significantly outperforms the other methods (p < 0.01, analysis of variation, ANOVA). In addition, we also verified the generality and practical effectiveness of these models on an independent validation dataset of 28 patients. The results showed that our RDL framework with an accuracy of 0.966 significantly surpassed other methods. CONCLUSION: High-grade lung ADC based on histologic pattern spectrum in GGO lesions might be predicted by the framework combining radiomics with deep learning, which reveals advantage over radiomics alone.",2020,10.1016/j.ejrad.2020.109150,cross-sectional,diagnosis,CT,Lung
Deep learning COVID-19 detection bias: accuracy through artificial intelligence,"BACKGROUND: Detection of COVID-19 cases' accuracy is posing a conundrum for scientists, physicians, and policy-makers. As of April 23, 2020, 2.7 million cases have been confirmed, over 190,000 people are dead, and about 750,000 people are reported recovered. Yet, there is no publicly available data on tests that could be missing infections. Complicating matters and furthering anxiety are specific instances of false-negative tests. METHODS: We developed a deep learning model to improve accuracy of reported cases and to precisely predict the disease from chest X-ray scans. Our model relied on convolutional neural networks (CNNs) to detect structural abnormalities and disease categorization that were keys to uncovering hidden patterns. To do so, a transfer learning approach was deployed to perform detections from the chest anterior-posterior radiographs of patients. We used publicly available datasets to achieve this. RESULTS: Our results offer very high accuracy (96.3%) and loss (0.151 binary cross-entropy) using the public dataset consisting of patients from different countries worldwide. As the confusion matrix indicates, our model is able to accurately identify true negatives (74) and true positives (32); this deep learning model identified three cases of false-positive and one false-negative finding from the healthy patient scans. CONCLUSIONS: Our COVID-19 detection model minimizes manual interaction dependent on radiologists as it automates identification of structural abnormalities in patient's CXRs, and our deep learning model is likely to detect true positives and true negatives and weed out false positive and false negatives with > 96.3% accuracy.",2020,10.1007/s00264-020-04609-7,cross-sectional,diagnosis,x-ray,Lung
Deep Learning COVID-19 Features on CXR Using Limited Training Data Sets,"Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage.",2020,10.1109/tmi.2020.2993291,cross-sectional,diagnosis,x-ray,Lung
Deep learning driven predictive treatment planning for adaptive radiotherapy of lung cancer,"BACKGROUND AND PURPOSE: To develop a novel deep learning algorithm of sequential analysis, Seq2Seq, for predicting weekly anatomical changes of lung tumor and esophagus during definitive radiotherapy, incorporate the potential tumor shrinkage into a predictive treatment planning paradigm, and improve the therapeutic ratio. METHODS AND MATERIALS: Seq2Seq starts with the primary tumor and esophagus observed on the planning CT to predict their geometric evolution during radiotherapy on a weekly basis, and subsequently updates the predictions with new snapshots acquired via weekly CBCTs. Seq2Seq is equipped with convolutional long short term memory to analyze the spatial-temporal changes of longitudinal images, trained and validated using a dataset including sixty patients. Predictive plans were optimized according to each weekly prediction and made ready for weekly deployment to mitigate the clinical burden of online weekly replanning. RESULTS: Seq2Seq tracks structural changes well: DICE between predicted and actual weekly tumor and esophagus were (0.83 ± 0.10, 0.79 ± 0.14, 0.78 ± 0.12, 0.77 ± 0.12, 0.75 ± 0.12, 0.71 ± 0.17), and (0.72 ± 0.16, 0.73 ± 0.11, 0.75 ± 0.08, 0.74 ± 0.09, 0.72 ± 0.14, 0.71 ± 0.14), respectively, while the average Hausdorff distances were within 2 mm. Evaluating dose to the actual weekly tumor and esophagus, a 4.2 Gy reduction in esophagus mean dose while maintaining 60 Gy tumor coverage was achieved with the predictive weekly plans, compared to the plan optimized using the initial tumor and esophagus alone, primarily due to noticeable tumor shrinkage during radiotherapy. CONCLUSION: It is feasible to predict the longitudinal changes of tumor and esophagus with the Seq2Seq, which could lead to improving the efficiency and effectiveness of lung adaptive radiotherapy.",2022,10.1016/j.radonc.2022.02.013,cross-sectional,treatment,CT,Lung
Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) With CT Images,"A novel coronavirus (COVID-19) recently emerged as an acute respiratory syndrome, and has caused a pneumonia outbreak world-widely. As the COVID-19 continues to spread rapidly across the world, computed tomography (CT) has become essentially important for fast diagnoses. Thus, it is urgent to develop an accurate computer-aided method to assist clinicians to identify COVID-19-infected patients by CT images. Here, we have collected chest CT scans of 88 patients diagnosed with COVID-19 from hospitals of two provinces in China, 100 patients infected with bacteria pneumonia, and 86 healthy persons for comparison and modeling. Based on the data, a deep learning-based CT diagnosis system was developed to identify patients with COVID-19. The experimental results showed that our model could accurately discriminate the COVID-19 patients from the bacteria pneumonia patients with an AUC of 0.95, recall (sensitivity) of 0.96, and precision of 0.79. When integrating three types of CT images, our model achieved a recall of 0.93 with precision of 0.86 for discriminating COVID-19 patients from others. Moreover, our model could extract main lesion features, especially the ground-glass opacity (GGO), which are visually helpful for assisted diagnoses by doctors. An online server is available for online diagnoses with CT images by our server (http://biomed.nscc-gz.cn/model.php). Source codes and datasets are available at our GitHub (https://github.com/SY575/COVID19-CT).",2021,10.1109/tcbb.2021.3065361,cross-sectional,diagnosis,CT,Lung
Deep Learning Enables Automatic Classification of Emphysema Pattern at CT,"BackgroundPattern of emphysema at chest CT, scored visually by using the Fleischner Society system, is associated with physiologic impairment and mortality risk.PurposeTo determine whether participant-level emphysema pattern could predict impairment and mortality when classified by using a deep learning method.Materials and MethodsThis retrospective analysis of Genetic Epidemiology of COPD (COPDGene) study participants enrolled between 2007 and 2011 included those with baseline CT, visual emphysema scores, and survival data through 2018. Participants were partitioned into nonoverlapping sets of 2407 for algorithm training, 100 for validation and parameter tuning, and 7143 for testing. A deep learning algorithm using convolutional neural network and long short-term memory architectures was trained to classify pattern of emphysema according to Fleischner criteria. Deep learning scores were compared with visual scores and clinical parameters including pulmonary function tests. Cox proportional hazard models were used to evaluate relationships between emphysema scores and survival. The algorithm was also tested by using CT and clinical data in 1962 participants enrolled in the Evaluation of COPD Longitudinally to Identify Predictive Surrogate End-points (ECLIPSE) study.ResultsA total of 7143 COPDGene participants (mean age ± standard deviation, 59.8 years ± 8.9; 3734 men and 3409 women) were evaluated. Deep learning emphysema classifications were associated with impaired pulmonary function tests, 6-minute walk distance, and St George's Respiratory Questionnaire at univariate analysis (P < .001 for each). Testing in the ECLIPSE cohort showed similar associations (P < .001). In the COPDGene test cohort, deep learning emphysema classification improved the fit of linear mixed models in the prediction of these clinical parameters compared with visual scoring (P < .001). Compared with participants without emphysema, mortality was greater in participants classified by the deep learning algorithm as having any grade of emphysema (adjusted hazard ratios were 1.5, 1.7, 2.9, 5.3, and 9.7, respectively, for trace, mild, moderate, confluent, and advanced destructive emphysema; P < .05).ConclusionDeep learning automation of the Fleischner grade of emphysema at chest CT is associated with clinical measures of pulmonary insufficiency and the risk of mortality.© RSNA, 2019Online supplemental material is available for this article.",2020,10.1148/radiol.2019191022,cross-sectional,diagnosis,CT,Lung
Deep Learning for Automatic Calcium Scoring in CT: Validation Using Multiple Cardiac CT and Chest CT Protocols,"Background Although several deep learning (DL) calcium scoring methods have achieved excellent performance for specific CT protocols, their performance in a range of CT examination types is unknown. Purpose To evaluate the performance of a DL method for automatic calcium scoring across a wide range of CT examination types and to investigate whether the method can adapt to different types of CT examinations when representative images are added to the existing training data set. Materials and Methods The study included 7240 participants who underwent various types of nonenhanced CT examinations that included the heart: coronary artery calcium (CAC) scoring CT, diagnostic CT of the chest, PET attenuation correction CT, radiation therapy treatment planning CT, CAC screening CT, and low-dose CT of the chest. CAC and thoracic aorta calcification (TAC) were quantified using a convolutional neural network trained with (a) 1181 low-dose chest CT examinations (baseline), (b) a small set of examinations of the respective type supplemented to the baseline (data specific), and (c) a combination of examinations of all available types (combined). Supplemental training sets contained 199-568 CT images depending on the calcium burden of each population. The DL algorithm performance was evaluated with intraclass correlation coefficients (ICCs) between DL and manual (Agatston) CAC and (volume) TAC scoring and with linearly weighted κ values for cardiovascular risk categories (Agatston score; cardiovascular disease risk categories: 0, 1-10, 11-100, 101-400, >400). Results At baseline, the DL algorithm yielded ICCs of 0.79-0.97 for CAC and 0.66-0.98 for TAC across the range of different types of CT examinations. ICCs improved to 0.84-0.99 (CAC) and 0.92-0.99 (TAC) for CT protocol-specific training and to 0.85-0.99 (CAC) and 0.96-0.99 (TAC) for combined training. For assignment of cardiovascular disease risk category, the κ value for all test CT scans was 0.90 (95% confidence interval [CI]: 0.89, 0.91) for the baseline training. It increased to 0.92 (95% CI: 0.91, 0.93) for both data-specific and combined training. Conclusion A deep learning calcium scoring algorithm for quantification of coronary and thoracic calcium was robust, despite substantial differences in CT protocol and variations in subject population. Augmenting the algorithm training with CT protocol-specific images further improved algorithm performance. © RSNA, 2020 See also the editorial by Vannier in this issue.",2020,10.1148/radiol.2020191621,cross-sectional,informatics,CT,Thorax
Deep Learning for Classification and Localization of COVID-19 Markers in Point-of-Care Lung Ultrasound,"Deep learning (DL) has proved successful in medical imaging and, in the wake of the recent COVID-19 pandemic, some works have started to investigate DL-based solutions for the assisted diagnosis of lung diseases. While existing works focus on CT scans, this paper studies the application of DL techniques for the analysis of lung ultrasonography (LUS) images. Specifically, we present a novel fully-annotated dataset of LUS images collected from several Italian hospitals, with labels indicating the degree of disease severity at a frame-level, video-level, and pixel-level (segmentation masks). Leveraging these data, we introduce several deep models that address relevant tasks for the automatic analysis of LUS images. In particular, we present a novel deep network, derived from Spatial Transformer Networks, which simultaneously predicts the disease severity score associated to a input frame and provides localization of pathological artefacts in a weakly-supervised way. Furthermore, we introduce a new method based on uninorms for effective frame score aggregation at a video-level. Finally, we benchmark state of the art deep models for estimating pixel-level segmentations of COVID-19 imaging biomarkers. Experiments on the proposed dataset demonstrate satisfactory results on all the considered tasks, paving the way to future research on DL for the assisted diagnosis of COVID-19 from LUS data.",2020,10.1109/tmi.2020.2994459,cross-sectional,diagnosis,LUS,Lung
Deep learning for COVID-19 detection based on CT images,"COVID-19 has tremendously impacted patients and medical systems globally. Computed tomography images can effectively complement the reverse transcription-polymerase chain reaction testing. This study adopted a convolutional neural network for COVID-19 testing. We examined the performance of different pre-trained models on CT testing and identified that larger, out-of-field datasets boost the testing power of the models. This suggests that a priori knowledge of the models from out-of-field training is also applicable to CT images. The proposed transfer learning approach proves to be more successful than the current approaches described in literature. We believe that our approach has achieved the state-of-the-art performance in identification thus far. Based on experiments with randomly sampled training datasets, the results reveal a satisfactory performance by our model. We investigated the relevant visual characteristics of the CT images used by the model; these may assist clinical doctors in manual screening.",2021,10.1038/s41598-021-93832-2,cross-sectional,diagnosis,CT,Lung
Deep Learning for Detecting Pneumothorax on Chest Radiographs after Needle Biopsy: Clinical Implementation,"Background Accurate detection of pneumothorax on chest radiographs, the most common complication of percutaneous transthoracic needle biopsies (PTNBs), is not always easy in practice. A computer-aided detection (CAD) system may help detect pneumothorax. Purpose To investigate whether a deep learning-based CAD system can improve detection performance for pneumothorax on chest radiographs after PTNB in clinical practice. Materials and Methods A CAD system for post-PTNB pneumothorax detection on chest radiographs was implemented in an institution in February 2020. This retrospective cohort study consecutively included chest radiographs interpreted with CAD assistance (CAD-applied group; February 2020 to November 2020) and those interpreted before implementation (non-CAD group; January 2018 to January 2020). The reference standard was defined by consensus reading by two radiologists. The diagnostic accuracy for pneumothorax was compared between the two groups using generalized estimating equations. Matching was performed according to whether the radiograph reader and PTNB operator were the same using the greedy method. Results A total of 676 radiographs from 655 patients (mean age: 67 years ± 11; 390 men) in the CAD-applied group and 676 radiographs from 664 patients (mean age: 66 years ± 12; 400 men) in the non-CAD group were included. The incidence of pneumothorax was 18.2% (123 of 676 radiographs) in the CAD-applied group and 22.5% (152 of 676 radiographs) in the non-CAD group (P = .05). The CAD-applied group showed higher sensitivity (85.4% vs 67.1%), negative predictive value (96.8% vs 91.3%), and accuracy (96.8% vs 92.3%) than the non-CAD group (all P < .001). The sensitivity for a small amount of pneumothorax improved in the CAD-applied group (pneumothorax of <10%: 74.5% vs 51.4%, P = .009; pneumothorax of 10%-15%: 92.7% vs 70.2%, P = .008). Among patients with pneumothorax, 34 of 655 (5.0%) in the non-CAD group and 16 of 664 (2.4%) in the CAD-applied group (P = .009) required subsequent drainage catheter insertion. Conclusion A deep learning-based computer-aided detection system improved the detection performance for pneumothorax on chest radiographs after lung biopsy. © RSNA, 2022 See also the editorial by Schiebler and Hartung in this issue.",2022,10.1148/radiol.211706,retrospective cohort,diagnosis,Radiograph,Lung
Deep learning for diagnosis of COVID-19 using 3D CT scans,"A new pneumonia-type coronavirus, COVID-19, recently emerged in Wuhan, China. COVID-19 has subsequently infected many people and caused many deaths worldwide. Isolating infected people is one of the methods of preventing the spread of this virus. CT scans provide detailed imaging of the lungs and assist radiologists in diagnosing COVID-19 in hospitals. However, a person's CT scan contains hundreds of slides, and the diagnosis of COVID-19 using such scans can lead to delays in hospitals. Artificial intelligence techniques could assist radiologists with rapidly and accurately detecting COVID-19 infection from these scans. This paper proposes an artificial intelligence (AI) approach to classify COVID-19 and normal CT volumes. The proposed AI method uses the ResNet-50 deep learning model to predict COVID-19 on each CT image of a 3D CT scan. Then, this AI method fuses image-level predictions to diagnose COVID-19 on a 3D CT volume. We show that the proposed deep learning model provides 96% AUC value for detecting COVID-19 on CT scans.",2021,10.1016/j.compbiomed.2021.104306,cross-sectional,diagnosis,CT,Lung
Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study,"BACKGROUND: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. METHODS AND FINDINGS: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5-93.3], survival median = 1.7 years [range 0.0-11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5-93.3], survival median = 1.3 years [range 0.0-11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2-88.0], survival median = 3.1 years [range 0.0-8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95% CI 0.63-0.78], p < 0.001) and surgery (AUC = 0.71 [95% CI 0.60-0.82], p < 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p < 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters-including age, sex, and tumor node metastasis stage-as well as demonstrate high robustness against test-retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. CONCLUSIONS: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.",2018,10.1371/journal.pmed.1002711,cross-sectional,prognosis,CT,Lung
Deep learning for lung disease segmentation on CT: Which reconstruction kernel should be used?,"PURPOSE: The purpose of this study was to determine whether a single reconstruction kernel or both high and low frequency kernels should be used for training deep learning models for the segmentation of diffuse lung disease on chest computed tomography (CT). MATERIALS AND METHODS: Two annotated datasets of COVID-19 pneumonia (323,960 slices) and interstitial lung disease (ILD) (4,284 slices) were used. Annotated CT images were used to train a U-Net architecture to segment disease. All CT slices were reconstructed using both a lung kernel (LK) and a mediastinal kernel (MK). Three different trainings, resulting in three different models were compared for each disease: training on LK only, MK only or LK+MK images. Dice similarity scores (DSC) were compared using the Wilcoxon signed-rank test. RESULTS: Models only trained on LK images performed better on LK images than on MK images (median DSC = 0.62 [interquartile range (IQR): 0.54, 0.69] vs. 0.60 [IQR: 0.50, 0.70], P < 0.001 for COVID-19 and median DSC = 0.62 [IQR: 0.56, 0.69] vs. 0.50 [IQR 0.43, 0.57], P < 0.001 for ILD). Similarly, models only trained on MK images performed better on MK images (median DSC = 0.62 [IQR: 0.53, 0.68] vs. 0.54 [IQR: 0.47, 0.63], P < 0.001 for COVID-19 and 0.69 [IQR: 0.61, 0.73] vs. 0.63 [IQR: 0.53, 0.70], P < 0.001 for ILD). Models trained on both kernels performed better or similarly than those trained on only one kernel. For COVID-19, median DSC was 0.67 (IQR: =0.59, 0.73) when applied on LK images and 0.67 (IQR: 0.60, 0.74) when applied on MK images (P < 0.001 for both). For ILD, median DSC was 0.69 (IQR: 0.63, 0.73) when applied on LK images (P = 0.006) and 0.68 (IQR: 0.62, 0.72) when applied on MK images (P > 0.99). CONCLUSION: Reconstruction kernels impact the performance of deep learning-based models for lung disease segmentation. Training on both LK and MK images improves the performance.",2021,10.1016/j.diii.2021.10.001,cross-sectional,diagnosis,CT,Lung
Deep Learning for Malignancy Risk Estimation of Pulmonary Nodules Detected at Low-Dose Screening CT,"Background Accurate estimation of the malignancy risk of pulmonary nodules at chest CT is crucial for optimizing management in lung cancer screening. Purpose To develop and validate a deep learning (DL) algorithm for malignancy risk estimation of pulmonary nodules detected at screening CT. Materials and Methods In this retrospective study, the DL algorithm was developed with 16 077 nodules (1249 malignant) collected -between 2002 and 2004 from the National Lung Screening Trial. External validation was performed in the following three -cohorts -collected between 2004 and 2010 from the Danish Lung Cancer Screening Trial: a full cohort containing all 883 nodules (65 -malignant) and two cancer-enriched cohorts with size matching (175 nodules, 59 malignant) and without size matching (177 -nodules, 59 malignant) of benign nodules selected at random. Algorithm performance was measured by using the area under the receiver operating characteristic curve (AUC) and compared with that of the Pan-Canadian Early Detection of Lung Cancer (PanCan) model in the full cohort and a group of 11 clinicians composed of four thoracic radiologists, five radiology residents, and two pulmonologists in the cancer-enriched cohorts. Results The DL algorithm significantly outperformed the PanCan model in the full cohort (AUC, 0.93 [95% CI: 0.89, 0.96] vs 0.90 [95% CI: 0.86, 0.93]; P = .046). The algorithm performed comparably to thoracic radiologists in cancer-enriched cohorts with both random benign nodules (AUC, 0.96 [95% CI: 0.93, 0.99] vs 0.90 [95% CI: 0.81, 0.98]; P = .11) and size-matched benign nodules (AUC, 0.86 [95% CI: 0.80, 0.91] vs 0.82 [95% CI: 0.74, 0.89]; P = .26). Conclusion The deep learning algorithm showed excellent performance, comparable to thoracic radiologists, for malignancy risk estimation of pulmonary nodules detected at screening CT. This algorithm has the potential to provide reliable and reproducible malignancy risk scores for clinicians, which may help optimize management in lung cancer screening. © RSNA, 2021 Online supplemental material is available for this article. See also the editorial by Tammemägi in this issue.",2021,10.1148/radiol.2021204433,cross-sectional,diagnosis,CT,Lung
Deep learning for predicting COVID-19 malignant progression,"As COVID-19 is highly infectious, many patients can simultaneously flood into hospitals for diagnosis and treatment, which has greatly challenged public medical systems. Treatment priority is often determined by the symptom severity based on first assessment. However, clinical observation suggests that some patients with mild symptoms may quickly deteriorate. Hence, it is crucial to identify patient early deterioration to optimize treatment strategy. To this end, we develop an early-warning system with deep learning techniques to predict COVID-19 malignant progression. Our method leverages CT scans and the clinical data of outpatients and achieves an AUC of 0.920 in the single-center study. We also propose a domain adaptation approach to improve the generalization of our model and achieve an average AUC of 0.874 in the multicenter study. Moreover, our model automatically identifies crucial indicators that contribute to the malignant progression, including Troponin, Brain natriuretic peptide, White cell count, Aspartate aminotransferase, Creatinine, and Hypersensitive C-reactive protein.",2021,10.1016/j.media.2021.102096,cross-sectional,diagnosis,CT,Lung
A semi-automatic approach for epicardial adipose tissue segmentation and quantification on cardiac CT scans,"Many studies have shown that epicardial fat is associated with a higher risk of heart diseases. Accurate epicardial adipose tissue quantification is still an open research issue. Considering that manual approaches are generally user-dependent and time-consuming, computer-assisted tools can considerably improve the result repeatability as well as reduce the time required for performing an accurate segmentation. Unfortunately, fully automatic strategies might not always identify the Region of Interest (ROI) correctly. Moreover, they could require user interaction for handling unexpected events. This paper proposes a semi-automatic method for Epicardial Fat Volume (EFV) segmentation and quantification. Unlike supervised Machine Learning approaches, the method does not require any initial training or modeling phase to set up the system. As a further key novelty, the method also yields a subdivision into quartiles of the adipose tissue density. Quartile-based analysis conveys information about fat densities distribution, enabling an in-depth study towards a possible correlation between fat amounts, fat distribution, and heart diseases. Experimental tests were performed on 50 Calcium Score (CaSc) series and 95 Coronary Computed Tomography Angiography (CorCTA) series. Area-based and distance-based metrics were used to evaluate the segmentation accuracy, by obtaining Dice Similarity Coefficient (DSC) = 93.74% and Mean Absolute Distance (MAD) = 2.18 for CaSc, as well as DSC = 92.48% and MAD = 2.87 for CorCTA. Moreover, the Pearson and Spearman coefficients were computed for quantifying the correlation between the ground-truth EFV and the corresponding automated measurement, by obtaining 0.9591 and 0.9490 for CaSc, and 0.9513 and 0.9319 for CorCTA, respectively. In conclusion, the proposed EFV quantification and analysis method represents a clinically useable tool assisting the cardiologist to gain insights into a specific clinical scenario and leading towards personalized diagnosis and therapy.",2019,10.1016/j.compbiomed.2019.103424,cross-sectional,diagnosis,CT,Heart
Artificial intelligence based automatic quantification of epicardial adipose tissue suitable for large scale population studies,"To develop a fully automatic model capable of reliably quantifying epicardial adipose tissue (EAT) volumes and attenuation in large scale population studies to investigate their relation to markers of cardiometabolic risk. Non-contrast cardiac CT images from the SCAPIS study were used to train and test a convolutional neural network based model to quantify EAT by: segmenting the pericardium, suppressing noise-induced artifacts in the heart chambers, and, if image sets were incomplete, imputing missing EAT volumes. The model achieved a mean Dice coefficient of 0.90 when tested against expert manual segmentations on 25 image sets. Tested on 1400 image sets, the model successfully segmented 99.4% of the cases. Automatic imputation of missing EAT volumes had an error of less than 3.1% with up to 20% of the slices in image sets missing. The most important predictors of EAT volumes were weight and waist, while EAT attenuation was predicted mainly by EAT volume. A model with excellent performance, capable of fully automatic handling of the most common challenges in large scale EAT quantification has been developed. In studies of the importance of EAT in disease development, the strong co-variation with anthropometric measures needs to be carefully considered.",2021,10.1038/s41598-021-03150-w,cross-sectional,diagnosis,CT,Heart
Relationship of epicardial fat volume from noncontrast CT with impaired myocardial flow reserve by positron emission tomography,"BACKGROUND: Impaired myocardial flow reserve (MFR) is a marker of coronary vascular dysfunction with prognostic significance. OBJECTIVES: We aimed to investigate the relationship between epicardial fat volume (EFV) measured from noncontrast CT and impaired MFR derived from rest-stress Rb-82 positron emission tomography (PET). METHODS: We retrospectively studied 85 consecutive patients without known coronary artery disease who underwent rest-stress Rb-82 myocardial PET/CT and were subsequently referred for invasive coronary angiography. EFV was computed from noncontrast CT by validated software and indexed to body surface area (EFVi, cm3/m2). Global stress and rest MFR were automatically derived from PET. Patient age, sex, cardiovascular risk factors, coronary calcium score (CCS), and EFVi were combined by boosted ensemble machine learning algorithm into a novel composite risk score, using 10-fold cross-validation, to predict impaired global MFR (MFR ≤2.0) by PET. RESULTS: Patients with impaired MFR (44 of 85; 52%) were older (71 vs. 65 years; P = .03) and had higher frequency of CCS (≥400; P = .02) with significantly higher EFVi (63.1 ± 20.4 vs. 51.3 ± 14.1 cm3/m2; P = .003). On multivariate logistic regression (with age, sex, number of risk factors, CCS, and EFVi), EFVi was the only independent predictor of impaired MFR (odds ratio, 7.39; P = .02). The machine learning composite risk score significantly improved risk reclassification of impaired MFR compared to CCS or EFVi alone (integrated discrimination improvement = 0.19; P = .007 and IDI = 0.22; P = .002, respectively). CONCLUSIONS: Increased EFVi and composite risk score combining EFVi and CCS significantly improve identification of impaired global MFR by PET.",2015,10.1016/j.jcct.2015.03.005,cross-sectional,diagnosis,PET/CT,Heart
Adaptive Fruitfly Based Modified Region Growing Algorithm for Cardiac Fat Segmentation Using Optimal Neural Network,"Epicardial adipose tissue is a visceral fat that has remained an entity of concern for decades owing to its high correlation with coronary heart disease. It continues to stump medical practitioners on the pretext of its relevance with pericardial fat and its dependence on a numerous other parameters including ethnicity and physique of an individual. This calls for a fool-proof algorithm that promises accurate classification and segmentation, hence an immaculate prediction. CT is immensely popular and widely preferred for diagnosis. Implementation of an improvised algorithm in CT would be a natural necessity. This research work proposes a Fruitfly Algorithm based Modified region growing algorithm is applied to the acquired CT images to segment fat accurately. The proposed methodology promises image registration and classification in order to segment two cardiac fats namely epicardial, pericardial and mediastinal. The main contributions are (1) Fat feature extraction: Construction of GLCM features CT image (2) Development of GWO based optimal neural network for classification; (3) Modeling the fat segmentation using modified region growing algorithm with Fruitfly optimization. The entire experimentation has been implemented in MATLAB simulation environment and final result is expected to flaunt a definite distinction between cardiac mediastinal and epicardial fats. Parallely, the accuracy, sensitivity, specificity, FPR and FNR have been stated and contrasted methodically with the existing methodology. This venture aims at spurring the healthcare industry towards smarter computational techniques that multiplies efficiency manifold.",2019,10.1007/s10916-019-1227-3,cross-sectional,diagnosis,CT,Heart
Deep Learning for Quantification of Epicardial and Thoracic Adipose Tissue From Non-Contrast CT,"Epicardial adipose tissue (EAT) is a visceral fat deposit related to coronary artery disease. Fully automated quantification of EAT volume in clinical routine could be a timesaving and reliable tool for cardiovascular risk assessment. We propose a new fully automated deep learning framework for EAT and thoracic adipose tissue (TAT) quantification from non-contrast coronary artery calcium computed tomography (CT) scans. The first multi-task convolutional neural network (ConvNet) is used to determine heart limits and perform segmentation of heart and adipose tissues. The second ConvNet, combined with a statistical shape model, allows for pericardium detection. EAT and TAT segmentations are then obtained from outputs of both ConvNets. We evaluate the performance of the method on CT data sets from 250 asymptomatic individuals. Strong agreement between automatic and expert manual quantification is obtained for both EAT and TAT with median Dice score coefficients of 0.823 (inter-quartile range (IQR): 0.779-0.860) and 0.905 (IQR: 0.862-0.928), respectively; with excellent correlations of 0.924 and 0.945 for EAT and TAT volumes. Computations are performed in <6 s on a standard personal computer for one CT scan. Therefore, the proposed method represents a tool for rapid fully automated quantification of adipose tissue and may improve cardiovascular risk stratification in patients referred for routine CT calcium scans.",2018,10.1109/tmi.2018.2804799,retrospective cohort,prognosis,CT,Pericardium
Deep learning for semi-automated unidirectional measurement of lung tumor size in CT,"BACKGROUND: Performing Response Evaluation Criteria in Solid Tumor (RECISTS) measurement is a non-trivial task requiring much expertise and time. A deep learning-based algorithm has the potential to assist with rapid and consistent lesion measurement. PURPOSE: The aim of this study is to develop and evaluate deep learning (DL) algorithm for semi-automated unidirectional CT measurement of lung lesions. METHODS: This retrospective study included 1617 lung CT images from 8 publicly open datasets. A convolutional neural network was trained using 1373 training and validation images annotated by two radiologists. Performance of the DL algorithm was evaluated 244 test images annotated by one radiologist. DL algorithm's measurement consistency with human radiologist was evaluated using Intraclass Correlation Coefficient (ICC) and Bland-Altman plotting. Bonferroni's method was used to analyze difference in their diagnostic behavior, attributed by tumor characteristics. Statistical significance was set at p < 0.05. RESULTS: The DL algorithm yielded ICC score of 0.959 with human radiologist. Bland-Altman plotting suggested 240 (98.4 %) measurements realized within the upper and lower limits of agreement (LOA). Some measurements outside the LOA revealed difference in clinical reasoning between DL algorithm and human radiologist. Overall, the algorithm marginally overestimated the size of lesion by 2.97 % compared to human radiologists. Further investigation indicated tumor characteristics may be associated with the DL algorithm's diagnostic behavior of over or underestimating the lesion size compared to human radiologist. CONCLUSIONS: The DL algorithm for unidirectional measurement of lung tumor size demonstrated excellent agreement with human radiologist.",2021,10.1186/s40644-021-00413-7,retrospective cohort,prognosis,CT,Lung
Deep Learning for the Classification of Small (≤2 cm) Pulmonary Nodules on CT Imaging: A Preliminary Study,"RATIONALE AND OBJECTIVES: We aimed to present a deep learning-based malignancy prediction model (CT-lungNET) that is simpler and faster to use in the diagnosis of small (≤2 cm) pulmonary nodules on nonenhanced chest CT and to preliminarily evaluate its performance and usefulness for human reviewers. MATERIALS AND METHODS: A total of 173 whole nonenhanced chest CT images containing 208 pulmonary nodules (94 malignant and 11 benign nodules) ranging in size from 5 mm to 20 mm were collected. Pathologically confirmed nodules or nodules that remained unchanged for more than 1 year were included, and 30 benign and 30 malignant nodules were randomly assigned into the test set. We designed CT-lungNET to include three convolutional layers followed by two fully-connected layers and compared its diagnostic performance and processing time with those of AlexNET by using the area under the receiver operating curve (AUROC). An observer performance test was conducted involving eight human reviewers of four different groups (medical students, physicians, radiologic residents, and thoracic radiologists) at test 1 and test 2, referring to the CT-lungNET's malignancy prediction rate with pairwise comparison receiver operating curve analysis. RESULTS: CT-lungNET showed an improved AUROC (0.85; 95% confidence interval: 0.74-0.93), compared to that of the AlexNET (0.82; 95% confidence interval: 0.71-0.91). The processing speed per one image slice for CT-lungNET was about 10 times faster than that for AlexNET (0.90 vs. 8.79 seconds). During the observer performance test, the classification performance of nonradiologists was increased with the aid of CTlungNET, (mean AUC improvement: 0.13; range: 0.03-0.19) but not significantly so in the radiologists group (mean AUC improvement: 0.02; range: -0.02 to 0.07). CONCLUSION: CT-lungNET was able to provide better classification results with a significantly shorter amount of processing time as compared to AlexNET in the diagnosis of small pulmonary nodules on nonenhanced chest CT. In this preliminary observer performance test, CT-lungNET may have a role acting as a second reviewer for less experienced reviewers, resulting in enhanced performance in the diagnosis of early lung cancer.",2020,10.1016/j.acra.2019.05.018,retrospective cohort,diagnosis,CT,Lung
Deep learning model for automatic contouring of cardiovascular substructures on radiotherapy planning CT images: Dosimetric validation and reader study based clinical acceptability testing,"BACKGROUND AND PURPOSE: Large radiotherapy (RT) planning imaging datasets with consistently contoured cardiovascular structures are essential for robust cardiac radiotoxicity research in thoracic cancers. This study aims to develop and validate a highly accurate automatic contouring model for the heart, cardiac chambers, and great vessels for RT planning computed tomography (CT) images that can be used for dose-volume parameter estimation. MATERIALS AND METHODS: A neural network model was trained using a dataset of 127 expertly contoured planning CT images from RT treatment of locally advanced non-small-cell lung cancer (NSCLC) patients. Evaluation of geometric accuracy and quality of dosimetric parameter estimation was performed on 50 independent scans with contrast and without contrast enhancement. The model was further evaluated regarding the clinical acceptability of the contours in 99 scans randomly sampled from the RTOG-0617 dataset by three experienced radiation oncologists. RESULTS: Median surface dice at 3 mm tolerance for all dedicated thoracic structures was 90% in the test set. Median absolute difference between mean dose computed with model contours and expert contours was 0.45 Gy averaged over all structures. The mean clinical acceptability rate by majority vote in the RTOG-0617 scans was 91%. CONCLUSION: This model can be used to contour the heart, cardiac chambers, and great vessels in large datasets of RT planning thoracic CT images accurately, quickly, and consistently. Additionally, the model can be used as a time-saving tool for contouring in clinic practice.",2021,10.1016/j.radonc.2021.10.008,retrospective cohort,others,CT,Cardiovasculature
Deep learning model for distinguishing novel coronavirus from other chest related infections in X-ray images,"Novel Coronavirus is deadly for humans and animals. The ease of its dispersion, coupled with its tremendous capability for ailment and death in infected people, makes it a risk to society. The chest X-ray is conventional but hard to interpret radiographic test for initial diagnosis of coronavirus from other related infections. It bears a considerable amount of information on physiological and anatomical features. To extract relevant information from it can occasionally become challenging even for a professional radiologist. In this regard, deep-learning models can help in swift, accurate and reliable outcomes. Existing datasets are small and suffer from the balance issue. In this paper, we prepare a relatively larger and well-balanced dataset as compared to the available datasets. Furthermore, we analyze deep learning models, namely, AlexNet, SqueezeNet, DenseNet201, MobileNetV2 and InceptionV3 with numerous variations such as training the models from scratch, fine-tuning without pre-trained weights, fine-tuning along with updating pre-trained weights of all layers, and fine-tuning with pre-trained weights along with applying augmentation. Our results show that fine-tuning with augmentation generates best results in pre-trained models. Finally, we have made architectural adjustments in MobileNetV2 and InceptionV3 models to learn more intricate features, which are then merged in our proposed ensemble model. The performance of our model is statistically analyzed against other models using four different performance metrics with paired two-sided t-test on 5 different splits of training and test sets of our dataset. We find that it is statistically better than its competing methods for the four metrics. Thus, the computer-aided classification based on the proposed model can assist radiologists in identifying coronavirus from other related infections in chest X-rays with higher accuracy. This can help in a reliable and speedy diagnosis, thereby saving valuable lives and mitigating the adverse impact on the socioeconomics of our community.",2021,10.1016/j.compbiomed.2021.104401,retrospective cohort,diagnosis,CXR,Lung
"Deep learning model for the automatic classification of COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy: a multi-center retrospective study","This retrospective study aimed to develop and validate a deep learning model for the classification of coronavirus disease-2019 (COVID-19) pneumonia, non-COVID-19 pneumonia, and the healthy using chest X-ray (CXR) images. One private and two public datasets of CXR images were included. The private dataset included CXR from six hospitals. A total of 14,258 and 11,253 CXR images were included in the 2 public datasets and 455 in the private dataset. A deep learning model based on EfficientNet with noisy student was constructed using the three datasets. The test set of 150 CXR images in the private dataset were evaluated by the deep learning model and six radiologists. Three-category classification accuracy and class-wise area under the curve (AUC) for each of the COVID-19 pneumonia, non-COVID-19 pneumonia, and healthy were calculated. Consensus of the six radiologists was used for calculating class-wise AUC. The three-category classification accuracy of our model was 0.8667, and those of the six radiologists ranged from 0.5667 to 0.7733. For our model and the consensus of the six radiologists, the class-wise AUC of the healthy, non-COVID-19 pneumonia, and COVID-19 pneumonia were 0.9912, 0.9492, and 0.9752 and 0.9656, 0.8654, and 0.8740, respectively. Difference of the class-wise AUC between our model and the consensus of the six radiologists was statistically significant for COVID-19 pneumonia (p value = 0.001334). Thus, an accurate model of deep learning for the three-category classification could be constructed; the diagnostic performance of our model was significantly better than that of the consensus interpretation by the six radiologists for COVID-19 pneumonia.",2022,10.1038/s41598-022-11990-3,retrospective cohort,diagnosis,CXR,Lung
Deep learning model to quantify left atrium volume on routine non-contrast chest CT and predict adverse outcomes,"BACKGROUND: Low-dose computed tomography (LDCT) are performed routinely for lung cancer screening. However, a large amount of nonpulmonary data from these scans remains unassessed. We aimed to validate a deep learning model to automatically segment and measure left atrial (LA) volumes from routine NCCT and evaluate prediction of cardiovascular outcomes. METHODS: We retrospectively evaluated 273 patients (median age 69 years, 55.5% male) who underwent LDCT for lung cancer screening. LA volumes were quantified by three expert cardiothoracic radiologists and a prototype AI algorithm. LA volumes were then indexed to the body surface area (BSA). Expert and AI LA volume index (LAVi) were compared and used to predict cardiovascular outcomes within five years. Logistic regression with appropriate univariate statistics were used for modelling outcomes. RESULTS: There was excellent correlation between AI and expert results with an LAV intraclass correlation of 0.950 (0.936-0.960). Bland-Altman plot demonstrated the AI underestimated LAVi by a mean 5.86 ​mL/m(2). AI-LAVi was associated with new-onset atrial fibrillation (AUC 0.86; OR 1.12, 95% CI 1.08-1.18, p ​< ​0.001), HF hospitalization (AUC 0.90; OR 1.07, 95% CI 1.04-1.13, p ​< ​0.001), and MACCE (AUC 0.68; OR 1.04, 95% CI 1.01-1.07, p ​= ​0.01). CONCLUSION: This novel deep learning algorithm for automated measurement of LA volume on lung cancer screening scans had excellent agreement with manual quantification. AI-LAVi is significantly associated with increased risk of new-onset atrial fibrillation, HF hospitalization, and major adverse cardiac and cerebrovascular events within 5 years.",2022,10.1016/j.jcct.2021.12.005,retrospective cohort,prognosis,CT,Heart
Deep Learning of Cancer Stem Cell Morphology Using Conditional Generative Adversarial Networks,"Deep-learning workflows of microscopic image analysis are sufficient for handling the contextual variations because they employ biological samples and have numerous tasks. The use of well-defined annotated images is important for the workflow. Cancer stem cells (CSCs) are identified by specific cell markers. These CSCs were extensively characterized by the stem cell (SC)-like gene expression and proliferation mechanisms for the development of tumors. In contrast, the morphological characterization remains elusive. This study aims to investigate the segmentation of CSCs in phase contrast imaging using conditional generative adversarial networks (CGAN). Artificial intelligence (AI) was trained using fluorescence images of the Nanog-Green fluorescence protein, the expression of which was maintained in CSCs, and the phase contrast images. The AI model segmented the CSC region in the phase contrast image of the CSC cultures and tumor model. By selecting images for training, several values for measuring segmentation quality increased. Moreover, nucleus fluorescence overlaid-phase contrast was effective for increasing the values. We show the possibility of mapping CSC morphology to the condition of undifferentiation using deep-learning CGAN workflows.",2020,10.3390/biom10060931,,,,
Deep Learning on MRI Images for Diagnosis of Lung Cancer Spinal Bone Metastasis,"This paper aimed to explore the adoption of deep learning algorithms in lung cancer spinal bone metastasis diagnosis. Comprehensive analysis was carried out with the aid of AdaBoost algorithm and Chan-Vese (CV) algorithm. 87 patients with lung cancer spinal bone metastasis were taken as research subjects, and comprehensive evaluation was made in terms of preliminary classification of images, segmentation results, Dice index, and Jaccard coefficient. After the case of misjudgment on whether there was hot spot was excluded, the initial classification accuracy of the AdaBoost algorithm can reach 96.55%. True positive rate (TPR) was 2.3%, and false negative rate (FNR) was 1.15%. 45 MRI images with hot spots were utilized as test set to detect the segmentation accuracy of CV, maximum between-cluster variance method (OTSU), and region growing algorithm. The results showed that the Dice index and Jaccard coefficient of the CV algorithm were 0.8591 and 0.8002, respectively, which were considerably superior to OTSU (0.6125 and 0.5541) and region growing algorithm (0.7293 and 0.6598). In summary, the AdaBoost algorithm was adopted for image preliminary classification, and CV algorithm for image segmentation was ideal for the diagnosis of lung cancer spinal bone metastasis and it was worthy of clinical promotion.",2021,10.1155/2021/5294379,retrospective cohort,diagnosis,MRI,Vertebral bone
Deep learning predicts cardiovascular disease risks from lung cancer screening low dose computed tomography,"Cancer patients have a higher risk of cardiovascular disease (CVD) mortality than the general population. Low dose computed tomography (LDCT) for lung cancer screening offers an opportunity for simultaneous CVD risk estimation in at-risk patients. Our deep learning CVD risk prediction model, trained with 30,286 LDCTs from the National Lung Cancer Screening Trial, achieves an area under the curve (AUC) of 0.871 on a separate test set of 2,085 subjects and identifies patients with high CVD mortality risks (AUC of 0.768). We validate our model against ECG-gated cardiac CT based markers, including coronary artery calcification (CAC) score, CAD-RADS score, and MESA 10-year risk score from an independent dataset of 335 subjects. Our work shows that, in high-risk patients, deep learning can convert LDCT for lung cancer screening into a dual-screening quantitative tool for CVD risk estimation.",2021,10.1038/s41467-021-23235-4,retrospective cohort,prognosis,CT,Heart
Deep learning predicts epidermal growth factor receptor mutation subtypes in lung adenocarcinoma,"PURPOSE: This study aimed to explore the predictive ability of deep learning (DL) for the common epidermal growth factor receptor (EGFR) mutation subtypes in patients with lung adenocarcinoma. METHODS: A total of 665 patients with lung adenocarcinoma (528/137) were recruited from two different institutions. In the training set, an 18-layer convolutional neural network (CNN) and fivefold cross-validation strategy were used to establish a CNN model. Subsequently, an independent external validation cohort from the other institution was used to evaluate the predictive efficacy of the CNN model. Grad-weighted class activation mapping (Grad-CAM) technology was used for the visual interpretation of the CNN model. In addition, this study also compared the prediction abilities of the radiomics and CNN models. Receiver operating characteristic (ROC) curves, accuracy and precision values, and recall and F1-score were used to evaluate the effectiveness of the CNN model and compare its performance with that of the radiomics model. RESULTS: In the validation set, the micro- and macroaverage values of the area under the ROC curve of the CNN model to identify the three EGFR subtypes were 0.78 and 0.79, respectively. All evaluation indicators of the CNN model were better than those of the radiomics model. CONCLUSIONS: Our study confirmed the potential of DL for predicting the EGFR mutation status in lung adenocarcinoma. The imaging phenotypes of the three mutation subtypes were found to be different, which can provide a basis for choosing more accurate and personalized treatment in patients with lung adenocarcinoma.",2021,10.1002/mp.15307,cross-sectional,prognosis,CT,Lung
Deep Learning Predicts Lung Cancer Treatment Response from Serial Medical Imaging,"PURPOSE: Tumors are continuously evolving biological systems, and medical imaging is uniquely positioned to monitor changes throughout treatment. Although qualitatively tracking lesions over space and time may be trivial, the development of clinically relevant, automated radiomics methods that incorporate serial imaging data is far more challenging. In this study, we evaluated deep learning networks for predicting clinical outcomes through analyzing time series CT images of patients with locally advanced non-small cell lung cancer (NSCLC).Experimental Design: Dataset A consists of 179 patients with stage III NSCLC treated with definitive chemoradiation, with pretreatment and posttreatment CT images at 1, 3, and 6 months follow-up (581 scans). Models were developed using transfer learning of convolutional neural networks (CNN) with recurrent neural networks (RNN), using single seed-point tumor localization. Pathologic response validation was performed on dataset B, comprising 89 patients with NSCLC treated with chemoradiation and surgery (178 scans). RESULTS: Deep learning models using time series scans were significantly predictive of survival and cancer-specific outcomes (progression, distant metastases, and local-regional recurrence). Model performance was enhanced with each additional follow-up scan into the CNN model (e.g., 2-year overall survival: AUC = 0.74, P < 0.05). The models stratified patients into low and high mortality risk groups, which were significantly associated with overall survival [HR = 6.16; 95% confidence interval (CI), 2.17-17.44; P < 0.001]. The model also significantly predicted pathologic response in dataset B (P = 0.016). CONCLUSIONS: We demonstrate that deep learning can integrate imaging scans at multiple timepoints to improve clinical outcome predictions. AI-based noninvasive radiomics biomarkers can have a significant impact in the clinic given their low cost and minimal requirements for human input.",2019,10.1158/1078-0432.Ccr-18-2495,retrospective cohort,treatment,CT,Lung
Deep Learning Reconstruction Shows Better Lung Nodule Detection for Ultra-Low-Dose Chest CT,"Background Ultra-low-dose (ULD) CT could facilitate the clinical implementation of large-scale lung cancer screening while minimizing the radiation dose. However, traditional image reconstruction methods are associated with image noise in low-dose acquisitions. Purpose To compare the image quality and lung nodule detectability of deep learning image reconstruction (DLIR) and adaptive statistical iterative reconstruction-V (ASIR-V) in ULD CT. Materials and Methods Patients who underwent noncontrast ULD CT (performed at 0.07 or 0.14 mSv, similar to a single chest radiograph) and contrast-enhanced chest CT (CECT) from April to June 2020 were included in this prospective study. ULD CT images were reconstructed with filtered back projection (FBP), ASIR-V, and DLIR. Three-dimensional segmentation of lung tissue was performed to evaluate image noise. Radiologists detected and measured nodules with use of a deep learning-based nodule assessment system and recognized malignancy-related imaging features. Bland-Altman analysis and repeated-measures analysis of variance were used to evaluate the differences between ULD CT images and CECT images. Results A total of 203 participants (mean age ± standard deviation, 61 years ± 12; 129 men) with 1066 nodules were included, with 100 scans at 0.07 mSv and 103 scans at 0.14 mSv. The mean lung tissue noise ± standard deviation was 46 HU ± 4 for CECT and 59 HU ± 4, 56 HU ± 4, 53 HU ± 4, 54 HU ± 4, and 51 HU ± 4 in FBP, ASIR-V level 40%, ASIR-V level 80% (ASIR-V-80%), medium-strength DLIR, and high-strength DLIR (DLIR-H), respectively, of ULD CT scans (P < .001). The nodule detection rates of FBP reconstruction, ASIR-V-80%, and DLIR-H were 62.5% (666 of 1066 nodules), 73.3% (781 of 1066 nodules), and 75.8% (808 of 1066 nodules), respectively (P < .001). Bland-Altman analysis showed the percentage difference in long diameter from that of CECT was 9.3% (95% CI of the mean: 8.0, 10.6), 9.2% (95% CI of the mean: 8.0, 10.4), and 6.2% (95% CI of the mean: 5.0, 7.4) in FBP reconstruction, ASIR-V-80%, and DLIR-H, respectively (P < .001). Conclusion Compared with adaptive statistical iterative reconstruction-V, deep learning image reconstruction reduced image noise, increased nodule detection rate, and improved measurement accuracy on ultra-low-dose chest CT images. © RSNA, 2022 Online supplemental material is available for this article. See also the editorial by Lee in this issue.",2022,10.1148/radiol.210551,,,,
Deep learning representations to support COVID-19 diagnosis on CT slices,"INTRODUCTION: The coronavirus disease 2019 (COVID-19) has become a significant public health problem worldwide. In this context, CT-scan automatic analysis has emerged as a COVID-19 complementary diagnosis tool allowing for radiological finding characterization, patient categorization, and disease follow-up. However, this analysis depends on the radiologist's expertise, which may result in subjective evaluations. OBJECTIVE: To explore deep learning representations, trained from thoracic CT-slices, to automatically distinguish COVID-19 disease from control samples. MATERIALS AND METHODS: Two datasets were used: SARS-CoV-2 CT Scan (Set-1) and FOSCAL clinic's dataset (Set-2). The deep representations took advantage of supervised learning models previously trained on the natural image domain, which were adjusted following a transfer learning scheme. The deep classification was carried out: (a) via an end-to-end deep learning approach and (b) via random forest and support vector machine classifiers by feeding the deep representation embedding vectors into these classifiers. RESULTS: The end-to-end classification achieved an average accuracy of 92.33% (89.70% precision) for Set-1 and 96.99% (96.62% precision) for Set-2. The deep feature embedding with a support vector machine achieved an average accuracy of 91.40% (95.77% precision) and 96.00% (94.74% precision) for Set-1 and Set-2, respectively. CONCLUSION: Deep representations have achieved outstanding performance in the identification of COVID-19 cases on CT scans demonstrating good characterization of the COVID-19 radiological patterns. These representations could potentially support the COVID-19 diagnosis in clinical settings.",2022,10.7705/biomedica.5927,cross-sectional,diagnosis,CT,Lung
Deep Learning to Assess Long-term Mortality From Chest Radiographs,"IMPORTANCE: Chest radiography is the most common diagnostic imaging test in medicine and may also provide information about longevity and prognosis. OBJECTIVE: To develop and test a convolutional neural network (CNN) (named CXR-risk) to predict long-term mortality, including noncancer death, from chest radiographs. DESIGN, SETTING, AND PARTICIPANTS: In this prognostic study, CXR-risk CNN development (n = 41 856) and testing (n = 10 464) used data from the screening radiography arm of the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) (n = 52 320), a community cohort of asymptomatic nonsmokers and smokers (aged 55-74 years) enrolled at 10 US sites from November 8, 1993, through July 2, 2001. External testing used data from the screening radiography arm of the National Lung Screening Trial (NLST) (n = 5493), a community cohort of heavy smokers (aged 55-74 years) enrolled at 21 US sites from August 2002, through April 2004. Data analysis was performed from January 1, 2018, to May 23, 2019. EXPOSURE: Deep learning CXR-risk score (very low, low, moderate, high, and very high) based on CNN analysis of the enrollment radiograph. MAIN OUTCOMES AND MEASURES: All-cause mortality. Prognostic value was assessed in the context of radiologists' diagnostic findings (eg, lung nodule) and standard risk factors (eg, age, sex, and diabetes) and for cause-specific mortality. RESULTS: Among 10 464 PLCO participants (mean [SD] age, 62.4 [5.4] years; 5405 men [51.6%]; median follow-up, 12.2 years [interquartile range, 10.5-12.9 years]) and 5493 NLST test participants (mean [SD] age, 61.7 [5.0] years; 3037 men [55.3%]; median follow-up, 6.3 years [interquartile range, 6.0-6.7 years]), there was a graded association between CXR-risk score and mortality. The very high-risk group had mortality of 53.0% (PLCO) and 33.9% (NLST), which was higher compared with the very low-risk group (PLCO: unadjusted hazard ratio [HR], 18.3 [95% CI, 14.5-23.2]; NLST: unadjusted HR, 15.2 [95% CI, 9.2-25.3]; both P < .001). This association was robust to adjustment for radiologists' findings and risk factors (PLCO: adjusted HR [aHR], 4.8 [95% CI, 3.6-6.4]; NLST: aHR, 7.0 [95% CI, 4.0-12.1]; both P < .001). Comparable results were seen for lung cancer death (PLCO: aHR, 11.1 [95% CI, 4.4-27.8]; NLST: aHR, 8.4 [95% CI, 2.5-28.0]; both P ≤ .001) and for noncancer cardiovascular death (PLCO: aHR, 3.6 [95% CI, 2.1-6.2]; NLST: aHR, 47.8 [95% CI, 6.1-374.9]; both P < .001) and respiratory death (PLCO: aHR, 27.5 [95% CI, 7.7-97.8]; NLST: aHR, 31.9 [95% CI, 3.9-263.5]; both P ≤ .001). CONCLUSIONS AND RELEVANCE: In this study, the deep learning CXR-risk score stratified the risk of long-term mortality based on a single chest radiograph. Individuals at high risk of mortality may benefit from prevention, screening, and lifestyle interventions.",2019,10.1001/jamanetworkopen.2019.7416,retrospective cohort,prognosis,CXR,N/A
Deep learning to detect acute respiratory distress syndrome on chest radiographs: a retrospective study with external validation,"BACKGROUND: Acute respiratory distress syndrome (ARDS) is a common, but under-recognised, critical illness syndrome associated with high mortality. An important factor in its under-recognition is the variability in chest radiograph interpretation for ARDS. We sought to train a deep convolutional neural network (CNN) to detect ARDS findings on chest radiographs. METHODS: CNNs were pretrained on 595 506 radiographs from two centres to identify common chest findings (eg, opacity and effusion), and then trained on 8072 radiographs annotated for ARDS by multiple physicians using various transfer learning approaches. The best performing CNN was tested on chest radiographs in an internal and external cohort, including a subset reviewed by six physicians, including a chest radiologist and physicians trained in intensive care medicine. Chest radiograph data were acquired from four US hospitals. FINDINGS: In an internal test set of 1560 chest radiographs from 455 patients with acute hypoxaemic respiratory failure, a CNN could detect ARDS with an area under the receiver operator characteristics curve (AUROC) of 0·92 (95% CI 0·89-0·94). In the subgroup of 413 images reviewed by at least six physicians, its AUROC was 0·93 (95% CI 0·88-0·96), sensitivity 83·0% (95% CI 74·0-91·1), and specificity 88·3% (95% CI 83·1-92·8). Among images with zero of six ARDS annotations (n=155), the median CNN probability was 11%, with six (4%) assigned a probability above 50%. Among images with six of six ARDS annotations (n=27), the median CNN probability was 91%, with two (7%) assigned a probability below 50%. In an external cohort of 958 chest radiographs from 431 patients with sepsis, the AUROC was 0·88 (95% CI 0·85-0·91). When radiographs annotated as equivocal were excluded, the AUROC was 0·93 (0·92-0·95). INTERPRETATION: A CNN can be trained to achieve expert physician-level performance in ARDS detection on chest radiographs. Further research is needed to evaluate the use of these algorithms to support real-time identification of ARDS patients to ensure fidelity with evidence-based care or to support ongoing ARDS research. FUNDING: National Institutes of Health, Department of Defense, and Department of Veterans Affairs.",2021,10.1016/s2589-7500(21)00056-x,retrospective cohort,diagnosis,CXR,Lung
Deep Learning to Estimate Biological Age From Chest Radiographs,"OBJECTIVES: The goal of this study was to assess whether a deep learning estimate of age from a chest radiograph image (CXR-Age) can predict longevity beyond chronological age. BACKGROUND: Chronological age is an imperfect measure of longevity. Biological age, a measure of overall health, may improve personalized care. This paper proposes a new way to estimate biological age using a convolutional neural network that takes as input a CXR image and outputs a chest x-ray age (in years) as a measure of long-term mortality risk. METHODS: CXR-Age was developed using CXR from 116,035 individuals and validated in 2 held-out testing sets: 1) 75% of the CXR arm of PLCO (Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial) (N = 40,967); and 2) the CXR arm of NLST (National Lung Screening Trial) (N = 5,414). CXR-Age was compared to chronological age and a multivariable regression model of chronological age, risk factors, and radiograph findings to predict all-cause and cardiovascular mortality with a maximum 23 years and 13 years of follow-up, respectively. The primary outcome was observed mortality; results are provided for the testing datasets only. RESULTS: In the PLCO testing dataset, a 5-year increase in CXR-Age carried a higher risk of all-cause mortality than a 5-year increase in chronological age (CXR-Age hazard ratio [HR]: 2.26 [95% confidence interval (CI): 2.24 to 2.29] vs. chronological age HR: 1.77 [95% CI: 1.75 to 1.78]; p < 0.001). A similar pattern was found for cardiovascular mortality (CXR-Age cause-specific HR: 2.45 per 5 years [95% CI: 2.34 to 2.56] vs. chronological age HR: 1.82 per 5 years [95% CI: 1.74 to 1.90]). Similar results were seen for both outcomes in the NLST external testing dataset. Adding CXR-Age to the multivariable model resulted in significant improvements for predicting both outcomes in both testing datasets (p < 0.001 for all comparisons). CONCLUSIONS: Based on a CXR image, CXR-Age predicted long-term all-cause and cardiovascular mortality.",2021,10.1016/j.jcmg.2021.01.008,retrospective cohort,diagnosis,CXR,N/A
Deep Learning Using Chest Radiographs to Identify High-Risk Smokers for Lung Cancer Screening Computed Tomography: Development and Validation of a Prediction Model,"BACKGROUND: Lung cancer screening with chest computed tomography (CT) reduces lung cancer death. Centers for Medicare & Medicaid Services (CMS) eligibility criteria for lung cancer screening with CT require detailed smoking information and miss many incident lung cancers. An automated deep-learning approach based on chest radiograph images may identify more smokers at high risk for lung cancer who could benefit from screening with CT. OBJECTIVE: To develop and validate a convolutional neural network (CXR-LC) that predicts long-term incident lung cancer using data commonly available in the electronic medical record (EMR) (chest radiograph, age, sex, and whether currently smoking). DESIGN: Risk prediction study. SETTING: U.S. lung cancer screening trials. PARTICIPANTS: The CXR-LC model was developed in the PLCO (Prostate, Lung, Colorectal, and Ovarian) Cancer Screening Trial (n = 41 856). The final CXR-LC model was validated in additional PLCO smokers (n = 5615, 12-year follow-up) and NLST (National Lung Screening Trial) heavy smokers (n = 5493, 6-year follow-up). Results are reported for validation data sets only. MEASUREMENTS: Up to 12-year lung cancer incidence predicted by CXR-LC. RESULTS: The CXR-LC model had better discrimination (area under the receiver-operating characteristic curve [AUC]) for incident lung cancer than CMS eligibility (PLCO AUC, 0.755 vs. 0.634; P < 0.001). The CXR-LC model's performance was similar to that of PLCO(M2012), a state-of-the-art risk score with 11 inputs, in both the PLCO data set (CXR-LC AUC of 0.755 vs. PLCO(M2012) AUC of 0.751) and the NLST data set (0.659 vs. 0.650). When compared in equal-sized screening populations, CXR-LC was more sensitive than CMS eligibility in the PLCO data set (74.9% vs. 63.8%; P = 0.012) and missed 30.7% fewer incident lung cancers. On decision curve analysis, CXR-LC had higher net benefit than CMS eligibility and similar benefit to PLCO(M2012). LIMITATION: Validation in lung cancer screening trials and not a clinical setting. CONCLUSION: The CXR-LC model identified smokers at high risk for incident lung cancer, beyond CMS eligibility and using information commonly available in the EMR. PRIMARY FUNDING SOURCE: None.",2020,10.7326/m20-1868,retrospective cohort,prognosis,CXR,Lung
Deep learning with robustness to missing data: A novel approach to the detection of COVID-19,"In the context of the current global pandemic and the limitations of the RT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising Fully Connected Network). Since medical facilities around the world differ enormously in what laboratory tests or chest imaging may be available, DFCN is designed to be robust to missing input data. An ablation study extensively evaluates the performance benefits of the DFCN as well as its robustness to missing inputs. Data from 1088 patients with confirmed RT-PCR results are obtained from two independent medical facilities. The data includes results from 27 laboratory tests and a chest x-ray scored by a deep learning model. Training and test datasets are taken from different medical facilities. Data is made publicly available. The performance of DFCN in predicting the RT-PCR result is compared with 3 related architectures as well as a Random Forest baseline. All models are trained with varying levels of masked input data to encourage robustness to missing inputs. Missing data is simulated at test time by masking inputs randomly. DFCN outperforms all other models with statistical significance using random subsets of input data with 2-27 available inputs. When all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any other model. Furthermore, with clinically meaningful subsets of parameters consisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than any other model, with values of 0.909 and 0.919.",2021,10.1371/journal.pone.0255301,cross-sectional,diagnosis,CXR,Lung
Deep learning-based algorithm for lung cancer detection on chest radiographs using the segmentation method,"We developed and validated a deep learning (DL)-based model using the segmentation method and assessed its ability to detect lung cancer on chest radiographs. Chest radiographs for use as a training dataset and a test dataset were collected separately from January 2006 to June 2018 at our hospital. The training dataset was used to train and validate the DL-based model with five-fold cross-validation. The model sensitivity and mean false positive indications per image (mFPI) were assessed with the independent test dataset. The training dataset included 629 radiographs with 652 nodules/masses and the test dataset included 151 radiographs with 159 nodules/masses. The DL-based model had a sensitivity of 0.73 with 0.13 mFPI in the test dataset. Sensitivity was lower in lung cancers that overlapped with blind spots such as pulmonary apices, pulmonary hila, chest wall, heart, and sub-diaphragmatic space (0.50-0.64) compared with those in non-overlapped locations (0.87). The dice coefficient for the 159 malignant lesions was on average 0.52. The DL-based model was able to detect lung cancers on chest radiographs, with low mFPI.",2022,10.1038/s41598-021-04667-w,cross-sectional,diagnosis,CXR,Lung
Deep Learning-based Artificial Intelligence Improves Accuracy of Error-prone Lung Nodules,"Introduction: Early detection of lung cancer is one way to improve outcomes. Improving the detection of nodules on chest CT scans is important. Previous artificial intelligence (AI) modules show rapid advantages, which improves the performance of detecting lung nodules in some datasets. However, they have a high false-positive (FP) rate. Its effectiveness in clinical practice has not yet been fully proven. We aimed to use AI assistance in CT scans to decrease FP. Materials and methods: CT images of 60 patients were obtained. Five senior doctors who were blinded to these cases participated in this study for the detection of lung nodules. Two doctors performed manual detection and labeling of lung nodules without AI assistance. Another three doctors used AI assistance to detect and label lung nodules before manual interpretation. The AI program is based on a deep learning framework. Results: In total, 266 nodules were identified. For doctors without AI assistance, the FP was 0.617-0.650/scan and the sensitivity was 59.2-67.0%. For doctors with AI assistance, the FP was 0.067 to 0.2/scan and the sensitivity was 59.2-77.3% This AI-assisted program significantly reduced FP. The error-prone characteristics of lung nodules were central locations, ground-glass appearances, and small sizes. The AI-assisted program improved the detection of error-prone nodules. Conclusions: Detection of lung nodules is important for lung cancer treatment. When facing a large number of CT scans, error-prone nodules are a great challenge for doctors. The AI-assisted program improved the performance of detecting lung nodules, especially for error-prone nodules.",2022,10.7150/ijms.69400,cross-sectional,diagnosis,CXR,Lung
Deep Learning-based Automatic Detection Algorithm for Reducing Overlooked Lung Cancers on Chest Radiographs,"Background It is uncertain whether a deep learning-based automatic detection algorithm (DLAD) for identifying malignant nodules on chest radiographs will help diagnose lung cancers. Purpose To evaluate the efficacy of using a DLAD in observer performance for the detection of lung cancers on chest radiographs. Materials and Methods Among patients diagnosed with lung cancers between January 2010 and December 2014, 117 patients (median age, 69 years; interquartile range [IQR], 64-74 years; 57 women) were retrospectively identified in whom lung cancers were visible on previous chest radiographs. For the healthy control group, 234 patients (median age, 58 years; IQR, 48-68 years; 123 women) with normal chest radiographs were randomly selected. Nine observers reviewed each chest radiograph, with and without a DLAD. They detected potential lung cancers and determined whether they would recommend chest CT for follow-up. Observer performance was compared with use of the area under the alternative free-response receiver operating characteristic curve (AUC), sensitivity, and rates of chest CT recommendation. Results In total, 105 of the 117 patients had lung cancers that were overlooked on their original radiographs. The average AUC for all observers significantly rose from 0.67 (95% confidence interval [CI]: 0.62, 0.72) without a DLAD to 0.76 (95% CI: 0.71, 0.81) with a DLAD (P < .001). With a DLAD, observers detected more overlooked lung cancers (average sensitivity, 53% [56 of 105 patients] with a DLAD vs 40% [42 of 105 patients] without a DLAD) (P < .001) and recommended chest CT for more patients (62% [66 of 105 patients] with a DLAD vs 47% [49 of 105 patients] without a DLAD) (P < .001). In the healthy control group, no difference existed in the rate of chest CT recommendation (10% [23 of 234 patients] without a DLAD and 8% [20 of 234 patients] with a DLAD) (P = .13). Conclusion Using a deep learning-based automatic detection algorithm may help observers reduce the number of overlooked lung cancers on chest radiographs, without a proportional increase in the number of follow-up chest CT examinations. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020200165,,,,
Deep Learning-Based Chest CT Image Features in Diagnosis of Lung Cancer,"This study was to evaluate the diagnostic value of deep learning-optimized chest CT in the patients with lung cancer. 90 patients who were diagnosed with lung cancer by surgery or puncture in hospital were selected as the research subjects. The Mask Region Convolutional Neural Network (Mask-RCNN) model was a typical end-to-end image segmentation model, and Dual Path Network (DPN) was used in nodule detection. The results showed that the accuracy of DPN algorithm model in detecting lung lesions in lung cancer patients was 88.74%, the accuracy of CT diagnosis of lung cancer was 88.37%, the sensitivity was 82.91%, and the specificity was 87.43%. Deep learning-based CT examination combined with serum tumor detection, factoring into Neurospecific enolase (N S E), cytokeratin 19 fragment (CYFRA21), Carcinoembryonic antigen (CEA), and squamous cell carcinoma (SCC) antigen, improved the accuracy to 97.94%, the sensitivity to 98.12%, and the specificity to 100%, all showing significant differences (P < 0.05). In conclusion, this study provides a scientific basis for improving the diagnostic efficiency of CT imaging in lung cancer and theoretical support for subsequent lung cancer diagnosis and treatment.",2022,10.1155/2022/4153211,case control,diagnosis,CT,Lung
Deep Learning-Based Computed Tomography Imaging to Diagnose the Lung Nodule and Treatment Effect of Radiofrequency Ablation,"This study aimed to detect and diagnose the lung nodules as early as possible to effectively treat them, thereby reducing the burden on the medical system and patients. A lung computed tomography (CT) image segmentation algorithm was constructed based on the deep learning convolutional neural network (CNN). The clinical data of 69 patients with lung nodules diagnosed by needle biopsy and pathological comprehensive diagnosis at hospital were collected for specific analysis. The CT image segmentation algorithm was used to distinguish the nature and volume of lung nodules and compared with other computer aided design (CAD) software (Philips ISP). 69 patients with lung nodules were treated by radiofrequency ablation (RFA). The results showed that the diagnostic sensitivity of the CT image segmentation algorithm based on the CNN was obviously higher than that of the Philips ISP for solid nodules <5 mm (63 cases vs. 33 cases) (P < 0.05); it was the same result for the subsolid nodule <5 mm (33 case vs. 5 cases) (P < 0.05) that was slightly higher for solid and subsolid nodules with a diameter of 5-10 mm (37 cases vs. 28 cases) (P < 0.05). In addition, the CNN algorithm can reach all detection for calcified nodules and pleural nodules (7 cases; 5 cases), and the diagnostic sensitivities were much better than those of Philips ISP (2 cases; 3 cases) (P < 0.05). Patients with pulmonary nodules treated by RFA were in good postoperative condition, with a half-year survival rate of 100% and a one-year survival rate of 72.4%. Therefore, it could be concluded that the CT image segmentation algorithm based on the CNN could effectively detect and diagnose the lung nodules early, and the RFA could effectively treat the lung nodules.",2021,10.1155/2021/6556266,cross-sectional,diagnosis,CT,Lung
Deep Learning-Based Computer-Aided Pneumothorax Detection Using Chest X-ray Images,"Pneumothorax is a thoracic disease leading to failure of the respiratory system, cardiac arrest, or in extreme cases, death. Chest X-ray (CXR) imaging is the primary diagnostic imaging technique for the diagnosis of pneumothorax. A computerized diagnosis system can detect pneumothorax in chest radiographic images, which provide substantial benefits in disease diagnosis. In the present work, a deep learning neural network model is proposed to detect the regions of pneumothoraces in the chest X-ray images. The model incorporates a Mask Regional Convolutional Neural Network (Mask RCNN) framework and transfer learning with ResNet101 as a backbone feature pyramid network (FPN). The proposed model was trained on a pneumothorax dataset prepared by the Society for Imaging Informatics in Medicine in association with American college of Radiology (SIIM-ACR). The present work compares the operation of the proposed MRCNN model based on ResNet101 as an FPN with the conventional model based on ResNet50 as an FPN. The proposed model had lower class loss, bounding box loss, and mask loss as compared to the conventional model based on ResNet50 as an FPN. Both models were simulated with a learning rate of 0.0004 and 0.0006 with 10 and 12 epochs, respectively.",2022,10.3390/s22062278,cross-sectional,diagnosis,CXR,Lung
Deep learning-based detection system for multiclass lesions on chest radiographs: comparison with observer readings,"OBJECTIVE: To investigate the feasibility of a deep learning-based detection (DLD) system for multiclass lesions on chest radiograph, in comparison with observers. METHODS: A total of 15,809 chest radiographs were collected from two tertiary hospitals (7204 normal and 8605 abnormal with nodule/mass, interstitial opacity, pleural effusion, or pneumothorax). Except for the test set (100 normal and 100 abnormal (nodule/mass, 70; interstitial opacity, 10; pleural effusion, 10; pneumothorax, 10)), radiographs were used to develop a DLD system for detecting multiclass lesions. The diagnostic performance of the developed model and that of nine observers with varying experiences were evaluated and compared using area under the receiver operating characteristic curve (AUROC), on a per-image basis, and jackknife alternative free-response receiver operating characteristic figure of merit (FOM) on a per-lesion basis. The false-positive fraction was also calculated. RESULTS: Compared with the group-averaged observations, the DLD system demonstrated significantly higher performances on image-wise normal/abnormal classification and lesion-wise detection with pattern classification (AUROC, 0.985 vs. 0.958; p = 0.001; FOM, 0.962 vs. 0.886; p < 0.001). In lesion-wise detection, the DLD system outperformed all nine observers. In the subgroup analysis, the DLD system exhibited consistently better performance for both nodule/mass (FOM, 0.913 vs. 0.847; p < 0.001) and the other three abnormal classes (FOM, 0.995 vs. 0.843; p < 0.001). The false-positive fraction of all abnormalities was 0.11 for the DLD system and 0.19 for the observers. CONCLUSIONS: The DLD system showed the potential for detection of lesions and pattern classification on chest radiographs, performing normal/abnormal classifications and achieving high diagnostic performance. KEY POINTS: • The DLD system was feasible for detection with pattern classification of multiclass lesions on chest radiograph. • The DLD system had high performance of image-wise classification as normal or abnormal chest radiographs (AUROC, 0.985) and showed especially high specificity (99.0%). • In lesion-wise detection of multiclass lesions, the DLD system outperformed all 9 observers (FOM, 0.962 vs. 0.886; p < 0.001).",2020,10.1007/s00330-019-06532-x,case control,diagnosis,CXR,Lung
Deep learning-based differentiation of invasive adenocarcinomas from preinvasive or minimally invasive lesions among pulmonary subsolid nodules,"OBJECTIVES: To evaluate a deep learning-based model using model-generated segmentation masks to differentiate invasive pulmonary adenocarcinoma (IPA) from preinvasive lesions or minimally invasive adenocarcinoma (MIA) on CT, making comparisons with radiologist-derived measurements of solid portion size. METHODS: Four hundred eleven subsolid nodules (SSNs) (120 preinvasive lesions or MIAs and 291 IPAs) in 333 patients who underwent surgery between June 2010 and August 2016 were retrospectively included to develop the model (370 SSNs in 293 patients for training and 41 SSNs in 40 patients for tuning). Ninety SSNs of 2 cm or smaller (45 preinvasive lesions or MIAs and 45 IPAs) resected in 2018 formed a validation set. Six radiologists measured the solid portion of each nodule. Performances of the model and radiologists were assessed using receiver operating characteristics curve analysis. RESULTS: The deep learning model differentiated IPA from preinvasive lesions or MIA with areas under the curve (AUCs) of 0.914, 0.956, and 0.833 for the training, tuning, and validation sets, respectively. The mean AUC of the radiologists was 0.835 in the validation set, without significant differences between radiologists and the model (p = 0.97). The sensitivity, specificity, and accuracy of the model were 71% (32/45), 87% (39/45), and 79% (71/90), respectively, whereas the corresponding values of the radiologists were 75.2% (203/270), 76.7% (207/270), and 75.9% (410/540) with a 5-mm threshold for the solid portion size. CONCLUSIONS: The performance of the model for differentiating IPA from preinvasive lesions or MIA was comparable to that of the radiologists' measurements of solid portion size. KEY POINTS: • A deep learning-based model differentiated IPA from preinvasive lesions or MIA with AUCs of 0.914 and 0.956 for the training and tuning sets, respectively. • In the validation set including subsolid nodules of 2 cm or smaller, the model showed an AUC of 0.833, being on par with the performance of the solid portion size measurements made by the radiologists (AUC, 0.835; p = 0.97). • SSNs with a solid portion measuring > 10 mm on CT showed a high probability of being IPA (positive predictive value, 93.5-100.0%).",2021,10.1007/s00330-020-07620-z,cross-sectional,diagnosis,CT,Lung
Deep Learning-based Image Conversion of CT Reconstruction Kernels Improves Radiomics Reproducibility for Pulmonary Nodules or Masses,"Background Intratumor heterogeneity in lung cancer may influence outcomes. CT radiomics seeks to assess tumor features to provide detailed imaging features. However, CT radiomic features vary according to the reconstruction kernel used for image generation. Purpose To investigate the effect of different reconstruction kernels on radiomic features and assess whether image conversion using a convolutional neural network (CNN) could improve reproducibility of radiomic features between different kernels. Materials and Methods In this retrospective analysis, patients underwent non-contrast material-enhanced and contrast material-enhanced axial chest CT with soft kernel (B30f) and sharp kernel (B50f) reconstruction using a single CT scanner from April to June 2017. To convert different kernels without sinogram, the CNN model was developed using residual learning and an end-to-end way. Kernel-converted images were generated, from B30f to B50f and from B50f to B30f. Pulmonary nodules or masses were semiautomatically segmented and 702 radiomic features (tumor intensity, texture, and wavelet features) were extracted. Measurement variability in radiomic features was evaluated using the concordance correlation coefficient (CCC). Results A total of 104 patients were studied, including 54 women and 50 men, with pulmonary nodules or masses (mean age, 63.2 years ± 10.5). The CCC between two readers using the same kernel was 0.92, and 592 of 702 (84.3%) of the radiomic features were reproducible (CCC ≥ 0.85); using different kernels, the CCC was 0.38 and only 107 of 702 (15.2%) of the radiomic features were reliable. Texture features and wavelet features were predominantly affected by reconstruction kernel (CCC, from 0.88 to 0.61 for texture features and from 0.92 to 0.35 for wavelet features). After applying image conversion, CCC improved to 0.84 and 403 of 702 (57.4%) radiomic features were reproducible (CCC, 0.85 for texture features and 0.84 for wavelet features). Conclusion Chest CT image conversion using a convolutional neural network effectively reduced the effect of two different reconstruction kernels and may improve the reproducibility of radiomic features in pulmonary nodules or masses. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Park in this issue.",2019,10.1148/radiol.2019181960,cross-sectional,others,CT,Lung
Deep Learning-Based Internal Target Volume (ITV) Prediction Using Cone-Beam CT Images in Lung Stereotactic Body Radiotherapy,"Purpose:This study aims to develop a deep learning (DL)-based (Mask R-CNN) method to predict the internal target volume (ITV) in cone beam computed tomography (CBCT) images for lung stereotactic body radiotherapy (SBRT) patients and to evaluate the prediction accuracy of the model using 4DCT as ground truth. Methods and Materials: This study enrolled 78 phantom cases and 156 patient cases who received SBRT treatment. We used a novel DL model (Mask R-CNN) to identify and delineate lung tumor ITV in CBCT images. The results of the DL-based method were compared quantitatively with the ground truth (4DCT) using 4 metrics, including Dice Similarity Coefficient (DSC), Relative Volume Index (RVI), 3D Motion Range (R(3D)), and Hausdorff Surface Distance (HD). Paired t-tests were used to determine the differences between the DL-based method and manual contouring. Results: The DSC value for 4DCT(MIP) versus CBCT is 0.86 ± 0.16 and for 4DCT(AVG) versus CBCT is 0.83 ± 0.18, indicating a high similarity of tumor delineation in CBCT and 4DCT. The mean Accuracy Precision (mAP), R(3D), RVI, and HD values for phantom evaluation are 0.94 ± 0.04, 1.37 ± 0.36, 0.79 ± 0.02, and 6.79 ± 0.68, respectively. For patient evaluation, the mAP, R(3D), RVI, and HD achieved averaged values of 0.74 ± 0.23, 2.39 ± 1.59, 1.27 ± 0.47, and 17.00 ± 19.89, respectively. These results showed a good correlation between predicted ITV and manually contoured ITV. The phantom p-value for RVI, R(3D), and HD are 0.75, 0.08, 0.86, and patient p-value are 0.53, 0.07, 0.28, respectively. These p-values for phantom and patient showed no significant difference between the predicted ITV and physician's manual contouring. Conclusion:The current improved method (Mask R-CNN) yielded a good similarity between predicted ITV in CBCT and the manual contouring in 4DCT, thus indicating great potential for using CBCT for patient ITV contouring.",2022,10.1177/15330338211073380,cross-sectional,treatment,CT,Lung
Deep learning-based method to accurately estimate breast tissue optical properties in the presence of the chest wall,"SIGNIFICANCE: In general, image reconstruction methods used in diffuse optical tomography (DOT) are based on diffusion approximation, and they consider the breast tissue as a homogenous, semi-infinite medium. However, the semi-infinite medium assumption used in DOT reconstruction is not valid when the chest wall is underneath the breast tissue. AIM: We aim to reduce the chest wall's effect on the estimated average optical properties of breast tissue and obtain accurate forward model for DOT reconstruction. APPROACH: We propose a deep learning-based neural network approach where a convolution neural network (CNN) is trained to simultaneously obtain accurate optical property values for both the breast tissue and the chest wall. RESULTS: The CNN model shows great promise in reducing errors in estimating the optical properties of the breast tissue in the presence of a shallow chest wall. For patient data, the CNN model predicted the breast tissue optical absorption coefficient, which was independent of chest wall depth. CONCLUSIONS: Our proposed method can be readily used in DOT and diffuse spectroscopy measurements to improve the accuracy of estimated tissue optical properties.",2021,10.1117/1.Jbo.26.10.106004,,,,
Deep learning-based multi-view fusion model for screening 2019 novel coronavirus pneumonia: A multicentre study,"PURPOSE: To develop a deep learning-based method to assist radiologists to fast and accurately identify patients with COVID-19 by CT images. METHODS: We retrospectively collected chest CT images of 495 patients from three hospitals in China. 495 datasets were randomly divided into 395 cases (80%, 294 of COVID-19, 101 of other pneumonia) of the training set, 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the validation set and 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the testing set. We trained a multi-view fusion model using deep learning network to screen patients with COVID-19 using CT images with the maximum lung regions in axial, coronal and sagittal views. The performance of the proposed model was evaluated by both the validation and testing sets. RESULTS: The multi-view deep learning fusion model achieved the area under the receiver-operating characteristics curve (AUC) of 0.732, accuracy of 0.700, sensitivity of 0.730 and specificity of 0.615 in validation set. In the testing set, we can achieve AUC, accuracy, sensitivity and specificity of 0.819, 0.760, 0.811 and 0.615 respectively. CONCLUSIONS: Based on deep learning method, the proposed diagnosis model trained on multi-view images of chest CT images showed great potential to improve the efficacy of diagnosis and mitigate the heavy workload of radiologists for the initial screening of COVID-19 pneumonia.",2020,10.1016/j.ejrad.2020.109041,cross-sectional,diagnosis,CT,Lung
Deep learning-based pulmonary nodule detection: Effect of slab thickness in maximum intensity projections at the nodule candidate detection stage,"BACKGROUND AND OBJECTIVE: To investigate the effect of the slab thickness in maximum intensity projections (MIPs) on the candidate detection performance of a deep learning-based computer-aided detection (DL-CAD) system for pulmonary nodule detection in CT scans. METHODS: The public LUNA16 dataset includes 888 CT scans with 1186 nodules annotated by four radiologists. From those scans, MIP images were reconstructed with slab thicknesses of 5 to 50 mm (at 5 mm intervals) and 3 to 13 mm (at 2 mm intervals). The architecture in the nodule candidate detection part of the DL-CAD system was trained separately using MIP images with various slab thicknesses. Based on ten-fold cross-validation, the sensitivity and the F(2) score were determined to evaluate the performance of using each slab thickness at the nodule candidate detection stage. The free-response receiver operating characteristic (FROC) curve was used to assess the performance of the whole DL-CAD system that took the results combined from 16 MIP slab thickness settings. RESULTS: At the nodule candidate detection stage, the combination of results from 16 MIP slab thickness settings showed a high sensitivity of 98.0% with 46 false positives (FPs) per scan. Regarding a single MIP slab thickness of 10 mm, the highest sensitivity of 90.0% with 8 FPs/scan was reached before false positive reduction. The sensitivity increased (82.8% to 90.0%) for slab thickness of 1 to 10 mm and decreased (88.7% to 76.6%) for slab thickness of 15-50 mm. The number of FPs was decreasing with increasing slab thickness, but was stable at 5 FPs/scan at a slab thickness of 30 mm or more. After false positive reduction, the DL-CAD system, utilizing 16 MIP slab thickness settings, had the sensitivity of 94.4% with 1 FP/scan. CONCLUSIONS: The utilization of multi-MIP images could improve the performance at the nodule candidate detection stage, even for the whole DL-CAD system. For a single slab thickness of 10 mm, the highest sensitivity for pulmonary nodule detection was reached at the nodule candidate detection stage, similar to the slab thickness usually applied by radiologists.",2020,10.1016/j.cmpb.2020.105620,cross-sectional,diagnosis,CT,Lung
Deep learning-based real-time volumetric imaging for lung stereotactic body radiation therapy: a proof of concept study,"Due to the inter- and intra- variation of respiratory motion, it is highly desired to provide real-time volumetric images during the treatment delivery of lung stereotactic body radiation therapy (SBRT) for accurate and active motion management. In this proof-of-concept study, we propose a novel generative adversarial network integrated with perceptual supervision to derive instantaneous volumetric images from a single 2D projection. Our proposed network, named TransNet, consists of three modules, i.e. encoding, transformation and decoding modules. Rather than only using image distance loss between the generated 3D images and the ground truth 3D CT images to supervise the network, perceptual loss in feature space is integrated into loss function to force the TransNet to yield accurate lung boundary. Adversarial supervision is also used to improve the realism of generated 3D images. We conducted a simulation study on 20 patient cases, who had received lung SBRT treatments in our institution and undergone 4D-CT simulation, and evaluated the efficacy and robustness of our method for four different projection angles, i.e. 0°, 30°, 60° and 90°. For each 3D CT image set of a breathing phase, we simulated its 2D projections at these angles. For each projection angle, a patient's 3D CT images of 9 phases and the corresponding 2D projection data were used to train our network for that specific patient, with the remaining phase used for testing. The mean absolute error of the 3D images obtained by our method are 99.3 ± 14.1 HU. The peak signal-to-noise ratio and structural similarity index metric within the tumor region of interest are 15.4 ± 2.5 dB and 0.839 ± 0.090, respectively. The center of mass distance between the manual tumor contours on the 3D images obtained by our method and the manual tumor contours on the corresponding 3D phase CT images are within 2.6 mm, with a mean value of 1.26 mm averaged over all the cases. Our method has also been validated in a simulated challenging scenario with increased respiratory motion amplitude and tumor shrinkage, and achieved acceptable results. Our experimental results demonstrate the feasibility and efficacy of our 2D-to-3D method for lung cancer patients, which provides a potential solution for in-treatment real-time on-board volumetric imaging for tumor tracking and dose delivery verification to ensure the effectiveness of lung SBRT treatment.",2020,10.1088/1361-6560/abc303,cross-sectional,treatment,CXR,Lung
Deep learning-based segmentation of the lung in MR-images acquired by a stack-of-spirals trajectory at ultra-short echo-times,"BACKGROUND: Functional lung MRI techniques are usually associated with time-consuming post-processing, where manual lung segmentation represents the most cumbersome part. The aim of this study was to investigate whether deep learning-based segmentation of lung images which were scanned by a fast UTE sequence exploiting the stack-of-spirals trajectory can provide sufficiently good accuracy for the calculation of functional parameters. METHODS: In this study, lung images were acquired in 20 patients suffering from cystic fibrosis (CF) and 33 healthy volunteers, by a fast UTE sequence with a stack-of-spirals trajectory and a minimum echo-time of 0.05 ms. A convolutional neural network was then trained for semantic lung segmentation using 17,713 2D coronal slices, each paired with a label obtained from manual segmentation. Subsequently, the network was applied to 4920 independent 2D test images and results were compared to a manual segmentation using the Sørensen-Dice similarity coefficient (DSC) and the Hausdorff distance (HD). Obtained lung volumes and fractional ventilation values calculated from both segmentations were compared using Pearson's correlation coefficient and Bland Altman analysis. To investigate generalizability to patients outside the CF collective, in particular to those exhibiting larger consolidations inside the lung, the network was additionally applied to UTE images from four patients with pneumonia and one with lung cancer. RESULTS: The overall DSC for lung tissue was 0.967 ± 0.076 (mean ± standard deviation) and HD was 4.1 ± 4.4 mm. Lung volumes derived from manual and deep learning based segmentations as well as values for fractional ventilation exhibited a high overall correlation (Pearson's correlation coefficent = 0.99 and 1.00). For the additional cohort with unseen pathologies / consolidations, mean DSC was 0.930 ± 0.083, HD = 12.9 ± 16.2 mm and the mean difference in lung volume was 0.032 ± 0.048 L. CONCLUSIONS: Deep learning-based image segmentation in stack-of-spirals based lung MRI allows for accurate estimation of lung volumes and fractional ventilation values and promises to replace the time-consuming step of manual image segmentation in the future.",2021,10.1186/s12880-021-00608-1,case control,prognosis,MRI,Lung
Deep learning-based segmentation of the thorax in mouse micro-CT scans,"For image-guided small animal irradiations, the whole workflow of imaging, organ contouring, irradiation planning, and delivery is typically performed in a single session requiring continuous administration of anaesthetic agents. Automating contouring leads to a faster workflow, which limits exposure to anaesthesia and thereby, reducing its impact on experimental results and on animal wellbeing. Here, we trained the 2D and 3D U-Net architectures of no-new-Net (nnU-Net) for autocontouring of the thorax in mouse micro-CT images. We trained the models only on native CTs and evaluated their performance using an independent testing dataset (i.e., native CTs not included in the training and validation). Unlike previous studies, we also tested the model performance on an external dataset (i.e., contrast-enhanced CTs) to see how well they predict on CTs completely different from what they were trained on. We also assessed the interobserver variability using the generalized conformity index ([Formula: see text]) among three observers, providing a stronger human baseline for evaluating automated contours than previous studies. Lastly, we showed the benefit on the contouring time compared to manual contouring. The results show that 3D models of nnU-Net achieve superior segmentation accuracy and are more robust to unseen data than 2D models. For all target organs, the mean surface distance (MSD) and the Hausdorff distance (95p HD) of the best performing model for this task (nnU-Net 3d_fullres) are within 0.16 mm and 0.60 mm, respectively. These values are below the minimum required contouring accuracy of 1 mm for small animal irradiations, and improve significantly upon state-of-the-art 2D U-Net-based AIMOS method. Moreover, the conformity indices of the 3d_fullres model also compare favourably to the interobserver variability for all target organs, whereas the 2D models perform poorly in this regard. Importantly, the 3d_fullres model offers 98% reduction in contouring time.",2022,10.1038/s41598-022-05868-7,,,,
Deep Learning-Based Stroke Disease Prediction System Using Real-Time Bio Signals,"The emergence of an aging society is inevitable due to the continued increases in life expectancy and decreases in birth rate. These social changes require new smart healthcare services for use in daily life, and COVID-19 has also led to a contactless trend necessitating more non-face-to-face health services. Due to the improvements that have been achieved in healthcare technologies, an increasing number of studies have attempted to predict and analyze certain diseases in advance. Research on stroke diseases is actively underway, particularly with the aging population. Stroke, which is fatal to the elderly, is a disease that requires continuous medical observation and monitoring, as its recurrence rate and mortality rate are very high. Most studies examining stroke disease to date have used MRI or CT images for simple classification. This clinical approach (imaging) is expensive and time-consuming while requiring bulky equipment. Recently, there has been increasing interest in using non-invasive measurable EEGs to compensate for these shortcomings. However, the prediction algorithms and processing procedures are both time-consuming because the raw data needs to be separated before the specific attributes can be obtained. Therefore, in this paper, we propose a new methodology that allows for the immediate application of deep learning models on raw EEG data without using the frequency properties of EEG. This proposed deep learning-based stroke disease prediction model was developed and trained with data collected from real-time EEG sensors. We implemented and compared different deep-learning models (LSTM, Bidirectional LSTM, CNN-LSTM, and CNN-Bidirectional LSTM) that are specialized in time series data classification and prediction. The experimental results confirmed that the raw EEG data, when wielded by the CNN-bidirectional LSTM model, can predict stroke with 94.0% accuracy with low FPR (6.0%) and FNR (5.7%), thus showing high confidence in our system. These experimental results demonstrate the feasibility of non-invasive methods that can easily measure brain waves alone to predict and monitor stroke diseases in real time during daily life. These findings are expected to lead to significant improvements for early stroke detection with reduced cost and discomfort compared to other measuring techniques.",2021,10.3390/s21134269,,,,
Deep learning-enabled accurate normalization of reconstruction kernel effects on emphysema quantification in low-dose CT,"Lung densitometry is being frequently adopted in CT-based emphysema quantification, yet known to be affected by the choice of reconstruction kernel. This study presents a two-step deep learning architecture that enables accurate normalization of reconstruction kernel effects on emphysema quantification in low-dose CT. Deep learning is used to convert a CT image of a sharp kernel to that of a standard kernel with restoration of truncation artifacts and smoothing-free pixel size normalization. We selected 353 scans reconstructed by both standard and sharp kernels from four different CT scanners from the United States National Lung Screening Trial program database. A truncation artifact correction model was constructed with a combination of histogram extrapolation and a deep learning model trained with truncated and non-truncated image sets. Then, we performed frequency domain zero-padding to normalize reconstruction field of view effects while preventing image smoothing effects. The kernel normalization model has a U-Net based architecture trained for each CT scanner dataset. Three lung density measurements including relative lung area under 950 HU (RA950), lower 15th percentile threshold (perc15), and mean lung density were obtained in the datasets from standard, sharp, and normalized kernels. The effect of kernel normalization was evaluated with pair-wise differences in lung density metrics. The mean of pair-wise differences in RA950 between standard and sharp kernel reconstructions was reduced from 10.75% to -0.07% using kernel normalization. The difference for perc15 decreased from -31.03 HU to -0.30 HU after kernel normalization. Our study demonstrated the feasibility of applying deep learning techniques for normalizing CT kernel effects, thereby reducing the kernel-induced variability in lung density measurements. The deep learning model could increase the accuracy of emphysema quantification, thereby allowing reliable surveillance of emphysema in lung cancer screening even when follow-up CT scans are acquired with different reconstruction kernels.",2019,10.1088/1361-6560/ab28a1,RCT,others,CT,Lung
Deep Mining External Imperfect Data for Chest X-Ray Disease Screening,"Deep learning approaches have demonstrated remarkable progress in automatic Chest X-ray analysis. The data-driven feature of deep models requires training data to cover a large distribution. Therefore, it is substantial to integrate knowledge from multiple datasets, especially for medical images. However, learning a disease classification model with extra Chest X-ray (CXR) data is yet challenging. Recent researches have demonstrated that performance bottleneck exists in joint training on different CXR datasets, and few made efforts to address the obstacle. In this paper, we argue that incorporating an external CXR dataset leads to imperfect training data, which raises the challenges. Specifically, the imperfect data is in two folds: domain discrepancy, as the image appearances vary across datasets; and label discrepancy, as different datasets are partially labeled. To this end, we formulate the multi-label thoracic disease classification problem as weighted independent binary tasks according to the categories. For common categories shared across domains, we adopt task-specific adversarial training to alleviate the feature differences. For categories existing in a single dataset, we present uncertainty-aware temporal ensembling of model predictions to mine the information from the missing labels further. In this way, our framework simultaneously models and tackles the domain and label discrepancies, enabling superior knowledge mining ability. We conduct extensive experiments on three datasets with more than 360,000 Chest X-ray images. Our method outperforms other competing models and sets state-of-the-art performance on the official NIH test set with 0.8349 AUC, demonstrating its effectiveness of utilizing the external dataset to improve the internal classification.",2020,10.1109/tmi.2020.3000949,cross-sectional,diagnosis,CXR,Lung
Deep Mining Generation of Lung Cancer Malignancy Models from Chest X-ray Images,"Lung cancer is the leading cause of cancer death and morbidity worldwide. Many studies have shown machine learning models to be effective in detecting lung nodules from chest X-ray images. However, these techniques have yet to be embraced by the medical community due to several practical, ethical, and regulatory constraints stemming from the ""black-box"" nature of deep learning models. Additionally, most lung nodules visible on chest X-rays are benign; therefore, the narrow task of computer vision-based lung nodule detection cannot be equated to automated lung cancer detection. Addressing both concerns, this study introduces a novel hybrid deep learning and decision tree-based computer vision model, which presents lung cancer malignancy predictions as interpretable decision trees. The deep learning component of this process is trained using a large publicly available dataset on pathological biomarkers associated with lung cancer. These models are then used to inference biomarker scores for chest X-ray images from two independent data sets, for which malignancy metadata is available. Next, multi-variate predictive models were mined by fitting shallow decision trees to the malignancy stratified datasets and interrogating a range of metrics to determine the best model. The best decision tree model achieved sensitivity and specificity of 86.7% and 80.0%, respectively, with a positive predictive value of 92.9%. Decision trees mined using this method may be considered as a starting point for refinement into clinically useful multi-variate lung cancer malignancy models for implementation as a workflow augmentation tool to improve the efficiency of human radiologists.",2021,10.3390/s21196655,cross-sectional,diagnosis,CXR,Lung
Deep monocular 3D reconstruction for assisted navigation in bronchoscopy,"PURPOSE: In bronchoschopy, computer vision systems for navigation assistance are an attractive low-cost solution to guide the endoscopist to target peripheral lesions for biopsy and histological analysis. We propose a decoupled deep learning architecture that projects input frames onto the domain of CT renderings, thus allowing offline training from patient-specific CT data. METHODS: A fully convolutional network architecture is implemented on GPU and tested on a phantom dataset involving 32 video sequences and [Formula: see text]60k frames with aligned ground truth and renderings, which is made available as the first public dataset for bronchoscopy navigation. RESULTS: An average estimated depth accuracy of 1.5 mm was obtained, outperforming conventional direct depth estimation from input frames by 60%, and with a computational time of [Formula: see text]30 ms on modern GPUs. Qualitatively, the estimated depth and renderings closely resemble the ground truth. CONCLUSIONS: The proposed method shows a novel architecture to perform real-time monocular depth estimation without losing patient specificity in bronchoscopy. Future work will include integration within SLAM systems and collection of in vivo datasets.",2017,10.1007/s11548-017-1609-2,,,,
Deep multi-instance transfer learning for pneumothorax classification in chest X-ray images,"PURPOSE: Pneumothorax is a life-threatening emergency that requires immediate treatment. Frontal-view chest X-ray images are typically used for pneumothorax detection in clinical practice. However, manual review of radiographs is time-consuming, labor-intensive, and highly dependent on the experience of radiologists, which may lead to misdiagnosis. Here, we aim to develop a reliable automatic classification method to assist radiologists in rapidly and accurately diagnosing pneumothorax in frontal chest radiographs. METHODS: A novel residual neural network (ResNet)-based two-stage deep-learning strategy is proposed for pneumothorax identification: local feature learning (LFL) followed by global multi-instance learning (GMIL). Most of the nonlesion regions in the images are removed for learning discriminative features. Two datasets are used for large-scale validation: a private dataset (27 955 frontal-view chest X-ray images) and a public dataset (the National Institutes of Health [NIH] ChestX-ray14; 112 120 frontal-view X-ray images). The model performance of the identification was evaluated using the accuracy, precision, recall, specificity, F1-score, receiver operating characteristic (ROC), and area under ROC curve (AUC). Fivefold cross-validation is conducted on the datasets, and then the mean and standard deviation of the above-mentioned metrics are calculated to assess the overall performance of the model. RESULTS: The experimental results demonstrate that the proposed learning strategy can achieve state-of-the-art performance on the NIH dataset with an accuracy, AUC, precision, recall, specificity, and F1-score of 94.4% ± 0.7%, 97.3% ± 0.5%, 94.2% ± 0.3%, 94.6% ± 1.5%, 94.2% ± 0.4%, and 94.4% ± 0.7%, respectively. CONCLUSIONS: The experimental results demonstrate that our proposed CAD system is an efficient assistive tool in the identification of pneumothorax.",2022,10.1002/mp.15328,cross-sectional,diagnosis,CXR,Lung
Deep neural network analyses of spirometry for structural phenotyping of chronic obstructive pulmonary disease,"BACKGROUNDCurrently recommended traditional spirometry outputs do not reflect the relative contributions of emphysema and airway disease to airflow obstruction. We hypothesized that machine-learning algorithms can be trained on spirometry data to identify these structural phenotypes.METHODSParticipants enrolled in a large multicenter study (COPDGene) were included. The data points from expiratory flow-volume curves were trained using a deep-learning model to predict structural phenotypes of chronic obstructive pulmonary disease (COPD) on CT, and results were compared with traditional spirometry metrics and an optimized random forest classifier. Area under the receiver operating characteristic curve (AUC) and weighted F-score were used to measure the discriminative accuracy of a fully convolutional neural network, random forest, and traditional spirometry metrics to phenotype CT as normal, emphysema-predominant (>5% emphysema), airway-predominant (Pi10 > median), and mixed phenotypes. Similar comparisons were made for the detection of functional small airway disease phenotype (>20% on parametric response mapping).RESULTSAmong 8980 individuals, the neural network was more accurate in discriminating predominant emphysema/airway phenotypes (AUC 0.80, 95%CI 0.79-0.81) compared with traditional measures of spirometry, FEV1/FVC (AUC 0.71, 95%CI 0.69-0.71), FEV1% predicted (AUC 0.70, 95%CI 0.68-0.71), and random forest classifier (AUC 0.78, 95%CI 0.77-0.79). The neural network was also more accurate in discriminating predominant emphysema/small airway phenotypes (AUC 0.91, 95%CI 0.90-0.92) compared with FEV1/FVC (AUC 0.80, 95%CI 0.78-0.82), FEV1% predicted (AUC 0.83, 95%CI 0.80-0.84), and with comparable accuracy with random forest classifier (AUC 0.90, 95%CI 0.88-0.91).CONCLUSIONSStructural phenotypes of COPD can be identified from spirometry using deep-learning and machine-learning approaches, demonstrating their potential to identify individuals for targeted therapies.TRIAL REGISTRATIONClinicalTrials.gov NCT00608764.FUNDINGThis study was supported by NIH grants K23 HL133438 and R21EB027891 and an American Thoracic Foundation 2018 Unrestricted Research Grant. The COPDGene study is supported by NIH grants NHLBI U01 HL089897 and U01 HL089856. The COPDGene study (NCT00608764) is also supported by the COPD Foundation through contributions made to an Industry Advisory Committee comprising AstraZeneca, Boehringer-Ingelheim, GlaxoSmithKline, Novartis, and Sunovion.",2020,10.1172/jci.insight.132781,,,,
Deep radiomics-based survival prediction in patients with chronic obstructive pulmonary disease,"Heterogeneous clinical manifestations and progression of chronic obstructive pulmonary disease (COPD) affect patient health risk assessment, stratification, and management. Pulmonary function tests are used to diagnose and classify the severity of COPD, but they cannot fully represent the type or range of pathophysiologic abnormalities of the disease. To evaluate whether deep radiomics from chest computed tomography (CT) images can predict mortality in patients with COPD, we designed a convolutional neural network (CNN) model for extracting representative features from CT images and then performed random survival forest to predict survival in COPD patients. We trained CNN-based binary classifier based on six-minute walk distance results (> 440 m or not) and extracted high-throughput image features (i.e., deep radiomics) directly from the last fully connected layer of it. The various sizes of fully connected layers and combinations of deep features were experimented using a discovery cohort with 344 patients from the Korean Obstructive Lung Disease cohort and an external validation cohort with 102 patients from Penang General Hospital in Malaysia. In the integrative analysis of discovery and external validation cohorts, with combining 256 deep features from the coronal slice of the vertebral body and two sagittal slices of the left/right lung, deep radiomics for survival prediction achieved concordance indices of 0.8008 (95% CI, 0.7642-0.8373) and 0.7156 (95% CI, 0.7024-0.7288), respectively. Deep radiomics from CT images could be used to predict mortality in COPD patients.",2021,10.1038/s41598-021-94535-4,prospective cohort,prognosis,CT,Lung
Deep reinforcement learning for automated radiation adaptation in lung cancer,"PURPOSE: To investigate deep reinforcement learning (DRL) based on historical treatment plans for developing automated radiation adaptation protocols for nonsmall cell lung cancer (NSCLC) patients that aim to maximize tumor local control at reduced rates of radiation pneumonitis grade 2 (RP2). METHODS: In a retrospective population of 114 NSCLC patients who received radiotherapy, a three-component neural networks framework was developed for deep reinforcement learning (DRL) of dose fractionation adaptation. Large-scale patient characteristics included clinical, genetic, and imaging radiomics features in addition to tumor and lung dosimetric variables. First, a generative adversarial network (GAN) was employed to learn patient population characteristics necessary for DRL training from a relatively limited sample size. Second, a radiotherapy artificial environment (RAE) was reconstructed by a deep neural network (DNN) utilizing both original and synthetic data (by GAN) to estimate the transition probabilities for adaptation of personalized radiotherapy patients' treatment courses. Third, a deep Q-network (DQN) was applied to the RAE for choosing the optimal dose in a response-adapted treatment setting. This multicomponent reinforcement learning approach was benchmarked against real clinical decisions that were applied in an adaptive dose escalation clinical protocol. In which, 34 patients were treated based on avid PET signal in the tumor and constrained by a 17.2% normal tissue complication probability (NTCP) limit for RP2. The uncomplicated cure probability (P+) was used as a baseline reward function in the DRL. RESULTS: Taking our adaptive dose escalation protocol as a blueprint for the proposed DRL (GAN + RAE + DQN) architecture, we obtained an automated dose adaptation estimate for use at ∼2/3 of the way into the radiotherapy treatment course. By letting the DQN component freely control the estimated adaptive dose per fraction (ranging from 1-5 Gy), the DRL automatically favored dose escalation/de-escalation between 1.5 and 3.8 Gy, a range similar to that used in the clinical protocol. The same DQN yielded two patterns of dose escalation for the 34 test patients, but with different reward variants. First, using the baseline P+ reward function, individual adaptive fraction doses of the DQN had similar tendencies to the clinical data with an RMSE = 0.76 Gy; but adaptations suggested by the DQN were generally lower in magnitude (less aggressive). Second, by adjusting the P+ reward function with higher emphasis on mitigating local failure, better matching of doses between the DQN and the clinical protocol was achieved with an RMSE = 0.5 Gy. Moreover, the decisions selected by the DQN seemed to have better concordance with patients eventual outcomes. In comparison, the traditional temporal difference (TD) algorithm for reinforcement learning yielded an RMSE = 3.3 Gy due to numerical instabilities and lack of sufficient learning. CONCLUSION: We demonstrated that automated dose adaptation by DRL is a feasible and a promising approach for achieving similar results to those chosen by clinicians. The process may require customization of the reward function if individual cases were to be considered. However, development of this framework into a fully credible autonomous system for clinical decision support would require further validation on larger multi-institutional datasets.",2017,10.1002/mp.12625,,,,
Deep Reinforcement Learning for Fractionated Radiotherapy in Non-Small Cell Lung Carcinoma,"Lung cancer is by far the leading cause of cancer death among both men and women. Radiation therapy is one of the main approaches to lung cancer treatment, and its planning is crucial for the therapy outcome. However, the current practice that uniformly delivers the dose does not take into account the patient-specific tumour features that may affect treatment success. Since radiation therapy is by its very nature a sequential procedure, Deep Reinforcement Learning (DRL) is a well-suited methodology to overcome this limitation. In this respect, in this work we present a DRL controller optimizing the daily dose fraction delivered to the patient on the basis of CT scans collected over time during the therapy, offering a personalized treatment not only for volume adaptation, as currently intended, but also for daily fractionation. Furthermore, this contribution introduces a virtual radiotherapy environment based on a set of ordinary differential equations modelling the tissue radiosensitivity by combining both the effect of the radiotherapy treatment and cell growth. Their parameters are estimated from CT scans routinely collected using the Particle Swarm Optimization algorithm. This permits the DRL to learn the optimal behaviour through an iterative trial and error process with the environment. We performed several experiments considering three rewards functions modelling treatment strategies with different tissue aggressiveness and two exploration strategies for the exploration-exploitation dilemma. The results show that our DRL approach can adapt to radiation therapy treatment, optimizing its behaviour according to the different reward functions and outperforming the current clinical practice.",2021,10.1016/j.artmed.2021.102137,retrospective cohort,treatment,CT,Lung
Deep Residual Separable Convolutional Neural Network for lung tumor segmentation,"Lung cancer is one of the deadliest types of cancers. Computed Tomography (CT) is a widely used technique to detect tumors present inside the lungs. Delineation of such tumors is particularly essential for analysis and treatment purposes. With the advancement in hardware technologies, Machine Learning and Deep Learning methods are outperforming the traditional methods in the field of medical imaging. In order to delineate lung cancer tumors, we have proposed a deep learning-based methodology which includes a maximum intensity projection based pre-processing method, two novel deep learning networks and an ensemble strategy. The two proposed networks named Deep Residual Separable Convolutional Neural Network 1 and 2 (DRS-CNN1 and DRS-CNN2) achieved better performance over the state-of-the-art U-net network and other segmentation networks. For fair comparison, we have evaluated the performances of all networks on Medical Segmentation Decathlon (MSD) and StructSeg 2019 datasets. The DRS-CNN2 achieved a mean Dice Similarity Coefficient (DSC) of 0.649, mean 95 Hausdorff Distance (HD95) of 18.26, mean Sensitivity 0.737 and a mean Precision of 0.765 on independent test sets.",2022,10.1016/j.compbiomed.2021.105161,cross-sectional,diagnosis,CT,Lung
Deep segmentation networks predict survival of non-small cell lung cancer,"Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung cancer diagnoses and is the leading cause of cancer-related death worldwide. Recent studies indicate that image-based radiomics features from positron emission tomography/computed tomography (PET/CT) images have predictive power for NSCLC outcomes. To this end, easily calculated functional features such as the maximum and the mean of standard uptake value (SUV) and total lesion glycolysis (TLG) are most commonly used for NSCLC prognostication, but their prognostic value remains controversial. Meanwhile, convolutional neural networks (CNN) are rapidly emerging as a new method for cancer image analysis, with significantly enhanced predictive power compared to hand-crafted radiomics features. Here we show that CNNs trained to perform the tumor segmentation task, with no other information than physician contours, identify a rich set of survival-related image features with remarkable prognostic value. In a retrospective study on pre-treatment PET-CT images of 96 NSCLC patients before stereotactic-body radiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net) trained for tumor segmentation in PET and CT images, contained features having strong correlation with 2- and 5-year overall and disease-specific survivals. The U-Net algorithm has not seen any other clinical information (e.g. survival, age, smoking history, etc.) than the images and the corresponding tumor contours provided by physicians. In addition, we observed the same trend by validating the U-Net features against an extramural data set provided by Stanford Cancer Institute. Furthermore, through visualization of the U-Net, we also found convincing evidence that the regions of metastasis and recurrence appear to match with the regions where the U-Net features identified patterns that predicted higher likelihoods of death. We anticipate our findings will be a starting point for more sophisticated non-intrusive patient specific cancer prognosis determination. For example, the deep learned PET/CT features can not only predict survival but also visualize high-risk regions within or adjacent to the primary tumor and hence potentially impact therapeutic outcomes by optimal selection of therapeutic strategy or first-line therapy adjustment.",2019,10.1038/s41598-019-53461-2,retrospective cohort,prognosis,PET/CT,Lung
Deep semantic lung segmentation for tracking potential pulmonary perfusion biomarkers in chronic obstructive pulmonary disease (COPD): The multi-ethnic study of atherosclerosis COPD study,"BACKGROUND: Chronic obstructive pulmonary disease (COPD) is associated with high morbidity and mortality. Identification of imaging biomarkers for phenotyping is necessary for future treatment and therapy monitoring. However, translation of visual analytic pipelines into clinics or their use in large-scale studies is significantly slowed by time-consuming postprocessing steps. PURPOSE: To implement an automated tool chain for regional quantification of pulmonary microvascular blood flow in order to reduce analysis time and user variability. STUDY TYPE: Prospective. POPULATION: In all, 90 MRI scans of 63 patients, of which 31 had a COPD with a mean Global Initiative for Chronic Obstructive Lung Disease status of 1.9 ± 0.64 (μ ± σ). FIELD STRENGTH/SEQUENCE: 1.5T dynamic gadolinium-enhanced MRI measurement using 4D dynamic contrast material-enhanced (DCE) time-resolved angiography acquired in a single breath-hold in inspiration. [Correction added on August 20, 2019, after first online publication: The field strength in the preceding sentence was corrected.] ASSESSMENT: We built a 3D convolutional neural network for semantic segmentation using 29 manually segmented perfusion maps. All five lobes of the lung are denoted, including the middle lobe. Evaluation was performed on 61 independent cases from two sites of the Multi-Ethnic Study of Arteriosclerosis (MESA)-COPD study. We publish our implementation of a model-free deconvolution filter according to Sourbron et al for 4D DCE MRI scans as open source. STATISTICAL TEST: Cross-validation 29/61 (# training / # testing), intraclass correlation coefficient (ICC), Spearman ρ, Pearson r, Sørensen-Dice coefficient, and overlap. RESULTS: Segmentations and derived clinical parameters were processed in ~90 seconds per case on a Xeon E5-2637v4 workstation with Tesla P40 GPUs. Clinical parameters and predicted segmentations exhibit high concordance with the ground truth regarding median perfusion for all lobes with an ICC of 0.99 and a Sørensen-Dice coefficient of 93.4 ± 2.8 (μ ± σ). DATA CONCLUSION: We present a robust end-to-end pipeline that allows for the extraction of perfusion-based biomarkers for all lung lobes in 4D DCE MRI scans by combining model-free deconvolution with deep learning. LEVEL OF EVIDENCE: 3 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:571-579.",2020,10.1002/jmri.26853,cross-sectional,prognosis,MRI,Lung
Deep Semi Supervised Generative Learning for Automated Tumor Proportion Scoring on NSCLC Tissue Needle Biopsies,"The level of PD-L1 expression in immunohistochemistry (IHC) assays is a key biomarker for the identification of Non-Small-Cell-Lung-Cancer (NSCLC) patients that may respond to anti PD-1/PD-L1 treatments. The quantification of PD-L1 expression currently includes the visual estimation by a pathologist of the percentage (tumor proportional scoring or TPS) of tumor cells showing PD-L1 staining. Known challenges like differences in positivity estimation around clinically relevant cut-offs and sub-optimal quality of samples makes visual scoring tedious and subjective, yielding a scoring variability between pathologists. In this work, we propose a novel deep learning solution that enables the first automated and objective scoring of PD-L1 expression in late stage NSCLC needle biopsies. To account for the low amount of tissue available in biopsy images and to restrict the amount of manual annotations necessary for training, we explore the use of semi-supervised approaches against standard fully supervised methods. We consolidate the manual annotations used for training as well the visual TPS scores used for quantitative evaluation with multiple pathologists. Concordance measures computed on a set of slides unseen during training provide evidence that our automatic scoring method matches visual scoring on the considered dataset while ensuring repeatability and objectivity.",2018,10.1038/s41598-018-35501-5,,,,
Deep Transfer Learning for COVID-19 Prediction: Case Study for Limited Data Problems,"OBJECTIVE: Automatic prediction of COVID-19 using deep convolution neural networks based pre-trained transfer models and Chest X-ray images. METHODS: This research employs the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of the disease. Using Deep Learning models, the research aims at evaluating the effectiveness and accuracy of different convolutional neural networks models in the automatic diagnosis of COVID-19 from X-ray images as compared to diagnosis performed by experts in the medical community. RESULTS: Due to the fact that the dataset available for COVID-19 is still limited, the best model to use is the InceptionNetV3. Performance results show that the InceptionNetV3 model yielded the highest accuracy of 98.63% (with data augmentation) and 98.90% (without data augmentation) among the three models designed. However, as the dataset gets bigger, the Inception ResNetV2 and NASNetlarge will do a better job of classification. All the performed networks tend to over-fit when data augmentation is not used, this is due to the small amount of data used for training and validation. CONCLUSION: A deep transfer learning is proposed to detecting the COVID-19 automatically from chest X-ray by training it with X-ray images gotten from both COVID-19 patients and people with normal chest X-rays. The study is aimed at helping doctors in making decisions in their clinical practice due its high performance and effectiveness, the study also gives an insight to how transfer learning was used to automatically detect the COVID-19.",2021,10.2174/1573405616666201123120417,cross-sectional,diagnosis,CXR,Lung
Deep transfer learning to quantify pleural effusion severity in chest X-rays,"PURPOSE: The detection of pleural effusion in chest radiography is crucial for doctors to make timely treatment decisions for patients with chronic obstructive pulmonary disease. We used the MIMIC-CXR database to develop a deep learning model to quantify pleural effusion severity in chest radiographs. METHODS: The Medical Information Mart for Intensive Care Chest X-ray (MIMIC-CXR) dataset was divided into patients 'with' or 'without' chronic obstructive pulmonary disease (COPD). The label of pleural effusion severity was obtained from the extracted COPD radiology reports and classified into four categories: no effusion, small effusion, moderate effusion, and large effusion. A total of 200 datasets were randomly sampled to manually check each item and determine whether the tags are correct. A professional doctor re-tagged these items as a verification cohort without knowing their previous tags. The learning models include eight common network structures including Resnet, DenseNet, and GoogleNET. Three data processing methods (no sampling, downsampling, and upsampling) and two loss algorithms (focal loss and cross-entropy loss) were used for unbalanced data. The Neural Network Intelligence tool was applied to train the model. Receiver operating characteristic curves, Area under the curve, and confusion matrix were employed to evaluate the model results. Grad-CAM was used for model interpretation. RESULTS: Among the 8533 patients, 15,620 chest X-rays with clearly marked pleural effusion severity were obtained (no effusion, 5685; small effusion, 4877; moderate effusion, 3657; and large effusion, 1401). The error rate of the manual check label was 6.5%, and the error rate of the doctor's relabeling was 11.0%. The highest accuracy rate of the optimized model was 73.07. The micro-average AUCs of the testing and validation cohorts was 0.89 and 0.90, respectively, and their macro-average AUCs were 0.86 and 0.89, respectively. The AUC of the distinguishing results of each class and the other three classes were 0.95 and 0.94, 0.76 and 0.83, 0.85 and 0.83, and 0.87 and 0.93. CONCLUSION: The deep transfer learning model can grade the severity of pleural effusion.",2022,10.1186/s12880-022-00827-0,cross-sectional,diagnosis,CXR,Lung
"Deep-chest: Multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer chest diseases","Corona Virus Disease (COVID-19) has been announced as a pandemic and is spreading rapidly throughout the world. Early detection of COVID-19 may protect many infected people. Unfortunately, COVID-19 can be mistakenly diagnosed as pneumonia or lung cancer, which with fast spread in the chest cells, can lead to patient death. The most commonly used diagnosis methods for these three diseases are chest X-ray and computed tomography (CT) images. In this paper, a multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer from a combination of chest x-ray and CT images is proposed. This combination has been used because chest X-ray is less powerful in the early stages of the disease, while a CT scan of the chest is useful even before symptoms appear, and CT can precisely detect the abnormal features that are identified in images. In addition, using these two types of images will increase the dataset size, which will increase the classification accuracy. To the best of our knowledge, no other deep learning model choosing between these diseases is found in the literature. In the present work, the performance of four architectures are considered, namely: VGG19-CNN, ResNet152V2, ResNet152V2 + Gated Recurrent Unit (GRU), and ResNet152V2 + Bidirectional GRU (Bi-GRU). A comprehensive evaluation of different deep learning architectures is provided using public digital chest x-ray and CT datasets with four classes (i.e., Normal, COVID-19, Pneumonia, and Lung cancer). From the results of the experiments, it was found that the VGG19 +CNN model outperforms the three other proposed models. The VGG19+CNN model achieved 98.05% accuracy (ACC), 98.05% recall, 98.43% precision, 99.5% specificity (SPC), 99.3% negative predictive value (NPV), 98.24% F1 score, 97.7% Matthew's correlation coefficient (MCC), and 99.66% area under the curve (AUC) based on X-ray and CT images.",2021,10.1016/j.compbiomed.2021.104348,cross-sectional,diagnosis,CT/CXR,Lung
Deep-COVID: Predicting COVID-19 from chest X-ray images using deep transfer learning,"The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3000 images, and most of these networks achieved a sensitivity rate of 98% ( ± 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git.",2020,10.1016/j.media.2020.101794,cross-sectional,diagnosis,CXR,Lung
Deep-learning algorithms for the interpretation of chest radiographs to aid in the triage of COVID-19 patients: A multicenter retrospective study,"The recent medical applications of deep-learning (DL) algorithms have demonstrated their clinical efficacy in improving speed and accuracy of image interpretation. If the DL algorithm achieves a performance equivalent to that achieved by physicians in chest radiography (CR) diagnoses with Coronavirus disease 2019 (COVID-19) pneumonia, the automatic interpretation of the CR with DL algorithms can significantly reduce the burden on clinicians and radiologists in sudden surges of suspected COVID-19 patients. The aim of this study was to evaluate the efficacy of the DL algorithm for detecting COVID-19 pneumonia on CR compared with formal radiology reports. This is a retrospective study of adult patients that were diagnosed as positive COVID-19 cases based on the reverse transcription polymerase chain reaction among all the patients who were admitted to five emergency departments and one community treatment center in Korea from February 18, 2020 to May 1, 2020. The CR images were evaluated with a publicly available DL algorithm. For reference, CR images without chest computed tomography (CT) scans classified as positive for COVID-19 pneumonia were used given that the radiologist identified ground-glass opacity, consolidation, or other infiltration in retrospectively reviewed CR images. Patients with evidence of pneumonia on chest CT scans were also classified as COVID-19 pneumonia positive outcomes. The overall sensitivity and specificity of the DL algorithm for detecting COVID-19 pneumonia on CR were 95.6%, and 88.7%, respectively. The area under the curve value of the DL algorithm for the detection of COVID-19 with pneumonia was 0.921. The DL algorithm demonstrated a satisfactory diagnostic performance comparable with that of formal radiology reports in the CR-based diagnosis of pneumonia in COVID-19 patients. The DL algorithm may offer fast and reliable examinations that can facilitate patient screening and isolation decisions, which can reduce the medical staff workload during COVID-19 pandemic situations.",2020,10.1371/journal.pone.0242759,retrospective cohort,diagnosis,CXR,Lung
Deep-learning based classification distinguishes sarcomatoid malignant mesotheliomas from benign spindle cell mesothelial proliferations,"Sarcomatoid mesothelioma is an aggressive malignancy that can be challenging to distinguish from benign spindle cell mesothelial proliferations based on biopsy, and this distinction is crucial to patient treatment and prognosis. A novel deep learning based classifier may be able to aid pathologists in making this critical diagnostic distinction. SpindleMesoNET was trained on cases of malignant sarcomatoid mesothelioma and benign spindle cell mesothelial proliferations. Performance was assessed through cross-validation on the training set, on an independent set of challenging cases referred for expert opinion ('referral' test set), and on an externally stained set from outside institutions ('externally stained' test set). SpindleMesoNET predicted the benign or malignant status of cases with AUC's of 0.932, 0.925, and 0.989 on the cross-validation, referral and external test sets, respectively. The accuracy of SpindleMesoNET on the referral set cases (92.5%) was comparable to the average accuracy of 3 experienced pathologists on the same slide set (91.7%). We conclude that SpindleMesoNET can accurately distinguish sarcomatoid mesothelioma from benign spindle cell mesothelial proliferations. A deep learning system of this type holds potential for future use as an ancillary test in diagnostic pathology.",2021,10.1038/s41379-021-00850-6,,,,
Deep-learning reconstruction for ultra-low-dose lung CT: Volumetric measurement accuracy and reproducibility of artificial ground-glass nodules in a phantom study,"OBJECTIVES: The lung nodule volume determined by CT is used for nodule diagnoses and monitoring tumor responses to therapy. Increased image noise on low-dose CT degrades the measurement accuracy of the lung nodule volume. We compared the volumetric accuracy among deep-learning reconstruction (DLR), model-based iterative reconstruction (MBIR), and hybrid iterative reconstruction (HIR) at an ultra-low-dose setting. METHODS: Artificial ground-glass nodules (6 mm and 10 mm diameters, -660 HU) placed at the lung-apex and the middle-lung field in chest phantom were scanned by 320-row CT with the ultra-low-dose setting of 6.3 mAs. Each scan data set was reconstructed by DLR, MBIR, and HIR. The volumes of nodules were measured semi-automatically, and the absolute percent volumetric error (APEvol) was calculated. The APEvol provided by each reconstruction were compared by the Tukey-Kramer method. Inter- and intraobserver variabilities were evaluated by a Bland-Altman analysis with limits of agreements. RESULTS: DLR provided a lower APEvol compared to MBIR and HIR. The APEvol of DLR (1.36%) was significantly lower than those of the HIR (8.01%, p = 0.0022) and MBIR (7.30%, p = 0.0053) on a 10-mm-diameter middle-lung nodule. DLR showed narrower limits of agreement compared to MBIR and HIR in the inter- and intraobserver agreement of the volumetric measurement. CONCLUSIONS: DLR showed higher accuracy compared to MBIR and HIR for the volumetric measurement of artificial ground-glass nodules by ultra-low-dose CT. ADVANCES IN KNOWLEDGE: DLR with ultra-low-dose setting allows a reduction of dose exposure, maintaining accuracy for the volumetry of lung nodule, especially in patients which deserve a long-term follow-up.",2022,10.1259/bjr.20210915,,,,
DeepBTS: Prediction of Recurrence-free Survival of Non-small Cell Lung Cancer Using a Time-binned Deep Neural Network,"Accurate prediction of non-small cell lung cancer (NSCLC) prognosis after surgery remains challenging. The Cox proportional hazard (PH) model is widely used, however, there are some limitations associated with it. In this study, we developed novel neural network models called binned time survival analysis (DeepBTS) models using 30 clinico-pathological features of surgically resected NSCLC patients (training cohort, n = 1,022; external validation cohort, n = 298). We employed the root-mean-square error (in the supervised learning model, s- DeepBTS) or negative log-likelihood (in the semi-unsupervised learning model, su-DeepBTS) as the loss function. The su-DeepBTS algorithm achieved better performance (C-index = 0.7306; AUC = 0.7677) than the other models (Cox PH: C-index = 0.7048 and AUC = 0.7390; s-DeepBTS: C-index = 0.7126 and AUC = 0.7420). The top 14 features were selected using su-DeepBTS model as a selector and could distinguish the low- and high-risk groups in the training cohort (p = 1.86 × 10(-11)) and validation cohort (p = 1.04 × 10(-10)). When trained with the optimal feature set for each model, the su-DeepBTS model could predict the prognoses of NSCLC better than the traditional model, especially in stage I patients. Follow-up studies using combined radiological, pathological imaging, and genomic data to enhance the performance of our model are ongoing.",2020,10.1038/s41598-020-58722-z,,,,
DeepCOVID-XR: An Artificial Intelligence Algorithm to Detect COVID-19 on Chest Radiographs Trained and Tested on a Large U.S. Clinical Data Set,"Background There are characteristic findings of coronavirus disease 2019 (COVID-19) on chest images. An artificial intelligence (AI) algorithm to detect COVID-19 on chest radiographs might be useful for triage or infection control within a hospital setting, but prior reports have been limited by small data sets, poor data quality, or both. Purpose To present DeepCOVID-XR, a deep learning AI algorithm to detect COVID-19 on chest radiographs, that was trained and tested on a large clinical data set. Materials and Methods DeepCOVID-XR is an ensemble of convolutional neural networks developed to detect COVID-19 on frontal chest radiographs, with reverse-transcription polymerase chain reaction test results as the reference standard. The algorithm was trained and validated on 14 788 images (4253 positive for COVID-19) from sites across the Northwestern Memorial Health Care System from February 2020 to April 2020 and was then tested on 2214 images (1192 positive for COVID-19) from a single hold-out institution. Performance of the algorithm was compared with interpretations from five experienced thoracic radiologists on 300 random test images using the McNemar test for sensitivity and specificity and the DeLong test for the area under the receiver operating characteristic curve (AUC). Results A total of 5853 patients (mean age, 58 years ± 19 [standard deviation]; 3101 women) were evaluated across data sets. For the entire test set, accuracy of DeepCOVID-XR was 83%, with an AUC of 0.90. For 300 random test images (134 positive for COVID-19), accuracy of DeepCOVID-XR was 82%, compared with that of individual radiologists (range, 76%-81%) and the consensus of all five radiologists (81%). DeepCOVID-XR had a significantly higher sensitivity (71%) than one radiologist (60%, P < .001) and significantly higher specificity (92%) than two radiologists (75%, P < .001; 84%, P = .009). AUC of DeepCOVID-XR was 0.88 compared with the consensus AUC of 0.85 (P = .13 for comparison). With consensus interpretation as the reference standard, the AUC of DeepCOVID-XR was 0.95 (95% CI: 0.92, 0.98). Conclusion DeepCOVID-XR, an artificial intelligence algorithm, detected coronavirus disease 2019 on chest radiographs with a performance similar to that of experienced thoracic radiologists in consensus. © RSNA, 2020 Supplemental material is available for this article. See also the editorial by van Ginneken in this issue.",2021,10.1148/radiol.2020203511,retrospective cohort,diagnosis,CXR,Lung
DeepLNAnno: a Web-Based Lung Nodules Annotating System for CT Images,"Lung cancer is one of the most common and fatal types of cancer, and pulmonary nodule detection plays a crucial role in the screening and diagnosis of this disease. A well-trained deep neural network model can help doctors to find nodules on computed tomography(CT) images while requiring lots of labeled data. However, currently available annotating systems are not suitable for annotating pulmonary nodules in CT images. We propose a web-based lung nodules annotating system named as DeepLNAnno. DeepLNAnno has a unique three-tier working process and loads of features like semi-automatic annotation, which not only make it much easier for doctors to annotate compared to some other annotating systems but also increase the accuracy of the labels. We invited a medical group from West China Hospital to annotate the CT images using our DeepLNAnno system, and collected a large number of labeled data. The results of our experiments demonstrated that a usable nodule-detection system is developed, and good benchmark scores on our evaluation data are obtained.",2019,10.1007/s10916-019-1258-9,,,,
DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View Projections by Deep Deformation Network,"This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT).",2020,10.1109/tvcg.2019.2934369,retrospective cohort,others,CXR/CT,Lung
"DenResCov-19: A deep transfer learning network for robust automatic classification of COVID-19, pneumonia, and tuberculosis from X-rays","The global pandemic of coronavirus disease 2019 (COVID-19) is continuing to have a significant effect on the well-being of the global population, thus increasing the demand for rapid testing, diagnosis, and treatment. As COVID-19 can cause severe pneumonia, early diagnosis is essential for correct treatment, as well as to reduce the stress on the healthcare system. Along with COVID-19, other etiologies of pneumonia and Tuberculosis (TB) constitute additional challenges to the medical system. Pneumonia (viral as well as bacterial) kills about 2 million infants every year and is consistently estimated as one of the most important factor of childhood mortality (according to the World Health Organization). Chest X-ray (CXR) and computed tomography (CT) scans are the primary imaging modalities for diagnosing respiratory diseases. Although CT scans are the gold standard, they are more expensive, time consuming, and are associated with a small but significant dose of radiation. Hence, CXR have become more widespread as a first line investigation. In this regard, the objective of this work is to develop a new deep transfer learning pipeline, named DenResCov-19, to diagnose patients with COVID-19, pneumonia, TB or healthy based on CXR images. The pipeline consists of the existing DenseNet-121 and the ResNet-50 networks. Since the DenseNet and ResNet have orthogonal performances in some instances, in the proposed model we have created an extra layer with convolutional neural network (CNN) blocks to join these two models together to establish superior performance as compared to the two individual networks. This strategy can be applied universally in cases where two competing networks are observed. We have tested the performance of our proposed network on two-class (pneumonia and healthy), three-class (COVID-19 positive, healthy, and pneumonia), as well as four-class (COVID-19 positive, healthy, TB, and pneumonia) classification problems. We have validated that our proposed network has been able to successfully classify these lung-diseases on our four datasets and this is one of our novel findings. In particular, the AUC-ROC are 99.60, 96.51, 93.70, 96.40% and the F1 values are 98.21, 87.29, 76.09, 83.17% on our Dataset X-Ray 1, 2, 3, and 4 (DXR1, DXR2, DXR3, DXR4), respectively.",2021,10.1016/j.compmedimag.2021.102008,cross-sectional,diagnosis,CXR,Lung
DenseCapsNet: Detection of COVID-19 from X-ray images using a capsule neural network,"At present, the global pandemic as it relates to novel coronavirus pneumonia is still a very difficult situation. Due to the recent outbreak of novel coronavirus pneumonia, novel chest X-ray (CXR) images that can be used for deep learning analysis are very rare. To solve this problem, we propose a deep learning framework that integrates a convolutional neural network and a capsule network. DenseCapsNet, a new deep learning framework, is formed by the fusion of a dense convolutional network (DenseNet) and the capsule neural network (CapsNet), leveraging their respective advantages and reducing the dependence of convolutional neural networks on a large amount of data. Using 750 CXR images of lungs of healthy patients as well as those of patients with other pneumonia and novel coronavirus pneumonia, the method can obtain an accuracy of 90.7% and an F1 score of 90.9%, and the sensitivity for detecting COVID-19 can reach 96%. These results show that the deep fusion neural network DenseCapsNet has good performance in novel coronavirus pneumonia CXR radiography detection.",2021,10.1016/j.compbiomed.2021.104399,cross-sectional,diagnosis,CXR,Lung
Densely connected attention network for diagnosing COVID-19 based on chest CT,"BACKGROUND: To fully enhance the feature extraction capabilities of deep learning models, so as to accurately diagnose coronavirus disease 2019 (COVID-19) based on chest CT images, a densely connected attention network (DenseANet) was constructed by utilizing the self-attention mechanism in deep learning. METHODS: During the construction of the DenseANet, we not only densely connected attention features within and between the feature extraction blocks with the same scale, but also densely connected attention features with different scales at the end of the deep model, thereby further enhancing the high-order features. In this way, as the depth of the deep model increases, the spatial attention features generated by different layers can be densely connected and gradually transferred to deeper layers. The DenseANet takes CT images of the lung fields segmented by an improved U-Net as inputs and outputs the probability of the patients suffering from COVID-19. RESULTS: Compared with exiting attention networks, DenseANet can maximize the utilization of self-attention features at different depths in the model. A five-fold cross-validation experiment was performed on a dataset containing 2993 CT scans of 2121 patients, and experiments showed that the DenseANet can effectively locate the lung lesions of patients infected with SARS-CoV-2, and distinguish COVID-19, common pneumonia and normal controls with an average of 96.06% Acc and 0.989 AUC. CONCLUSIONS: The DenseANet we proposed can generate strong attention features and achieve the best diagnosis results. In addition, the proposed method of densely connecting attention features can be easily extended to other advanced deep learning methods to improve their performance in related tasks.",2021,10.1016/j.compbiomed.2021.104857,cross-sectional,diagnosis,CT,Lung
Deploying Machine and Deep Learning Models for Efficient Data-Augmented Detection of COVID-19 Infections,"This generation faces existential threats because of the global assault of the novel Corona virus 2019 (i.e., COVID-19). With more than thirteen million infected and nearly 600000 fatalities in 188 countries/regions, COVID-19 is the worst calamity since the World War II. These misfortunes are traced to various reasons, including late detection of latent or asymptomatic carriers, migration, and inadequate isolation of infected people. This makes detection, containment, and mitigation global priorities to contain exposure via quarantine, lockdowns, work/stay at home, and social distancing that are focused on ""flattening the curve"". While medical and healthcare givers are at the frontline in the battle against COVID-19, it is a crusade for all of humanity. Meanwhile, machine and deep learning models have been revolutionary across numerous domains and applications whose potency have been exploited to birth numerous state-of-the-art technologies utilised in disease detection, diagnoses, and treatment. Despite these potentials, machine and, particularly, deep learning models are data sensitive, because their effectiveness depends on availability and reliability of data. The unavailability of such data hinders efforts of engineers and computer scientists to fully contribute to the ongoing assault against COVID-19. Faced with a calamity on one side and absence of reliable data on the other, this study presents two data-augmentation models to enhance learnability of the Convolutional Neural Network (CNN) and the Convolutional Long Short-Term Memory (ConvLSTM)-based deep learning models (DADLMs) and, by doing so, boost the accuracy of COVID-19 detection. Experimental results reveal improvement in terms of accuracy of detection, logarithmic loss, and testing time relative to DLMs devoid of such data augmentation. Furthermore, average increases of 4% to 11% in COVID-19 detection accuracy are reported in favour of the proposed data-augmented deep learning models relative to the machine learning techniques. Therefore, the proposed algorithm is effective in performing a rapid and consistent Corona virus diagnosis that is primarily aimed at assisting clinicians in making accurate identification of the virus.",2020,10.3390/v12070769,,,,
Design of lung nodules segmentation and recognition algorithm based on deep learning,"BACKGROUND: Accurate segmentation and recognition algorithm of lung nodules has great important value of reference for early diagnosis of lung cancer. An algorithm is proposed for 3D CT sequence images in this paper based on 3D Res U-Net segmentation network and 3D ResNet50 classification network. The common convolutional layers in encoding and decoding paths of U-Net are replaced by residual units while the loss function is changed to Dice loss after using cross entropy loss to accelerate network convergence. Since the lung nodules are small and rich in 3D information, the ResNet50 is improved by replacing the 2D convolutional layers with 3D convolutional layers and reducing the sizes of some convolution kernels, 3D ResNet50 network is obtained for the diagnosis of benign and malignant lung nodules. RESULTS: 3D Res U-Net was trained and tested on 1044 CT subcases in the LIDC-IDRI database. The segmentation result shows that the Dice coefficient of 3D Res U-Net is above 0.8 for the segmentation of lung nodules larger than 10 mm in diameter. 3D ResNet50 was trained and tested on 2960 lung nodules in the LIDC-IDRI database. The classification result shows that the diagnostic accuracy of 3D ResNet50 is 87.3% and AUC is 0.907. CONCLUSION: The 3D Res U-Net module improves segmentation performance significantly with the comparison of 3D U-Net model based on residual learning mechanism. 3D Res U-Net can identify small nodules more effectively and improve its segmentation accuracy for large nodules. Compared with the original network, the classification performance of 3D ResNet50 is significantly improved, especially for small benign nodules.",2021,10.1186/s12859-021-04234-0,cross-sectional,diagnosis,CT,Lung
Designing Futuristic Telemedicine Using Artificial Intelligence and Robotics in the COVID-19 Era,"Technological innovations such as artificial intelligence and robotics may be of potential use in telemedicine and in building capacity to respond to future pandemics beyond the current COVID-19 era. Our international consortium of interdisciplinary experts in clinical medicine, health policy, and telemedicine have identified gaps in uptake and implementation of telemedicine or telehealth across geographics and medical specialties. This paper discusses various artificial intelligence and robotics-assisted telemedicine or telehealth applications during COVID-19 and presents an alternative artificial intelligence assisted telemedicine framework to accelerate the rapid deployment of telemedicine and improve access to quality and cost-effective healthcare. We postulate that the artificial intelligence assisted telemedicine framework would be indispensable in creating futuristic and resilient health systems that can support communities amidst pandemics.",2020,10.3389/fpubh.2020.556789,,,,
Detailed identification of epidermal growth factor receptor mutations in lung adenocarcinoma: Combining radiomics with machine learning,"PURPOSE: To investigate the use of radiomics in the in-depth identification of epidermal growth factor receptor (EGFR) mutation status in patients with lung adenocarcinoma. METHODS: Computed tomography images of 438 patients with lung adenocarcinoma were collected in two different institutions, and 496 radiomic features were extracted. In the training set, lasso logistic regression was used to establish radiomic signatures. Combining radiomic index and clinical features, five machine learning methods, and a tenfold cross-validation strategy were used to establish combined models for EGFR(+) vs EGFR(-) , and 19Del vs L858R, groups. The predictive power of the models was then evaluated using an independent external validation cohort. RESULTS: In the EGFR(+) vs EGFR(-) and 19Del vs L858R groups, radiomic signatures consisting of 12 and 7 radiomic features were established, respectively; the area under the curves (AUCs) of the lasso logistic regression model on the validation set was 0.76 and 0.71, respectively. After inclusion of the clinical features, the maximum AUC of combined models on the validation set was 0.79 and 0.74, respectively. Logistic regression analysis showed good performance in the two groups, with AUCs of 0.79 and 0.71 on the validation set. Additionally, the AUC of combined models in the EGFR(+) vs EGFR(-) group was higher than that of the 19Del vs L858R group. CONCLUSIONS: Our study shows the potential of radiomics to predict EGFR mutation status. There are imaging phenotypic differences between EGFR(+) and EGFR(-) , and between 19Del and L858R; these can be used to allow patients with lung adenocarcinoma to choose more appropriate and personalized treatment options.",2020,10.1002/mp.14238,retrospective cohort,prognosis,CT,Lung
Detecting COVID-19 in Chest X-Ray Images via MCFF-Net,"COVID-19 is a respiratory disease caused by severe acute respiratory syndrome coronavirus (SARS-CoV-2). Due to the rapid spread of COVID-19 around the world, the number of COVID-19 cases continues to increase, and lots of countries are facing tremendous pressure on both public and medical resources. Although RT-PCR is the most widely used detection technology with COVID-19 detection, it still has some limitations, such as high cost, being time-consuming, and having low sensitivity. According to the characteristics of chest X-ray (CXR) images, we design the Parallel Channel Attention Feature Fusion Module (PCAF), as well as a new structure of convolutional neural network MCFF-Net proposed based on PCAF. In order to improve the recognition efficiency, the network adopts 3 classifiers: 1-FC, GAP-FC, and Conv1-GAP. The experimental results show that the overall accuracy of MCFF-Net66-Conv1-GAP model is 94.66% for 4-class classification. Simultaneously, the classification accuracy, precision, sensitivity, specificity, and F1-score of COVID-19 are 100%. MCFF-Net may not only assist clinicians in making appropriate decisions for COVID-19 diagnosis but also mitigate the lack of testing kits.",2021,10.1155/2021/3604900,cross-sectional,diagnosis,CXR,Lung
Detecting COVID-19 Pneumonia over Fuzzy Image Enhancement on Computed Tomography Images,"COVID-19 is the worst pandemic that has hit the globe in recent history, causing an increase in deaths. As a result of this pandemic, a number of research interests emerged in several fields such as medicine, health informatics, medical imaging, artificial intelligence and social sciences. Lung infection or pneumonia is the regular complication of COVID-19, and Reverse Transcription Polymerase Chain Reaction (RT-PCR) and computed tomography (CT) have played important roles to diagnose the disease. This research proposes an image enhancement method employing fuzzy expected value to improve the quality of the image for the detection of COVID-19 pneumonia. The principal objective of this research is to detect COVID-19 in patients using CT scan images collected from different sources, which include patients suffering from pneumonia and healthy people. The method is based on fuzzy histogram equalization and is organized with the improvement of the image contrast using fuzzy normalized histogram of the image. The effectiveness of the algorithm has been justified over several experiments on different features of CT images of lung for COVID-19 patients, like Ground-Glass Opacity (GGO), crazy paving, and consolidation. Experimental investigations indicate that among the 254 patients, 81.89% had features on both lungs; 9.5% on the left lung; and 10.24% on the right lung. The predominantly affected lobe was the right lower lobe (79.53%).",2022,10.1155/2022/1043299,,,,
Detecting Pneumonia using Convolutions and Dynamic Capsule Routing for Chest X-ray Images,"An entity's existence in an image can be depicted by the activity instantiation vector from a group of neurons (called capsule). Recently, multi-layered capsules, called CapsNet, have proven to be state-of-the-art for image classification tasks. This research utilizes the prowess of this algorithm to detect pneumonia from chest X-ray (CXR) images. Here, an entity in the CXR image can help determine if the patient (whose CXR is used) is suffering from pneumonia or not. A simple model of capsules (also known as Simple CapsNet) has provided results comparable to best Deep Learning models that had been used earlier. Subsequently, a combination of convolutions and capsules is used to obtain two models that outperform all models previously proposed. These models-Integration of convolutions with capsules (ICC) and Ensemble of convolutions with capsules (ECC)-detect pneumonia with a test accuracy of 95.33% and 95.90%, respectively. The latter model is studied in detail to obtain a variant called EnCC, where n = 3, 4, 8, 16. Here, the E4CC model works optimally and gives test accuracy of 96.36%. All these models had been trained, validated, and tested on 5857 images from Mendeley.",2020,10.3390/s20041068,cross-sectional,diagnosis,CXR,Lung
Detecting Racial/Ethnic Health Disparities Using Deep Learning From Frontal Chest Radiography,"PURPOSE: The aim of this study was to assess racial/ethnic and socioeconomic disparities in the difference between atherosclerotic vascular disease prevalence measured by a multitask convolutional neural network (CNN) deep learning model using frontal chest radiographs (CXRs) and the prevalence reflected by administrative hierarchical condition category codes in two cohorts of patients with coronavirus disease 2019 (COVID-19). METHODS: A CNN model, previously published, was trained to predict atherosclerotic disease from ambulatory frontal CXRs. The model was then validated on two cohorts of patients with COVID-19: 814 ambulatory patients from a suburban location (presenting from March 14, 2020, to October 24, 2020, the internal ambulatory cohort) and 485 hospitalized patients from an inner-city location (hospitalized from March 14, 2020, to August 12, 2020, the external hospitalized cohort). The CNN model predictions were validated against electronic health record administrative codes in both cohorts and assessed using the area under the receiver operating characteristic curve (AUC). The CXRs from the ambulatory cohort were also reviewed by two board-certified radiologists and compared with the CNN-predicted values for the same cohort to produce a receiver operating characteristic curve and the AUC. The atherosclerosis diagnosis discrepancy, Δ(vasc), referring to the difference between the predicted value and presence or absence of the vascular disease HCC categorical code, was calculated. Linear regression was performed to determine the association of Δ(vasc) with the covariates of age, sex, race/ethnicity, language preference, and social deprivation index. Logistic regression was used to look for an association between the presence of any hierarchical condition category codes with Δ(vasc) and other covariates. RESULTS: The CNN prediction for vascular disease from frontal CXRs in the ambulatory cohort had an AUC of 0.85 (95% confidence interval, 0.82-0.89) and in the hospitalized cohort had an AUC of 0.69 (95% confidence interval, 0.64-0.75) against the electronic health record data. In the ambulatory cohort, the consensus radiologists' reading had an AUC of 0.89 (95% confidence interval, 0.86-0.92) relative to the CNN. Multivariate linear regression of Δ(vasc) in the ambulatory cohort demonstrated significant negative associations with non-English-language preference (β = -0.083, P < .05) and Black or Hispanic race/ethnicity (β = -0.048, P < .05) and positive associations with age (β = 0.005, P < .001) and sex (β = 0.044, P < .05). For the hospitalized cohort, age was also significant (β = 0.003, P < .01), as was social deprivation index (β = 0.002, P < .05). The Δ(vasc) variable (odds ratio [OR], 0.34), Black or Hispanic race/ethnicity (OR, 1.58), non-English-language preference (OR, 1.74), and site (OR, 0.22) were independent predictors of having one or more hierarchical condition category codes (P < .01 for all) in the combined patient cohort. CONCLUSIONS: A CNN model was predictive of aortic atherosclerosis in two cohorts (one ambulatory and one hospitalized) with COVID-19. The discrepancy between the CNN model and the administrative code, Δ(vasc), was associated with language preference in the ambulatory cohort; in the hospitalized cohort, this discrepancy was associated with social deprivation index. The absence of administrative code(s) was associated with Δ(vasc) in the combined cohorts, suggesting that Δ(vasc) is an independent predictor of health disparities. This may suggest that biomarkers extracted from routine imaging studies and compared with electronic health record data could play a role in enhancing value-based health care for traditionally underserved or disadvantaged patients for whom barriers to care exist.",2022,10.1016/j.jacr.2021.09.010,retrospective cohort,prognosis,CXR,NA
Detection and analysis of COVID-19 in medical images using deep learning techniques,"The main purpose of this work is to investigate and compare several deep learning enhanced techniques applied to X-ray and CT-scan medical images for the detection of COVID-19. In this paper, we used four powerful pre-trained CNN models, VGG16, DenseNet121, ResNet50,and ResNet152, for the COVID-19 CT-scan binary classification task. The proposed Fast.AI ResNet framework was designed to find out the best architecture, pre-processing, and training parameters for the models largely automatically. The accuracy and F1-score were both above 96% in the diagnosis of COVID-19 using CT-scan images. In addition, we applied transfer learning techniques to overcome the insufficient data and to improve the training time. The binary and multi-class classification of X-ray images tasks were performed by utilizing enhanced VGG16 deep transfer learning architecture. High accuracy of 99% was achieved by enhanced VGG16 in the detection of X-ray images from COVID-19 and pneumonia. The accuracy and validity of the algorithms were assessed on X-ray and CT-scan well-known public datasets. The proposed methods have better results for COVID-19 diagnosis than other related in literature. In our opinion, our work can help virologists and radiologists to make a better and faster diagnosis in the struggle against the outbreak of COVID-19.",2021,10.1038/s41598-021-99015-3,cross-sectional,diagnosis,CXR,Lung
Detection and characterization of COVID-19 findings in chest CT: Feasibility and applicability of an AI-based software tool,"The COVID-19 pandemic has challenged institutions' diagnostic processes worldwide. The aim of this study was to assess the feasibility of an artificial intelligence (AI)-based software tool that automatically evaluates chest computed tomography for findings of suspected COVID-19.Two groups were retrospectively evaluated for COVID-19-associated ground glass opacities of the lungs (group A: real-time polymerase chain reaction positive COVID patients, n = 108; group B: asymptomatic pre-operative group, n = 88). The performance of an AI-based software assessment tool for detection of COVID-associated abnormalities was compared with human evaluation based on COVID-19 reporting and data system (CO-RADS) scores performed by 3 readers.All evaluated variables of the AI-based assessment showed significant differences between the 2 groups (P < .01). The inter-reader reliability of CO-RADS scoring was 0.87. The CO-RADS scores were substantially higher in group A (mean 4.28) than group B (mean 1.50). The difference between CO-RADS scoring and AI assessment was statistically significant for all variables but showed good correlation with the clinical context of the CO-RADS score. AI allowed to predict COVID positive cases with an accuracy of 0.94.The evaluated AI-based algorithm detects COVID-19-associated findings with high sensitivity and may support radiologic workflows during the pandemic.",2021,10.1097/md.0000000000027478,,,,
Detection and characterization of lung cancer using cell-free DNA fragmentomes,"Non-invasive approaches for cell-free DNA (cfDNA) assessment provide an opportunity for cancer detection and intervention. Here, we use a machine learning model for detecting tumor-derived cfDNA through genome-wide analyses of cfDNA fragmentation in a prospective study of 365 individuals at risk for lung cancer. We validate the cancer detection model using an independent cohort of 385 non-cancer individuals and 46 lung cancer patients. Combining fragmentation features, clinical risk factors, and CEA levels, followed by CT imaging, detected 94% of patients with cancer across stages and subtypes, including 91% of stage I/II and 96% of stage III/IV, at 80% specificity. Genome-wide fragmentation profiles across ~13,000 ASCL1 transcription factor binding sites distinguished individuals with small cell lung cancer from those with non-small cell lung cancer with high accuracy (AUC = 0.98). A higher fragmentation score represented an independent prognostic indicator of survival. This approach provides a facile avenue for non-invasive detection of lung cancer.",2021,10.1038/s41467-021-24994-w,,,,
Detection Methods of COVID-19,"Since being first detected in China, coronavirus disease 2019 (COVID-19) has spread rapidly across the world, triggering a global pandemic with no viable cure in sight. As a result, national responses have focused on the effective minimization of the spread. Border control measures and travel restrictions have been implemented in a number of countries to limit the import and export of the virus. The detection of COVID-19 is a key task for physicians. The erroneous results of early laboratory tests and their delays led researchers to focus on different options. Information obtained from computed tomography (CT) and radiological images is important for clinical diagnosis. Therefore, it is worth developing a rapid method of detection of viral diseases through the analysis of radiographic images. We propose a novel method of detection of COVID-19. The purpose is to provide clinical decision support to healthcare workers and researchers. The article is to support researchers working on early detection of COVID-19 as well as similar viral diseases.",2020,10.1177/2472630320962002,cross-sectional,diagnosis,CXR,Lung
Detection of coronavirus disease from X-ray images using deep learning and transfer learning algorithms,"OBJECTIVE: This study aims to employ the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of novel coronavirus (COVID-19) infected disease. METHOD: This study applied transfer learning method to develop deep learning models for detecting COVID-19 disease. Three existing state-of-the-art deep learning models namely, Inception ResNetV2, InceptionNetV3 and NASNetLarge, were selected and fine-tuned to automatically detect and diagnose COVID-19 disease using chest X-ray images. A dataset involving 850 images with the confirmed COVID-19 disease, 500 images of community-acquired (non-COVID-19) pneumonia cases and 915 normal chest X-ray images was used in this study. RESULTS: Among the three models, InceptionNetV3 yielded the best performance with accuracy levels of 98.63% and 99.02% with and without using data augmentation in model training, respectively. All the performed networks tend to overfitting (with high training accuracy) when data augmentation is not used, this is due to the limited amount of image data used for training and validation. CONCLUSION: This study demonstrated that a deep transfer learning is feasible to detect COVID-19 disease automatically from chest X-ray by training the learning model with chest X-ray images mixed with COVID-19 patients, other pneumonia affected patients and people with healthy lungs, which may help doctors more effectively make their clinical decisions. The study also gives an insight to how transfer learning was used to automatically detect the COVID-19 disease. In future studies, as the amount of available dataset increases, different convolution neutral network models could be designed to achieve the goal more efficiently.",2020,10.3233/xst-200720,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 from Chest CT Images Using CNN with MLP Hybrid Model,"COVID-19 when left undetected can lead to a hazardous infection spread, leading to an unfortunate loss of life. It's of utmost importance to diagnose COVID-19 in Infected patients at the earliest, to avoid further complications. RT-PCR, the gold standard method is routinely used for the diagnosis of COVID-19 infection. Yet, this method comes along with few limitations such as its time-consuming nature, a scarcity of trained manpower, sophisticated laboratory equipment and the possibility of false positive and negative results. Physicians and global health care centers use CT scan as an alternate for the diagnosis of COVID-19. But this process of detection too, might demand more manual work, effort and time. Thus, automating the detection of COVID-19 using an intelligent system has been a recent research topic, in the view of pandemic. This will also help in saving the physician's time for carrying out further treatment. In this paper, a hybrid learning model has been proposed to identify the COVID-19 infection using CT scan images. The Convolutional Neural Network (CNN) was used for feature extraction and Multilayer Perceptron was used for classification. This hybrid learning model's results were also compared with traditional CNN and MLP models in terms of Accuracy, F1-Score, Precision and Recall. This Hybrid CNN-MLP model showed an Accuracy of 94.89% when compared with CNN and MLP giving 86.95% and 80.77% respectively.",2021,10.3233/shti210617,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 from Chest X-Ray Images Using Convolutional Neural Networks,"The detection of severe acute respiratory syndrome coronavirus 2 (SARS CoV-2), which is responsible for coronavirus disease 2019 (COVID-19), using chest X-ray images has life-saving importance for both patients and doctors. In addition, in countries that are unable to purchase laboratory kits for testing, this becomes even more vital. In this study, we aimed to present the use of deep learning for the high-accuracy detection of COVID-19 using chest X-ray images. Publicly available X-ray images (1583 healthy, 4292 pneumonia, and 225 confirmed COVID-19) were used in the experiments, which involved the training of deep learning and machine learning classifiers. Thirty-eight experiments were performed using convolutional neural networks, 10 experiments were performed using five machine learning models, and 14 experiments were performed using the state-of-the-art pre-trained networks for transfer learning. Images and statistical data were considered separately in the experiments to evaluate the performances of models, and eightfold cross-validation was used. A mean sensitivity of 93.84%, mean specificity of 99.18%, mean accuracy of 98.50%, and mean receiver operating characteristics-area under the curve scores of 96.51% are achieved. A convolutional neural network without pre-processing and with minimized layers is capable of detecting COVID-19 in a limited number of, and in imbalanced, chest X-ray images.",2020,10.1177/2472630320958376,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 from Chest X-ray Images Using Deep Convolutional Neural Networks,"The COVID-19 global pandemic has wreaked havoc on every aspect of our lives. More specifically, healthcare systems were greatly stretched to their limits and beyond. Advances in artificial intelligence have enabled the implementation of sophisticated applications that can meet clinical accuracy requirements. In this study, customized and pre-trained deep learning models based on convolutional neural networks were used to detect pneumonia caused by COVID-19 respiratory complications. Chest X-ray images from 368 confirmed COVID-19 patients were collected locally. In addition, data from three publicly available datasets were used. The performance was evaluated in four ways. First, the public dataset was used for training and testing. Second, data from the local and public sources were combined and used to train and test the models. Third, the public dataset was used to train the model and the local data were used for testing only. This approach adds greater credibility to the detection models and tests their ability to generalize to new data without overfitting the model to specific samples. Fourth, the combined data were used for training and the local dataset was used for testing. The results show a high detection accuracy of 98.7% with the combined dataset, and most models handled new data with an insignificant drop in accuracy.",2021,10.3390/s21175940,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 from CT Lung Scans Using Transfer Learning,"This paper aims to investigate the use of transfer learning architectures in the detection of COVID-19 from CT lung scans. The study evaluates the performances of various transfer learning architectures, as well as the effects of the standard Histogram Equalization and Contrast Limited Adaptive Histogram Equalization. The findings of this study suggest that transfer learning-based frameworks are an alternative to the contemporary methods used to detect the presence of the virus in patients. The highest performing model, the VGG-19 implemented with the Contrast Limited Adaptive Histogram Equalization, on a SARS-CoV-2 dataset, achieved an accuracy and recall of 95.75% and 97.13%, respectively.",2021,10.1155/2021/5527923,cross-sectional,diagnosis,CT,Lung
Detection of COVID-19 in Chest X-ray Images: A Big Data Enabled Deep Learning Approach,"Coronavirus disease (COVID-19) spreads from one person to another rapidly. A recently discovered coronavirus causes it. COVID-19 has proven to be challenging to detect and cure at an early stage all over the world. Patients showing symptoms of COVID-19 are resulting in hospitals becoming overcrowded, which is becoming a significant challenge. Deep learning's contribution to big data medical research has been enormously beneficial, offering new avenues and possibilities for illness diagnosis techniques. To counteract the COVID-19 outbreak, researchers must create a classifier distinguishing between positive and negative corona-positive X-ray pictures. In this paper, the Apache Spark system has been utilized as an extensive data framework and applied a Deep Transfer Learning (DTL) method using Convolutional Neural Network (CNN) three architectures -InceptionV3, ResNet50, and VGG19-on COVID-19 chest X-ray images. The three models are evaluated in two classes, COVID-19 and normal X-ray images, with 100 percent accuracy. But in COVID/Normal/pneumonia, detection accuracy was 97 percent for the inceptionV3 model, 98.55 percent for the ResNet50 Model, and 98.55 percent for the VGG19 model, respectively.",2021,10.3390/ijerph181910147,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 Using Deep Learning Algorithms on Chest Radiographs,"PURPOSE: To evaluate the performance of a deep learning (DL) algorithm for the detection of COVID-19 on chest radiographs (CXR). MATERIALS AND METHODS: In this retrospective study, a DL model was trained on 112,120 CXR images with 14 labeled classifiers (ChestX-ray14) and fine-tuned using initial CXR on hospital admission of 509 patients, who had undergone COVID-19 reverse transcriptase-polymerase chain reaction (RT-PCR). The test set consisted of a CXR on presentation of 248 individuals suspected of COVID-19 pneumonia between February 16 and March 3, 2020 from 4 centers (72 RT-PCR positives and 176 RT-PCR negatives). The CXR were independently reviewed by 3 radiologists and using the DL algorithm. Diagnostic performance was compared with radiologists' performance and was assessed by area under the receiver operating characteristics (AUC). RESULTS: The median age of the subjects in the test set was 61 (interquartile range: 39 to 79) years (51% male). The DL algorithm achieved an AUC of 0.81, sensitivity of 0.85, and specificity of 0.72 in detecting COVID-19 using RT-PCR as the reference standard. On subgroup analyses, the model achieved an AUC of 0.79, sensitivity of 0.80, and specificity of 0.74 in detecting COVID-19 in patients presented with fever or respiratory systems and an AUC of 0.87, sensitivity of 0.85, and specificity of 0.81 in distinguishing COVID-19 from other forms of pneumonia. The algorithm significantly outperforms human readers (P<0.001 using DeLong test) with higher sensitivity (P=0.01 using McNemar test). CONCLUSIONS: A DL algorithm (COV19NET) for the detection of COVID-19 on chest radiographs can potentially be an effective tool in triaging patients, particularly in resource-stretched health-care systems.",2020,10.1097/rti.0000000000000559,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 Using Transfer Learning and Grad-CAM Visualization on Indigenously Collected X-ray Dataset,"The COVID-19 outbreak began in December 2019 and has dreadfully affected our lives since then. More than three million lives have been engulfed by this newest member of the corona virus family. With the emergence of continuously mutating variants of this virus, it is still indispensable to successfully diagnose the virus at early stages. Although the primary technique for the diagnosis is the PCR test, the non-contact methods utilizing the chest radiographs and CT scans are always preferred. Artificial intelligence, in this regard, plays an essential role in the early and accurate detection of COVID-19 using pulmonary images. In this research, a transfer learning technique with fine tuning was utilized for the detection and classification of COVID-19. Four pre-trained models i.e., VGG16, DenseNet-121, ResNet-50, and MobileNet were used. The aforementioned deep neural networks were trained using the dataset (available on Kaggle) of 7232 (COVID-19 and normal) chest X-ray images. An indigenous dataset of 450 chest X-ray images of Pakistani patients was collected and used for testing and prediction purposes. Various important parameters, e.g., recall, specificity, F1-score, precision, loss graphs, and confusion matrices were calculated to validate the accuracy of the models. The achieved accuracies of VGG16, ResNet-50, DenseNet-121, and MobileNet are 83.27%, 92.48%, 96.49%, and 96.48%, respectively. In order to display feature maps that depict the decomposition process of an input image into various filters, a visualization of the intermediate activations is performed. Finally, the Grad-CAM technique was applied to create class-specific heatmap images in order to highlight the features extracted in the X-ray images. Various optimizers were used for error minimization purposes. DenseNet-121 outperformed the other three models in terms of both accuracy and prediction.",2021,10.3390/s21175813,cross-sectional,diagnosis,CXR,Lung
Detection of COVID-19 With CT Images Using Hybrid Complex Shearlet Scattering Networks,"With the ongoing worldwide coronavirus disease 2019 (COVID-19) pandemic, it is desirable to develop effective algorithms to automatically detect COVID-19 with chest computed tomography (CT) images. Recently, a considerable number of methods based on deep learning have indeed been proposed. However, training an accurate deep learning model requires a large-scale chest CT dataset, which is hard to collect due to the high contagiousness of COVID-19. To achieve improved detection performance, this paper proposes a hybrid framework that fuses the complex shearlet scattering transform (CSST) and a suitable convolutional neural network into a single model. The introduced CSST cascades complex shearlet transforms with modulus nonlinearities and low-pass filter convolutions to compute a sparse and locally invariant image representation. The features computed from the input chest CT images are discriminative for COVID-19 detection. Furthermore, a wide residual network with a redesigned residual block (WR2N) is developed to learn more granular multiscale representations by applying it to scattering features. The combination of model-based CSST and data-driven WR2N leads to a more convenient neural network for image representation, where the idea is to learn only the image parts that the CSST cannot handle instead of all parts. Experiments on two public datasets demonstrate the superiority of our method. We can obtain more accurate results than several state-of-the-art COVID-19 classification methods in terms of measures such as accuracy, the F1-score, and the area under the receiver operating characteristic curve.",2022,10.1109/jbhi.2021.3132157,cross-sectional,diagnosis,CT,Lung
Detection of Lung Cancer Lymph Node Metastases from Whole-Slide Histopathologic Images Using a Two-Step Deep Learning Approach,"The application of deep learning for the detection of lymph node metastases on histologic slides has attracted worldwide attention due to its potentially important role in patient treatment and prognosis. Despite this attention, false-positive predictions remain problematic, particularly in the case of reactive lymphoid follicles. In this study, a novel two-step deep learning algorithm was developed to address the issue of false-positive prediction while maintaining accurate cancer detection. Three-hundred and forty-nine whole-slide lung cancer lymph node images, including 233 slides for algorithm training, 10 slides for validation, and 106 slides for evaluation, were collected. In the first step, a deep learning algorithm was used to eliminate frequently misclassified noncancerous regions (lymphoid follicles). In the second step, a deep learning classifier was developed to detect cancer cells. Using this two-step approach, errors were reduced by 36.4% on average and up to 89% in slides with reactive lymphoid follicles. Furthermore, 100% sensitivity was reached in cases of macrometastases, micrometastases, and isolated tumor cells. To reduce the small number of remaining false positives, a receiver-operating characteristic curve was created using foci size thresholds of 0.6 mm and 0.7 mm, achieving sensitivity and specificity of 79.6% and 96.5%, and 75.5% and 98.2%, respectively. A two-step approach can be used to detect lung cancer metastases in lymph node tissue effectively and with few false positives.",2019,10.1016/j.ajpath.2019.08.014,,,,
Detection of Lung Cancer on Computed Tomography Using Artificial Intelligence Applications Developed by Deep Learning Methods and the Contribution of Deep Learning to the Classification of Lung Carcinoma,"BACKGROUND: Every year, lung cancer contributes to a high percentage deaths in the world. Early detection of lung cancer is important for its effective treatment, and non-invasive rapid methods are usually used for diagnosis. INTRODUCTION: In this study, we aimed to detect lung cancer using deep learning methods and determine the contribution of deep learning to the classification of lung carcinoma using a convolutional neural network (CNN). METHODS: A total of 301 patients diagnosed with lung carcinoma pathologies in our hospital were included in the study. In the thorax, Computed Tomography (CT) was performed for diagnostic purposes prior to the treatment. After tagging the section images, tumor detection, small and non-small cell lung carcinoma differentiation, adenocarcinoma-squamous cell lung carcinoma differentiation, and adenocarcinoma-squamous cell-small cell lung carcinoma differentiation were sequentially performed using deep CNN methods. RESULTS: In total, 301 lung carcinoma images were used to detect tumors, and the model obtained with the deep CNN system exhibited 0.93 sensitivity, 0.82 precision, and 0.87 F1 score in detecting lung carcinoma. In the differentiation of small cell-non-small cell lung carcinoma, the sensitivity, precision and F1 score of the CNN model at the test stage were 0.92, 0.65, and 0.76, respectively. In the adenocarcinoma-squamous cancer differentiation, the sensitivity, precision, and F1 score were 0.95, 0.80, and 0.86, respectively. The patients were finally grouped as small cell lung carcinoma, adenocarcinoma, and squamous cell lung carcinoma, and the CNN model was used to determine whether it could differentiate these groups. The sensitivity, specificity, and F1 score of this model were 0.90, 0.44, and 0.59, respectively, in this differentiation. CONCLUSION: In this study, we successfully detected tumors and differentiated between adenocarcinoma- squamous cell carcinoma groups with the deep learning method using the CNN model. Due to their non-invasive nature and the success of the deep learning methods, they should be integrated into radiology to diagnose lung carcinoma.",2021,10.2174/1573405617666210204210500,cross-sectional,diagnosis,CT,Lung
Detection of Lung Nodules in Micro-CT Imaging Using Deep Learning,"We are developing imaging methods for a co-clinical trial investigating synergy between immunotherapy and radiotherapy. We perform longitudinal micro-computed tomography (micro-CT) of mice to detect lung metastasis after treatment. This work explores deep learning (DL) as a fast approach for automated lung nodule detection. We used data from control mice both with and without primary lung tumors. To augment the number of training sets, we have simulated data using real augmented tumors inserted into micro-CT scans. We employed a convolutional neural network (CNN), trained with four competing types of training data: (1) simulated only, (2) real only, (3) simulated and real, and (4) pretraining on simulated followed with real data. We evaluated our model performance using precision and recall curves, as well as receiver operating curves (ROC) and their area under the curve (AUC). The AUC appears to be almost identical (0.76-0.77) for all four cases. However, the combination of real and synthetic data was shown to improve precision by 8%. Smaller tumors have lower rates of detection than larger ones, with networks trained on real data showing better performance. Our work suggests that DL is a promising approach for fast and relatively accurate detection of lung tumors in mice.",2021,10.3390/tomography7030032,,,,
Detection of pulmonary ground-glass opacity based on deep learning computer artificial intelligence,"BACKGROUND: A deep learning computer artificial intelligence system is helpful for early identification of ground glass opacities (GGOs). METHODS: Images from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) database were used in AlexNet and GoogLeNet to detect pulmonary nodules, and 221 GGO images provided by Xinhua Hospital were used in ResNet50 for detecting GGOs. We used computed tomography image radial reorganization to create the input image of the three-dimensional features, and used the extracted features for deep learning, network training, testing, and analysis. RESULTS: In the final evaluation results, we found that the accuracy of identification of lung nodule could reach 88.0%, with an F-score of 0.891. In terms of performance and accuracy, our method was better than the existing solutions. The GGO nodule classification achieved the best F-score of 0.87805. We propose a preprocessing method of red, green, and blue (RGB) superposition in the region of interest to effectively increase the differentiation between nodules and normal tissues, and that is the innovation of our research. CONCLUSIONS: The method of deep learning proposed in this study is more sensitive than other systems in recent years, and the average false positive is lower than that of others.",2019,10.1186/s12938-019-0627-4,cross-sectional,diagnosis,CT,Lung
Detection of pulmonary nodules based on a multiscale feature 3D U-Net convolutional neural network of transfer learning,"A new computer-aided detection scheme is proposed, the 3D U-Net convolutional neural network, based on multiscale features of transfer learning to automatically detect pulmonary nodules from the thoracic region containing background and noise. The test results can be used as reference information for doctors to assist in the detection of early lung cancer. The proposed scheme is composed of three major steps: First, the pulmonary parenchyma area is segmented by various methods. Then, the 3D U-Net convolutional neural network model with a multiscale feature structure is built. The network model structure is subsequently fine-tuned by the transfer learning method based on weight, and the optimal parameters are selected in the network model. Finally, datasets are extracted to train the fine-tuned 3D U-Net network model to detect pulmonary nodules. The five-fold cross-validation method is used to obtain the experimental results for the LUNA16 and TIANCHI17 datasets. The experimental results show that the scheme not only has obvious advantages in the detection of medium and large-sized nodules but also has an accuracy rate of more than 70% for the detection of small-sized nodules. The scheme provides automatic and accurate detection of pulmonary nodules that reduces the overfitting rate and training time and improves the efficiency of the algorithm. It can assist doctors in the diagnosis of lung cancer and can be extended to other medical image detection and recognition fields.",2020,10.1371/journal.pone.0235672,cross-sectional,diagnosis,CT,Lung
Detection of pulmonary nodules in CT images based on fuzzy integrated active contour model and hybrid parametric mixture model,"The segmentation and detection of various types of nodules in a Computer-aided detection (CAD) system present various challenges, especially when (1) the nodule is connected to a vessel and they have very similar intensities; (2) the nodule with ground-glass opacity (GGO) characteristic possesses typical weak edges and intensity inhomogeneity, and hence it is difficult to define the boundaries. Traditional segmentation methods may cause problems of boundary leakage and ""weak"" local minima. This paper deals with the above mentioned problems. An improved detection method which combines a fuzzy integrated active contour model (FIACM)-based segmentation method, a segmentation refinement method based on Parametric Mixture Model (PMM) of juxta-vascular nodules, and a knowledge-based C-SVM (Cost-sensitive Support Vector Machines) classifier, is proposed for detecting various types of pulmonary nodules in computerized tomography (CT) images. Our approach has several novel aspects: (1) In the proposed FIACM model, edge and local region information is incorporated. The fuzzy energy is used as the motivation power for the evolution of the active contour. (2) A hybrid PMM Model of juxta-vascular nodules combining appearance and geometric information is constructed for segmentation refinement of juxta-vascular nodules. Experimental results of detection for pulmonary nodules show desirable performances of the proposed method.",2013,10.1155/2013/515386,cross-sectional,diagnosis,CT,Lung
Detection of the location of pneumothorax in chest X-rays using small artificial neural networks and a simple training process,"The purpose of this study was to evaluate the diagnostic performance achieved by using fully-connected small artificial neural networks (ANNs) and a simple training process, the Kim-Monte Carlo algorithm, to detect the location of pneumothorax in chest X-rays. A total of 1,000 chest X-ray images with pneumothorax were taken randomly from NIH (the National Institutes of Health) public image database and used as the training and test sets. Each X-ray image with pneumothorax was divided into 49 boxes for pneumothorax localization. For each of the boxes in the chest X-ray images contained in the test set, the area under the receiver operating characteristic (ROC) curve (AUC) was 0.882, and the sensitivity and specificity were 80.6% and 83.0%, respectively. In addition, a common currently used deep-learning method for image recognition, the convolution neural network (CNN), was also applied to the same dataset for comparison purposes. The performance of the fully-connected small ANN was better than that of the CNN. Regarding the diagnostic performances of the CNN with different activation functions, the CNN with a sigmoid activation function for fully-connected hidden nodes was better than the CNN with the rectified linear unit (RELU) activation function. This study showed that our approach can accurately detect the location of pneumothorax in chest X-rays, significantly reduce the time delay incurred when diagnosing urgent diseases such as pneumothorax, and increase the effectiveness of clinical practice and patient care.",2021,10.1038/s41598-021-92523-2,cross-sectional,diagnosis,CXR,Lung
Determination of disease severity in COVID-19 patients using deep learning in chest X-ray images,"PURPOSE: Chest X-ray plays a key role in diagnosis and management of COVID-19 patients and imaging features associated with clinical elements may assist with the development or validation of automated image analysis tools. We aimed to identify associations between clinical and radiographic features as well as to assess the feasibility of deep learning applied to chest X-rays in the setting of an acute COVID-19 outbreak. METHODS: A retrospective study of X-rays, clinical, and laboratory data was performed from 48 SARS-CoV-2 RT-PCR positive patients (age 60±17 years, 15 women) between February 22 and March 6, 2020 from a tertiary care hospital in Milan, Italy. Sixty-five chest X-rays were reviewed by two radiologists for alveolar and interstitial opacities and classified by severity on a scale from 0 to 3. Clinical factors (age, symptoms, comorbidities) were investigated for association with opacity severity and also with placement of central line or endotracheal tube. Deep learning models were then trained for two tasks: lung segmentation and opacity detection. Imaging characteristics were compared to clinical datapoints using the unpaired student's t-test or Mann-Whitney U test. Cohen's kappa analysis was used to evaluate the concordance of deep learning to conventional radiologist interpretation. RESULTS: Fifty-six percent of patients presented with alveolar opacities, 73% had interstitial opacities, and 23% had normal X-rays. The presence of alveolar or interstitial opacities was statistically correlated with age (P = 0.008) and comorbidities (P = 0.005). The extent of alveolar or interstitial opacities on baseline X-ray was significantly associated with the presence of endotracheal tube (P = 0.0008 and P = 0.049) or central line (P = 0.003 and P = 0.007). In comparison to human interpretation, the deep learning model achieved a kappa concordance of 0.51 for alveolar opacities and 0.71 for interstitial opacities. CONCLUSION: Chest X-ray analysis in an acute COVID-19 outbreak showed that the severity of opacities was associated with advanced age, comorbidities, as well as acuity of care. Artificial intelligence tools based upon deep learning of COVID-19 chest X-rays are feasible in the acute outbreak setting.",2021,10.5152/dir.2020.20205,retrospective cohort,prognosis,CXR,Lung
Determining the invasiveness of ground-glass nodules using a 3D multi-task network,"OBJECTIVES: The aim of this study was to determine the invasiveness of ground-glass nodules (GGNs) using a 3D multi-task deep learning network. METHODS: We propose a novel architecture based on 3D multi-task learning to determine the invasiveness of GGNs. In total, 770 patients with 909 GGNs who underwent lung CT scans were enrolled. The patients were divided into the training (n = 626) and test sets (n = 144). In the test set, invasiveness was classified using deep learning into three categories: atypical adenomatous hyperplasia (AAH) and adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive pulmonary adenocarcinoma (IA). Furthermore, binary classifications (AAH/AIS/MIA vs. IA) were made by two thoracic radiologists and compared with the deep learning results. RESULTS: In the three-category classification task, the sensitivity, specificity, and accuracy were 65.41%, 82.21%, and 64.9%, respectively. In the binary classification task, the sensitivity, specificity, accuracy, and area under the ROC curve (AUC) values were 69.57%, 95.24%, 87.42%, and 0.89, respectively. In the visual assessment of GGN invasiveness of binary classification by the two thoracic radiologists, the sensitivity, specificity, and accuracy of the senior and junior radiologists were 58.93%, 90.51%, and 81.35% and 76.79%, 55.47%, and 61.66%, respectively. CONCLUSIONS: The proposed multi-task deep learning model achieved good classification results in determining the invasiveness of GGNs. This model may help to select patients with invasive lesions who need surgery and the proper surgical methods. KEY POINTS: • The proposed multi-task model has achieved good classification results for the invasiveness of GGNs. • The proposed network includes a classification and segmentation branch to learn global and regional features, respectively. • The multi-task model could assist doctors in selecting patients with invasive lesions who need surgery and choosing appropriate surgical methods.",2021,10.1007/s00330-021-07794-0,retrospective cohort,prognosis,CT,Lung
"Determining Top Fully Connected Layer's Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19","Deep convolutional neural network (CNN)-assisted classification of images is one of the most discussed topics in recent years. Continuously innovation of neural network architectures is making it more correct and efficient every day. But training a neural network from scratch is very time-consuming and requires a lot of sophisticated computational equipment and power. So, using some pre-trained neural network as feature extractor for any image classification task or ""transfer learning"" is a very popular approach that saves time and computational power for practical use of CNNs. In this paper, an efficient way of building full model from any pre-trained model with high accuracy and low memory is proposed using knowledge distillation. Using the distilled knowledge of the last layer of pre-trained networks passes through fully connected layers with different hidden layers, followed by Softmax layer. The accuracies of student networks are mildly lesser than the whole models, but accuracy of student models clearly indicates the accuracy of the real network. In this way, the best number of hidden layers for dense layer for that pre-trained network with best accuracy and no-overfitting can be found with less time. Here, VGG16 and VGG19 (pre-trained upon ""ImageNet"" dataset) is tested upon chest X-rays (pneumonia and COVID-19). For finding the best total number of hidden layers, it saves nearly 44 min for VGG19 and 36 min and 37 s for VGG16 feature extractor.",2021,10.1007/s10278-021-00518-2,cross-sectional,diagnosis,CXR,Lung
Development and application of an elastic net logistic regression model to investigate the impact of cardiac substructure dose on radiation-induced pericardial effusion in patients with NSCLC,"BACKGROUND: Typically, cardiac substructures are neither delineated nor analyzed during radiation treatment planning. Therefore, we developed a novel machine learning model to evaluate the impact of cardiac substructure dose for predicting radiation-induced pericardial effusion (PCE). MATERIALS AND METHODS: One-hundred and forty-one stage III NSCLC patients, who received radiation therapy in a prospective clinical trial, were included in this analysis. The impact of dose-volume histogram (DVH) metrics (mean and max dose, V5Gy[%]-V70Gy[%]) for the whole heart, left and right atrium, and left and right ventricle, on pericardial effusion toxicity (≥grade 2, CTCAE v4.0 grading) were examined. Elastic net logistic regression, using repeat cross-validation (n = 100 iterations, 75%/25% training/test set data split), was conducted with cardiac-based DVH metrics as covariates. The following model types were constructed and analyzed: (i) standard model type, which only included whole-heart DVH metrics; and (ii) a model type trained with both whole-heart and substructure DVH metrics. Model performance was analyzed on the test set using area under the curve (AUC), accuracy, calibration slope and calibration intercept. A final fitted model, based on the optimal model type, was developed from the entire study population for future comparisons. RESULTS: Grade 2 PCE incidence was 49.6% (n = 70). Models using whole heart and substructure dose had the highest performance (median values: AUC = 0.820; calibration slope/intercept = 1.356/-0.235; accuracy = 0.743) and outperformed the standard whole-heart only model type (median values: AUC = 0.799; calibration slope/intercept = 2.456/-0.729; accuracy = 0.713). The final fitted elastic net model showed high performance in predicting PCE (median values: AUC = 0.879; calibration slope/intercept = 1.352/-0.174; accuracy = 0.801). CONCLUSIONS: We developed and evaluated elastic net regression toxicity models of radiation-induced PCE. We found the model type that included cardiac substructure dose had superior predictive performance. A final toxicity model that included cardiac substructure dose metrics was developed and reported for comparison with external datasets.",2020,10.1080/0284186x.2020.1794034,,,,
Development and clinical application of deep learning model for lung nodules screening on CT images,"Lung cancer screening based on low-dose CT (LDCT) has now been widely applied because of its effectiveness and ease of performance. Radiologists who evaluate a large LDCT screening images face enormous challenges, including mechanical repetition and boring work, the easy omission of small nodules, lack of consistent criteria, etc. It requires an efficient method for helping radiologists improve nodule detection accuracy with efficiency and cost-effectiveness. Many novel deep neural network-based systems have demonstrated the potential for use in the proposed technique to detect lung nodules. However, the effectiveness of clinical practice has not been fully recognized or proven. Therefore, the aim of this study to develop and assess a deep learning (DL) algorithm in identifying pulmonary nodules (PNs) on LDCT and investigate the prevalence of the PNs in China. Radiologists and algorithm performance were assessed using the FROC score, ROC-AUC, and average time consumption. Agreement between the reference standard and the DL algorithm in detecting positive nodules was assessed per-study by Bland-Altman analysis. The Lung Nodule Analysis (LUNA) public database was used as the external test. The prevalence of NCPNs was investigated as well as other detailed information regarding the number of pulmonary nodules, their location, and characteristics, as interpreted by two radiologists.",2020,10.1038/s41598-020-70629-3,cross-sectional,diagnosis,CT,Lung
"Development and clinical implementation of tailored image analysis tools for COVID-19 in the midst of the pandemic: The synergetic effect of an open, clinically embedded software development platform and machine learning","PURPOSE: During the emerging COVID-19 pandemic, radiology departments faced a substantial increase in chest CT admissions coupled with the novel demand for quantification of pulmonary opacities. This article describes how our clinic implemented an automated software solution for this purpose into an established software platform in 10 days. The underlying hypothesis was that modern academic centers in radiology are capable of developing and implementing such tools by their own efforts and fast enough to meet the rapidly increasing clinical needs in the wake of a pandemic. METHOD: Deep convolutional neural network algorithms for lung segmentation and opacity quantification on chest CTs were trained using semi-automatically and manually created ground-truth (N(total) = 172). The performance of the in-house method was compared to an externally developed algorithm on a separate test subset (N = 66). RESULTS: The final algorithm was available at day 10 and achieved human-like performance (Dice coefficient = 0.97). For opacity quantification, a slight underestimation was seen both for the in-house (1.8 %) and for the external algorithm (0.9 %). In contrast to the external reference, the underestimation for the in-house algorithm showed no dependency on total opacity load, making it more suitable for follow-up. CONCLUSIONS: The combination of machine learning and a clinically embedded software development platform enabled time-efficient development, instant deployment, and rapid adoption in clinical routine. The algorithm for fully automated lung segmentation and opacity quantification that we developed in the midst of the COVID-19 pandemic was ready for clinical use within just 10 days and achieved human-level performance even in complex cases.",2020,10.1016/j.ejrad.2020.109233,cross-sectional,diagnosis,CT,Lung
Development and Cost Analysis of a Lung Nodule Management Strategy Combining Artificial Intelligence and Lung-RADS for Baseline Lung Cancer Screening,"OBJECTIVES: To develop a lung nodule management strategy combining the Lung CT Screening Reporting and Data System (Lung-RADS) with an artificial intelligence (AI) malignancy risk score and determine its impact on follow-up investigations and associated costs in a baseline lung cancer screening population. MATERIALS AND METHODS: Secondary analysis was undertaken of a data set consisting of AI malignancy risk scores and Lung-RADS classifications from six radiologists for 192 baseline low-dose CT studies. Low-dose CT studies were weighted to model a representative cohort of 3,197 baseline screening patients. An AI risk score threshold was defined to match average sensitivity of six radiologists applying Lung-RADS. Cases initially Lung-RADS category 1 or 2 with a high AI risk score were upgraded to category 3, and cases initially category 3 or higher with a low AI risk score were downgraded to category 2. Follow-up investigations resulting from Lung-RADS and the AI-informed management strategy were determined. Investigation costs were based on the 2019 US Medicare Physician Fee Schedule. RESULTS: The AI-informed management strategy achieved sensitivity and specificity of 91% and 96%, respectively. Average sensitivity and specificity of six radiologists using Lung-RADS only was 91% and 66%, respectively. Using the AI-informed management strategy, 41 (0.2%) category 1 or 2 classifications were upgraded to category 3, and 5,750 (30%) category 3 or higher classifications were downgraded to category 2. Minimum net cost savings using the AI-informed management strategy was estimated to be $72 per patient screened. CONCLUSION: Using an AI risk score combined with Lung-RADS at baseline lung cancer screening may result in fewer follow-up investigations and substantial cost savings.",2021,10.1016/j.jacr.2020.11.014,,,,
Development and evaluation of an artificial intelligence system for COVID-19 diagnosis,"Early detection of COVID-19 based on chest CT enables timely treatment of patients and helps control the spread of the disease. We proposed an artificial intelligence (AI) system for rapid COVID-19 detection and performed extensive statistical analysis of CTs of COVID-19 based on the AI system. We developed and evaluated our system on a large dataset with more than 10 thousand CT volumes from COVID-19, influenza-A/B, non-viral community acquired pneumonia (CAP) and non-pneumonia subjects. In such a difficult multi-class diagnosis task, our deep convolutional neural network-based system is able to achieve an area under the receiver operating characteristic curve (AUC) of 97.81% for multi-way classification on test cohort of 3,199 scans, AUC of 92.99% and 93.25% on two publicly available datasets, CC-CCII and MosMedData respectively. In a reader study involving five radiologists, the AI system outperforms all of radiologists in more challenging tasks at a speed of two orders of magnitude above them. Diagnosis performance of chest x-ray (CXR) is compared to that of CT. Detailed interpretation of deep network is also performed to relate system outputs with CT presentations. The code is available at https://github.com/ChenWWWeixiang/diagnosis_covid19 .",2020,10.1038/s41467-020-18685-1,cross-sectional,diagnosis,CT,Lung
Development and prospective validation of COVID-19 chest X-ray screening model for patients attending emergency departments,"Chest X-rays (CXRs) are the first-line investigation in patients presenting to emergency departments (EDs) with dyspnoea and are a valuable adjunct to clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to facilitate rapid triage of CXRs for further patient testing and/or isolation. In this work we develop an AI algorithm, CovIx, to differentiate normal, abnormal, non-COVID-19 pneumonia, and COVID-19 CXRs using a multicentre cohort of 293,143 CXRs. The algorithm is prospectively validated in 3289 CXRs acquired from patients presenting to ED with symptoms of COVID-19 across four sites in NHS Greater Glasgow and Clyde. CovIx achieves area under receiver operating characteristic curve for COVID-19 of 0.86, with sensitivity and F1-score up to 0.83 and 0.71 respectively, and performs on-par with four board-certified radiologists. AI-based algorithms can identify CXRs with COVID-19 associated pneumonia, as well as distinguish non-COVID pneumonias in symptomatic patients presenting to ED. Pre-trained models and inference scripts are freely available at https://github.com/beringresearch/bravecx-covid .",2021,10.1038/s41598-021-99986-3,retrospective cohort,diagnosis,CXR,Lung
Development and validation of a clinically applicable deep learning strategy (HONORS) for pulmonary nodule classification at CT: A retrospective multicentre study,"PURPOSE: To propose a practical strategy for the clinical application of deep learning algorithm, i.e., Hierarchical-Ordered Network-ORiented Strategy (HONORS), and a new approach to pulmonary nodule classification in various clinical scenarios, i.e., Filter-Guided Pyramid NETwork (FGP-NET). MATERIALS AND METHODS: We developed and validated FGP-NET on a collection of 2106 pulmonary nodules on computed tomography images which combined screened and clinically detected nodules, and performed external test (n = 341). The area under the curves (AUCs) of FGP-NET were assessed. A comparison study with a group of 126 skilled radiologists was conducted. On top of FGP-NET, we built up our HONORS which was composed of two solutions. In the Human Free Solution, we used the high sensitivity operating point for screened nodules, but the high specificity operating point for clinically detected nodules. In the Human-Machine Coupling Solution, we used the Youden point. RESULTS: FGP-NET achieved AUCs of 0.969 and 0.847 for internal and external test. The AUCs of the subsets of the external test set ranged from 0.890 to 0.942. The average sensitivity and specificity of the 126 radiologists were 72.2 ± 15.1 % and 71.7 ± 15.5 %, respectively, while a higher sensitivity (93.3 %) but a relatively inferior specificity (64.0 %) were achieved by FGP-NET. HONORS-guided FGP-NET identified benign nodules with high sensitivity (sensitivity,95.5 %; specificity, 72.5 %) in the screened nodules, and identified malignant nodules with high specificity (sensitivity, 31.0 %; specificity, 97.5 %) in the clinically detected nodules. These nodules could be reliably diagnosed without any intervention from radiologists, via the Human Free Solution. The remaining ambiguous nodules were diagnosed with high performance, which however required manual confirmation by radiologists, via the Human-Machine Coupling Solution. CONCLUSIONS: FGP-NET performed comparably to skilled radiologists in terms of diagnosing pulmonary nodules. HONORS, due to its high performance, might reliably contribute a second opinion, aiding in optimizing the clinical workflow.",2021,10.1016/j.lungcan.2021.03.008,cross-sectional,diagnosis,CT,Lung
Development and Validation of a Deep Learning-based Automatic Detection Algorithm for Active Pulmonary Tuberculosis on Chest Radiographs,"BACKGROUND: Detection of active pulmonary tuberculosis on chest radiographs (CRs) is critical for the diagnosis and screening of tuberculosis. An automated system may help streamline the tuberculosis screening process and improve diagnostic performance. METHODS: We developed a deep learning-based automatic detection (DLAD) algorithm using 54c221 normal CRs and 6768 CRs with active pulmonary tuberculosis that were labeled and annotated by 13 board-certified radiologists. The performance of DLAD was validated using 6 external multicenter, multinational datasets. To compare the performances of DLAD with physicians, an observer performance test was conducted by 15 physicians including nonradiology physicians, board-certified radiologists, and thoracic radiologists. Image-wise classification and lesion-wise localization performances were measured using area under the receiver operating characteristic (ROC) curves and area under the alternative free-response ROC curves, respectively. Sensitivities and specificities of DLAD were calculated using 2 cutoffs (high sensitivity [98%] and high specificity [98%]) obtained through in-house validation. RESULTS: DLAD demonstrated classification performance of 0.977-1.000 and localization performance of 0.973-1.000. Sensitivities and specificities for classification were 94.3%-100% and 91.1%-100% using the high-sensitivity cutoff and 84.1%-99.0% and 99.1%-100% using the high-specificity cutoff. DLAD showed significantly higher performance in both classification (0.993 vs 0.746-0.971) and localization (0.993 vs 0.664-0.925) compared to all groups of physicians. CONCLUSIONS: Our DLAD demonstrated excellent and consistent performance in the detection of active pulmonary tuberculosis on CR, outperforming physicians, including thoracic radiologists.",2019,10.1093/cid/ciy967,cross-sectional,diagnosis,CXR,Lung
Development and Validation of a Modified Three-Dimensional U-Net Deep-Learning Model for Automated Detection of Lung Nodules on Chest CT Images From the Lung Image Database Consortium and Japanese Datasets,"RATIONALE AND OBJECTIVES: A more accurate lung nodule detection algorithm is needed. We developed a modified three-dimensional (3D) U-net deep-learning model for the automated detection of lung nodules on chest CT images. The purpose of this study was to evaluate the accuracy of the developed modified 3D U-net deep-learning model. MATERIALS AND METHODS: In this Health Insurance Portability and Accountability Act-compliant, Institutional Review Board-approved retrospective study, the 3D U-net based deep-learning model was trained using the Lung Image Database Consortium and Image Database Resource Initiative dataset. For internal model validation, we used 89 chest CT scans that were not used for model training. For external model validation, we used 450 chest CT scans taken at an urban university hospital in Japan. Each case included at least one nodule of >5 mm identified by an experienced radiologist. We evaluated model accuracy using the competition performance metric (CPM) (average sensitivity at 1/8, 1/4, 1/2, 1, 2, 4, and 8 false-positives per scan). The 95% confidence interval (CI) was computed by bootstrapping 1000 times. RESULTS: In the internal validation, the CPM was 94.7% (95% CI: 89.1%-98.6%). In the external validation, the CPM was 83.3% (95% CI: 79.4%-86.1%). CONCLUSION: The modified 3D U-net deep-learning model showed high performance in both internal and external validation.",2022,10.1016/j.acra.2020.07.030,cross-sectional,diagnosis,CT,Lung
Development and validation of a plasma biomarker panel for discerning clinical significance of indeterminate pulmonary nodules,"INTRODUCTION: The recent findings of the National Lung Screening Trial showed 24.2% of individuals at high risk for lung cancer having one or more indeterminate nodules detected by low-dose computed tomography-based screening, 96.4% of which were eventually confirmed as false positives. These positive scans necessitate additional diagnostic procedures to establish a definitive diagnosis that adds cost and risk to the paradigm. A plasma test able to assign benign versus malignant pathology in high-risk patients would be an invaluable tool to complement low-dose computed tomography-based screening and promote its rapid implementation. METHODS: We evaluated 17 biomarkers, previously shown to have value in detecting lung cancer, against a discovery cohort, comprising benign (n = 67) cases and lung cancer (n = 69) cases. A Random Forest method based analysis was used to identify the optimal biomarker panel for assigning disease status, which was then validated against a cohort from the Mayo Clinic, comprising patients with benign (n = 61) or malignant (n = 20) indeterminate lung nodules. RESULTS: Our discovery efforts produced a seven-analyte plasma biomarker panel consisting of interleukin 6 (IL-6), IL-10, IL-1ra, sIL-2Rα, stromal cell-derived factor-1α+β, tumor necrosis factor α, and macrophage inflammatory protein 1 α. The sensitivity and specificity of our panel in our validation cohort is 95.0% and 23.3%, respectively. The validated negative predictive value of our panel was 93.8%. CONCLUSION: We developed a seven-analyte plasma biomarker panel able to identify benign nodules, otherwise deemed indeterminate, with a high degree of accuracy. This panel may have clinical utility in risk-stratifying screen-detected lung nodules, decrease unnecessary follow-up imaging or invasive procedures, and potentially avoid unnecessary morbidity, mortality, and health care costs.",2013,10.1097/JTO.0b013e31827627f8,,,,
Development and Validation of a Predictive Radiomics Model for Clinical Outcomes in Stage I Non-small Cell Lung Cancer,"PURPOSE: To develop and validate a radiomics signature that can predict the clinical outcomes for patients with stage I non-small cell lung cancer (NSCLC). METHODS AND MATERIALS: We retrospectively analyzed contrast-enhanced computed tomography images of patients from a training cohort (n = 147) treated with surgery and an independent validation cohort (n = 295) treated with stereotactic ablative radiation therapy. Twelve radiomics features with established strategies for filtering and preprocessing were extracted. The random survival forests (RSF) method was used to build models from subsets of the 12 candidate features based on their survival relevance and generate a mortality risk index for each observation in the training set. An optimal model was selected, and its ability to predict clinical outcomes was evaluated in the validation set using predicted mortality risk indexes. RESULTS: The optimal RSF model, consisting of 2 predictive features, kurtosis and the gray level co-occurrence matrix feature homogeneity2, allowed for significant risk stratification (log-rank P < .0001) and remained an independent predictor of overall survival after adjusting for age, tumor volume and histologic type, and Karnofsky performance status (hazard ratio [HR] 1.27; P < 2e-16) in the training set. The resultant mortality risk indexes were significantly associated with overall survival in the validation set (log-rank P = .0173; HR 1.02, P = .0438). They were also significant for distant metastasis (log-rank P < .05; HR 1.04, P = .0407) and were borderline significant for regional recurrence on univariate analysis (log-rank P < .05; HR 1.04, P = .0617). CONCLUSIONS: Our radiomics model accurately predicted several clinical outcomes and allowed pretreatment risk stratification in stage I NSCLC, allowing the choice of treatment to be tailored to each patient's individual risk profile.",2018,10.1016/j.ijrobp.2017.10.046,case control,diagnosis,CT,Lung
"Development and validation of a preoperative CT-based radiomic nomogram to predict pathology invasiveness in patients with a solitary pulmonary nodule: a machine learning approach, multicenter, diagnostic study","OBJECTIVES: To develop and validate a preoperative CT-based nomogram combined with radiomic and clinical-radiological signatures to distinguish preinvasive lesions from pulmonary invasive lesions. METHODS: This was a retrospective, diagnostic study conducted from August 1, 2018, to May 1, 2020, at three centers. Patients with a solitary pulmonary nodule were enrolled in the GDPH center and were divided into two groups (7:3) randomly: development (n = 149) and internal validation (n = 54). The SYSMH center and the ZSLC Center formed an external validation cohort of 170 patients. The least absolute shrinkage and selection operator (LASSO) algorithm and logistic regression analysis were used to feature signatures and transform them into models. RESULTS: The study comprised 373 individuals from three independent centers (female: 225/373, 60.3%; median [IQR] age, 57.0 [48.0-65.0] years). The AUCs for the combined radiomic signature selected from the nodular area and the perinodular area were 0.93, 0.91, and 0.90 in the three cohorts. The nomogram combining the clinical and combined radiomic signatures could accurately predict interstitial invasion in patients with a solitary pulmonary nodule (AUC, 0.94, 0.90, 0.92) in the three cohorts, respectively. The radiomic nomogram outperformed any clinical or radiomic signature in terms of clinical predictive abilities, according to a decision curve analysis and the Akaike information criteria. CONCLUSIONS: This study demonstrated that a nomogram constructed by identified clinical-radiological signatures and combined radiomic signatures has the potential to precisely predict pathology invasiveness. KEY POINTS: • The radiomic signature from the perinodular area has the potential to predict pathology invasiveness of the solitary pulmonary nodule. • The new radiomic nomogram was useful in clinical decision-making associated with personalized surgical intervention and therapeutic regimen selection in patients with early-stage non-small-cell lung cancer.",2022,10.1007/s00330-021-08268-z,cross-sectional,diagnosis,CT,Lung
Development and Validation of a Risk Stratification Model of Pulmonary Ground-Glass Nodules Based on Complementary Lung-RADS 1.1 and Deep Learning Scores,"PURPOSE: To assess the value of novel deep learning (DL) scores combined with complementary lung imaging reporting and data system 1.1 (cLung-RADS 1.1) in managing the risk stratification of ground-glass nodules (GGNs) and therefore improving the efficiency of lung cancer (LC) screening in China. MATERIALS AND METHODS: Overall, 506 patients with 561 GGNs on routine computed tomography images, obtained between January 2017 and March 2021, were enrolled in this single-center, retrospective Chinese study. Moreover, the cLung-RADS 1.1 was previously validated, and the DL algorithms were based on a multi-stage, three-dimensional DL-based convolutional neural network. Therefore, the DL-based cLung-RADS 1.1 model was created using a combination of the risk scores of DL and category of cLung-RADS 1.1. The recall rate, precision, accuracy, per-class F1 score, weighted average F1 score (F1(weighted)), Matthews correlation coefficient (MCC), and area under the curve (AUC) were used to evaluate the performance of DL-based cLung-RADS 1.1. RESULTS: The percentage of neoplastic lesions appeared as GGNs in our study was 95.72% (537/561) after long-period follow-up.Compared to cLung-RADS 1.1 model or DL model, The DL-based cLung-RADS 1.1 model achieved the excellent performance with F1 scores of 95.96% and 95.58%, F1(weighted) values of 97.49 and 96.62%, accuracies of 92.38 and 91.77%, and MCCs of 32.43 and 37.15% in the training and validation tests, respectively. The combined model achieved the best AUCs of 0.753 (0.526-0.980) and 0.734 (0.585-0.884) for the training and validation tests, respectively. CONCLUSION: The DL-based cLung-RADS 1.1 model shows the best performance in risk stratification management of GGNs, which demonstrates substantial promise for developing a more effective personalized lung neoplasm management paradigm for LC screening in China.",2022,10.3389/fpubh.2022.891306,cross-sectional,diagnosis,CT,Lung
Development and Validation of Deep Learning-based Automatic Detection Algorithm for Malignant Pulmonary Nodules on Chest Radiographs,"Purpose To develop and validate a deep learning-based automatic detection algorithm (DLAD) for malignant pulmonary nodules on chest radiographs and to compare its performance with physicians including thoracic radiologists. Materials and Methods For this retrospective study, DLAD was developed by using 43 292 chest radiographs (normal radiograph-to-nodule radiograph ratio, 34 067:9225) in 34 676 patients (healthy-to-nodule ratio, 30 784:3892; 19 230 men [mean age, 52.8 years; age range, 18-99 years]; 15 446 women [mean age, 52.3 years; age range, 18-98 years]) obtained between 2010 and 2015, which were labeled and partially annotated by 13 board-certified radiologists, in a convolutional neural network. Radiograph classification and nodule detection performances of DLAD were validated by using one internal and four external data sets from three South Korean hospitals and one U.S. hospital. For internal and external validation, radiograph classification and nodule detection performances of DLAD were evaluated by using the area under the receiver operating characteristic curve (AUROC) and jackknife alternative free-response receiver-operating characteristic (JAFROC) figure of merit (FOM), respectively. An observer performance test involving 18 physicians, including nine board-certified radiologists, was conducted by using one of the four external validation data sets. Performances of DLAD, physicians, and physicians assisted with DLAD were evaluated and compared. Results According to one internal and four external validation data sets, radiograph classification and nodule detection performances of DLAD were a range of 0.92-0.99 (AUROC) and 0.831-0.924 (JAFROC FOM), respectively. DLAD showed a higher AUROC and JAFROC FOM at the observer performance test than 17 of 18 and 15 of 18 physicians, respectively (P < .05), and all physicians showed improved nodule detection performances with DLAD (mean JAFROC FOM improvement, 0.043; range, 0.006-0.190; P < .05). Conclusion This deep learning-based automatic detection algorithm outperformed physicians in radiograph classification and nodule detection performance for malignant pulmonary nodules on chest radiographs, and it enhanced physicians' performances when used as a second reader. © RSNA, 2018 Online supplemental material is available for this article.",2019,10.1148/radiol.2018180237,cross-sectional,diagnosis,CT,Lung
Development and Validation of Machine Learning Models to Predict Epidermal Growth Factor Receptor Mutation in Non-Small Cell Lung Cancer: A Multi-Center Retrospective Radiomics Study,"OBJECTIVE: To develop and validate a generalized prediction model that can classify epidermal growth factor receptor (EGFR) mutation status in non-small cell lung cancer patients. METHODS: A total of 346 patients (296 in the training cohort and 50 in the validation cohort) from four centers were included in this retrospective study. First, 1085 features were extracted using IBEX from the computed tomography images. The features were screened using the intraclass correlation coefficient, hypothesis tests and least absolute shrinkage and selection operator. Logistic regression (LR), decision tree (DT), random forest (RF), and support vector machine (SVM) were used to build a radiomics model for classification. The models were evaluated using the following metrics: area under the curve (AUC), calibration curve (CAL), decision curve analysis (DCA), concordance index (C-index), and Brier score. RESULTS: Sixteen features were selected, and models were built using LR, DT, RF, and SVM. In the training cohort, the AUCs was .723, .842, .995, and .883; In the validation cohort, the AUCs were .658, 0567, .88, and .765. RF model with the best AUC, its CAL, C-index (training cohort=.998; validation cohort=.883), and Brier score (training cohort=.007; validation cohort=0.137) showed a satisfactory predictive accuracy; DCA indicated that the RF model has better clinical application value. CONCLUSION: Machine learning models based on computed tomography images can be used to evaluate EGFR status in patients with non-small cell lung cancer, and the RF model outperformed LR, DT, and SVM.",2022,10.1177/10732748221092926,cross-sectional,prognosis,CT,Lung
Development and Validation of Machine Learning-based Model for the Prediction of Malignancy in Multiple Pulmonary Nodules: Analysis from Multicentric Cohorts,"PURPOSE: Nodule evaluation is challenging and critical to diagnose multiple pulmonary nodules (MPNs). We aimed to develop and validate a machine learning-based model to estimate the malignant probability of MPNs to guide decision-making. EXPERIMENTAL DESIGN: A boosted ensemble algorithm (XGBoost) was used to predict malignancy using the clinicoradiologic variables of 1,739 nodules from 520 patients with MPNs at a Chinese center. The model (PKU-M model) was trained using 10-fold cross-validation in which hyperparameters were selected and fine-tuned. The model was validated and compared with solitary pulmonary nodule (SPN) models, clinicians, and a computer-aided diagnosis (CADx) system in an independent transnational cohort and a prospective multicentric cohort. RESULTS: The PKU-M model showed excellent discrimination [area under the curve; AUC (95% confidence interval (95% CI)), 0.909 (0.854-0.946)] and calibration (Brier score, 0.122) in the development cohort. External validation (583 nodules) revealed that the AUC of the PKU-M model was 0.890 (0.859-0.916), higher than those of the Brock model [0.806 (0.771-0.838)], PKU model [0.780 (0.743-0.817)], Mayo model [0.739 (0.697-0.776)], and VA model [0.682 (0.640-0.722)]. Prospective comparison (200 nodules) showed that the AUC of the PKU-M model [0.871 (0.815-0.915)] was higher than that of surgeons [0.790 (0.711-0.852), 0.741 (0.662-0.804), and 0.727 (0.650-0.788)], radiologist [0.748 (0.671-0.814)], and the CADx system [0.757 (0.682-0.818)]. Furthermore, the model outperformed the clinicians with an increase of 14.3% in sensitivity and 7.8% in specificity. CONCLUSIONS: After its development using machine learning algorithms, validation using transnational multicentric cohorts, and prospective comparison with clinicians and the CADx system, this novel prediction model for MPNs presented solid performance as a convenient reference to help decision-making.",2021,10.1158/1078-0432.Ccr-20-4007,retrospective cohort,prognosis,CT,Lung
Development of a deep learning-based method to diagnose pulmonary ground-glass nodules by sequential computed tomography imaging,"BACKGROUND: Early identification of the malignant propensity of pulmonary ground-glass nodules (GGNs) can relieve the pressure from tracking lesions and personalized treatment adaptation. The purpose of this study was to develop a deep learning-based method using sequential computed tomography (CT) imaging for diagnosing pulmonary GGNs. METHODS: This diagnostic study retrospectively enrolled 762 patients with GGNs from West China Hospital of Sichuan University between July 2009 and March 2019. All patients underwent surgical resection and at least two consecutive time-point CT scans. We developed a deep learning-based method to identify GGNs using sequential CT imaging on a training set consisting of 1524 CT sections from 508 patients and then evaluated 256 patients in the testing set. Afterwards, an observer study was conducted to compare the diagnostic performance between the deep learning model and two trained radiologists in the testing set. We further performed stratified analysis to further relieve the impact of histological types, nodule size, time interval between two CTs, and the component of GGNs. Receiver operating characteristic (ROC) analysis was used to assess the performance of all models. RESULTS: The deep learning model that used integrated DL-features from initial and follow-up CT images yielded the best diagnostic performance, with an area under the curve of 0.841. The observer study showed that the accuracies for the deep learning model, junior radiologist, and senior radiologist were 77.17%, 66.89%, and 77.03%, respectively. Stratified analyses showed that the deep learning model and radiologists exhibited higher performance in the subgroup of nodule sizes larger than 10 mm. With a longer time interval between two CTs, the deep learning model yielded higher diagnostic accuracy, but no general rules were yielded for radiologists. Different densities of components did not affect the performance of the deep learning model. In contrast, the radiologists were affected by the nodule component. CONCLUSIONS: Deep learning can achieve diagnostic performance on par with or better than radiologists in identifying pulmonary GGNs.",2022,10.1111/1759-7714.14305,retrospective cohort,diagnosis,CT,Lung
Development of a deep neural network for generating synthetic dual-energy chest x-ray images with single x-ray exposure,"Dual-energy chest radiography (DECR) is a medical imaging technology that can improve diagnostic accuracy. This technique can decompose single-energy chest radiography (SECR) images into separate bone- and soft tissue-only images. This can, however, double the radiation exposure to the patient. To address this limitation, we developed an algorithm for the synthesis of DECR from a SECR through deep learning. To predict high resolution images, we developed a novel deep learning architecture by modifying a conventional U-net to take advantage of the high frequency-dominant information that propagates from the encoding part to the decoding part. In addition, we used the anticorrelated relationship (ACR) of DECR for improving the quality of the predicted images. For training data, 300 pairs of SECR and their corresponding DECR images were used. To test the trained model, 50 DECR images from Yonsei University Severance Hospital and 662 publicly accessible SECRs were used. To evaluate the performance of the proposed method, we compared DECR and predicted images using a structural similarity approach (SSIM). In addition, we quantitatively evaluated image quality calculating the modulation transfer function and coefficient of variation. The proposed model selectively predicted the bone- and soft tissue-only CR images from an SECR image. The strategy for improving the spatial resolution by ACR was effective. Quantitative evaluation showed that the proposed method with ACR showed relatively high SSIM (over 0.85). In addition, predicted images with the proposed ACR model achieved better image quality measures than those of U-net. In conclusion, the proposed method can obtain high-quality bone- and soft tissue-only CR images without the need for additional hardware for double x-ray exposures in clinical practice.",2019,10.1088/1361-6560/ab1cee,cross-sectional,others,CXR,NA
Development of a predictive radiomics model for lymph node metastases in pre-surgical CT-based stage IA non-small cell lung cancer,"OBJECTIVES: To develop and validate predictive models using clinical parameters, radiomic features and a combination of both for lymph node metastasis (LNM) in pre-surgical CT-based stage IA non-small cell lung cancer (NSCLC) patients. METHODS: This retrospective study included 649 pre-surgical CT-based stage IA NSCLC patients from our hospital. One hundred and thirty-eight (21 %) of the 649 patients had LNM after surgery. A total of 396 radiomic features were extracted from the venous phase contrast enhanced computed tomography (CECT). The training group included 455 patients (97 with and 358 without LNM) and the testing group included 194 patients (41 with and 153 without LNM). The least absolute shrinkage and selection operator (LASSO) algorithm was used for radiomic feature selection. The random forest (RF) was used for model development. Three models (a clinical model, a radiomics model, and a combined model) were developed to predict LNM in early stage NSCLC patients. The area under the receiver operating characteristic (ROC) curve (AUC) value and decision curve analysis were used to evaluate the performance in LNM status (with or without LNM) using the three models. RESULTS: The ROC analysis (also decision curve analysis) showed predictive performance for LNM of the radiomics model (AUC values for training and testing, respectively 0.898 and 0.851) and of the combined model (0.911 and 0.860, respectively). Both performed better than the clinical model (0.739 and 0.614, respectively; delong test p-values both<0.001). CONCLUSION: A radiomics model using the venous phase of CE-CT has potential for predicting LNM in pre-surgical CT-based stage IA NSCLC patients.",2020,10.1016/j.lungcan.2019.11.003,retrospective cohort,prognosis,CT,Lung
Development of a quantitative segmentation model to assess the effect of comorbidity on patients with COVID-19,"BACKGROUND: The coronavirus disease 2019 (COVID-19) has brought a global disaster. Quantitative lesions may provide the radiological evidence of the severity of pneumonia and further to assess the effect of comorbidity on patients with COVID-19. METHODS: 294 patients with COVID-19 were enrolled from February, 24, 2020 to June, 1, 2020 from six centers. Multi-task Unet network was used to segment the whole lung and lesions from chest CT images. This deep learning method was pre-trained in 650 CT images (550 in primary dataset and 100 in test dataset) with COVID-19 or community-acquired pneumonia and Dice coefficients in test dataset were calculated. 50 CT scans of 50 patients (15 with comorbidity and 35 without comorbidity) were random selected to mark lesions manually. The results will be compared with the automatic segmentation model. Eight quantitative parameters were calculated based on the segmentation results to evaluate the effect of comorbidity on patients with COVID-19. RESULTS: Quantitative segmentation model was proved to be effective and accurate with all Dice coefficients more than 0.85 and all accuracies more than 0.95. Of the 294 patients, 52 (17.7%) patients were reported having at least one comorbidity; 14 (4.8%) having more than one comorbidity. Patients with any comorbidity were older (P < 0.001), had longer incubation period (P < 0.001), were more likely to have abnormal laboratory findings (P < 0.05), and be in severity status (P < 0.001). More lesions (including larger volume of lesion, consolidation, and ground-glass opacity) were shown in patients with any comorbidity than patients without comorbidity (all P < 0.001). More lesions were found on CT images in patients with more comorbidities. The median volumes of lesion, consolidation, and ground-glass opacity in diabetes mellitus group were largest among the groups with single comorbidity that had the incidence rate of top three. CONCLUSIONS: Multi-task Unet network can make quantitative CT analysis of lesions to assess the effect of comorbidity on patients with COVID-19, further to provide the radiological evidence of the severity of pneumonia. More lesions (including GGO and consolidation) were found in CT images of cases with comorbidity. The more comorbidities patients have, the more lesions CT images show.",2020,10.1186/s40001-020-00450-1,retrospective cohort,prognosis,CT,Lung
Development of computer-aided model to differentiate COVID-19 from pulmonary edema in lung CT scan: EDECOVID-net,"The efforts made to prevent the spread of COVID-19 face specific challenges in diagnosing COVID-19 patients and differentiating them from patients with pulmonary edema. Although systemically administered pulmonary vasodilators and acetazolamide are of great benefit for treating pulmonary edema, they should not be used to treat COVID-19 as they carry the risk of several adverse consequences, including worsening the matching of ventilation and perfusion, impaired carbon dioxide transport, systemic hypotension, and increased work of breathing. This study proposes a machine learning-based method (EDECOVID-net) that automatically differentiates the COVID-19 symptoms from pulmonary edema in lung CT scans using radiomic features. To the best of our knowledge, EDECOVID-net is the first method to differentiate COVID-19 from pulmonary edema and a helpful tool for diagnosing COVID-19 at early stages. The EDECOVID-net has been proposed as a new machine learning-based method with some advantages, such as having simple structure and few mathematical calculations. In total, 13 717 imaging patches, including 5759 COVID-19 and 7958 edema images, were extracted using a CT incision by a specialist radiologist. The EDECOVID-net can distinguish the patients with COVID-19 from those with pulmonary edema with an accuracy of 0.98. In addition, the accuracy of the EDECOVID-net algorithm is compared with other machine learning methods, such as VGG-16 (Acc = 0.94), VGG-19 (Acc = 0.96), Xception (Acc = 0.95), ResNet101 (Acc = 0.97), and DenseNet20l (Acc = 0.97).",2022,10.1016/j.compbiomed.2021.105172,retrospective cohort,diagnosis,CT,Lung
Development of severity and mortality prediction models for covid-19 patients at emergency department including the chest x-ray,"OBJECTIVES: To develop prognosis prediction models for COVID-19 patients attending an emergency department (ED) based on initial chest X-ray (CXR), demographics, clinical and laboratory parameters. METHODS: All symptomatic confirmed COVID-19 patients admitted to our hospital ED between February 24th and April 24th 2020 were recruited. CXR features, clinical and laboratory variables and CXR abnormality indices extracted by a convolutional neural network (CNN) diagnostic tool were considered potential predictors on this first visit. The most serious individual outcome defined the three severity level: 0) home discharge or hospitalization ≤ 3 days, 1) hospital stay >3 days and 2) intensive care requirement or death. Severity and in-hospital mortality multivariable prediction models were developed and internally validated. The Youden index was used for the optimal threshold selection of the classification model. RESULTS: A total of 440 patients were enrolled (median 64 years; 55.9% male); 13.6% patients were discharged, 64% hospitalized, 6.6% required intensive care and 15.7% died. The severity prediction model included oxygen saturation/inspired oxygen fraction (SatO2/FiO2), age, C-reactive protein (CRP), lymphocyte count, extent score of lung involvement on CXR (ExtScoreCXR), lactate dehydrogenase (LDH), D-dimer level and platelets count, with AUC-ROC = 0.94 and AUC-PRC = 0.88. The mortality prediction model included age, SatO2/FiO2, CRP, LDH, CXR extent score, lymphocyte count and D-dimer level, with AUC-ROC = 0.97 and AUC-PRC = 0.78. The addition of CXR CNN-based indices did not improve significantly the predictive metrics. CONCLUSION: The developed and internally validated severity and mortality prediction models could be useful as triage tools in ED for patients with COVID-19 or other virus infections with similar behaviour.",2022,10.1016/j.rxeng.2021.09.004,retrospective cohort,prognosis,CXR,Lung
DIAG a Diagnostic Web Application Based on Lung CT Scan Images and Deep Learning,"Coronavirus disease is a pandemic that has infected millions of people around the world. Lung CT-scans are effective diagnostic tools, but radiologists can quickly become overwhelmed by the flow of infected patients. Therefore, automated image interpretation needs to be achieved. Deep learning (DL) can support critical medical tasks including diagnostics, and DL algorithms have successfully been applied to the classification and detection of many diseases. This work aims to use deep learning methods that can classify patients between Covid-19 positive and healthy patient. We collected 4 available datasets, and tested our convolutional neural networks (CNNs) on different distributions to investigate the generalizability of our models. In order to clearly explain the predictions, Grad-CAM and Fast-CAM visualization methods were used. Our approach reaches more than 92% accuracy on 2 different distributions. In addition, we propose a computer aided diagnosis web application for Covid-19 diagnosis. The results suggest that our proposed deep learning tool can be integrated to the Covid-19 detection process and be useful for a rapid patient management.",2021,10.3233/shti210175,cross-sectional,diagnosis,CT,Lung
Diagnosis of Coronavirus Disease 2019 (COVID-19) With Structured Latent Multi-View Representation Learning,"Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread rapidly across the world. Due to the large number of infected patients and heavy labor for doctors, computer-aided diagnosis with machine learning algorithm is urgently needed, and could largely reduce the efforts of clinicians and accelerate the diagnosis process. Chest computed tomography (CT) has been recognized as an informative tool for diagnosis of the disease. In this study, we propose to conduct the diagnosis of COVID-19 with a series of features extracted from CT images. To fully explore multiple features describing CT images from different views, a unified latent representation is learned which can completely encode information from different aspects of features and is endowed with promising class structure for separability. Specifically, the completeness is guaranteed with a group of backward neural networks (each for one type of features), while by using class labels the representation is enforced to be compact within COVID-19/community-acquired pneumonia (CAP) and also a large margin is guaranteed between different types of pneumonia. In this way, our model can well avoid overfitting compared to the case of directly projecting high-dimensional features into classes. Extensive experimental results show that the proposed method outperforms all comparison methods, and rather stable performances are observed when varying the number of training data.",2020,10.1109/tmi.2020.2992546,retrospective cohort,diagnosis,CT,Lung
Diagnosis of Coronavirus Disease 2019 Pneumonia by Using Chest Radiography: Value of Artificial Intelligence,"Background Radiologists are proficient in differentiating between chest radiographs with and without symptoms of pneumonia but have found it more challenging to differentiate coronavirus disease 2019 (COVID-19) pneumonia from non-COVID-19 pneumonia on chest radiographs. Purpose To develop an artificial intelligence algorithm to differentiate COVID-19 pneumonia from other causes of abnormalities at chest radiography. Materials and Methods In this retrospective study, a deep neural network, CV19-Net, was trained, validated, and tested on chest radiographs in patients with and without COVID-19 pneumonia. For the chest radiographs positive for COVID-19, patients with reverse transcription polymerase chain reaction results positive for severe acute respiratory syndrome coronavirus 2 with findings positive for pneumonia between February 1, 2020, and May 30, 2020, were included. For the non-COVID-19 chest radiographs, patients with pneumonia who underwent chest radiography between October 1, 2019, and December 31, 2019, were included. Area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were calculated to characterize diagnostic performance. To benchmark the performance of CV19-Net, a randomly sampled test data set composed of 500 chest radiographs in 500 patients was evaluated by the CV19-Net and three experienced thoracic radiologists. Results A total of 2060 patients (5806 chest radiographs; mean age, 62 years ± 16 [standard deviation]; 1059 men) with COVID-19 pneumonia and 3148 patients (5300 chest radiographs; mean age, 64 years ± 18; 1578 men) with non-COVID-19 pneumonia were included and split into training and validation and test data sets. For the test set, CV19-Net achieved an AUC of 0.92 (95% CI: 0.91, 0.93). This corresponded to a sensitivity of 88% (95% CI: 87, 89) and a specificity of 79% (95% CI: 77, 80) by using a high-sensitivity operating threshold, or a sensitivity of 78% (95% CI: 77, 79) and a specificity of 89% (95% CI: 88, 90) by using a high-specificity operating threshold. For the 500 sampled chest radiographs, CV19-Net achieved an AUC of 0.94 (95% CI: 0.93, 0.96) compared with an AUC of 0.85 (95% CI: 0.81, 0.88) achieved by radiologists. Conclusion CV19-Net was able to differentiate coronavirus disease 2019-related pneumonia from other types of pneumonia, with performance exceeding that of experienced thoracic radiologists. © RSNA, 2021 Online supplemental material is available for this article.",2021,10.1148/radiol.2020202944,retrospective cohort,diagnosis,CXR,Lung
Diagnosis of COVID-19 using CT scan images and deep learning techniques,"Early diagnosis of the coronavirus disease in 2019 (COVID-19) is essential for controlling this pandemic. COVID-19 has been spreading rapidly all over the world. There is no vaccine available for this virus yet. Fast and accurate COVID-19 screening is possible using computed tomography (CT) scan images. The deep learning techniques used in the proposed method is based on a convolutional neural network (CNN). Our manuscript focuses on differentiating the CT scan images of COVID-19 and non-COVID 19 CT using different deep learning techniques. A self-developed model named CTnet-10 was designed for the COVID-19 diagnosis, having an accuracy of 82.1%. Also, other models that we tested are DenseNet-169, VGG-16, ResNet-50, InceptionV3, and VGG-19. The VGG-19 proved to be superior with an accuracy of 94.52% as compared to all other deep learning models. Automated diagnosis of COVID-19 from the CT scan pictures can be used by the doctors as a quick and efficient method for COVID-19 screening.",2021,10.1007/s10140-020-01886-y,cross-sectional,diagnosis,CT,Lung
Diagnosis of Invasive Lung Adenocarcinoma Based on Chest CT Radiomic Features of Part-Solid Pulmonary Nodules: A Multicenter Study,"Background Solid components of part-solid nodules (PSNs) at CT are reflective of invasive adenocarcinoma, but studies describing radiomic features of PSNs and the perinodular region are lacking. Purpose To develop and to validate radiomic signatures diagnosing invasive lung adenocarcinoma in PSNs compared with the Brock, clinical-semantic features, and volumetric models. Materials and Methods This retrospective multicenter study (https://ClinicalTrials.gov, NCT03872362) included 291 patients (median age, 60 years; interquartile range, 55-65 years; 191 women) from January 2013 to October 2017 with 297 PSN lung adenocarcinomas split into training (n = 229) and test (n = 68) data sets. Radiomic features were extracted from the different regions (gross tumor volume [GTV], solid, ground-glass, and perinodular). Random-forest models were trained using clinical-semantic, volumetric, and radiomic features, and an online nodule calculator was used to compute the Brock model. Performances of models were evaluated using standard metrics such as area under the curve (AUC), accuracy, and calibration. The integrated discrimination improvement was applied to assess model performance changes after the addition of perinodular features. Results The radiomics model based on ground-glass and solid features yielded an AUC of 0.98 (95% confidence interval [CI]: 0.96, 1.00) on the test data set, which was significantly higher than the Brock (AUC, 0.83 [95% CI: 0.72, 0.94]; P = .007), clinical-semantic (AUC, 0.90 [95% CI: 0.83, 0.98]; P = .03), volumetric GTV (AUC, 0.87 [95% CI: 0.78, 0.96]; P = .008), and radiomics GTV (AUC, 0.88 [95% CI: 0.80, 0.96]; P = .01) models. It also achieved the best accuracy (93% [95% CI: 84%, 98%]). Both this model and the model with added perinodular features showed good calibration, whereas adding perinodular features did not improve the performance (integrated discrimination improvement, -0.02; P = .56). Conclusion Separating ground-glass and solid CT radiomic features of part-solid nodules was useful in diagnosing the invasiveness of lung adenocarcinoma, yielding a better predictive performance than the Brock, clinical-semantic, volumetric, and radiomics gross tumor volume models. Online supplemental material is available for this article. See also the editorial by Nishino in this issue. Published under a CC BY 4.0 license.",2020,10.1148/radiol.2020192431,retrospective cohort,diagnosis,CT,Lung
Diagnosis of Non-small Cell Lung Cancer for Early Stage Asymptomatic Patients,"BACKGROUND/AIM: In 2016 in the United States, 7 of 10 patients were estimated to die following lung cancer diagnosis. This is due to a lack of a reliable screening method that detects early-stage lung cancer. Our aim is to accurately detect early stage lung cancer using algorithms and protein biomarkers. PATIENTS AND METHODS: A total of 1,479 human plasma samples were processed using a multiplex immunoassay platform. 82 biomarkers and 6 algorithms were explored. There were 351 NSCLC samples (90.3% Stage I, 2.3% Stage II, and 7.4% Stage III/IV). RESULTS: We identified 33 protein biomarkers and developed a classifier using Random Forest. Our test detected early-stage Non-Small Cell Lung Cancer (NSCLC) with a 90% accuracy, 80% sensitivity, and 95% specificity in the validation set using the 33 markers. CONCLUSION: A specific, non-invasive, early-detection test, in combination with low-dose computed tomography, could increase survival rates and reduce false positives from screenings.",2019,10.21873/cgp.20128,,,,
Diagnostic Accuracy and Performance of Artificial Intelligence in Detecting Lung Nodules in Patients With Complex Lung Disease: A Noninferiority Study,"OBJECTIVES: The aim of the study is to investigate the performance of artificial intelligence (AI) convolutional neural networks (CNN) in detecting lung nodules on chest computed tomography of patients with complex lung disease, and demonstrate its noninferiority when compared against an experienced radiologist through clinically relevant assessments. METHODS: A CNN prototype was used to retrospectively evaluate 103 complex lung disease cases and 40 control cases without reported nodules. Computed tomography scans were blindly evaluated by an expert thoracic radiologist; a month after initial analyses, 20 positive cases were re-evaluated with the assistance of AI. For clinically relevant applications: (1) AI was asked to classify each patient into nodules present or absent and (2) AI results were compared against standard radiology reports. Standard statistics were performed to determine detection performance. RESULTS: AI was, on average, 27 seconds faster than the expert and detected 8.4% of nodules that would have been missed. AI had a sensitivity of 67.7%, similar to an accuracy reported for experienced radiologists. AI correctly classified each patient (nodules present/absent) with a sensitivity of 96.1%. When matched against radiology reports, AI performed with a sensitivity of 89.4%. Control group assessment demonstrated an overall specificity of 82.5%. When aided by AI, the expert decreased the average assessment time per case from 2:44 minutes to 35.7 seconds, while reporting an overall increase in confidence. CONCLUSION: In a group of patients with complex lung disease, the sensitivity of AI is similar to an experienced radiologist and the tool helps detect previously missed nodules. AI also helps experts analyze for lung nodules faster and more confidently, a feature that is beneficial to patients and favorable to hospitals due to increased patient load and need for shorter turnaround times.",2022,10.1097/rti.0000000000000613,retrospective cohort,diagnosis,CT,Lung
Diagnostic accuracy of infrared thermal imaging for detecting COVID-19 infection in minimally symptomatic patients,"INTRODUCTION: Despite being widely used as a screening tool, a rigorous scientific evaluation of infrared thermography for the diagnosis of minimally symptomatic patients suspected of having COVID-19 infection has not been performed. METHODS: A consecutive sample of 60 adult individuals with a history of close contact with COVID-19 infected individuals and mild respiratory symptoms for less than 7 days and 20 confirmed COVID-19 negative healthy volunteers were enrolled in the study. Infrared thermograms of the face were obtained with a mobile camera, and RT-PCR was used as the reference standard test to diagnose COVID-19 infection. Temperature values and distribution of the face of healthy volunteers and patients with and without COVID-19 infection were then compared. RESULTS: Thirty-four patients had an RT-PCR confirmed diagnosis of COVID-19 and 26 had negative test results. The temperature asymmetry between the lacrimal caruncles and the forehead was significantly higher in COVID-19 positive individuals. Through a random forest analysis, a cut-off value of 0.55°C was found to discriminate with an 82% accuracy between patients with and without COVID-19 confirmed infection. CONCLUSIONS: Among adults with a history of COVID-19 exposure and mild respiratory symptoms, a temperature asymmetry of ≥ 0.55°C between the lacrimal caruncle and the forehead is highly suggestive of COVID-19 infection. This finding questions the widespread use of the measurement of absolute temperature values of the forehead as a COVID-19 screening tool.",2021,10.1111/eci.13474,,,,
Diagnostic Approach for Accurate Diagnosis of COVID-19 Employing Deep Learning and Transfer Learning Techniques through Chest X-ray Images Clinical Data in E-Healthcare,"COVID-19 is a transferable disease that is also a leading cause of death for a large number of people worldwide. This disease, caused by SARS-CoV-2, spreads very rapidly and quickly affects the respiratory system of the human being. Therefore, it is necessary to diagnosis this disease at the early stage for proper treatment, recovery, and controlling the spread. The automatic diagnosis system is significantly necessary for COVID-19 detection. To diagnose COVID-19 from chest X-ray images, employing artificial intelligence techniques based methods are more effective and could correctly diagnosis it. The existing diagnosis methods of COVID-19 have the problem of lack of accuracy to diagnosis. To handle this problem we have proposed an efficient and accurate diagnosis model for COVID-19. In the proposed method, a two-dimensional Convolutional Neural Network (2DCNN) is designed for COVID-19 recognition employing chest X-ray images. Transfer learning (TL) pre-trained ResNet-50 model weight is transferred to the 2DCNN model to enhanced the training process of the 2DCNN model and fine-tuning with chest X-ray images data for final multi-classification to diagnose COVID-19. In addition, the data augmentation technique transformation (rotation) is used to increase the data set size for effective training of the R2DCNNMC model. The experimental results demonstrated that the proposed (R2DCNNMC) model obtained high accuracy and obtained 98.12% classification accuracy on CRD data set, and 99.45% classification accuracy on CXI data set as compared to baseline methods. This approach has a high performance and could be used for COVID-19 diagnosis in E-Healthcare systems.",2021,10.3390/s21248219,cross-sectional,diagnosis,CXR,Lung
Diagnostic classification of coronavirus disease 2019 (COVID-19) and other pneumonias using radiomics features in CT chest images,"We propose a classification method using the radiomics features of CT chest images to identify patients with coronavirus disease 2019 (COVID-19) and other pneumonias. The chest CT images of two groups of participants (90 COVID-19 patients who were confirmed as positive by nucleic acid test of RT-PCR and 90 other pneumonias patients) were collected, and the two groups of data were manually drawn to outline the region of interest (ROI) of pneumonias. The radiomics method was used to extract textural features and histogram features of the ROI and obtain a radiomics features vector from each sample. Then, we divided the data into two independent radiomic cohorts for training (70 COVID-19 patients and 70 other pneumonias patients), and validation (20 COVID-19 patients and 20 other pneumonias patients) by using support vector machine (SVM). This model used 20 rounds of tenfold cross-validation for training. Finally, single-shot testing of the final model was performed on the independent validation cohort. In the COVID-19 patients, correlation analysis (multiple comparison correction-Bonferroni correction, P < 0.05/7) was also conducted to determine whether the textural and histogram features were correlated with the laboratory test index of blood, i.e., blood oxygen, white blood cell, lymphocytes, neutrophils, C-reactive protein, hypersensitive C-reactive protein, and erythrocyte sedimentation rate. The final model showed good discrimination on the independent validation cohort, with an accuracy of 89.83%, sensitivity of 94.22%, specificity of 85.44%, and AUC of 0.940. This proved that the radiomics features were highly distinguishable, and this SVM model can effectively identify and diagnose patients with COVID-19 and other pneumonias. The correlation analysis results showed that some textural features were positively correlated with WBC, and NE, and also negatively related to SPO2H and NE. Our results showed that radiomic features can classify COVID-19 patients and other pneumonias patients. The SVM model can achieve an excellent diagnosis of COVID-19.",2021,10.1038/s41598-021-97497-9,retrospective cohort,diagnosis,CT,Lung
Diagnostic classification of solitary pulmonary nodules using dual time (18)F-FDG PET/CT image texture features in granuloma-endemic regions,"Lung cancer, the most commonly diagnosed cancer worldwide, usually presents as solid pulmonary nodules (SPNs) on early diagnostic images. Classification of malignant disease at this early timepoint is critical for improving the success of surgical resection and increasing 5-year survival rates. (18)F-fluorodeoxyglucose ((18)F-FDG) PET/CT has demonstrated value for SPNs diagnosis with high sensitivity to detect malignant SPNs, but lower specificity in diagnosing malignant SPNs in populations with endemic infectious lung disease. This study aimed to determine whether quantitative heterogeneity derived from various texture features on dual time FDG PET/CT images (DTPI) can differentiate between malignant and benign SPNs in patients from granuloma-endemic regions. Machine learning methods were employed to find optimal discrimination between malignant and benign nodules. Machine learning models trained by texture features on DTPI images achieved significant improvements over standard clinical metrics and visual interpretation for discriminating benign from malignant SPNs, especially by texture features on delayed FDG PET/CT images.",2017,10.1038/s41598-017-08764-7,retrospective cohort,diagnosis,PET/CT,Lung
Diagnostic performance for pulmonary adenocarcinoma on CT: comparison of radiologists with and without three-dimensional convolutional neural network,"OBJECTIVES: To compare diagnostic performance for pulmonary invasive adenocarcinoma among radiologists with and without three-dimensional convolutional neural network (3D-CNN). METHODS: Enrolled were 285 patients with adenocarcinoma in situ (AIS, n = 75), minimally invasive adenocarcinoma (MIA, n = 58), and invasive adenocarcinoma (IVA, n = 152). A 3D-CNN model was constructed with seven convolution-pooling and two max-pooling layers and fully connected layers, in which batch normalization, residual connection, and global average pooling were used. Only the flipping process was performed for augmentation. The output layer comprised two nodes for two conditions (AIS/MIA and IVA) according to prognosis. Diagnostic performance of the 3D-CNN model in 285 patients was calculated using nested 10-fold cross-validation. In 90 of 285 patients, results from each radiologist (R1, R2, and R3; with 9, 14, and 26 years of experience, respectively) with and without the 3D-CNN model were statistically compared. RESULTS: Without the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 70.0%, 52.1%, and 90.5%; R2, 72.2%, 75%, and 69%; and R3, 74.4%, 89.6%, and 57.1%, respectively. With the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 72.2%, 77.1%, and 66.7%; R2, 74.4%, 85.4%, and 61.9%; and R3, 74.4%, 93.8%, and 52.4%, respectively. Diagnostic performance of each radiologist with and without the 3D-CNN model had no significant difference (p > 0.88), but the accuracy of R1 and R2 was significantly higher with than without the 3D-CNN model (p < 0.01). CONCLUSIONS: The 3D-CNN model can support a less-experienced radiologist to improve diagnostic accuracy for pulmonary invasive adenocarcinoma without deteriorating any diagnostic performances. KEY POINTS: • The 3D-CNN model is a non-invasive method for predicting pulmonary invasive adenocarcinoma in CT images with high sensitivity. • Diagnostic accuracy by a less-experienced radiologist was better with the 3D-CNN model than without the model.",2021,10.1007/s00330-020-07339-x,cross-sectional,diagnosis,CT,Lung
Diagnostic performance of artificial intelligence model for pneumonia from chest radiography,"OBJECTIVE: The chest X-ray (CXR) is the most readily available and common imaging modality for the assessment of pneumonia. However, detecting pneumonia from chest radiography is a challenging task, even for experienced radiologists. An artificial intelligence (AI) model might help to diagnose pneumonia from CXR more quickly and accurately. We aim to develop an AI model for pneumonia from CXR images and to evaluate diagnostic performance with external dataset. METHODS: To train the pneumonia model, a total of 157,016 CXR images from the National Institutes of Health (NIH) and the Korean National Tuberculosis Association (KNTA) were used (normal vs. pneumonia = 120,722 vs.36,294). An ensemble model of two neural networks with DenseNet classifies each CXR image into pneumonia or not. To test the accuracy of the models, a separate external dataset of pneumonia CXR images (n = 212) from a tertiary university hospital (Gachon University Gil Medical Center GUGMC, Incheon, South Korea) was used; the diagnosis of pneumonia was based on both the chest CT findings and clinical information, and the performance evaluated using the area under the receiver operating characteristic curve (AUC). Moreover, we tested the change of the AI probability score for pneumonia using the follow-up CXR images (7 days after the diagnosis of pneumonia, n = 100). RESULTS: When the probability scores of the models that have a threshold of 0.5 for pneumonia, two models (models 1 and 4) having different pre-processing parameters on the histogram equalization distribution showed best AUC performances of 0.973 and 0.960, respectively. As expected, the ensemble model of these two models performed better than each of the classification models with 0.983 AUC. Furthermore, the AI probability score change for pneumonia showed a significant difference between improved cases and aggravated cases (Δ = -0.06 ± 0.14 vs. 0.06 ± 0.09, for 85 improved cases and 15 aggravated cases, respectively, P = 0.001) for CXR taken as a 7-day follow-up. CONCLUSIONS: The ensemble model combined two different classification models for pneumonia that performed at 0.983 AUC for an external test dataset from a completely different data source. Furthermore, AI probability scores showed significant changes between cases of different clinical prognosis, which suggest the possibility of increased efficiency and performance of the CXR reading at the diagnosis and follow-up evaluation for pneumonia.",2021,10.1371/journal.pone.0249399,cross-sectional,diagnosis,CXR,Lung
Diagnostic validation of a deep learning nodule detection algorithm in low-dose chest CT: determination of optimized dose thresholds in a virtual screening scenario,"OBJECTIVES: This study was conducted to evaluate the effect of dose reduction on the performance of a deep learning (DL)-based computer-aided diagnosis (CAD) system regarding pulmonary nodule detection in a virtual screening scenario. METHODS: Sixty-eight anthropomorphic chest phantoms were equipped with 329 nodules (150 ground glass, 179 solid) with four sizes (5 mm, 8 mm, 10 mm, 12 mm) and scanned with nine tube voltage/current combinations. The examinations were analyzed by a commercially available DL-based CAD system. The results were compared by a comparison of proportions. Logistic regression was performed to evaluate the impact of tube voltage, tube current, nodule size, nodule density, and nodule location. RESULTS: The combination with the lowest effective dose (E) and unimpaired detection rate was 80 kV/50 mAs (sensitivity: 97.9%, mean false-positive rate (FPR): 1.9, mean CTDIvol: 1.2 ± 0.4 mGy, mean E: 0.66 mSv). Logistic regression revealed that tube voltage and current had the greatest impact on the detection rate, while nodule size and density had no significant influence. CONCLUSIONS: The optimal tube voltage/current combination proposed in this study (80 kV/50 mAs) is comparable to the proposed combinations in similar studies, which mostly dealt with conventional CAD software. Modification of tube voltage and tube current has a significant impact on the performance of DL-based CAD software in pulmonary nodule detection regardless of their size and composition. KEY POINTS: • Modification of tube voltage and tube current has a significant impact on the performance of deep learning-based CAD software. • Nodule size and composition have no significant impact on the software's performance. • The optimal tube voltage/current combination for the examined software is 80 kV/50 mAs.",2022,10.1007/s00330-021-08511-7,,,,
Diagnostic value of circulating genetically abnormal cells to support computed tomography for benign and malignant pulmonary nodules,"BACKGROUND: The accuracy of CT and tumour markers in screening lung cancer needs to be improved. Computer-aided diagnosis has been reported to effectively improve the diagnostic accuracy of imaging data, and recent studies have shown that circulating genetically abnormal cell (CAC) has the potential to become a novel marker of lung cancer. The purpose of this research is explore new ways of lung cancer screening. METHODS: From May 2020 to April 2021, patients with pulmonary nodules who had received CAC examination within one week before surgery or biopsy at First Affiliated Hospital of Zhengzhou University were enrolled. CAC counts, CT scan images, serum tumour marker (CEA, CYFRA21-1, NSE) levels and demographic characteristics of the patients were collected for analysis. CT were uploaded to the Pulmonary Nodules Artificial Intelligence Diagnostic System (PNAIDS) to assess the malignancy probability of nodules. We compared diagnosis based on PNAIDS, CAC, Mayo Clinic Model, tumour markers alone and their combination. The combination models were built through logistic regression, and was compared through the area under (AUC) the ROC curve. RESULTS: A total of 93 of 111 patients were included. The AUC of PNAIDS was 0.696, which increased to 0.847 when combined with CAC. The sensitivity (SE), specificity (SP), and positive (PPV) and negative (NPV) predictive values of the combined model were 61.0%, 94.1%, 94.7% and 58.2%, respectively. In addition, we evaluated the diagnostic value of CAC, which showed an AUC of 0.779, an SE of 76.3%, an SP of 64.7%, a PPV of 78.9%, and an NPV of 61.1%, higher than those of any single serum tumour marker and Mayo Clinic Model. The combination of PNAIDS and CAC exhibited significantly higher AUC values than the PNAIDS (P = 0.009) or the CAC (P = 0.047) indicator alone. However, including additional tumour markers did not significantly alter the performance of CAC and PNAIDS. CONCLUSIONS: CAC had a higher diagnostic value than traditional tumour markers in early-stage lung cancer and a supportive value for PNAIDS in the diagnosis of cancer based on lung nodules. The results of this study offer a new mode of screening for early-stage lung cancer using lung nodules.",2022,10.1186/s12885-022-09472-w,,,,
Diagnostic Value of Deep Learning-Based CT Feature for Severe Pulmonary Infection,"The study aimed to explore the diagnostic value of computed tomography (CT) images based on cavity convolution U-Net algorithm for patients with severe pulmonary infection. A new lung CT image segmentation algorithm (U-Net+ deep convolution (DC)) was proposed based on U-Net network and compared with convolutional neural network (CNN) algorithm. Then, it was applied to CT image diagnosis of 100 patients with severe lung infection in The Second Affiliated Hospital of Fujian Medical University hospital and compared with traditional methods, and its sensitivity, specificity, and accuracy were compared. It was found that the single training time and loss of U-Net + DC algorithm were reduced by 59.4% and 9.8%, respectively, compared with CNN algorithm, while Dice increased by 3.6%. The lung contour segmented by the proposed model was smooth, which was the closest to the gold standard. Fungal infection, bacterial infection, viral infection, tuberculosis infection, and mixed infection accounted for 28%, 18%, 7%, 7%, and 40%, respectively. 36%, 38%, 26%, 17%, and 20% of the patients had ground-glass shadow, solid shadow, nodule or mass shadow, reticular or linear shadow, and hollow shadow in CT, respectively. The incidence of various CT characteristics in patients with fungal and bacterial infections was statistically significant (P < 0.05). The specificity (94.32%) and accuracy (97.22%) of CT image diagnosis based on U-Net + DC algorithm were significantly higher than traditional diagnostic method (75.74% and 74.23%), and the differences were statistically significant (P < 0.05). The network of the algorithm in this study demonstrated excellent image segmentation effect. The CT image based on the U-Net + DC algorithm can be used for the diagnosis of patients with severe pulmonary infection, with high diagnostic value.",2021,10.1155/2021/5359084,retrospective cohort,prognosis,CT,Lung
Different CT slice thickness and contrast-enhancement phase in radiomics models on the differential performance of lung adenocarcinoma,"BACKGROUND: To investigate the effects of computed tomography (CT) reconstruction slice thickness and contrast-enhancement phase on the differential diagnosis performance of radiomic signature in lung adenocarcinoma. METHODS: A total of 187 patients who had been pathologically confirmed with lung adenocarcinoma and nonadenocarcinoma were divided into a training cohort (n = 149) and validation cohort (n = 38). All the patients underwent contrast-enhanced CT and the images were reconstructed with different slice thickness. The radiomic features were extracted from different slice thickness and scan phase. The logistic regression (LR) algorithm was used to build a machine learning model for each group. The area under the curve (AUC) obtained from the receiver operating characteristic (ROC) curve and DeLong test was used to evaluate its discriminating performance. RESULTS: Finally, 34 image features and five semantic features were selected to establish a radiomics model. Based on the three contrast-enhanced CT phases and four reconstruction slice thickness, 12 groups of radiomics models showed good discrimination ability with the AUCs range from 0.9287 to 0.9631, sensitivity range from 0.8349 to 0.9083, specificity range from 0.825 to 0.925 in the training group. Similar results were observed in the validation group. However, there was no statistical significance between the different CT scan phase groups and different slice thickness (p > 0.05). CONCLUSIONS: The radiomic analysis of contrast-enhanced CT can be used for the differential diagnosis of lung adenocarcinoma. Moreover, different slice thickness and contrast-enhanced scan phase did not affect the discriminating ability in the radiomics models.",2022,10.1111/1759-7714.14459,cross-sectional,diagnosis,CT,Lung
Differential diagnosis of lung carcinoma with three-dimensional quantitative molecular vibrational imaging,"The advent of molecularly targeted therapies requires effective identification of the various cell types of non-small cell lung carcinomas (NSCLC). Currently, cell type diagnosis is performed using small biopsies or cytology specimens that are often insufficient for molecular testing after morphologic analysis. Thus, the ability to rapidly recognize different cancer cell types, with minimal tissue consumption, would accelerate diagnosis and preserve tissue samples for subsequent molecular testing in targeted therapy. We report a label-free molecular vibrational imaging framework enabling three-dimensional (3-D) image acquisition and quantitative analysis of cellular structures for identification of NSCLC cell types. This diagnostic imaging system employs superpixel-based 3-D nuclear segmentation for extracting such disease-related features as nuclear shape, volume, and cell-cell distance. These features are used to characterize cancer cell types using machine learning. Using fresh unstained tissue samples derived from cell lines grown in a mouse model, the platform showed greater than 97% accuracy for diagnosis of NSCLC cell types within a few minutes. As an adjunct to subsequent histology tests, our novel system would allow fast delineation of cancer cell types with minimum tissue consumption, potentially facilitating on-the-spot diagnosis, while preserving specimens for additional tests. Furthermore, 3-D measurements of cellular structure permit evaluation closer to the native state of cells, creating an alternative to traditional 2-D histology specimen evaluation, potentially increasing accuracy in diagnosing cell type of lung carcinomas.",2012,10.1117/1.Jbo.17.6.066017,,,,
Differentiating brain metastases from different pathological types of lung cancers using texture analysis of T1 postcontrast MR,"PURPOSE: The goal of this study was to investigate the feasibility of differentiating brain metastases from different types of lung cancers using texture analysis (TA) of T1 postcontrast MR images. METHODS: TA was performed, and four subset textures were extracted and calculated separately. The capability of each texture to classify the different types of lung carcinoma was investigated using the Kruskal-Wallis test and receiver operating characteristic analysis. K-nearest neighbor (KNN) classifier model and back-propagation artificial neural network (BP-ANN) classifier model were used to build models and improve the predictive ability of TA. RESULTS: Texture-based lesion classification was highly specific in differentiating brain metastases originated from different types of lung cancers, with misclassification rates of 3.1%, 4.3%, 5.8%, and 8.1%, respectively, for small cell lung carcinoma, squamous cell carcinoma, adenocarcinoma, and large cell lung carcinoma. The BP-ANN model had a better predictive ability than the KNN model. No texture feature could distinguish between all four types of lung cancer. CONCLUSIONS: TA may predict the differences among various pathological types of lung cancer with brain metastases. The texture parameters, which reflect the tumor histopathology structure, may serve as an adjunct tool for clinically accurate diagnoses and deserves further investigation. Magn Reson Med 76:1410-1419, 2016. © 2015 International Society for Magnetic Resonance in Medicine.",2016,10.1002/mrm.26029,,,,
Differentiating Central Lung Tumors from Atelectasis with Contrast-Enhanced CT-Based Radiomics Features,"OBJECTIVES: To evaluate the utility of radiomics features in differentiating central lung cancers and atelectasis on contrast-enhanced computed tomography (CT) images. This study is retrospective. MATERIALS AND METHODS: In this study, 36 patients with central pulmonary cancer and atelectasis between July 2013 and June 2018 were identified. A total of 1,653 2D and 2,327 3D radiomics features were extracted from segmented lung cancers and atelectasis on contrast-enhanced CT. The refined features were investigated for usefulness in classifying lung cancer and atelectasis according to the information gain, and 10 models were trained based on these features. The classification model is trained and tested at the region level and pixel level, respectively. RESULTS: Among all the extracted features, 334 2D features and 1,507 3D features had an information gain (IG) greater than 0.1. The highest accuracy (AC) of the region classifiers was 0.9375. The best Dice score, Hausdorff distance, and voxel AC were 0.2076, 45.28, and 0.8675, respectively. CONCLUSIONS: Radiomics features derived from contrast-enhanced CT images can differentiate lung cancers and atelectasis at the regional and voxel levels.",2021,10.1155/2021/5522452,cross-sectional,diagnosis,CT,Lung
Differentiating Small-Cell Lung Cancer From Non-Small-Cell Lung Cancer Brain Metastases Based on MRI Using Efficientnet and Transfer Learning Approach,"Differentiation between small-cell lung cancer (SCLC) from non-small-cell lung cancer (NSCLC) brain metastases is crucial due to the different clinical behaviors of the two tumor types. We propose the use of a deep learning and transfer learning approach based on conventional magnetic resonance imaging (MRI) for non-invasive classification of SCLC vs. NSCLC brain metastases. Sixty-nine patients with brain metastasis of lung cancer origin were included. Of them, 44 patients had NSCLC and 25 patients had SCLC. Classification was performed with EfficientNet architecture on crop images of lesion areas and based on post-contrast T1-weighted, T2-weighted and FLAIR imaging input data. Evaluation of the model was carried out in a 5-fold cross-validation manner, and based on accuracy, precision, recall, F1 score and area under the receiver operating characteristic curve. The best classification results were obtained with multiparametric MRI input data (T1WI+c+FLAIR+T2WI), with a mean overall accuracy of 0.90 ± 0.04, and F1 score of 0.92 ± 0.05 for NSCLC and 0.87 ± 0.08 for SCLC for the validation data and an accuracy of 0.87 ± 0.05, with an F1 score of 0.88 ± 0.05 for NSCLC and 0.85 ± 0.05 for SCLC for the test dataset. The proposed method provides an automatic noninvasive method for the classification of brain metastasis with high sensitivity and specificity for differentiation between NSCLC vs. SCLC brain metastases. It may be used as a diagnostic tool for improving decision-making in the treatment of patients with these metastases. Further studies on larger patient samples are required to validate the current results.",2021,10.1177/15330338211004919,,,,
Differentiation Between Anteroposterior and Posteroanterior Chest X-Ray View Position With Convolutional Neural Networks,"PURPOSE: Detection and validation of the chest X-ray view position with use of convolutional neural networks to improve meta-information for data cleaning within a hospital data infrastructure. MATERIAL AND METHODS: Within this paper we developed a convolutional neural network which automatically detects the anteroposterior and posteroanterior view position of a chest radiograph. We trained two different network architectures (VGG variant and ResNet-34) with data published by the RSNA (26 684 radiographs, class distribution 46 % AP, 54 % PA) and validated these on a self-compiled dataset with data from the University Hospital Essen (4507, radiographs, class distribution 55 % PA, 45 % AP) labeled by a human reader. For visualization and better understanding of the network predictions, a Grad-CAM was generated for each network decision. The network results were evaluated based on the accuracy, the area under the curve (AUC), and the F1-score against the human reader labels. Also a final performance comparison between model predictions and DICOM labels was performed. RESULTS: The ensemble models reached accuracy and F1-scores greater than 95 %. The AUC reaches more than 0.99 for the ensemble models. The Grad-CAMs provide insight as to which anatomical structures contributed to a decision by the networks which are comparable with the ones a radiologist would use. Furthermore, the trained models were able to generalize over mislabeled examples, which was found by comparing the human reader labels to the predicted labels as well as the DICOM labels. CONCLUSION: The results show that certain incorrectly entered meta-information of radiological images can be effectively corrected by deep learning in order to increase data quality in clinical application as well as in research. KEY POINTS: · The predictions for both view positions are accurate with respect to external validation data.. · The networks based their decisions on anatomical structures and key points that were in-line with prior knowledge and human understanding.. · Final models were able to detect labeling errors within the test dataset.. CITATION FORMAT: · Hosch R, Kroll L, Nensa F et al. Differentiation Between Anteroposterior and Posteroanterior Chest X-Ray View Position With Convolutional Neural Networks. Fortschr Röntgenstr 2021; 193: 168 - 176.",2021,10.1055/a-1183-5227,cross-sectional,diagnosis,CXR,NA
Differentiation between immune checkpoint inhibitor-related and radiation pneumonitis in lung cancer by CT radiomics and machine learning,"PURPOSE: Consolidation immunotherapy after completion of chemoradiotherapy has become the standard of care for unresectable locally advanced non-small cell lung cancer and can induce potentially severe and life-threatening adverse events, including both immune checkpoint inhibitor-related pneumonitis (CIP) and radiation pneumonitis (RP), which are very challenging for radiologists to diagnose. Differentiating between CIP and RP has significant implications for clinical management such as the treatments for pneumonitis and the decision to continue or restart immunotherapy. The purpose of this study is to differentiate between CIP and RP by a CT radiomics approach. METHODS: We retrospectively collected the CT images and clinical information of patients with pneumonitis who received immune checkpoint inhibitor (ICI) only (n = 28), radiotherapy (RT) only (n = 31), and ICI+RT (n = 14). Three kinds of radiomic features (intensity histogram, gray-level co-occurrence matrix [GLCM] based, and bag-of-words [BoW] features) were extracted from CT images, which characterize tissue texture at different scales. Classification models, including logistic regression, random forest, and linear SVM, were first developed and tested in patients who received ICI or RT only with 10-fold cross-validation and further tested in patients who received ICI+RT using clinicians' diagnosis as a reference. RESULTS: Using 10-fold cross-validation, the classification models built on the intensity histogram features, GLCM-based features, and BoW features achieved an area under curve (AUC) of 0.765, 0.848, and 0.937, respectively. The best model was then applied to the patients receiving combination treatment, achieving an AUC of 0.896. CONCLUSIONS: This study demonstrates the promising potential of radiomic analysis of CT images for differentiating between CIP and RP in lung cancer, which could be a useful tool to attribute the cause of pneumonitis in patients who receive both ICI and RT.",2022,10.1002/mp.15451,retrospective cohort,diagnosis,CT,Lung
Differentiation of benign and malignant regions in paraffin embedded tissue blocks of pulmonary adenocarcinoma using micro CT scanning of paraffin tissue blocks: a pilot study for method validation,"PURPOSE: Micro computed tomography (micro-CT) can provide detailed information about the internal structure of materials. This study aimed to demonstrate the diagnostic value of micro-CT in formalin fixed paraffin embedded pulmonary adenocarcinomas by correlating the micro-CT findings of tumoral and non-tumoral areas with hematoxylin and eosin (HE) sections. METHODS: Paraffin blocks obtained from three adenocarcinomas were scanned with micro-CT. Ten regions of interest (ROIs) from adenocarcinoma and 11 ROIs from pulmonary parenchyma (ROI-C and ROI-N, respectively) areas were compared regarding the various structural parameters. RESULTS: All parameters were significantly different regarding the tumoral and non-tumoral ROIs. The percent object volume, structure thickness, structure linear density, connectivity and connectivity density were higher in ROI-Cs (p < 0.000, p < 0.000, p = 0.001, p < 0.000, and p < 0.000 respectively); whereas intersection surface and structure model index were higher in ROI-Ns (p < 0.000 and p < 0.000). The open porosity percentage was higher in ROI-Ns (68.86 + 2.96 vs 48.29 + 5.11, p < 0.000) and the closed porosity percentage was higher in ROI-Cs (2.29 + 0.55 vs 0.57 + 0.17 p < 0.000). CONCLUSIONS: The tumoral and non-tumoral areas in paraffin blocks can be distinguished from each other, using the quantitative and qualitative information obtained by micro-CT. Making this distinction with quantitative data obtained from micro-CT can therefore be the basis of creating artificial intelligence algorithms in the future.",2021,10.1007/s00595-021-02252-2,,,,
Differentiation of Benign from Malignant Pulmonary Nodules by Using a Convolutional Neural Network to Determine Volume Change at Chest CT,"Background Deep learning may help to improve computer-aided detection of volume (CADv) measurement of pulmonary nodules at chest CT. Purpose To determine the efficacy of a deep learning method for improving CADv for measuring the solid and ground-glass opacity (GGO) volumes of a nodule, doubling time (DT), and the change in volume at chest CT. Materials and Methods From January 2014 to December 2016, patients with pulmonary nodules at CT were retrospectively reviewed. CADv without and with a convolutional neural network (CNN) automatically determined total nodule volume change per day and DT. Area under the curves (AUCs) on a per-nodule basis and diagnostic accuracy on a per-patient basis were compared among all indexes from CADv with and without CNN for differentiating benign from malignant nodules. Results The CNN training set was 294 nodules in 217 patients, the validation set was 41 nodules in 32 validation patients, and the test set was 290 nodules in 188 patients. A total of 170 patients had 290 nodules (mean size ± standard deviation, 11 mm ± 5; range, 4-29 mm) diagnosed as 132 malignant nodules and 158 benign nodules. There were 132 solid nodules (46%), 106 part-solid nodules (36%), and 52 ground-glass nodules (18%). The test set results showed that the diagnostic performance of the CNN with CADv for total nodule volume change per day was larger than DT of CADv with CNN (AUC, 0.94 [95% confidence interval {CI}: 0.90, 0.96] vs 0.67 [95% CI: 0.60, 0.74]; P < .001) and CADv without CNN (total nodule volume change per day: AUC, 0.69 [95% CI: 0.62, 0.75]; P < .001; DT: AUC, 0.58 [95% CI: 0.51, 0.65]; P < .001). The accuracy of total nodule volume change per day of CADv with CNN was significantly higher than that of CADv without CNN (P < .001) and DT of both methods (P < .001). Conclusion Convolutional neural network is useful for improving accuracy of computer-aided detection of volume measurement and nodule differentiation capability at CT for patients with pulmonary nodules. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020191740,retrospective cohort,diagnosis,CT,Lung
Differentiation of spinal metastases originated from lung and other cancers using radiomics and deep learning based on DCE-MRI,"PURPOSE: To differentiate metastatic lesions in the spine originated from primary lung cancer and other cancers using radiomics and deep learning, compared to traditional hot-spot ROI analysis. METHODS: In a retrospective review of clinical spinal MRI database with a dynamic contrast enhanced (DCE) sequence, a total of 61 patients without prior cancer diagnosis and later confirmed to have metastases (30 lung; 31 non-lung cancers) were identified. For hot-spot analysis, a manual ROI was placed to calculate three heuristic parameters from the wash-in, maximum, and wash-out phases in the DCE kinetics. For each case, the 3D tumor mask was generated by using the normalized-cut algorithm. Radiomics analysis was performed to extract histogram and texture features from three DCE parametric maps. Deep learning was performed using these maps as inputs into a conventional convolutional neural network (CNN), as well as using all 12 sets of DCE images into a convolutional long short term memory (CLSTM) network. RESULTS: For hot-spot ROI analysis, mean wash-out slope was 0.25 ± 10% for lung metastases and -9.8 ± 12.9% for other tumors. CHAID classification using a wash-out slope of -6.6% followed by wash-in enhancement ratio of 98% achieved a diagnostic accuracy of 0.79. Radiomics analysis using features representing tumor heterogeneity only reached the highest accuracy of 0.71. Classification using CNN achieved a mean accuracy of 0.71 ± 0.043, whereas a CLSTM improved accuracy to 0.81 ± 0.034. CONCLUSIONS: DCE-MRI machine-learning analysis methods have potential to predict lung cancer metastases in the spine, which may be used to guide subsequent workup for confirmed diagnosis.",2019,10.1016/j.mri.2019.02.013,,,,
Differentiation of supratentorial single brain metastasis and glioblastoma by using peri-enhancing oedema region-derived radiomic features and multiple classifiers,"OBJECTIVE: To differentiate supratentorial single brain metastasis (MET) from glioblastoma (GBM) by using radiomic features derived from the peri-enhancing oedema region and multiple classifiers. METHODS: One hundred and twenty single brain METs and GBMs were retrospectively reviewed and then randomly divided into a training data set (70%) and validation data set (30%). Quantitative radiomic features of each case were extracted from the peri-enhancing oedema region of conventional MR images. After feature selection, five classifiers were built. Additionally, the combined use of the classifiers was studied. Accuracy, sensitivity, and specificity were used to evaluate the classification performance. RESULTS: A total of 321 features were extracted, and 3 features were selected for each case. The 5 classifiers showed an accuracy of 0.70 to 0.76, sensitivity of 0.57 to 0.98, and specificity of 0.43 to 0.93 for the training data set, with an accuracy of 0.56 to 0.64, sensitivity of 0.39 to 0.78, and specificity of 0.50 to 0.89 for the validation data set. When combining the classifiers, the classification performance differed according to the combined mode and the agreement pattern of classifiers, and the greatest benefit was obtained when all the classifiers reached agreement using the same weight and simple majority vote method. CONCLUSIONS: Three features derived from the peri-enhancing oedema region had moderate value in differentiating supratentorial single brain MET from GBM with five single classifiers. Combined use of classifiers, like multi-disciplinary team (MDT) consultation, could confer extra benefits, especially for those cases when all classifiers reach agreement. KEY POINTS: • Radiomics provides a way to differentiate single brain MET between GBM by using conventional MR images. • The results of classifiers or algorithms themselves are also data, the transformation of the primary data. • Like MDT consultation, the combined use of multiple classifiers may confer extra benefits.",2020,10.1007/s00330-019-06460-w,,,,
Diffusion tensor and postcontrast T1-weighted imaging radiomics to differentiate the epidermal growth factor receptor mutation status of brain metastases from non-small cell lung cancer,"PURPOSE: To assess whether the radiomic features of diffusion tensor imaging (DTI) and conventional postcontrast T1-weighted (T1C) images can differentiate the epidermal growth factor receptor (EGFR) mutation status in brain metastases from non-small cell lung cancer (NSCLC). METHODS: A total of 99 brain metastases in 51 patients who underwent surgery or biopsy with underlying NSCLC and known EGFR mutation statuses (57 from EGFR wild type, 42 from EGFR mutant) were allocated to the training (57 lesions in 31 patients) and test (42 lesions in 20 patients) sets. Radiomic features (n = 526) were extracted from preoperative MR images including T1C and DTI. Radiomics classifiers were constructed by combinations of five feature selectors and four machine learning algorithms. The trained classifiers were validated on the test set, and the classifier performance was assessed by determining the area under the curve (AUC). RESULTS: EGFR mutation status showed an overall discordance rate of 12% between the primary tumors and corresponding brain metastases. The best performing classifier was a combination of the tree-based feature selection and linear discriminant algorithm and 5 features were selected (1 from ADC, 2 from fractional anisotropy, and 2 from T1C images), resulting in an AUC, accuracy, sensitivity, and specificity of 0.73, 78.6%, 81.3%, and 76.9% in the test set, respectively. CONCLUSIONS: Radiomics classifiers integrating multiparametric MRI parameters may have potential in differentiating the EGFR mutation status in brain metastases from NSCLC.",2021,10.1007/s00234-020-02529-2,,,,
"Digital holographic deep learning of red blood cells for field-portable, rapid COVID-19 screening","Rapid screening of red blood cells for active infection of COVID-19 is presented using a compact and field-portable, 3D-printed shearing digital holographic microscope. Video holograms of thin blood smears are recorded, individual red blood cells are segmented for feature extraction, then a bi-directional long short-term memory network is used to classify between healthy and COVID positive red blood cells based on their spatiotemporal behavior. Individuals are then classified based on the simple majority of their cells' classifications. The proposed system may be beneficial for under-resourced healthcare systems. To the best of our knowledge, this is the first report of digital holographic microscopy for rapid screening of COVID-19.",2021,10.1364/ol.426152,,,,
Discrimination between transient and persistent subsolid pulmonary nodules on baseline CT using deep transfer learning,"OBJECTIVES: To develop and validate a deep learning model to discriminate transient from persistent subsolid nodules (SSNs) on baseline CT. METHODS: A cohort of 1414 SSNs, consisting of 319 transient SSNs in 168 individuals and 1095 persistent SSNs in 816 individuals, were identified on chest CT. The cohort was assigned by examination date into a development set of 996 SSNs, a tuning set of 212 SSNs, and a validation set of 206 SSNs. Our model was built by transfer learning, which was transferred from a well-performed deep learning model for pulmonary nodule classification. The performance of the model was compared with that of two experienced radiologists. Each nodule was categorized by Lung CT Screening Reporting and Data System (Lung-RADS) to further evaluate the performance and the potential clinical benefit of the model. Two methods were employed to visualize the learned features. RESULTS: Our model achieved an AUC of 0.926 on the validation set with an accuracy of 0.859, a sensitivity of 0.863, and a specificity of 0.858, and outperformed the radiologists. The model performed the best among Lung-RADS 2 nodules and maintained well performance among Lung-RADS 4 nodules. Feature visualization demonstrated the model's effectiveness in extracting features from images. CONCLUSIONS: The transfer learning model presented good performance on the discrimination between transient and persistent SSNs. A reliable diagnosis on nodule persistence can be achieved at baseline CT; thus, an early diagnosis as well as better patient care is available. KEY POINTS: • Deep learning can be used for the discrimination between transient and persistent subsolid nodules. • A transfer learning model can achieve good performance when it is transferred from a model with a similar task. • With the assistance of deep learning model, a reliable diagnosis on nodule persistence can be achieved at baseline CT, which can bring a better patient care strategy.",2020,10.1007/s00330-020-07071-6,retrospective cohort,prognosis,CT,Lung
Disease Progression Detection via Deep Sequence Learning of Successive Radiographic Scans,"The highly rapid spread of the current pandemic has quickly overwhelmed hospitals all over the world and motivated extensive research to address a wide range of emerging problems. The unforeseen influx of COVID-19 patients to hospitals has made it inevitable to deploy a rapid and accurate triage system, monitor progression, and predict patients at higher risk of deterioration in order to make informed decisions regarding hospital resource management. Disease detection in radiographic scans, severity estimation, and progression and prognosis prediction have been extensively studied with the help of end-to-end methods based on deep learning. The majority of recent works have utilized a single scan to determine severity or predict progression of the disease. In this paper, we present a method based on deep sequence learning to predict improvement or deterioration in successive chest X-ray scans and build a mathematical model to determine individual patient disease progression profile using successive scans. A deep convolutional neural network pretrained on a diverse lung disease dataset was used as a feature extractor to generate the sequences. We devised three strategies for sequence modeling in order to obtain both fine-grained and coarse-grained features and construct sequences of different lengths. We also devised a strategy to quantify positive or negative change in successive scans, which was then combined with age-related risk factors to construct disease progression profile for COVID-19 patients. The age-related risk factors allowed us to model rapid deterioration and slower recovery in older patients. Experiments conducted on two large datasets showed that the proposed method could accurately predict disease progression. With the best feature extractor, the proposed method was able to achieve AUC of 0.98 with the features obtained from radiographs. Furthermore, the proposed patient profiling method accurately estimated the health profile of patients.",2022,10.3390/ijerph19010480,retrospective cohort,prognosis,CXR,Lung
Disease Progression Modeling in Chronic Obstructive Pulmonary Disease,"Rationale: The decades-long progression of chronic obstructive pulmonary disease (COPD) renders identifying different trajectories of disease progression challenging.Objectives: To identify subtypes of patients with COPD with distinct longitudinal progression patterns using a novel machine-learning tool called ""Subtype and Stage Inference"" (SuStaIn) and to evaluate the utility of SuStaIn for patient stratification in COPD.Methods: We applied SuStaIn to cross-sectional computed tomography imaging markers in 3,698 Global Initiative for Chronic Obstructive Lung Disease (GOLD) 1-4 patients and 3,479 controls from the COPDGene (COPD Genetic Epidemiology) study to identify subtypes of patients with COPD. We confirmed the identified subtypes and progression patterns using ECLIPSE (Evaluation of COPD Longitudinally to Identify Predictive Surrogate Endpoints) data. We assessed the utility of SuStaIn for patient stratification by comparing SuStaIn subtypes and stages at baseline with longitudinal follow-up data.Measurements and Main Results: We identified two trajectories of disease progression in COPD: a ""Tissue→Airway"" subtype (n = 2,354, 70.4%), in which small airway dysfunction and emphysema precede large airway wall abnormalities, and an ""Airway→Tissue"" subtype (n = 988, 29.6%), in which large airway wall abnormalities precede emphysema and small airway dysfunction. Subtypes were reproducible in ECLIPSE. Baseline stage in both subtypes correlated with future FEV(1)/FVC decline (r = -0.16 [P < 0.001] in the Tissue→Airway group; r = -0.14 [P = 0.011] in the Airway→Tissue group). SuStaIn placed 30% of smokers with normal lung function at elevated stages, suggesting imaging changes consistent with early COPD. Individuals with early changes were 2.5 times more likely to meet COPD diagnostic criteria at follow-up.Conclusions: We demonstrate two distinct patterns of disease progression in COPD using SuStaIn, likely representing different endotypes. One third of healthy smokers have detectable imaging changes, suggesting a new biomarker of ""early COPD.""",2020,10.1164/rccm.201908-1600OC,,,,
Disease Staging and Prognosis in Smokers Using Deep Learning in Chest Computed Tomography,"RATIONALE: Deep learning is a powerful tool that may allow for improved outcome prediction. OBJECTIVES: To determine if deep learning, specifically convolutional neural network (CNN) analysis, could detect and stage chronic obstructive pulmonary disease (COPD) and predict acute respiratory disease (ARD) events and mortality in smokers. METHODS: A CNN was trained using computed tomography scans from 7,983 COPDGene participants and evaluated using 1,000 nonoverlapping COPDGene participants and 1,672 ECLIPSE participants. Logistic regression (C statistic and the Hosmer-Lemeshow test) was used to assess COPD diagnosis and ARD prediction. Cox regression (C index and the Greenwood-Nam-D'Agnostino test) was used to assess mortality. MEASUREMENTS AND MAIN RESULTS: In COPDGene, the C statistic for the detection of COPD was 0.856. A total of 51.1% of participants in COPDGene were accurately staged and 74.95% were within one stage. In ECLIPSE, 29.4% were accurately staged and 74.6% were within one stage. In COPDGene and ECLIPSE, the C statistics for ARD events were 0.64 and 0.55, respectively, and the Hosmer-Lemeshow P values were 0.502 and 0.380, respectively, suggesting no evidence of poor calibration. In COPDGene and ECLIPSE, CNN predicted mortality with fair discrimination (C indices, 0.72 and 0.60, respectively), and without evidence of poor calibration (Greenwood-Nam-D'Agnostino P values, 0.307 and 0.331, respectively). CONCLUSIONS: A deep-learning approach that uses only computed tomography imaging data can identify those smokers who have COPD and predict who are most likely to have ARD events and those with the highest mortality. At a population level CNN analysis may be a powerful tool for risk assessment.",2018,10.1164/rccm.201705-0860OC,cross-sectional,prognosis,CT,Lung
Distinct composition and metabolic functions of human gut microbiota are associated with cachexia in lung cancer patients,"Cachexia is associated with decreased survival in cancer patients and has a prevalence of up to 80%. The etiology of cachexia is poorly understood, and limited treatment options exist. Here, we investigated the role of the human gut microbiome in cachexia by integrating shotgun metagenomics and plasma metabolomics of 31 lung cancer patients. The cachexia group showed significant differences in the gut microbial composition, functional pathways of the metagenome, and the related plasma metabolites compared to non-cachectic patients. Branched-chain amino acids (BCAAs), methylhistamine, and vitamins were significantly depleted in the plasma of cachexia patients, which was also reflected in the depletion of relevant gut microbiota functional pathways. The enrichment of BCAAs and 3-oxocholic acid in non-cachectic patients were positively correlated with gut microbial species Prevotella copri and Lactobacillus gasseri, respectively. Furthermore, the gut microbiota capacity for lipopolysaccharides biosynthesis was significantly enriched in cachectic patients. The involvement of the gut microbiome in cachexia was further observed in a high-performance machine learning model using solely gut microbial features. Our study demonstrates the links between cachectic host metabolism and specific gut microbial species and functions in a clinical setting, suggesting that the gut microbiota could have an influence on cachexia with possible therapeutic applications.",2021,10.1038/s41396-021-00998-8,,,,
Distinguishing granulomas from adenocarcinomas by integrating stable and discriminating radiomic features on non-contrast computed tomography scans,"OBJECTIVE: To identify stable and discriminating radiomic features on non-contrast CT scans to develop more generalisable radiomic classifiers for distinguishing granulomas from adenocarcinomas. METHODS: In total, 412 patients with adenocarcinomas and granulomas from three institutions were retrospectively included. Segmentations of the lung nodules were performed manually by an expert radiologist in a 2D axial view. Radiomic features were extracted from intra- and perinodular regions. A total of 145 patients were used as part of the training set (S(tr)), whereas 205 patients were used as part of test set I (S(te)(1)) and 62 patients were used as part of independent test set II (S(te)(2)). To mitigate the variation of CT acquisition parameters, we defined 'stable' radiomic features as those for which the feature expression remains relatively unchanged between different sites, as assessed using a Wilcoxon rank-sum test. These stable features were used to develop more generalisable radiomic classifiers that were more resilient to variations in lung CT scans. Features were ranked based on two criteria, firstly based on discriminability (i.e. maximising AUC) alone and subsequently based on maximising both feature stability and discriminability. Different machine-learning classifiers (Linear discriminant analysis, Quadratic discriminant analysis, Support vector machines and random forest) were trained with features selected using the two different criteria and then compared on the two independent test sets for distinguishing granulomas from adenocarcinomas, in terms of area under the receiver operating characteristic curve. RESULTS: In the test sets, classifiers constructed using the criteria involving maximising feature stability and discriminability simultaneously achieved higher AUC compared with the discriminating alone criteria (S(te)(1) [n = 205]: maximum AUCs of 0.85versus . 0.80; p-value = 0.047 and S(te)(2) [n = 62]: maximum AUCs of 0.87 versus. 0.79; p-value = 0.021). These differences held for features extracted from scans with <3 mm slice thickness (AUC = 0.88 versus. 0.80; p-value = 0.039, n = 100) and for the ≥3 mm cases (AUC = 0.81 versus. 0.76; p-value = 0.034, n = 105). In both experiments, shape and peritumoural texture features had a higher stability compared with intratumoural texture features. CONCLUSIONS: Our study suggests that explicitly accounting for both stability and discriminability results in more generalisable radiomic classifiers to distinguish adenocarcinomas from granulomas on non-contrast CT scans. Our results also showed that peritumoural texture and shape features were less affected by the scanner parameters compared with intratumoural texture features; however, they were also less discriminating compared with intratumoural features.",2021,10.1016/j.ejca.2021.02.008,retrospective cohort,diagnosis,CT,Lung
Does non-COVID-19 lung lesion help? investigating transferability in COVID-19 CT image segmentation,"BACKGROUND AND OBJECTIVE: Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading all around the world. Deep learning has been adopted as an effective technique to aid COVID-19 detection and segmentation from computed tomography (CT) images. The major challenge lies in the inadequate public COVID-19 datasets. Recently, transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem. However, it remains unclear whether various non-COVID19 lung lesions could contribute to segmenting COVID-19 infection areas and how to better conduct this transfer procedure. This paper provides a way to understand the transferability of non-COVID19 lung lesions and a better strategy to train a robust deep learning model for COVID-19 infection segmentation. METHODS: Based on a publicly available COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four transfer learning methods using 3D U-Net as a standard encoder-decoder method. i) We introduce the multi-task learning method to get a multi-lesion pre-trained model for COVID-19 infection. ii) We propose and compare four transfer learning strategies with various performance gains and training time costs. Our proposed Hybrid-encoder Learning strategy introduces a Dedicated-encoder and an Adapted-encoder to extract COVID-19 infection features and general lung lesion features, respectively. An attention-based Selective Fusion unit is designed for dynamic feature selection and aggregation. RESULTS: Experiments show that trained with limited data, proposed Hybrid-encoder strategy based on multi-lesion pre-trained model achieves a mean DSC, NSD, Sensitivity, F1-score, Accuracy and MCC of 0.704, 0.735, 0.682, 0.707, 0.994 and 0.716, respectively, with better genetalization and lower over-fitting risks for segmenting COVID-19 infection. CONCLUSIONS: The results reveal the benefits of transferring knowledge from non-COVID19 lung lesions, and learning from multiple lung lesion datasets can extract more general features, leading to accurate and robust pre-trained models. We further show the capability of the encoder to learn feature representations of lung lesions, which improves segmentation accuracy and facilitates training convergence. In addition, our proposed Hybrid-encoder learning method incorporates transferred lung lesion features from non-COVID19 datasets effectively and achieves significant improvement. These findings promote new insights into transfer learning for COVID-19 CT image segmentation, which can also be further generalized to other medical tasks.",2021,10.1016/j.cmpb.2021.106004,cross-sectional,diagnosis,CT,Lung
Domain Adaptation-Based Deep Learning for Automated Tumor Cell (TC) Scoring and Survival Analysis on PD-L1 Stained Tissue Images,"We report the ability of two deep learning-based decision systems to stratify non-small cell lung cancer (NSCLC) patients treated with checkpoint inhibitor therapy into two distinct survival groups. Both systems analyze functional and morphological properties of epithelial regions in digital histopathology whole slide images stained with the SP263 PD-L1 antibody. The first system learns to replicate the pathologist assessment of the Tumor Cell (TC) score with a cut-point for positivity at 25% for patient stratification. The second system is free from assumptions related to TC scoring and directly learns patient stratification from the overall survival time and event information. Both systems are built on a novel unpaired domain adaptation deep learning solution for epithelial region segmentation. This approach significantly reduces the need for large pixel-precise manually annotated datasets while superseding serial sectioning or re-staining of slides to obtain ground truth by cytokeratin staining. The capacity of the first system to replicate the TC scoring by pathologists is evaluated on 703 unseen cases, with an addition of 97 cases from an independent cohort. Our results show Lin's concordance values of 0.93 and 0.96 against pathologist scoring, respectively. The ability of the first and second system to stratify anti-PD-L1 treated patients is evaluated on 151 clinical samples. Both systems show similar stratification powers (first system: HR = 0.539, p = 0.004 and second system: HR = 0.525, p = 0.003) compared to TC scoring by pathologists (HR = 0.574, p = 0.01).",2021,10.1109/tmi.2021.3081396,,,,
Domain Progressive 3D Residual Convolution Network to Improve Low-Dose CT Imaging,"The wide applications of X-ray computed tomography (CT) bring low-dose CT (LDCT) into a clinical prerequisite, but reducing the radiation exposure in CT often leads to significantly increased noise and artifacts, which might lower the judgment accuracy of radiologists. In this paper, we put forward a domain progressive 3D residual convolution network (DP-ResNet) for the LDCT imaging procedure that contains three stages: sinogram domain network (SD-net), filtered back projection (FBP), and image domain network (ID-net). Though both are based on the residual network structure, the SD-net and ID-net provide complementary effect on improving the final LDCT quality. The experimental results with both simulated and real projection data show that this domain progressive deep-learning network achieves significantly improved performance by combing the network processing in the two domains.",2019,10.1109/tmi.2019.2917258,cross-sectional,others,CT,NA
Dose to Highly Functional Ventilation Zones Improves Prediction of Radiation Pneumonitis for Proton and Photon Lung Cancer Radiation Therapy,"PURPOSE: We hypothesized that the radiation dose in high-ventilation portions of the lung better predicts radiation pneumonitis (RP) outcome for patients treated with proton radiation therapy (PR) and photon radiation therapy (PH). METHODS AND MATERIALS: Seventy-four patients (38 protons, 36 photons) with locally advanced non-small cell lung cancer treated with concurrent chemoradiation therapy were identified, of whom 24 exhibited RP (graded using Common Terminology Criteria for Adverse Events v4.0) after PR or PH, and 50 were negative controls. The inhale and exhale simulation computed tomography scans were deformed using Advanced Normalization Tools. The 3-dimensional lung ventilation maps were derived from the deformation matrix and partitioned into low- and high-ventilation zones for dosimetric analysis. Receiver operating curve analysis was used to study the power of relationship between RP and ventilation zones to determine an optimal ventilation cutoff. Univariate logistic regression was used to correlate dose in high- and low-ventilation zones with risk of RP. A nonparametric random forest process was used for multivariate importance assessment. RESULTS: The optimal high-ventilation zone definition was determined to be the higher 45% to 60% of the ventilation values. The parameter vV20Gy_high (high ventilation volume receiving ≥20 Gy) was found to be a significant indicator for RP (PH: P = .002, PR: P = .035) with improved areas under the curve compared with the traditional V20Gy for both photon and proton cohorts. The relationship of RP with dose to the low-ventilation zone of the lung was insignificant (PH: P = .123, PR: P = .661). Similar trends were observed for ventilation mean lung dose and ventilation V5Gy. Multivariate importance assessment determined that vV20Gy_high, vV5_high, and mean lung dose were the most significant parameters for the proton cohort with a combined area under the curve of 0.78. CONCLUSION: Dose to the high-ventilated regions of the lung can improve predictions of RP for both PH and PR.",2020,10.1016/j.ijrobp.2020.01.014,,,,
Dosimetric Factors and Radiomics Features Within Different Regions of Interest in Planning CT Images for Improving the Prediction of Radiation Pneumonitis,"PURPOSE: This study aimed to establish machine learning models using dosimetric factors and radiomics features within 5 regions of interest (ROIs) in treatment planning computed tomography images to improve the prediction of symptomatic radiation pneumonitis (RP) (grade ≥2). METHODS AND MATERIALS: This study retrospectively collected data on 79 patients with lung cancer (25 RP ≥2) who underwent chemoradiotherapy between 2015 and 2018. We defined 5 ROIs in planning computed tomography images: gross tumor volume (GTV), planning tumor volume (PTV), PTV-GTV, total lung (TL)-GTV, and TL-PTV. We calculated the mean dose, V5, V10, V20, and V30 within TL-GTV and TL-PTV and the mean dose within the other ROIs. A total of 1924 radiomics features were extracted from all 5 ROIs. We selected the best predictors for classifying 2 groups of patients using a sequential backward elimination support vector machine model. A permutation test was used to assess its statistical significance (P < .05). RESULTS: The best predictors for symptomatic RP were the combination of 11 radiomics features, 5 dosimetric factors, age, and T stage, achieving an area under the curve (AUC) of 0.94 (95% confidence interval [CI], 0.85-1) (accuracy, 90%; sensitivity, 80% [95% CI, 44%-96%]; specificity, 95% [95% CI, 73%-100%]; P = 8 × 10(-4)). The clinical characteristics, dosimetric factors, and their combination showed limited predictive power (accuracy, 63.3%, 70%, and 70%; AUC [95% CI]: 0.73 [0.54-0.92], 0.53 [0.31-0.75], and 0.72 [0.51-0.92], respectively). The radiomics features of PTV-GTV and TL-PTV outperformed those of the other ROIs (accuracy, 76.7% and 76.7%; AUC [95% CI]: 0.82 [0.65-0.99] and 0.80 [0.59-1], respectively). CONCLUSIONS: Combining dosimetric factors and radiomics features within different ROIs can improve the prediction of symptomatic RP. Our results can help physicians adjust the radiation dose distribution of the dose-sensitive lungs and target volumes based on personalized RP estimates.",2021,10.1016/j.ijrobp.2021.01.049,retrospective cohort,prognosis,CT,Lung
Dosimetric Study of Deep Learning-Guided ITV Prediction in Cone-beam CT for Lung Stereotactic Body Radiotherapy,"PURPOSE: The purpose of this study was to evaluate the accuracy of a lung stereotactic body radiotherapy (SBRT) treatment plan with the target of a newly predicted internal target volume (ITV(predict)) and the feasibility of its clinical application. ITV(predict) was automatically generated by our in-house deep learning model according to the cone-beam CT (CBCT) image database. METHOD: A retrospective study of 45 patients who underwent SBRT was involved, and Mask R-CNN based algorithm model helped to predict the internal target volume (ITV) using the CBCT image database. The geometric accuracy of ITV(predict) was verified by the Dice Similarity Coefficient (DSC), 3D Motion Range (R(3D)), Relative Volume Index (RVI), and Hausdorff Distance (HD). The PTV(predict) was generated by ITV(predict), which was registered and then projected on free-breath CT (FBCT) images. The PTV(FBCT) was margined from the GTV on FBCT images gross tumor volume on free-breath CT (GTV(FBCT)). Treatment plans with the target of Predict planning target volume on CBCT images (PTV(predict)) and planning target volume on free-breath CT (PTV(FBCT)) were respectively re-established, and the dosimetric parameters included the ratio of the volume of patients receiving at least the prescribed dose to the volume of PTV (R(100%)), the ratio of the volume of patients receiving at least 50% of the prescribed dose to the volume of PTV in the Radiation Therapy Oncology Group (RTOG) 0813 Trial (R(50%)), Gradient Index (GI), and the maximum dose 2 cm from the PTV (D(2cm)), which were evaluated via Plan(4DCT), plan which based on PTV(predict) (Plan(predict)), and plan which based on PTV(FBCT) (Plan(FBCT)). RESULT: The geometric results showed that there existed a good correlation between ITV(predict) and ITV on the 4-dimensional CT [ITV(4DCT); DSC= 0.83 ±0.18]. However, the average volume of ITV(predict) was 10% less than that of ITV(4DCT) (p = 0.333). No significant difference in dose coverage was found in V(100%) for the ITV with 99.98 ± 0.04% in the ITV(4DCT) vs. 97.56 ± 4.71% in the ITV(predict) (p = 0.162). Dosimetry parameters of PTV, including R(100%), R(50%), GI and D(2cm) showed no statistically significant difference between each plan (p > 0.05). CONCLUSION: Dosimetric parameters of Plan(predict) are clinically comparable to those of the original Plan(4DCT.) This study confirmed that the treatment plan based on ITV(predict) produced by our model could automatically meet clinical requirements. Thus, for patients undergoing lung SBRT, the model has great potential for using CBCT images for ITV contouring which can be used in treatment planning.",2022,10.3389/fpubh.2022.860135,retrospective cohort,treatment,CT,Lung
DR-MIL: deep represented multiple instance learning distinguishes COVID-19 from community-acquired pneumonia in CT images,"BACKGROUND AND OBJECTIVE: Given that the novel coronavirus disease 2019 (COVID-19) has become a pandemic, a method to accurately distinguish COVID-19 from community-acquired pneumonia (CAP) is urgently needed. However, the spatial uncertainty and morphological diversity of COVID-19 lesions in the lungs, and subtle differences with respect to CAP, make differential diagnosis non-trivial. METHODS: We propose a deep represented multiple instance learning (DR-MIL) method to fulfill this task. A 3D volumetric CT scan of one patient is treated as one bag and ten CT slices are selected as the initial instances. For each instance, deep features are extracted from the pre-trained ResNet-50 with fine-tuning and represented as one deep represented instance score (DRIS). Each bag with a DRIS for each initial instance is then input into a citation k-nearest neighbor search to generate the final prediction. A total of 141 COVID-19 and 100 CAP CT scans were used. The performance of DR-MIL is compared with other potential strategies and state-of-the-art models. RESULTS: DR-MIL displayed an accuracy of 95% and an area under curve of 0.943, which were superior to those observed for comparable methods. COVID-19 and CAP exhibited significant differences in both the DRIS and the spatial pattern of lesions (p<0.001). As a means of content-based image retrieval, DR-MIL can identify images used as key instances, references, and citers for visual interpretation. CONCLUSIONS: DR-MIL can effectively represent the deep characteristics of COVID-19 lesions in CT images and accurately distinguish COVID-19 from CAP in a weakly supervised manner. The resulting DRIS is a useful supplement to visual interpretation of the spatial pattern of lesions when screening for COVID-19.",2021,10.1016/j.cmpb.2021.106406,cross-sectional,diagnosis,CT,Lung
Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: a study on 200 patients,"As the whole world is witnessing what novel coronavirus (COVID-19) can do to the mankind, it presents several unique features also. In the absence of specific vaccine for COVID-19, it is essential to detect the disease at an early stage and isolate an infected patient. Till today there is a global shortage of testing labs and testing kits for COVID-19. This paper discusses about the role of machine learning techniques for getting important insights like whether lung computed tomography (CT) scan should be the first screening/alternative test for real-time reverse transcriptase-polymerase chain reaction (RT-PCR), is COVID-19 pneumonia different from other viral pneumonia and if yes how to distinguish it using lung CT scan images from the carefully selected data of lung CT scan COVID-19-infected patients from the hospitals of Italy, China, Moscow and India? For training and testing the proposed system, custom vision software of Microsoft azure based on machine learning techniques is used. An overall accuracy of almost 91% is achieved for COVID-19 classification using the proposed methodology.",2020,10.1007/s11356-020-10133-3,retrospective cohort,diagnosis,CT,Lung
Dual attention multiple instance learning with unsupervised complementary loss for COVID-19 screening,"Chest computed tomography (CT) based analysis and diagnosis of the Coronavirus Disease 2019 (COVID-19) plays a key role in combating the outbreak of the pandemic that has rapidly spread worldwide. To date, the disease has infected more than 18 million people with over 690k deaths reported. Reverse transcription polymerase chain reaction (RT-PCR) is the current gold standard for clinical diagnosis but may produce false positives; thus, chest CT based diagnosis is considered more viable. However, accurate screening is challenging due to the difficulty in annotation of infected areas, curation of large datasets, and the slight discrepancies between COVID-19 and other viral pneumonia. In this study, we propose an attention-based end-to-end weakly supervised framework for the rapid diagnosis of COVID-19 and bacterial pneumonia based on multiple instance learning (MIL). We further incorporate unsupervised contrastive learning for improved accuracy with attention applied both in spatial and latent contexts, herein we propose Dual Attention Contrastive based MIL (DA-CMIL). DA-CMIL takes as input several patient CT slices (considered as bag of instances) and outputs a single label. Attention based pooling is applied to implicitly select key slices in the latent space, whereas spatial attention learns slice spatial context for interpretable diagnosis. A contrastive loss is applied at the instance level to encode similarity of features from the same patient against representative pooled patient features. Empirical results show that our algorithm achieves an overall accuracy of 98.6% and an AUC of 98.4%. Moreover, ablation studies show the benefit of contrastive learning with MIL.",2021,10.1016/j.media.2021.102105,cross-sectional,diagnosis,CT,Lung
Dual energy CT image prediction on primary tumor of lung cancer for nodal metastasis using deep learning,"Lymph node metastasis (LNM) identification is the most clinically important tasks related to survival and recurrence from lung cancer. However, the preoperative prediction of nodal metastasis remains a challenge to determine surgical plans and pretreatment decisions in patients with cancers. We proposed a novel deep prediction method with a size-related damper block for nodal metastasis (Nmet) identification from the primary tumor in lung cancer generated by gemstone spectral imaging (GSI) dual-energy computer tomography (CT). The best model is the proposed method trained by the 40 keV dataset achieves an accuracy of 86 % and a Kappa value of 72 % for Nmet prediction. In the experiment, we have 11 different monochromatic images from 40∼140 keV (the interval is 10 keV) for each patient. When we used the model of 40 keV dataset, there has significant difference in other energy levels (unit of keV). Therefore, we apply in 5-fold cross-validation to explain the lower keV is more efficient to predict Nmet of the primary tumor. The result shows that tumor heterogeneity and size contributed to the proposed model to estimate whether absence or presence of nodal metastasis from the primary tumor.",2021,10.1016/j.compmedimag.2021.101935,retrospective cohort,prognosis,CT,Lung
Dual-branch combination network (DCN): Towards accurate diagnosis and lesion segmentation of COVID-19 using CT images,"The recent global outbreak and spread of coronavirus disease (COVID-19) makes it an imperative to develop accurate and efficient diagnostic tools for the disease as medical resources are getting increasingly constrained. Artificial intelligence (AI)-aided tools have exhibited desirable potential; for example, chest computed tomography (CT) has been demonstrated to play a major role in the diagnosis and evaluation of COVID-19. However, developing a CT-based AI diagnostic system for the disease detection has faced considerable challenges, which is mainly due to the lack of adequate manually-delineated samples for training, as well as the requirement of sufficient sensitivity to subtle lesions in the early infection stages. In this study, we developed a dual-branch combination network (DCN) for COVID-19 diagnosis that can simultaneously achieve individual-level classification and lesion segmentation. To focus the classification branch more intensively on the lesion areas, a novel lesion attention module was developed to integrate the intermediate segmentation results. Furthermore, to manage the potential influence of different imaging parameters from individual facilities, a slice probability mapping method was proposed to learn the transformation from slice-level to individual-level classification. We conducted experiments on a large dataset of 1202 subjects from ten institutes in China. The results demonstrated that 1) the proposed DCN attained a classification accuracy of 96.74% on the internal dataset and 92.87% on the external validation dataset, thereby outperforming other models; 2) DCN obtained comparable performance with fewer samples and exhibited higher sensitivity, especially in subtle lesion detection; and 3) DCN provided good interpretability on the loci of infection compared to other deep models due to its classification guided by high-level semantic information. An online CT-based diagnostic platform for COVID-19 derived from our proposed framework is now available.",2021,10.1016/j.media.2020.101836,retrospective cohort,diagnosis,CT,Lung
Dual-Sampling Attention Network for Diagnosis of COVID-19 From Community Acquired Pneumonia,"The coronavirus disease (COVID-19) is rapidly spreading all over the world, and has infected more than 1,436,000 people in more than 200 countries and territories as of April 9, 2020. Detecting COVID-19 at early stage is essential to deliver proper healthcare to the patients and also to protect the uninfected population. To this end, we develop a dual-sampling attention network to automatically diagnose COVID-19 from the community acquired pneumonia (CAP) in chest computed tomography (CT). In particular, we propose a novel online attention module with a 3D convolutional network (CNN) to focus on the infection regions in lungs when making decisions of diagnoses. Note that there exists imbalanced distribution of the sizes of the infection regions between COVID-19 and CAP, partially due to fast progress of COVID-19 after symptom onset. Therefore, we develop a dual-sampling strategy to mitigate the imbalanced learning. Our method is evaluated (to our best knowledge) upon the largest multi-center CT data for COVID-19 from 8 hospitals. In the training-validation stage, we collect 2186 CT scans from 1588 patients for a 5-fold cross-validation. In the testing stage, we employ another independent large-scale testing dataset including 2796 CT scans from 2057 patients. Results show that our algorithm can identify the COVID-19 images with the area under the receiver operating characteristic curve (AUC) value of 0.944, accuracy of 87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With this performance, the proposed algorithm could potentially aid radiologists with COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19 outbreak.",2020,10.1109/tmi.2020.2995508,cross-sectional,diagnosis,CT,Lung
DUDA-Net: a double U-shaped dilated attention network for automatic infection area segmentation in COVID-19 lung CT images,"PURPOSE: The global health crisis caused by coronavirus disease 2019 (COVID-19) is a common threat facing all humankind. In the process of diagnosing COVID-19 and treating patients, automatic COVID-19 lesion segmentation from computed tomography images helps doctors and patients intuitively understand lung infection. To effectively quantify lung infections, a convolutional neural network for automatic lung infection segmentation based on deep learning is proposed. METHOD: This new type of COVID-19 lesion segmentation network is based on a U-Net backbone. First, a coarse segmentation network is constructed to extract the lung areas. Second, in the encoding and decoding process of the fine segmentation network, a new soft attention mechanism, namely the dilated convolutional attention (DCA) mechanism, is introduced to enable the network to focus on better quantitative information to strengthen the network's segmentation ability in the subtle areas of the lesions. RESULTS: The experimental results show that the average Dice similarity coefficient (DSC), sensitivity (SEN), specificity (SPE) and area under the curve of DUDA-Net are 87.06%, 90.85%, 99.59% and 0.965, respectively. In addition, the introduction of a cascade U-shaped network scheme and DCA mechanism can improve the DSC by 24.46% and 14.33%, respectively. CONCLUSION: The proposed DUDA-Net approach can automatically segment COVID-19 lesions with excellent performance, which indicates that the proposed method is of great clinical significance. In addition, the introduction of a coarse segmentation network and DCA mechanism can improve the COVID-19 segmentation performance.",2021,10.1007/s11548-021-02418-w,cross-sectional,diagnosis,CT,Lung
Dynamic deformable attention network (DDANet) for COVID-19 lesions semantic segmentation,"Deep learning based medical image segmentation is an important step within diagnosis, which relies strongly on capturing sufficient spatial context without requiring too complex models that are hard to train with limited labelled data. Training data is in particular scarce for segmenting infection regions of CT images of COVID-19 patients. Attention models help gather contextual information within deep networks and benefit semantic segmentation tasks. The recent criss-cross-attention module aims to approximate global self-attention while remaining memory and time efficient by separating horizontal and vertical self-similarity computations. However, capturing attention from all non-local locations can adversely impact the accuracy of semantic segmentation networks. We propose a new Dynamic Deformable Attention Network (DDANet) that enables a more accurate contextual information computation in a similarly efficient way. Our novel technique is based on a deformable criss-cross attention block that learns both attention coefficients and attention offsets in a continuous way. A deep U-Net (Schlemper et al., 2019) segmentation network that employs this attention mechanism is able to capture attention from pertinent non-local locations and also improves the performance on semantic segmentation tasks compared to criss-cross attention within a U-Net on a challenging COVID-19 lesion segmentation task. Our validation experiments show that the performance gain of the recursively applied dynamic deformable attention blocks comes from their ability to capture dynamic and precise attention context. Our DDANet achieves Dice scores of 73.4% and 61.3% for Ground-glass opacity and consolidation lesions for COVID-19 segmentation and improves the accuracy by 4.9% points compared to a baseline U-Net and 24.4% points compared to current state of art methods (Fan et al., 2020).",2021,10.1016/j.jbi.2021.103816,cross-sectional,diagnosis,CT,Lung
Dynamic evaluation of lung involvement during coronavirus disease-2019 (COVID-19) with quantitative lung CT,"PURPOSE: To identify and quantify lung changes associated with coronavirus disease-2019 (COVID-19) with quantitative lung CT during the disease. METHODS: This retrospective study reviewed COVID-19 patients who underwent multiple chest CT scans during their disease course. Quantitative lung CT was used to determine the nature and volume of lung involvement. A semi-quantitative scoring system was also used to evaluate lung lesions. RESULTS: This study included eighteen cases (4 cases in mild type, 10 cases in moderate type, 4 cases in severe type, and without critical type cases) with confirmed COVID-19. Patients had a mean hospitalized period of 24.1 ± 7.1 days (range: 14-38 days) and underwent an average CT scans of 3.9 ± 1.6 (range: 2-8). The total volumes of lung abnormalities reached a peak of 8.8 ± 4.1 days (range: 2-14 days). The ground-glass opacity (GGO) volume percentage was higher than the consolidative opacity (CO) volume percentage on the first CT examination (Z = 2.229, P = 0.026), and there was no significant difference between the GGO volume percentage and that of CO at the peak stage (Z = - 0.628, P = 0.53). The volume percentage of lung involvement identified by AI demonstrated a strong correlation with the total CT scores at each stage (r = 0.873, P = 0.0001). CONCLUSIONS: Quantitative lung CT can automatically identify the nature of lung involvement and quantify the dynamic changes of lung lesions on CT during COVID-19. For patients who recovered from COVID-19, GGO was the predominant imaging feature on the initial CT scan, while GGO and CO were the main appearances at peak stage.",2020,10.1007/s10140-020-01856-4,,,,
Dynamic evolution of COVID-19 on chest computed tomography: experience from Jiangsu Province of China,"OBJECTIVES: To determine the patterns of chest computed tomography (CT) evolution according to disease severity in a large coronavirus disease 2019 (COVID-19) cohort in Jiangsu Province, China. METHODS: This retrospective cohort study was conducted from January 10, 2020, to February 18, 2020. All patients diagnosed with COVID-19 in Jiangsu Province were included, retrospectively. Quantitative CT measurements of pulmonary opacities including volume, density, and location were extracted by deep learning algorithm. Dynamic evolution of these measurements was investigated from symptom onset (day 1) to beyond day 15. Comparison was made between severity groups. RESULTS: A total of 484 patients (median age of 47 years, interquartile range 33-57) with 954 CT examinations were included, and each was assigned to one of the three groups: asymptomatic/mild (n = 63), moderate (n = 378), severe/critically ill (n = 43). Time series showed different evolution patterns of CT measurements in the groups. Following disease onset, posteroinferior subpleural area of the lung was the most common location for pulmonary opacities. Opacity volume continued to increase beyond 15 days in the severe/critically ill group, compared with peaking on days 13-15 in the moderate group. Asymptomatic/mild group had the lowest opacity volume which almost resolved after 15 days. The opacity density began to drop from day 10 to day 12 for moderately ill patients. CONCLUSIONS: Volume, density, and location of the pulmonary opacity and their evolution on CT varied with disease severity in COVID-19. These findings are valuable in understanding the nature of the disease and monitoring the patient's condition during the course of illness. KEY POINTS: • Volume, density, and location of the pulmonary opacity on CT change over time in COVID-19. • The evolution of CT appearance follows specific pattern, varying with disease severity.",2020,10.1007/s00330-020-06976-6,,,,
Dynamic Profiling of β-Coronavirus 3CL M(pro) Protease Ligand-Binding Sites,"β-coronavirus (CoVs) alone has been responsible for three major global outbreaks in the 21st century. The current crisis has led to an urgent requirement to develop therapeutics. Even though a number of vaccines are available, alternative strategies targeting essential viral components are required as a backup against the emergence of lethal viral variants. One such target is the main protease (M(pro)) that plays an indispensable role in viral replication. The availability of over 270 M(pro) X-ray structures in complex with inhibitors provides unique insights into ligand-protein interactions. Herein, we provide a comprehensive comparison of all nonredundant ligand-binding sites available for SARS-CoV2, SARS-CoV, and MERS-CoV M(pro). Extensive adaptive sampling has been used to investigate structural conservation of ligand-binding sites using Markov state models (MSMs) and compare conformational dynamics employing convolutional variational auto-encoder-based deep learning. Our results indicate that not all ligand-binding sites are dynamically conserved despite high sequence and structural conservation across β-CoV homologs. This highlights the complexity in targeting all three M(pro) enzymes with a single pan inhibitor.",2021,10.1021/acs.jcim.1c00449,,,,
E-TBNet: Light Deep Neural Network for Automatic Detection of Tuberculosis with X-ray DR Imaging,"Currently, the tuberculosis (TB) detection model based on chest X-ray images has the problem of excessive reliance on hardware computing resources, high equipment performance requirements, and being harder to deploy in low-cost personal computer and embedded devices. An efficient tuberculosis detection model is proposed to achieve accurate, efficient, and stable tuberculosis screening on devices with lower hardware levels. Due to the particularity of the chest X-ray images of TB patients, there are fewer labeled data, and the deep neural network model is difficult to fully train. We first analyzed the data distribution characteristics of two public TB datasets, and found that the two-stage tuberculosis identification (first divide, then classify) is insufficient. Secondly, according to the particularity of the detection image(s), the basic residual module was optimized and improved, and this is regarded as a crucial component of this article's network. Finally, an efficient attention mechanism was introduced, which was used to fuse the channel features. The network architecture was optimally designed and adjusted according to the correct and sufficient experimental content. In order to evaluate the performance of the network, it was compared with other lightweight networks under personal computer and Jetson Xavier embedded devices. The experimental results show that the recall rate and accuracy of the E-TBNet proposed in this paper are better than those of classic lightweight networks such as SqueezeNet and ShuffleNet, and it also has a shorter reasoning time. E-TBNet will be more advantageous to deploy on equipment with low levels of hardware.",2022,10.3390/s22030821,cross-sectional,diagnosis,CXR,Lung
"Early detection of COVID-19 in the UK using self-reported symptoms: a large-scale, prospective, epidemiological surveillance study","BACKGROUND: Self-reported symptoms during the COVID-19 pandemic have been used to train artificial intelligence models to identify possible infection foci. To date, these models have only considered the culmination or peak of symptoms, which is not suitable for the early detection of infection. We aimed to estimate the probability of an individual being infected with SARS-CoV-2 on the basis of early self-reported symptoms to enable timely self-isolation and urgent testing. METHODS: In this large-scale, prospective, epidemiological surveillance study, we used prospective, observational, longitudinal, self-reported data from participants in the UK on 19 symptoms over 3 days after symptoms onset and COVID-19 PCR test results extracted from the COVID-19 Symptom Study mobile phone app. We divided the study population into a training set (those who reported symptoms between April 29, 2020, and Oct 15, 2020) and a test set (those who reported symptoms between Oct 16, 2020, and Nov 30, 2020), and used three models to analyse the self-reported symptoms: the UK's National Health Service (NHS) algorithm, logistic regression, and the hierarchical Gaussian process model we designed to account for several important variables (eg, specific COVID-19 symptoms, comorbidities, and clinical information). Model performance to predict COVID-19 positivity was compared in terms of sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) in the test set. For the hierarchical Gaussian process model, we also evaluated the relevance of symptoms in the early detection of COVID-19 in population subgroups stratified according to occupation, sex, age, and body-mass index. FINDINGS: The training set comprised 182 991 participants and the test set comprised 15 049 participants. When trained on 3 days of self-reported symptoms, the hierarchical Gaussian process model had a higher prediction AUC (0·80 [95% CI 0·80-0·81]) than did the logistic regression model (0·74 [0·74-0·75]) and the NHS algorithm (0·67 [0·67-0·67]). AUCs for all models increased with the number of days of self-reported symptoms, but were still high for the hierarchical Gaussian process model at day 1 (0·73 [95% CI 0·73-0·74]) and day 2 (0·79 [0·78-0·79]). At day 3, the hierarchical Gaussian process model also had a significantly higher sensitivity, but a non-statistically lower specificity, than did the two other models. The hierarchical Gaussian process model also identified different sets of relevant features to detect COVID-19 between younger and older subgroups, and between health-care workers and non-health-care workers. When used during different pandemic periods, the model was robust to changes in populations. INTERPRETATION: Early detection of SARS-CoV-2 infection is feasible with our model. Such early detection is crucial to contain the spread of COVID-19 and efficiently allocate medical resources. FUNDING: ZOE, the UK Government Department of Health and Social Care, the Wellcome Trust, the UK Engineering and Physical Sciences Research Council, the UK National Institute for Health Research, the UK Medical Research Council, the British Heart Foundation, the Alzheimer's Society, the Chronic Disease Research Foundation, and the Massachusetts Consortium on Pathogen Readiness.",2021,10.1016/s2589-7500(21)00131-x,,,,
Early Detection of Lung Cancer Using DNA Promoter Hypermethylation in Plasma and Sputum,"Purpose: CT screening can reduce death from lung cancer. We sought to improve the diagnostic accuracy of lung cancer screening using ultrasensitive methods and a lung cancer-specific gene panel to detect DNA methylation in sputum and plasma.Experimental Design: This is a case-control study of subjects with suspicious nodules on CT imaging. Plasma and sputum were obtained preoperatively. Cases (n = 150) had pathologic confirmation of node-negative (stages I and IIA) non-small cell lung cancer. Controls (n = 60) had non-cancer diagnoses. We detected promoter methylation using quantitative methylation-specific real-time PCR and methylation-on-beads for cancer-specific genes (SOX17, TAC1, HOXA7, CDO1, HOXA9, and ZFP42).Results: DNA methylation was detected in plasma and sputum more frequently in people with cancer compared with controls (P < 0.001) for five of six genes. The sensitivity and specificity for lung cancer diagnosis using the best individual genes was 63% to 86% and 75% to 92% in sputum, respectively, and 65% to 76% and 74% to 84% in plasma, respectively. A three-gene combination of the best individual genes has sensitivity and specificity of 98% and 71% using sputum and 93% and 62% using plasma. Area under the receiver operating curve for this panel was 0.89 [95% confidence interval (CI), 0.80-0.98] in sputum and 0.77 (95% CI, 0.68-0.86) in plasma. Independent blinded random forest prediction models combining gene methylation with clinical information correctly predicted lung cancer in 91% of subjects using sputum detection and 85% of subjects using plasma detection.Conclusions: High diagnostic accuracy for early-stage lung cancer can be obtained using methylated promoter detection in sputum or plasma. Clin Cancer Res; 23(8); 1998-2005. ©2016 AACR.",2017,10.1158/1078-0432.Ccr-16-1371,,,,
"Early prediction of in-hospital death of COVID-19 patients: a machine-learning model based on age, blood analyses, and chest x-ray score","An early-warning model to predict in-hospital mortality on admission of COVID-19 patients at an emergency department (ED) was developed and validated using a machine-learning model. In total, 2782 patients were enrolled between March 2020 and December 2020, including 2106 patients (first wave) and 676 patients (second wave) in the COVID-19 outbreak in Italy. The first-wave patients were divided into two groups with 1474 patients used to train the model, and 632 to validate it. The 676 patients in the second wave were used to test the model. Age, 17 blood analytes, and Brescia chest X-ray score were the variables processed using a random forests classification algorithm to build and validate the model. Receiver operating characteristic (ROC) analysis was used to assess the model performances. A web-based death-risk calculator was implemented and integrated within the Laboratory Information System of the hospital. The final score was constructed by age (the most powerful predictor), blood analytes (the strongest predictors were lactate dehydrogenase, D-dimer, neutrophil/lymphocyte ratio, C-reactive protein, lymphocyte %, ferritin std, and monocyte %), and Brescia chest X-ray score (https://bdbiomed.shinyapps.io/covid19score/). The areas under the ROC curve obtained for the three groups (training, validating, and testing) were 0.98, 0.83, and 0.78, respectively. The model predicts in-hospital mortality on the basis of data that can be obtained in a short time, directly at the ED on admission. It functions as a web-based calculator, providing a risk score which is easy to interpret. It can be used in the triage process to support the decision on patient allocation.",2021,10.7554/eLife.70640,retrospective cohort,prognosis,CXR,Lung
Early prediction of level-of-care requirements in patients with COVID-19,"This study examined records of 2566 consecutive COVID-19 patients at five Massachusetts hospitals and sought to predict level-of-care requirements based on clinical and laboratory data. Several classification methods were applied and compared against standard pneumonia severity scores. The need for hospitalization, ICU care, and mechanical ventilation were predicted with a validation accuracy of 88%, 87%, and 86%, respectively. Pneumonia severity scores achieve respective accuracies of 73% and 74% for ICU care and ventilation. When predictions are limited to patients with more complex disease, the accuracy of the ICU and ventilation prediction models achieved accuracy of 83% and 82%, respectively. Vital signs, age, BMI, dyspnea, and comorbidities were the most important predictors of hospitalization. Opacities on chest imaging, age, admission vital signs and symptoms, male gender, admission laboratory results, and diabetes were the most important risk factors for ICU admission and mechanical ventilation. The factors identified collectively form a signature of the novel COVID-19 disease.",2020,10.7554/eLife.60519,,,,
Early prediction of severity in coronavirus disease (COVID-19) using quantitative CT imaging,"PURPOSE: To evaluate whether the extent of COVID-19 pneumonia on CT scans using quantitative CT imaging obtained early in the illness can predict its future severity. METHODS: We conducted a retrospective single-center study on confirmed COVID-19 patients between January 18, 2020 and March 5, 2020. A quantitative AI algorithm was used to evaluate each patient's CT scan to determine the proportion of the lungs with pneumonia (VR) and the rate of change (RAR) in VR from scan to scan. Patients were classified as being in the severe or non-severe group based on their final symptoms. Penalized B-splines regression modeling was used to examine the relationship between mean VR and days from onset of symptoms in the two groups, with 95% and 99% confidence intervals. RESULTS: Median VR max was 18.6% (IQR 9.1-32.7%) in 21 patients in the severe group, significantly higher (P < 0.0001) than in the 53 patients in non-severe group (1.8% (IQR 0.4-5.7%)). RAR was increasing with a median RAR of 2.1% (IQR 0.4-5.5%) in severe and 0.4% (IQR 0.1-0.9%) in non-severe group, which was significantly different (P < 0.0001). Penalized B-spline analyses showed positive relationships between VR and days from onset of symptom. The 95% confidence limits of the predicted means for the two groups diverged 5 days after the onset of initial symptoms with a threshold of 11.9%. CONCLUSION: Five days after the initial onset of symptoms, CT could predict the patients who later developed severe symptoms with 95% confidence.",2021,10.1016/j.clinimag.2021.02.003,retrospective cohort,prognosis,CT,Lung
Early spatiotemporal-specific changes in intermediate signals are predictive of cytotoxic sensitivity to TNFα and co-treatments,"Signaling pathways can generate different cellular responses to the same cytotoxic agents. Current quantitative models for predicting these differential responses are usually based on large numbers of intracellular gene products or signals at different levels of signaling cascades. Here, we report a study to predict cellular sensitivity to tumor necrosis factor alpha (TNFα) using high-throughput cellular imaging and machine-learning methods. We measured and compared 1170 protein phosphorylation events in a panel of human lung cancer cell lines based on different signals, subcellular regions, and time points within one hour of TNFα treatment. We found that two spatiotemporal-specific changes in an intermediate signaling protein, p90 ribosomal S6 kinase (RSK), are sufficient to predict the TNFα sensitivity of these cell lines. Our models could also predict the combined effects of TNFα and other kinase inhibitors, many of which are not known to target RSK directly. Therefore, early spatiotemporal-specific changes in intermediate signals are sufficient to represent the complex cellular responses to these perturbations. Our study provides a general framework for the development of rapid, signaling-based cytotoxicity screens that may be used to predict cellular sensitivity to a cytotoxic agent, or identify co-treatments that may sensitize or desensitize cells to the agent.",2017,10.1038/srep43541,,,,
Early survival prediction in non-small cell lung cancer from PET/CT images using an intra-tumor partitioning method,"PURPOSE: To explore prognostic and predictive values of a novel quantitative feature set describing intra-tumor heterogeneity in patients with lung cancer treated with concurrent and sequential chemoradiotherapy. METHODS: Longitudinal PET-CT images of 30 patients with non-small cell lung cancer were analysed. To describe tumor cell heterogeneity, the tumors were partitioned into one to ten concentric regions depending on their sizes, and, for each region, the change in average intensity between the two scans was calculated for PET and CT images separately to form the proposed feature set. To validate the prognostic value of the proposed method, radiomics analysis was performed and a combination of the proposed novel feature set and the classic radiomic features was evaluated. A feature selection algorithm was utilized to identify the optimal features, and a linear support vector machine was trained for the task of overall survival prediction in terms of area under the receiver operating characteristic curve (AUROC). RESULTS: The proposed novel feature set was found to be prognostic and even outperformed the radiomics approach with a significant difference (AUROC(SALoP) = 0.90 vs. AUROC(radiomic) = 0.71) when feature selection was not employed, whereas with feature selection, a combination of the novel feature set and radiomics led to the highest prognostic values. CONCLUSION: A novel feature set designed for capturing intra-tumor heterogeneity was introduced. Judging by their prognostic power, the proposed features have a promising potential for early survival prediction.",2019,10.1016/j.ejmp.2019.03.024,retrospective cohort,prognosis,PET/CT,Lung
Early tumor response prediction for lung cancer patients using novel longitudinal pattern features from sequential PET/CT image scans,"PURPOSE: A new set of quantitative features that capture intensity changes in PET/CT images over time and space is proposed for assessing the tumor response early during chemoradiotherapy. The hypothesis whether the new features, combined with machine learning, improve outcome prediction is tested. METHODS: The proposed method is based on dividing the tumor volume into successive zones depending on the distance to the tumor border. Mean intensity changes are computed within each zone, for CT and PET scans separately, and used as image features for tumor response assessment. Doing so, tumors are described by accounting for temporal and spatial changes at the same time. Using linear support vector machines, the new features were tested on 30 non-small cell lung cancer patients who underwent sequential or concurrent chemoradiotherapy. Prediction of 2-years overall survival was based on two PET-CT scans, acquired before the start and during the first 3 weeks of treatment. The predictive power of the newly proposed longitudinal pattern features was compared to that of previously proposed radiomics features and radiobiological parameters. RESULTS: The highest areas under the receiver operating characteristic curves were 0.98 and 0.93 for patients treated with sequential and concurrent chemoradiotherapy, respectively. Results showed an overall comparable performance with respect to radiomics features and radiobiological parameters. CONCLUSIONS: A novel set of quantitative image features, based on underlying tumor physiology, was computed from PET/CT scans and successfully employed to distinguish between early responders and non-responders to chemoradiotherapy.",2018,10.1016/j.ejmp.2018.09.003,retrospective cohort,prognosis,PET/CT,Lung
ECM-CSD: An Efficient Classification Model for Cancer Stage Diagnosis in CT Lung Images Using FCM and SVM Techniques,"As is eminent, lung cancer is one of the death frightening syndromes among people in present cases. The earlier diagnosis and treatment of lung cancer can increase the endurance rate of the affected people. But, the structure of the cancer cell makes the diagnosis process more challenging, in which the most of the cells are superimposed. By adopting the efficient image processing techniques, the diagnosis process can be made effective, earlier and accurate, where the time aspect is extremely decisive. With those considerations, the main objective of this work is to propose a region based Fuzzy C-Means Clustering (FCM) technique for segmenting the lung cancer region and the Support Vector Machine (SVM) based classification for diagnosing the cancer stage, which helps in clinical practice in significant way to increase the morality rate. Moreover, the proposed ECM-CSD (Efficient Classification Model for Cancer Stage Diagnosis) uses Computed Tomography (CT) lung images for processing, since it poses higher imaging sensitivity, resolution with good isotopic acquisition in lung nodule identification. With those images, the pre-processing has been made with Gaussian Filter for smoothing and Gabor Filter for enhancement. Following, based on the extracted image features, the effective segmentation of lung nodules is performed using the FCM based clustering. And, the stages of cancer are identified based on the SVM classification technique. Further, the model is analyzed with MATLAB tool by incorporating the LIDC-IDRI lung CT images clinical dataset. The comparative experiments show the efficiency of the proposed model in terms of the performance evaluation factors like increased accuracy and reduced error rate.",2019,10.1007/s10916-019-1190-z,cross-sectional,diagnosis,CT,Lung
Effect of CT image acquisition parameters on diagnostic performance of radiomics in predicting malignancy of pulmonary nodules of different sizes,"OBJECTIVES: To investigate the effect of CT image acquisition parameters on the performance of radiomics in classifying benign and malignant pulmonary nodules (PNs) with respect to nodule size. METHODS: We retrospectively collected CT images of 696 patients with PNs from March 2015 to March 2018. PNs were grouped by nodule diameter: T1a (diameter ≤ 1.0 cm), T1b (1.0 cm < diameter ≤ 2.0 cm), and T1c (2.0 cm < diameter ≤ 3.0 cm). CT images were divided into four settings according to slice-thickness-convolution-kernels: setting 1 (slice thickness/reconstruction type: 1.25 mm sharp), setting 2 (5 mm sharp), setting 3 (5 mm smooth), and random setting. We created twelve groups from two interacting conditions. Each PN was segmented and had 1160 radiomics features extracted. Non-redundant features with high predictive ability in training were selected to build a distinct model under each of the twelve subsets. RESULTS: The performance (AUCs) on predicting PN malignancy were as follows: T1a group: 0.84, 0.64, 0.68, and 0.68; T1b group: 0.68, 0.74, 0.76, and 0.70; T1c group: 0.66, 0.64, 0.63, and 0.70, for the setting 1, setting 2, setting 3, and random setting, respectively. In the T1a group, the AUC of radiomics model in setting 1 was statistically significantly higher than all others; In the T1b group, AUCs of radiomics models in setting 3 were statistically significantly higher than some; and in the T1c group, there were no statistically significant differences among models. CONCLUSIONS: For PNs less than 1 cm, CT image acquisition parameters have a significant influence on diagnostic performance of radiomics in predicting malignancy, and a model created using images reconstructed with thin section and a sharp kernel algorithm achieved the best performance. For PNs larger than 1 cm, CT reconstruction parameters did not affect diagnostic performance substantially. KEY POINTS: • CT image acquisition parameters have a significant influence on the diagnostic performance of radiomics in pulmonary nodules less than 1 cm. • In pulmonary nodules less than 1 cm, a radiomics model created by using images reconstructed with thin section and a sharp kernel algorithm achieved the best diagnostic performance. • For PNs larger than 1 cm, CT image acquisition parameters do not affect diagnostic performance substantially.",2022,10.1007/s00330-021-08274-1,cross-sectional,diagnosis,CT,Lung
Effect of CT Reconstruction Algorithm on the Diagnostic Performance of Radiomics Models: A Task-Based Approach for Pulmonary Subsolid Nodules,"OBJECTIVE: We investigated whether the diagnostic performance of machine learning-based radiomics models for the discrimination of invasive pulmonary adenocarcinomas (IPAs) among subsolid nodules (SSNs) was affected by the proportion of images reconstructed with filtered back projection (FBP) and model-based iterative reconstruction (MBIR) in datasets used for feature extraction. MATERIALS AND METHODS: This retrospective study included 60 patients (23 men and 37 women; mean age, 61.4 years) with 69 SSNs (54 part-solid and 15 pure ground-glass nodules). Preoperative CT scans were reconstructed with both FBP and MBIR. A total of 860 radiomics features were obtained from the entire nodule volume, and 70 resampled nodule datasets with an increasing proportion of nodules with MBIR-derived features (from 0/69 to 69/69) were prepared. After feature selection using neighborhood component analysis, support vector machines (SVMs) and an ensemble model were used as classifiers for the differentiation of IPAs. The diagnostic performances of all blending proportions of reconstruction algorithms were calculated and analyzed. RESULTS: The ROC AUC and the diagnostic accuracy of the radiomics models decreased significantly as the number of nodules with MBIR-derived features increased, and this relationship followed cubic functions (R(2) = 0.993 and 0.926 for SVM; R(2) = 0.993 and 0.975 for the ensemble model; p < 0.001). The magnitude of variation in AUC due to the reconstruction algorithm heterogeneity was 0.39 for SVM and 0.39 for the ensemble model. CONCLUSION: Inclusion of CT scans reconstructed with MBIR for radiomics modeling can significantly decrease diagnostic performance for the identification of IPAs.",2019,10.2214/ajr.18.20018,cross-sectional,diagnosis,CT,Lung
Effect of CT reconstruction settings on the performance of a deep learning based lung nodule CAD system,"PURPOSE: To study the effect of different reconstruction parameter settings on the performance of a commercially available deep learning based pulmonary nodule CAD system. MATERIALS AND METHODS: We performed a retrospective analysis of 24 chest CT scans, reconstructed at 16 different reconstruction settings for two different iterative reconstruction algorithms (SAFIRE and ADMIRE) varying in slice thickness, kernel size and iterative reconstruction level strength using a commercially available deep learning pulmonary nodule CAD system. The DL-CAD software was evaluated at 25 different sensitivity threshold settings and nodules detected by the DL-CAD software were matched against a reference standard based on the consensus reading of three radiologists. RESULTS: A total of 384 CT reconstructions was analysed from 24 patients, resulting in a total of 5786 found nodules. We matched the detected nodules against the reference standard, defined by a team of thoracic radiologists, and showed a gradual drop in recall, and an improvement in precision when the iterative strength levels were increased for a constant kernel size. The optimal DL-CAD threshold setting for use in our clinical workflow was found to be 0.88 with an F(2) of 0.73 ± 0.053. CONCLUSIONS: The DL-CAD system behaves differently on IR data than on FBP data, there is a gradual drop in recall, and growth in precision when the iterative strength levels are increased. As a result, caution should be taken when implementing deep learning software in a hospital with multiple CT scanners and different reconstruction protocols. To the best of our knowledge, this is the first study that demonstrates this result from a DL-CAD system on clinical data.",2021,10.1016/j.ejrad.2021.109526,cross-sectional,diagnosis,CT,Lung
Effect of machine learning methods on predicting NSCLC overall survival time based on Radiomics analysis,"BACKGROUND: To investigate the effect of machine learning methods on predicting the Overall Survival (OS) for non-small cell lung cancer based on radiomics features analysis. METHODS: A total of 339 radiomic features were extracted from the segmented tumor volumes of pretreatment computed tomography (CT) images. These radiomic features quantify the tumor phenotypic characteristics on the medical images using tumor shape and size, the intensity statistics and the textures. The performance of 5 feature selection methods and 8 machine learning methods were investigated for OS prediction. The predicted performance was evaluated with concordance index between predicted and true OS for the non-small cell lung cancer patients. The survival curves were evaluated by the Kaplan-Meier algorithm and compared by the log-rank tests. RESULTS: The gradient boosting linear models based on Cox's partial likelihood method using the concordance index feature selection method obtained the best performance (Concordance Index: 0.68, 95% Confidence Interval: 0.62~ 0.74). CONCLUSIONS: The preliminary results demonstrated that certain machine learning and radiomics analysis method could predict OS of non-small cell lung cancer accuracy.",2018,10.1186/s13014-018-1140-9,cross-sectional,prognosis,CT,Lung
Effective and Reliable Framework for Lung Nodules Detection from CT Scan Images,"Lung cancer is considered more serious among other prevailing cancer types. One of the reasons for it is that it is usually not diagnosed until it has spread and by that time it becomes very difficult to treat. Early detection of lung cancer can significantly increase the chances of survival of a cancer patient. An effective nodule detection system can play a key role in early detection of lung cancer thus increasing the chances of successful treatment. In this research work, we have proposed a novel classification framework for nodule classification. The framework consists of multiple phases that include image contrast enhancement, segmentation, optimal feature extraction, followed by employment of these features for training and testing of Support Vector Machine. We have empirically tested the efficacy of our technique by utilizing the well-known Lung Image Consortium Database (LIDC) dataset. The empirical results suggest that the technique is highly effective for reducing the false positive rates. We were able to receive an impressive sensitivity rate of 97.45%.",2019,10.1038/s41598-019-41510-9,cross-sectional,diagnosis,CT,Lung
Effective deep learning approaches for predicting COVID-19 outcomes from chest computed tomography volumes,"The rapid evolution of the novel coronavirus disease (COVID-19) pandemic has resulted in an urgent need for effective clinical tools to reduce transmission and manage severe illness. Numerous teams are quickly developing artificial intelligence approaches to these problems, including using deep learning to predict COVID-19 diagnosis and prognosis from chest computed tomography (CT) imaging data. In this work, we assess the value of aggregated chest CT data for COVID-19 prognosis compared to clinical metadata alone. We develop a novel patient-level algorithm to aggregate the chest CT volume into a 2D representation that can be easily integrated with clinical metadata to distinguish COVID-19 pneumonia from chest CT volumes from healthy participants and participants with other viral pneumonia. Furthermore, we present a multitask model for joint segmentation of different classes of pulmonary lesions present in COVID-19 infected lungs that can outperform individual segmentation models for each task. We directly compare this multitask segmentation approach to combining feature-agnostic volumetric CT classification feature maps with clinical metadata for predicting mortality. We show that the combination of features derived from the chest CT volumes improve the AUC performance to 0.80 from the 0.52 obtained by using patients' clinical data alone. These approaches enable the automated extraction of clinically relevant features from chest CT volumes for risk stratification of COVID-19 patients.",2022,10.1038/s41598-022-05532-0,retrospective cohort,prognosis,CT,Lung
Effectiveness of convolutional neural networks in the interpretation of pulmonary cytologic images in endobronchial ultrasound procedures,"BACKGROUND: Rapid on-site cytologic evaluation (ROSE) helps to improve the diagnostic accuracy in endobronchial ultrasound (EBUS) procedures. However, cytologists are seldom available to perform ROSE in many institutions. Recent studies have investigated the application of deep learning in cytologic image analysis. As such, the present study analyzed lung cytologic images obtained by EBUS procedures, and employed deep-learning methods to distinguish between benign and malignant cells and to semantically segment malignant cells. METHODS: Ninety-seven patients who underwent 104 EBUS procedures were enrolled. Four hundred and ninety-nine lung cytologic images obtained via ROSE, including 425 malignant and 74 benign, and most malignant were lung adenocarcinoma (64.3%). All the images were used to train a residual network model with 101 layers (ResNet101), with suitable hyperparameters selected to classify benign and malignant lung cytologic images. An HRNet model was also employed to mark the area of malignant cells. Automatic patch-cropping was adopted to facilitate dataset preparation. RESULTS: Malignant cells were successfully classified by ResNet101 with 98.8% classification accuracy, 98.8% sensitivity, and 98.8% specificity in patch-based classification; 95.5% classification accuracy in image-based classification; and 92.9% classification accuracy in patient-based classification. Malignant cell area was successfully marked by HRNet with a mean intersection over union of 89.2%. The automatic cropping method enabled the system to complete diagnosis within 1 s. CONCLUSIONS: This is the first study to combine lung cytologic image deep-learning classification with semantic segmentation. The model was optimized for high accuracy and the automatic cropping facilitates the clinical application of our model. The success in both lung cytologic images classification and semantic segmentation on our dataset shows a promising result for clinical application in the future.",2021,10.1002/cam4.4383,,,,
Effects of sample size and data augmentation on U-Net-based automatic segmentation of various organs,"Deep learning has demonstrated high efficacy for automatic segmentation in contour delineation, which is crucial in radiation therapy planning. However, the collection, labeling, and management of medical imaging data can be challenging. This study aims to elucidate the effects of sample size and data augmentation on the automatic segmentation of computed tomography images using U-Net, a deep learning method. For the chest and pelvic regions, 232 and 556 cases are evaluated, respectively. We investigate multiple conditions by changing the sum of the training and validation datasets across a broad range of values: 10-200 and 10-500 cases for the chest and pelvic regions, respectively. A U-Net is constructed, and horizontal-flip data augmentation, which produces left and right inverse images resulting in twice the number of images, is compared with no augmentation for each training session. All lung cases and more than 100 prostate, bladder, and rectum cases indicate that adding horizontal-flip data augmentation is almost as effective as doubling the number of cases. The slope of the Dice similarity coefficient (DSC) in all organs decreases rapidly until approximately 100 cases, stabilizes after 200 cases, and shows minimal changes as the number of cases is increased further. The DSCs stabilize at a smaller sample size with the incorporation of data augmentation in all organs except the heart. This finding is applicable to the automation of radiation therapy for rare cancers, where large datasets may be difficult to obtain.",2021,10.1007/s12194-021-00630-6,,,,
Efficiency of a computer-aided diagnosis (CAD) system with deep learning in detection of pulmonary nodules on 1-mm-thick images of computed tomography,"PURPOSE: To evaluate the performance of a deep learning-based computer-aided diagnosis (CAD) system at detecting pulmonary nodules on CT by comparing radiologists' readings with and without CAD. MATERIALS AND METHODS: A total of 120 chest CT images were randomly selected from patients with suspected lung cancer. The gold standard of nodules ≥ 3 mm was established by a panel of three expert radiologists. Two less experienced radiologists read the images without and afterward with CAD system. Their reading times were recorded. RESULTS: The radiologists' sensitivity increased from 20.9% to 38.0% with the introduction of CAD. The positive predictive value (PPV) decreased from 70.5% to 61.8%, and the F1-score increased from 32.2% to 47.0%. The sensitivity significantly increased from 13.7% to 32.4% for small nodules (3-6 mm) and from 33.3% to 47.6% for medium nodules (6-10 mm). CAD alone showed a sensitivity of 70.3%, a PPV of 57.9%, and an F1-score of 63.5%. Reading time decreased by 11.3% with the use of CAD. CONCLUSION: CAD improved the less experienced radiologists' sensitivity in detecting pulmonary nodules of all sizes, especially including a significant improvement in the detection of clinically important-sized medium nodules (6-10 mm) as well as small nodules (3-6 mm) and reduced their reading time.",2020,10.1007/s11604-020-01009-0,,,,
Efficient and Effective Training of COVID-19 Classification Networks With Self-Supervised Dual-Track Learning to Rank,"Coronavirus Disease 2019 (COVID-19) has rapidly spread worldwide since first reported. Timely diagnosis of COVID-19 is crucial both for disease control and patient care. Non-contrast thoracic computed tomography (CT) has been identified as an effective tool for the diagnosis, yet the disease outbreak has placed tremendous pressure on radiologists for reading the exams and may potentially lead to fatigue-related mis-diagnosis. Reliable automatic classification algorithms can be really helpful; however, they usually require a considerable number of COVID-19 cases for training, which is difficult to acquire in a timely manner. Meanwhile, how to effectively utilize the existing archive of non-COVID-19 data (the negative samples) in the presence of severe class imbalance is another challenge. In addition, the sudden disease outbreak necessitates fast algorithm development. In this work, we propose a novel approach for effective and efficient training of COVID-19 classification networks using a small number of COVID-19 CT exams and an archive of negative samples. Concretely, a novel self-supervised learning method is proposed to extract features from the COVID-19 and negative samples. Then, two kinds of soft-labels ('difficulty' and 'diversity') are generated for the negative samples by computing the earth mover's distances between the features of the negative and COVID-19 samples, from which data 'values' of the negative samples can be assessed. A pre-set number of negative samples are selected accordingly and fed to the neural network for training. Experimental results show that our approach can achieve superior performance using about half of the negative samples, substantially reducing model training time.",2020,10.1109/jbhi.2020.3018181,retrospective cohort,diagnosis,CT,Lung
Efficient COVID-19 Segmentation from CT Slices Exploiting Semantic Segmentation with Integrated Attention Mechanism,"Coronavirus (COVID-19) is a pandemic, which caused suddenly unexplained pneumonia cases and caused a devastating effect on global public health. Computerized tomography (CT) is one of the most effective tools for COVID-19 screening. Since some specific patterns such as bilateral, peripheral, and basal predominant ground-glass opacity, multifocal patchy consolidation, crazy-paving pattern with a peripheral distribution can be observed in CT images and these patterns have been declared as the findings of COVID-19 infection. For patient monitoring, diagnosis and segmentation of COVID-19, which spreads into the lung, expeditiously and accurately from CT, will provide vital information about the stage of the disease. In this work, we proposed a SegNet-based network using the attention gate (AG) mechanism for the automatic segmentation of COVID-19 regions in CT images. AGs can be easily integrated into standard convolutional neural network (CNN) architectures with a minimum computing load as well as increasing model precision and predictive accuracy. Besides, the success of the proposed network has been evaluated based on dice, Tversky, and focal Tversky loss functions to deal with low sensitivity arising from the small lesions. The experiments were carried out using a fivefold cross-validation technique on a COVID-19 CT segmentation database containing 473 CT images. The obtained sensitivity, specificity, and dice scores were reported as 92.73%, 99.51%, and 89.61%, respectively. The superiority of the proposed method has been highlighted by comparing with the results reported in previous studies and it is thought that it will be an auxiliary tool that accurately detects automatic COVID-19 regions from CT images.",2021,10.1007/s10278-021-00434-5,cross-sectional,diagnosis,CT,Lung
Efficient Deep Network Architectures for Fast Chest X-Ray Tuberculosis Screening and Visualization,"Automated diagnosis of tuberculosis (TB) from chest X-Rays (CXR) has been tackled with either hand-crafted algorithms or machine learning approaches such as support vector machines (SVMs) and convolutional neural networks (CNNs). Most deep neural network applied to the task of tuberculosis diagnosis have been adapted from natural image classification. These models have a large number of parameters as well as high hardware requirements, which makes them prone to overfitting and harder to deploy in mobile settings. We propose a simple convolutional neural network optimized for the problem which is faster and more efficient than previous models but preserves their accuracy. Moreover, the visualization capabilities of CNNs have not been fully investigated. We test saliency maps and grad-CAMs as tuberculosis visualization methods, and discuss them from a radiological perspective.",2019,10.1038/s41598-019-42557-4,cross-sectional,diagnosis,CXR,Lung
Efficient Framework for Detection of COVID-19 Omicron and Delta Variants Based on Two Intelligent Phases of CNN Models,"INTRODUCTION: While the COVID-19 pandemic was waning in most parts of the world, a new wave of COVID-19 Omicron and Delta variants in Central Asia and the Middle East caused a devastating crisis and collapse of health-care systems. As the diagnostic methods for this COVID-19 variant became more complex, health-care centers faced a dramatic increase in patients. Thus, the need for less expensive and faster diagnostic methods led researchers and specialists to work on improving diagnostic testing. METHOD: Inspired by the COVID-19 diagnosis methods, the latest and most efficient deep learning algorithms in the field of extracting X-ray and CT scan image features were used to identify COVID-19 in the early stages of the disease. RESULTS: We presented a general framework consisting of two models which are developed by convolutional neural network (CNN) using the concept of transfer learning and parameter optimization. The proposed phase of the framework was evaluated on the test dataset and yielded remarkable results and achieved a detection sensitivity, specificity, and accuracy of 0.99, 0.986, and 0.988, for the first phase and 0.997, 0.9976, and 0.997 for the second phase, respectively. In all cases, the whole framework was able to successfully classify COVID-19 and non-COVID-19 cases from CT scans and X-ray images. CONCLUSION: Since the proposed framework was based on two deep learning models that used two radiology modalities, it was able to significantly assist radiologists in detecting COVID-19 in the early stages. The use of models with this feature can be considered as a powerful and reliable tool, compared to the previous models used in the past pandemics.",2022,10.1155/2022/4838009,cross-sectional,diagnosis,CXR/CT,Lung
Elaboration of a multimodal MRI-based radiomics signature for the preoperative prediction of the histological subtype in patients with non-small-cell lung cancer,"BACKGROUND: Non-invasive discrimination between lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD) subtypes of non-small-cell lung cancer (NSCLC) could be very beneficial to the patients unfit for the invasive diagnostic procedures. The aim of this study was to investigate the feasibility of utilizing the multimodal magnetic resonance imaging (MRI) radiomics and clinical features in classifying NSCLC. This retrospective study involved 148 eligible patients with postoperative pathologically confirmed NSCLC. The study was conducted in three steps: (1) feature extraction was performed using the online freely available package with the multimodal MRI data; (2) feature selection was performed using the Student's t test and support vector machine (SVM)-based recursive feature elimination method with the training cohort (n = 100), and the performance of these selected features was evaluated using both the training and the validation cohorts (n = 48) with a non-linear SVM classifier; (3) a Radscore model was then generated using logistic regression algorithm; (4) Integrating the Radscore with the semantic clinical features, a radiomics-clinical nomogram was developed, and its overall performance was evaluated with both cohorts. RESULTS: Thirteen optimal features achieved favorable discrimination performance with both cohorts, with area under the curve (AUC) of 0.819 and 0.824, respectively. The radiomics-clinical nomogram integrating the Radscore with the independent clinical predictors exhibited more favorable discriminative power, with AUC improved to 0.901 and 0.872 in both cohorts, respectively. The Hosmer-Lemeshow test and decision curve analysis results furtherly showed good predictive precision and clinical usefulness of the nomogram. CONCLUSION: Non-invasive histological subtype stratification of NSCLC can be done favorably using multimodal MRI radiomics features. Integrating the radiomics features with the clinical features could further improve the performance of the histological subtype stratification in patients with NSCLC.",2020,10.1186/s12938-019-0744-0,cross-sectional,diagnosis,MRI,Lung
Elevated Coronary Artery Calcium Quantified by a Validated Deep Learning Model From Lung Cancer Radiotherapy Planning Scans Predicts Mortality,"PURPOSE: Coronary artery calcium (CAC) quantified on computed tomography (CT) scans is a robust predictor of atherosclerotic coronary disease; however, the feasibility and relevance of quantitating CAC from lung cancer radiotherapy planning CT scans is unknown. We used a previously validated deep learning (DL) model to assess whether CAC is a predictor of all-cause mortality and major adverse cardiac events (MACEs). METHODS: Retrospective analysis of non-contrast-enhanced radiotherapy planning CT scans from 428 patients with locally advanced lung cancer is performed. The DL-CAC algorithm was previously trained on 1,636 cardiac-gated CT scans and tested on four clinical trial cohorts. Plaques ≥ 1 cubic millimeter were measured to generate an Agatston-like DL-CAC score and grouped as DL-CAC = 0 (very low risk) and DL-CAC ≥ 1 (elevated risk). Cox and Fine and Gray regressions were adjusted for lung cancer and cardiovascular factors. RESULTS: The median follow-up was 18.1 months. The majority (61.4%) had a DL-CAC ≥ 1. There was an increased risk of all-cause mortality with DL-CAC ≥ 1 versus DL-CAC = 0 (adjusted hazard ratio, 1.51; 95% CI, 1.01 to 2.26; P = .04), with 2-year estimates of 56.2% versus 45.4%, respectively. There was a trend toward increased risk of major adverse cardiac events with DL-CAC ≥ 1 versus DL-CAC = 0 (hazard ratio, 1.80; 95% CI, 0.87 to 3.74; P = .11), with 2-year estimates of 7.3% versus 1.2%, respectively. CONCLUSION: In this proof-of-concept study, CAC was effectively measured from routinely acquired radiotherapy planning CT scans using an automated model. Elevated CAC, as predicted by the DL model, was associated with an increased risk of mortality, suggesting a potential benefit for automated cardiac risk screening before cancer therapy begins.",2022,10.1200/cci.21.00095,,,,
Emphysema quantification using low-dose computed tomography with deep learning-based kernel conversion comparison,"OBJECTIVE: This study determined the effect of dose reduction and kernel selection on quantifying emphysema using low-dose computed tomography (LDCT) and evaluated the efficiency of a deep learning-based kernel conversion technique in normalizing kernels for emphysema quantification. METHODS: A sample of 131 participants underwent LDCT and standard-dose computed tomography (SDCT) at 1- to 2-year intervals. LDCT images were reconstructed with B31f and B50f kernels, and SDCT images were reconstructed with B30f kernels. A deep learning model was used to convert the LDCT image from a B50f kernel to a B31f kernel. Emphysema indices (EIs), lung attenuation at 15th percentile (perc15), and mean lung density (MLD) were calculated. Comparisons among the different kernel types for both LDCT and SDCT were performed using Friedman's test and Bland-Altman plots. RESULTS: All values of LDCT B50f were significantly different compared with the values of LDCT B31f and SDCT B30f (p < 0.05). Although there was a statistical difference, the variation of the values of LDCT B50f significantly decreased after kernel normalization. The 95% limits of agreement between the SDCT and LDCT kernels (B31f and converted B50f) ranged from - 2.9 to 4.3% and from - 3.2 to 4.4%, respectively. However, there were no significant differences in EIs and perc15 between SDCT and LDCT converted B50f in the non-chronic obstructive pulmonary disease (COPD) participants (p > 0.05). CONCLUSION: The deep learning-based CT kernel conversion of sharp kernel in LDCT significantly reduced variation in emphysema quantification, and could be used for emphysema quantification. KEY POINTS: • Low-dose computed tomography with smooth kernel showed adequate performance in quantifying emphysema compared with standard-dose CT. • Emphysema quantification is affected by kernel selection and the application of a sharp kernel resulted in a significant overestimation of emphysema. • Deep learning-based kernel normalization of sharp kernel significantly reduced variation in emphysema quantification.",2020,10.1007/s00330-020-07020-3,cross-sectional,diagnosis,CT,Lung
End-to-end automatic differentiation of the coronavirus disease 2019 (COVID-19) from viral pneumonia based on chest CT,"PURPOSE: In the absence of a virus nucleic acid real-time reverse transcriptase-polymerase chain reaction (RT-PCR) test and experienced radiologists, clinical diagnosis is challenging for viral pneumonia with clinical symptoms and CT signs similar to that of coronavirus disease 2019 (COVID-19). We developed an end-to-end automatic differentiation method based on CT images to identify COVID-19 pneumonia patients in real time. METHODS: From January 18 to February 23, 2020, we conducted a retrospective study and enrolled 201 patients from two hospitals in China who underwent chest CT and RT-PCR tests, of which 98 patients tested positive for COVID-19 (118 males and 83 females, with an average age of 42 years). Patient CT images from one hospital were divided among training, validation and test datasets with an 80%:10%:10% ratio. An end-to-end representation learning method using a large-scale bi-directional generative adversarial network (BigBiGAN) architecture was designed to extract semantic features from the CT images. The semantic feature matrix was input for linear classifier construction. Patients from the other hospital were used for external validation. Differentiation accuracy was evaluated using a receiver operating characteristic curve. RESULTS: Based on the 120-dimensional semantic features extracted by BigBiGAN from each image, the linear classifier results indicated that the area under the curve (AUC) in the training, validation and test datasets were 0.979, 0.968 and 0.972, respectively, with an average sensitivity of 92% and specificity of 91%. The AUC for external validation was 0.850, with a sensitivity of 80% and specificity of 75%. Publicly available architecture and computing resources were used throughout the study to ensure reproducibility. CONCLUSION: This study provides an efficient recognition method for coronavirus disease 2019 pneumonia, using an end-to-end design to implement targeted and effective isolation for the containment of this communicable disease.",2020,10.1007/s00259-020-04929-1,cross-sectional,diagnosis,CT,Lung
End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography,"With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States(1). Lung cancer screening using low-dose computed tomography has been shown to reduce mortality by 20-43% and is now included in US screening guidelines(1-6). Existing challenges include inter-grader variability and high false-positive and false-negative rates(7-10). We propose a deep learning algorithm that uses a patient's current and prior computed tomography volumes to predict the risk of lung cancer. Our model achieves a state-of-the-art performance (94.4% area under the curve) on 6,716 National Lung Cancer Screening Trial cases, and performs similarly on an independent clinical validation set of 1,139 cases. We conducted two reader studies. When prior computed tomography imaging was not available, our model outperformed all six radiologists with absolute reductions of 11% in false positives and 5% in false negatives. Where prior computed tomography imaging was available, the model performance was on-par with the same radiologists. This creates an opportunity to optimize the screening process via computer assistance and automation. While the vast majority of patients remain unscreened, we show the potential for deep learning models to increase the accuracy, consistency and adoption of lung cancer screening worldwide.",2019,10.1038/s41591-019-0447-x,cross-sectional,diagnosis,CT,Lung
End-to-End Non-Small-Cell Lung Cancer Prognostication Using Deep Learning Applied to Pretreatment Computed Tomography,"PURPOSE: Clinical TNM staging is a key prognostic factor for patients with lung cancer and is used to inform treatment and monitoring. Computed tomography (CT) plays a central role in defining the stage of disease. Deep learning applied to pretreatment CTs may offer additional, individualized prognostic information to facilitate more precise mortality risk prediction and stratification. METHODS: We developed a fully automated imaging-based prognostication technique (IPRO) using deep learning to predict 1-year, 2-year, and 5-year mortality from pretreatment CTs of patients with stage I-IV lung cancer. Using six publicly available data sets from The Cancer Imaging Archive, we performed a retrospective five-fold cross-validation using pretreatment CTs of 1,689 patients, of whom 1,110 were diagnosed with non-small-cell lung cancer and had available TNM staging information. We compared the association of IPRO and TNM staging with patients' survival status and assessed an Ensemble risk score that combines IPRO and TNM staging. Finally, we evaluated IPRO's ability to stratify patients within TNM stages using hazard ratios (HRs) and Kaplan-Meier curves. RESULTS: IPRO showed similar prognostic power (concordance index [C-index] 1-year: 0.72, 2-year: 0.70, 5-year: 0.68) compared with that of TNM staging (C-index 1-year: 0.71, 2-year: 0.71, 5-year: 0.70) in predicting 1-year, 2-year, and 5-year mortality. The Ensemble risk score yielded superior performance across all time points (C-index 1-year: 0.77, 2-year: 0.77, 5-year: 0.76). IPRO stratified patients within TNM stages, discriminating between highest- and lowest-risk quintiles in stages I (HR: 8.60), II (HR: 5.03), III (HR: 3.18), and IV (HR: 1.91). CONCLUSION: Deep learning applied to pretreatment CT combined with TNM staging enhances prognostication and risk stratification in patients with lung cancer.",2021,10.1200/cci.21.00096,retrospective cohort,prognosis,CT,Lung
Ensemble Deep Learning and Internet of Things-Based Automated COVID-19 Diagnosis Framework,"Coronavirus disease (COVID-19) is a viral infection caused by SARS-CoV-2. The modalities such as computed tomography (CT) have been successfully utilized for the early stage diagnosis of COVID-19 infected patients. Recently, many researchers have utilized deep learning models for the automated screening of COVID-19 suspected cases. An ensemble deep learning and Internet of Things (IoT) based framework is proposed for screening of COVID-19 suspected cases. Three well-known pretrained deep learning models are ensembled. The medical IoT devices are utilized to collect the CT scans, and automated diagnoses are performed on IoT servers. The proposed framework is compared with thirteen competitive models over a four-class dataset. Experimental results reveal that the proposed ensembled deep learning model yielded 98.98% accuracy. Moreover, the model outperforms all competitive models in terms of other performance metrics achieving 98.56% precision, 98.58% recall, 98.75% F-score, and 98.57% AUC. Therefore, the proposed framework can improve the acceleration of COVID-19 diagnosis.",2022,10.1155/2022/7377502,cross-sectional,diagnosis,CT,Lung
Ensemble Learning Framework with GLCM Texture Extraction for Early Detection of Lung Cancer on CT Images,"Lung cancer has emerged as a major cause of death among all demographics worldwide, largely caused by a proliferation of smoking habits. However, early detection and diagnosis of lung cancer through technological improvements can save the lives of millions of individuals affected globally. Computerized tomography (CT) scan imaging is a proven and popular technique in the medical field, but diagnosing cancer with only CT scans is a difficult task even for doctors and experts. This is why computer-assisted diagnosis has revolutionized disease diagnosis, especially cancer detection. This study looks at 20 CT scan images of lungs. In a preprocessing step, we chose the best filter to be applied to medical CT images between median, Gaussian, 2D convolution, and mean. From there, it was established that the median filter is the most appropriate. Next, we improved image contrast by applying adaptive histogram equalization. Finally, the preprocessed image with better quality is subjected to two optimization algorithms, fuzzy c-means and k-means clustering. The performance of these algorithms was then compared. Fuzzy c-means showed the highest accuracy of 98%. The feature was extracted using Gray Level Cooccurrence Matrix (GLCM). In classification, a comparison between three algorithms-bagging, gradient boosting, and ensemble (SVM, MLPNN, DT, logistic regression, and KNN)-was performed. Gradient boosting performed the best among these three, having an accuracy of 90.9%.",2022,10.1155/2022/2733965,cross-sectional,diagnosis,CT,Lung
"Epidemiologic Features, Radiological Findings andClinical Outcomes of 19 Patients with COVID-19in a Single Center in Beijing, China","ObjectiveTo describe the epidemiologic, clinical, laboratory, and radiological characteristics and prognoses of COVID-19 confirmed patients in a single center in Beijing, China. Methods The study retrospectively included 19 patients with nucleic acid-confirmed SARS-CoV-2 infection at our hospital from January 20 to March 5, 2020. The final follow-up date was March 14, 2020. The epidemiologic and clinical information was obtained through direct communication with the patients or their family members. Laboratory results retrieved from medical records and radiological images were analyzed both qualitatively by two senior chest radiologists as well as quantitatively via an artificial intelligence software. Results We identified 5 family clusters (13/19, 68.4%) from the study cohort. All cases had good clinical prognoses and were either mild (3/19) or moderate (16/19) clinical types. Fever (15/19, 78.9%) and dry cough (11/19, 57.9%) were common symptoms. Two patients received negative results for more than three consecutive viral nucleic acid tests. The longest interval between an initial CT abnormal finding and a confirmed diagnosis was 30 days. One patient's nucleic acid test turned positive on the follow-up examination after discharge. The presence of radiological abnormalities was non-specific for the diagnosis of COVID-19. Conclusions COVID-19 patients with mild or no clinical symptoms are common in Beijing, China. Radiological abnormalities are mostly non-specific and massive CT examinations for COVID-19 screening should be avoided. Analyses of the contact histories of diagnosed cases in combination with clinical, radiological and laboratory findings are crucial for the early detection of COVID-19. Close monitoring after discharge is also recommended.",2021,10.24920/003821,,,,
Epithelial-to-mesenchymal transition of A549 lung cancer cells exposed to electronic cigarettes,"OBJECTIVES: Epithelial-to-mesenchymal transition (EMT) is the initial step enabling the metastasis of cancer cells, which often leads to death. Although smoking is a major risk factor for lung cancer, there is still widespread use of conventional cigarettes. Recently, the tobacco industry has been transformed by the introduction of electronic cigarettes (ECs), which have lower levels of carcinogens and may provide a safer alternative. Here, we investigate the ability of EC liquids and aerosols to induce an EMT in A549 lung cancer cells. MATERIALS AND METHODS: Human adenocarcinoma alveolar basal epithelial cells (A549) were exposed to EC liquids and aerosols from a popular product for 3-8 days. Live cell imaging, EMT biomarker analysis, and machine learning/image processing algorithms were used to characterize changes associated with EMT. RESULTS: Long-term exposure of A549 cells to menthol or tobacco-flavored EC liquids or aerosols induced an EMT that was characterized by acquisition of a fibroblast-like morphology, loss of cell-to-cell junctions, internalization of E-cadherin, increased motility, and upregulation of other EMT markers. The EMT was concurrent with plasma membrane to nuclear translocation of active β-catenin. CONCLUSION: This is the first known study to show an EMT of lung cancer cells during exposure to EC products. Because an EMT is an initial step leading to metastasis, an intractable problem that often leads to patient death, this critical finding has significant implications for former or heavy cigarette smokers who are using EC and may be at risk for lung cancer or who may already have a lung tumor.",2018,10.1016/j.lungcan.2018.06.010,,,,
Establishing Classifiers With Clinical Laboratory Indicators to Distinguish COVID-19 From Community-Acquired Pneumonia: Retrospective Cohort Study,"BACKGROUND: The initial symptoms of patients with COVID-19 are very much like those of patients with community-acquired pneumonia (CAP); it is difficult to distinguish COVID-19 from CAP with clinical symptoms and imaging examination. OBJECTIVE: The objective of our study was to construct an effective model for the early identification of COVID-19 that would also distinguish it from CAP. METHODS: The clinical laboratory indicators (CLIs) of 61 COVID-19 patients and 60 CAP patients were analyzed retrospectively. Random combinations of various CLIs (ie, CLI combinations) were utilized to establish COVID-19 versus CAP classifiers with machine learning algorithms, including random forest classifier (RFC), logistic regression classifier, and gradient boosting classifier (GBC). The performance of the classifiers was assessed by calculating the area under the receiver operating characteristic curve (AUROC) and recall rate in COVID-19 prediction using the test data set. RESULTS: The classifiers that were constructed with three algorithms from 43 CLI combinations showed high performance (recall rate >0.9 and AUROC >0.85) in COVID-19 prediction for the test data set. Among the high-performance classifiers, several CLIs showed a high usage rate; these included procalcitonin (PCT), mean corpuscular hemoglobin concentration (MCHC), uric acid, albumin, albumin to globulin ratio (AGR), neutrophil count, red blood cell (RBC) count, monocyte count, basophil count, and white blood cell (WBC) count. They also had high feature importance except for basophil count. The feature combination (FC) of PCT, AGR, uric acid, WBC count, neutrophil count, basophil count, RBC count, and MCHC was the representative one among the nine FCs used to construct the classifiers with an AUROC equal to 1.0 when using the RFC or GBC algorithms. Replacing any CLI in these FCs would lead to a significant reduction in the performance of the classifiers that were built with them. CONCLUSIONS: The classifiers constructed with only a few specific CLIs could efficiently distinguish COVID-19 from CAP, which could help clinicians perform early isolation and centralized management of COVID-19 patients.",2021,10.2196/23390,,,,
Estimating heterogeneous survival treatment effects of lung cancer screening approaches: A causal machine learning analysis,"The National Lung Screening Trial (NLST) found that low-dose computed tomography (LDCT) screening provided lung cancer (LC) mortality benefit compared to chest radiography (CXR). Considerable research concerns identifying the differential treatment effects that may exist in certain subpopulations. We shed light on several important issues in existing research and highlight the need for further investigation of the heterogeneous comparative effect of LDCT versus CXR, using more flexible and rigorous statistical approaches. We used a high-performance Bayesian machine learning approach designed for censored survival data, accelerated failure time Bayesian additive regression trees model (AFT-BART), to flexibly capture the relationships between the failure time and predictors. We then used the counterfactual framework to draw Markov chain Monte Carlo samples of the individual treatment effect for each participant. Using these posterior samples, we explored the possible treatment effect heterogeneity via a stepwise binary tree approach. When re-analyzed with AFT-BART, LDCT did not have a statistically significant LC or overall mortality benefit compared to CXR. The Asian and Black (particularly those with pack-year ≥ 37 years and without emphysema) NLST population were shown to have enhanced overall mortality benefit from LDCT than the population average. Although inconclusive for LC mortality benefit, Asians, Blacks and Whites with history of chronic obstructive pulmonary disease showed a small trend towards benefit from LDCT. Causal inference with flexible machine learning modeling can provide valuable knowledge for informing treatment decision and planning targeted clinical trials emphasizing personalized medicine approaches.",2021,10.1016/j.annepidem.2021.06.008,prospective cohort,prognosis,CXR,Lung
Estimation of malignancy of pulmonary nodules at CT scans: Effect of computer-aided diagnosis on diagnostic performance of radiologists,"OBJECTIVES: To develop a computer-aided diagnosis (CAD) system for distinguishing malignant from benign pulmonary nodules on computed tomography (CT) scans, and to assess whether the diagnostic performance of radiologists with different experiences can be improved with the assistant of CAD. MATERIALS AND METHODS: A total of 857 malignant nodules from 601 patients and 426 benign nodules from 278 patients were retrospectively collected from four hospitals. In this study, we exploited convolutional neural network in the framework of deep learning to classify whether a nodule was benign or malignant. A total of 745 malignant nodules and 370 benign nodules were used as the training data of our CAD system. The remaining 112 malignant nodules and 56 benign nodules were used as the test data. The participants were two senior chest radiologists, two secondary chest radiologists, and two junior radiology residents. The readers estimated the likelihood of malignancy of pulmonary nodules first without and then with CAD output. Receiver-operating characteristic (ROC) curve was used to evaluate readers' diagnostic performance. RESULTS: When a threshold level of 58% was used to estimate the likelihood of malignancy, the sensitivity, specificity, and diagnostic accuracy values of our CAD scheme alone were 93.8%, 83.9%, and 90.5%, respectively. For all six readers, the mean area under the ROC curve (A(z) ) values without and with CAD system were 0.913 and 0.938, respectively. For each reader, there is a large difference in A(z) values that assessed without and with CAD system. With CAD output, the readers made correct changes an average of 15.7 times and incorrect changes an average of 2 times. CONCLUSION: Our CAD system significantly improved the diagnostic performance of readers regardless of their experience levels for assessment of the likelihood of malignancy of pulmonary nodules.",2021,10.1111/ajco.13362,,,,
Estimation of Respiratory Rate from Thermography Using Respiratory Likelihood Index,"Respiration is a key vital sign used to monitor human health status. Monitoring respiratory rate (RR) under non-contact is particularly important for providing appropriate pre-hospital care in emergencies. We propose an RR estimation system using thermal imaging cameras, which are increasingly being used in the medical field, such as recently during the COVID-19 pandemic. By measuring temperature changes during exhalation and inhalation, we aim to track the respiration of the subject in a supine or seated position in real-time without any physical contact. The proposed method automatically selects the respiration-related regions from the detected facial regions and estimates the respiration rate. Most existing methods rely on signals from nostrils and require close-up or high-resolution images, while our method only requires the facial region to be captured. Facial region is detected using YOLO v3, an object detection model based on deep learning. The detected facial region is divided into subregions. By calculating the respiratory likelihood of each segmented region using the newly proposed index, called the Respiratory Quality Index, the respiratory region is automatically selected and the RR is estimated. An evaluation of the proposed RR estimation method was conducted on seven subjects in their early twenties, with four 15 s measurements being taken. The results showed a mean absolute error of 0.66 bpm. The proposed method can be useful as an RR estimation method.",2021,10.3390/s21134406,,,,
Evaluate the Malignancy of Pulmonary Nodules Using the 3-D Deep Leaky Noisy-OR Network,"Automatic diagnosing lung cancer from computed tomography scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3-D deep neural network to solve this problem. The model consists of two modules. The first one is a 3-D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities, and combines them with a leaky noisy-OR gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The overfitting caused by the shortage of the training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition.",2019,10.1109/tnnls.2019.2892409,cross-sectional,diagnosis,CT,Lung
Evaluate the performance of four artificial intelligence-aided diagnostic systems in identifying and measuring four types of pulmonary nodules,"PURPOSE: This study aims to evaluate the performance of four artificial intelligence-aided diagnostic systems in identifying and measuring four types of pulmonary nodules. METHODS: Four types of nodules were implanted in a commercial lung phantom. The phantom was scanned with multislice spiral computed tomography, after which four systems (A, B, C, D) were used to identify the nodules and measure their volumes. RESULTS: The relative volume error (RVE) of system A was the lowest for all nodules, except for small ground glass nodules (SGGNs). System C had the smallest RVE for SGGNs, -0.13 (-0.56, 0.00). In the Bland-Altman test, only systems A and C passed the consistency test, P = 0.40. In terms of precision, the miss rate (MR) of system C was 0.00% for small solid nodules (SSNs), ground glass nodules (GGNs), and solid nodules (SNs) but 4.17% for SGGNs. The comparable system D MRs for SGGNs, SSNs, and GGNs were 71.30%, 25.93%, and 47.22%, respectively, the highest among all the systems. Receiver operating characteristic curve analysis indicated that system A had the best performance in recognizing SSNs and GGNs, with areas under the curve of 0.91 and 0.68. System C had the best performance for SGGNs (AUC = 0.91). CONCLUSION: Among four types nodules, SGGNs are the most difficult to recognize, indicating the need to improve higher accuracy and precision of artificial systems. System A most accurately measured nodule volume. System C was most precise in recognizing all four types of nodules, especially SGGN.",2021,10.1002/acm2.13142,,,,
Evaluating Deep Neural Network Architectures with Transfer Learning for Pneumonitis Diagnosis,"Pneumonitis is an infectious disease that causes the inflammation of the air sac. It can be life-threatening to the very young and elderly. Detection of pneumonitis from X-ray images is a significant challenge. Early detection and assistance with diagnosis can be crucial. Recent developments in the field of deep learning have significantly improved their performance in medical image analysis. The superior predictive performance of the deep learning methods makes them ideal for pneumonitis classification from chest X-ray images. However, training deep learning models can be cumbersome and resource-intensive. Reusing knowledge representations of public models trained on large-scale datasets through transfer learning can help alleviate these challenges. In this paper, we compare various image classification models based on transfer learning with well-known deep learning architectures. The Kaggle chest X-ray dataset was used to evaluate and compare our models. We apply basic data augmentation and fine-tune our feed-forward classification head on the models pretrained on the ImageNet dataset. We observed that the DenseNet201 model outperforms other models with an AUROC score of 0.966 and a recall score of 0.99. We also visualize the class activation maps from the DenseNet201 model to interpret the patterns recognized by the model for prediction.",2021,10.1155/2021/8036304,cross-sectional,diagnosis,CXR,Lung
Evaluating the clinical trends and benefits of low-dose computed tomography in lung cancer patients,"BACKGROUND: Despite guideline recommendations, utilization of low-dose computed tomography (LDCT) for lung cancer screening remains low. The driving factors behind these low rates and the real-world effect of LDCT utilization on lung cancer outcomes remain limited. METHODS: We identified patients diagnosed with non-small cell lung cancer (NSCLC) from 2015 to 2017 within the Veterans Health Administration. Multivariable logistic regression assessed the influence of LDCT screening on stage at diagnosis. Lead time correction using published LDCT lead times was performed. Cancer-specific mortality (CSM) was evaluated using Fine-Gray regression with non-cancer death as a competing risk. A lasso machine learning model identified important predictors for receiving LDCT screening. RESULTS: Among 4664 patients, mean age was 67.8 with 58-month median follow-up, 95% CI = [7-71], and 118 patients received ≥1 screening LDCT before NSCLC diagnosis. From 2015 to 2017, LDCT screening increased (0.1%-6.6%, mean = 1.3%). Compared with no screening, patients with ≥1 LDCT were more than twice as likely to present with stage I disease at diagnosis (odds ratio [OR] 2.16 [95% CI 1.46-3.20]) and less than half as likely to present with stage IV (OR 0.38 [CI 0.21-0.70]). Screened patients had lower risk of CSM even after adjusting for LDCT lead time (subdistribution hazard ratio 0.60 [CI 0.42-0.85]). The machine learning model achieved an area under curve of 0.87 and identified diagnosis year and region as the most important predictors for receiving LDCT. White, non-Hispanic patients were more likely to receive LDCT screening, whereas minority, older, female, and unemployed patients were less likely. CONCLUSIONS: Utilization of LDCT screening is increasing, although remains low. Consistent with randomized data, LDCT-screened patients were diagnosed at earlier stages and had lower CSM. LDCT availability appeared to be the main predictor of utilization. Providing access to more patients, including those in diverse racial and socioeconomic groups, should be a priority.",2021,10.1002/cam4.4229,retrospective cohort,prognosis,CT,Lung
Evaluating the performance of a deep learning-based computer-aided diagnosis (DL-CAD) system for detecting and characterizing lung nodules: Comparison with the performance of double reading by radiologists,"BACKGROUND: The study was conducted to evaluate the performance of a state-of-the-art commercial deep learning-based computer-aided diagnosis (DL-CAD) system for detecting and characterizing pulmonary nodules. METHODS: Pulmonary nodules in 346 healthy subjects (male: female = 221:125, mean age 51 years) from a lung cancer screening program conducted from March to November 2017 were screened using a DL-CAD system and double reading independently, and their performance in nodule detection and characterization were evaluated. An expert panel combined the results of the DL-CAD system and double reading as the reference standard. RESULTS: The DL-CAD system showed a higher detection rate than double reading, regardless of nodule size (86.2% vs. 79.2%; P < 0.001): nodules ≥ 5 mm (96.5% vs. 88.0%; P = 0.008); nodules < 5 mm (84.3% vs. 77.5%; P < 0.001). However, the false positive rate (per computed tomography scan) of the DL-CAD system (1.53, 529/346) was considerably higher than that of double reading (0.13, 44/346; P < 0.001). Regarding nodule characterization, the sensitivity and specificity of the DL-CAD system for distinguishing solid nodules > 5 mm (90.3% and 100.0%, respectively) and ground-glass nodules (100.0% and 96.1%, respectively) were close to that of double reading, but dropped to 55.5% and 93%, respectively, when discriminating part solid nodules. CONCLUSION: Our DL-CAD system detected significantly more nodules than double reading. In the future, false positive findings should be further reduced and characterization accuracy improved.",2019,10.1111/1759-7714.12931,,,,
Evaluating the Use of Circulating MicroRNA Profiles for Lung Cancer Detection in Symptomatic Patients,"IMPORTANCE: The overall low survival rate of patients with lung cancer calls for improved detection tools to enable better treatment options and improved patient outcomes. Multivariable molecular signatures, such as blood-borne microRNA (miRNA) signatures, may have high rates of sensitivity and specificity but require additional studies with large cohorts and standardized measurements to confirm the generalizability of miRNA signatures. OBJECTIVE: To investigate the use of blood-borne miRNAs as potential circulating markers for detecting lung cancer in an extended cohort of symptomatic patients and control participants. DESIGN, SETTING, AND PARTICIPANTS: This multicenter, cohort study included patients from case-control and cohort studies (TREND and COSYCONET) with 3102 patients being enrolled by convenience sampling between March 3, 2009, and March 19, 2018. For the cohort study TREND, population sampling was performed. Clinical diagnoses were obtained for 3046 patients (606 patients with non-small cell and small cell lung cancer, 593 patients with nontumor lung diseases, 883 patients with diseases not affecting the lung, and 964 unaffected control participants). No samples were removed because of experimental issues. The collected data were analyzed between April 2018 and November 2019. MAIN OUTCOMES AND MEASURES: Sensitivity and specificity of liquid biopsy using miRNA signatures for detection of lung cancer. RESULTS: A total of 3102 patients with a mean (SD) age of 61.1 (16.2) years were enrolled. Data on the sex of the participants were available for 2856 participants; 1727 (60.5%) were men. Genome-wide miRNA profiles of blood samples from 3046 individuals were evaluated by machine-learning methods. Three classification scenarios were investigated by splitting the samples equally into training and validation sets. First, a 15-miRNA signature from the training set was used to distinguish patients diagnosed with lung cancer from all other individuals in the validation set with an accuracy of 91.4% (95% CI, 91.0%-91.9%), a sensitivity of 82.8% (95% CI, 81.5%-84.1%), and a specificity of 93.5% (95% CI, 93.2%-93.8%). Second, a 14-miRNA signature from the training set was used to distinguish patients with lung cancer from patients with nontumor lung diseases in the validation set with an accuracy of 92.5% (95% CI, 92.1%-92.9%), sensitivity of 96.4% (95% CI, 95.9%-96.9%), and specificity of 88.6% (95% CI, 88.1%-89.2%). Third, a 14-miRNA signature from the training set was used to distinguish patients with early-stage lung cancer from all individuals without lung cancer in the validation set with an accuracy of 95.9% (95% CI, 95.7%-96.2%), sensitivity of 76.3% (95% CI, 74.5%-78.0%), and specificity of 97.5% (95% CI, 97.2%-97.7%). CONCLUSIONS AND RELEVANCE: The findings of the study suggest that the identified patterns of miRNAs may be used as a component of a minimally invasive lung cancer test, complementing imaging, sputum cytology, and biopsy tests.",2020,10.1001/jamaoncol.2020.0001,,,,
Evaluation of a deep learning-based computer-aided detection algorithm on chest radiographs: Case-control study,"Along with recent developments in deep learning techniques, computer-aided diagnosis (CAD) has been growing rapidly in the medical imaging field. In this work, we evaluate the deep learning-based CAD algorithm (DCAD) for detecting and localizing 3 major thoracic abnormalities visible on chest radiographs (CR) and to compare the performance of physicians with and without the assistance of the algorithm. A subset of 244 subjects (60% abnormal CRs) was evaluated. Abnormal findings included mass/nodules (55%), consolidation (21%), and pneumothorax (24%). Observer performance tests were conducted to assess whether the performance of physicians could be enhanced with the algorithm. The area under the receiver operating characteristic (ROC) curve (AUC) and the area under the jackknife alternative free-response ROC (JAFROC) were measured to evaluate the performance of the algorithm and physicians in image classification and lesion detection, respectively. The AUCs for nodule/mass, consolidation, and pneumothorax were 0.9883, 1.000, and 0.9997, respectively. For the image classification, the overall AUC of the pooled physicians was 0.8679 without DCAD and 0.9112 with DCAD. Regarding lesion detection, the pooled observers exhibited a weighted JAFROC figure of merit (FOM) of 0.8426 without DCAD and 0.9112 with DCAD. DCAD for CRs could enhance physicians' performance in the detection of 3 major thoracic abnormalities.",2021,10.1097/md.0000000000025663,,,,
Evaluation of a novel deep learning-based classifier for perifissural nodules,"OBJECTIVES: To evaluate the performance of a novel convolutional neural network (CNN) for the classification of typical perifissural nodules (PFN). METHODS: Chest CT data from two centers in the UK and The Netherlands (1668 unique nodules, 1260 individuals) were collected. Pulmonary nodules were classified into subtypes, including ""typical PFNs"" on-site, and were reviewed by a central clinician. The dataset was divided into a training/cross-validation set of 1557 nodules (1103 individuals) and a test set of 196 nodules (158 individuals). For the test set, three radiologically trained readers classified the nodules into three nodule categories: typical PFN, atypical PFN, and non-PFN. The consensus of the three readers was used as reference to evaluate the performance of the PFN-CNN. Typical PFNs were considered as positive results, and atypical PFNs and non-PFNs were grouped as negative results. PFN-CNN performance was evaluated using the ROC curve, confusion matrix, and Cohen's kappa. RESULTS: Internal validation yielded a mean AUC of 91.9% (95% CI 90.6-92.9) with 78.7% sensitivity and 90.4% specificity. For the test set, the reader consensus rated 45/196 (23%) of nodules as typical PFN. The classifier-reader agreement (k = 0.62-0.75) was similar to the inter-reader agreement (k = 0.64-0.79). Area under the ROC curve was 95.8% (95% CI 93.3-98.4), with a sensitivity of 95.6% (95% CI 84.9-99.5), and specificity of 88.1% (95% CI 81.8-92.8). CONCLUSION: The PFN-CNN showed excellent performance in classifying typical PFNs. Its agreement with radiologically trained readers is within the range of inter-reader agreement. Thus, the CNN-based system has potential in clinical and screening settings to rule out perifissural nodules and increase reader efficiency. KEY POINTS: • Agreement between the PFN-CNN and radiologically trained readers is within the range of inter-reader agreement. • The CNN model for the classification of typical PFNs achieved an AUC of 95.8% (95% CI 93.3-98.4) with 95.6% (95% CI 84.9-99.5) sensitivity and 88.1% (95% CI 81.8-92.8) specificity compared to the consensus of three readers.",2021,10.1007/s00330-020-07509-x,cross-sectional,diagnosis,CT,Lung
Evaluation of Computer-Aided Nodule Assessment and Risk Yield (CANARY) in Korean patients for prediction of invasiveness of ground-glass opacity nodule,"Differentiating the invasiveness of ground-glass nodules (GGN) is clinically important, and several institutions have attempted to develop their own solutions by using computed tomography images. The purpose of this study is to evaluate Computer-Aided Analysis of Risk Yield (CANARY), a validated virtual biopsy and risk-stratification machine-learning tool for lung adenocarcinomas, in a Korean patient population. To this end, a total of 380 GGNs from 360 patients who underwent pulmonary resection in a single institution were reviewed. Based on the Score Indicative of Lung Cancer Aggression (SILA), a quantitative indicator of CANARY analysis results, all of the GGNs were classified as ""indolent"" (atypical adenomatous hyperplasia, adenocarcinomas in situ, or minimally invasive adenocarcinoma) or ""invasive"" (invasive adenocarcinoma) and compared with the pathology reports. By considering the possibility of uneven class distribution, statistical analysis was performed on the 1) entire cohort and 2) randomly extracted six sets of class-balanced samples. For each trial, the optimal cutoff SILA was obtained from the receiver operating characteristic curve. The classification results were evaluated using several binary classification metrics. Of a total of 380 GGNs, the mean SILA for 65 (17.1%) indolent and 315 (82.9%) invasive lesions were 0.195±0.124 and 0.391±0.208 (p < 0.0001). The area under the curve (AUC) of each trial was 0.814 and 0.809, with an optimal threshold SILA of 0.229 for both. The macro F1-score and geometric mean were found to be 0.675 and 0.745 for the entire cohort, while both scored 0.741 in the class-equalized dataset. From these results, CANARY could be confirmed acceptable in classifying GGN for Korean patients after the cutoff SILA was calibrated. We found that adjusting the cutoff SILA is needed to use CANARY in other countries or races, and geometric mean could be more objective than F1-score or AUC in the binary classification of imbalanced data.",2021,10.1371/journal.pone.0253204,,,,
Evaluation of deep learning for COVID-19 diagnosis: Impact of image dataset organization,"INTRODUCTION: Coronavirus disease 2019 (COVID-19) has spread all over the world showing high transmissibility. Many studies have proposed diverse diagnostic methods based on deep learning using chest X-ray images focusing on performance improvement. In reviewing them, this study noticed that evaluation results might be influenced by dataset organization. Therefore, this study identified whether the high-performance values can prove the clinical application potential. METHODS: This study selected chest X-ray image databases which have been widely applied in previous studies. One database includes images for COVID-19, while the others consist of normal and pneumonia images. Then, the COVID-19 classification model was designed and trained on diverse database compositions and evaluated using confusion matrix-based metrics. Also, each database was analyzed by graphical representation methods. RESULTS: The performance was significantly different according to dataset composition. Overall, higher performance was identified on the dataset organized with different databases for each class, compared with the dataset from same database. Also, there were significant differences in the image characteristics between different databases. CONCLUSIONS: The experimental results indicate that model may be trained based on differences of the image characteristics between databases and not on lesion features. This shows that evaluation metrics can be influenced by dataset organization, and high metric values would not directly mean the potential for clinical application. These emphasize the importance of suitable dataset organization for applying COVID-19 diagnosis methods to real clinical sites. Radiologists should sufficiently understand about this issue as actual user of these methods.",2021,10.1002/acm2.13320,,,,
Evaluation of Glucocorticoid Therapy in Asthma Children with Small Airway Obstruction Based on CT Features of Deep Learning,"This study was aimed at exploring the treatment of asthma children with small airway obstruction in CT imaging features of deep learning and glucocorticoid. A total of 145 patients meeting the requirements in hospital were included in this study, and they were randomly assigned to receive aerosolized glucocorticoid (n = 45), aerosolized glucocorticoid combined with bronchodilator (n = 50), or oral steroids (n = 50) for 4 weeks after discharge. The lung function and fractional exhaled nitric oxide (FENO) indexes of the three groups were measured, respectively, and then the effective rates were compared to evaluate the clinical efficacy of glucocorticoids with different administration methods and combined medications in the short-term maintenance treatment after acute exacerbation of asthma. Deep learning algorithm was used for CT image segmentation. The CT image is sent to the workbench for processing on the workbench, and then the convolution operation is performed on each input pixel point during the image processing. After 4 weeks of maintenance treatment, FEF50 %, FEF75 %, and MMEF75/25 increased significantly, and FENO decreased significantly (P < 0.01). The improvement results of FEF50 %, FEF75 %, MMEF75/25, and FENO after maintenance treatment were as follows: the oral hormone group was the most effective, followed by the combined atomization inhalation group, and the hormone atomization inhalation group was the least effective. The differences among them were statistically significant (P < 0.05). The accuracy of artificial intelligence segmentation algorithm was 81%. All the hormones were more effective than local medication in the treatment of small airway function and airway inflammation. In the treatment of aerosol inhalation, the hormone combined with bronchiectasis drug was the most effective in improving small airway obstruction and reducing airway inflammation compared with single drug inhalation. Deep learning CT images are simple, noninvasive, and intuitively observe lung changes in asthma with small airway functional obstruction. Asthma with small airway functional obstruction has high clinical diagnosis and evaluation value.",2021,10.1155/2021/7936548,cross-sectional,treatment,CT,Lung
Evaluation of the Effectiveness of Artificial Intelligence Chest CT Lung Nodule Detection Based on Deep Learning,"Lung cancer is one of the most malignant tumors. If it can be detected early and treated actively, it can effectively improve a patient's survival rate. Therefore, early diagnosis of lung cancer is very important. Early-stage lung cancer usually appears as a solitary lung nodule on medical imaging. It usually appears as a round or nearly round dense shadow in the chest radiograph. It is difficult to distinguish lung nodules and lung soft tissues with the naked eye. Therefore, this article proposes a deep learning-based artificial intelligence chest CT lung nodule detection performance evaluation study, aiming to evaluate the value of chest CT imaging technology in the detection of noncalcified nodules and provide help for the detection and treatment of lung cancer. In this article, the Lung Medical Imaging Database Consortium (LIDC) was selected to obtain 536 usable cases based on inclusion criteria; 80 cases were selected for examination, artificial intelligence software, radiologists, and thoracic imaging specialists. Using 80 pulmonary nodules detection in each case, the pathological type of pulmonary nodules, nonlime tuberculous test results, detection sensitivity, false negative rate, false positive rate, and CT findings were individually analyzed, and the detection efficiency software of artificial intelligence was evaluated. Experiments have proved that the sensitivity of artificial intelligence software to detect noncalcified nodules in the pleural, peripheral, central, and hilar areas is higher than that of radiologists, indicating that the method proposed in this article has achieved good detection results. It has a better nodule detection sensitivity than a radiologist, reducing the complexity of the detection process.",2021,10.1155/2021/9971325,,,,
Evaluation of the feasibility of explainable computer-aided detection of cardiomegaly on chest radiographs using deep learning,"We examined the feasibility of explainable computer-aided detection of cardiomegaly in routine clinical practice using segmentation-based methods. Overall, 793 retrospectively acquired posterior-anterior (PA) chest X-ray images (CXRs) of 793 patients were used to train deep learning (DL) models for lung and heart segmentation. The training dataset included PA CXRs from two public datasets and in-house PA CXRs. Two fully automated segmentation-based methods using state-of-the-art DL models for lung and heart segmentation were developed. The diagnostic performance was assessed and the reliability of the automatic cardiothoracic ratio (CTR) calculation was determined using the mean absolute error and paired t-test. The effects of thoracic pathological conditions on performance were assessed using subgroup analysis. One thousand PA CXRs of 1000 patients (480 men, 520 women; mean age 63 ± 23 years) were included. The CTR values derived from the DL models and diagnostic performance exhibited excellent agreement with reference standards for the whole test dataset. Performance of segmentation-based methods differed based on thoracic conditions. When tested using CXRs with lesions obscuring heart borders, the performance was lower than that for other thoracic pathological findings. Thus, segmentation-based methods using DL could detect cardiomegaly; however, the feasibility of computer-aided detection of cardiomegaly without human intervention was limited.",2021,10.1038/s41598-021-96433-1,cross-sectional,diagnosis,CXR,Heart
Evidence of Gender Differences in the Diagnosis and Management of Coronavirus Disease 2019 Patients: An Analysis of Electronic Health Records Using Natural Language Processing and Machine Learning,"Background: The impact of sex and gender in the incidence and severity of coronavirus disease 2019 (COVID-19) remains controversial. Here, we aim to describe the characteristics of COVID-19 patients at disease onset, with special focus on the diagnosis and management of female patients with COVID-19. Methods: We explored the unstructured free text in the electronic health records (EHRs) within the SESCAM Healthcare Network (Castilla La-Mancha, Spain). The study sample comprised the entire population with available EHRs (1,446,452 patients) from January 1st to May 1st, 2020. We extracted patients' clinical information upon diagnosis, progression, and outcome for all COVID-19 cases. Results: A total of 4,780 patients with a confirmed diagnosis of COVID-19 were identified. Of these, 2,443 (51%) were female, who were on average 1.5 years younger than male patients (61.7 ± 19.4 vs. 63.3 ± 18.3, p = 0.0025). There were more female COVID-19 cases in the 15-59-year-old interval, with the greatest sex ratio (95% confidence interval) observed in the 30-39-year-old range (1.69; 1.35-2.11). Upon diagnosis, headache, anosmia, and ageusia were significantly more frequent in females than males. Imaging by chest X-ray or blood tests were performed less frequently in females (65.5% vs. 78.3% and 49.5% vs. 63.7%, respectively), all p < 0.001. Regarding hospital resource use, females showed less frequency of hospitalization (44.3% vs. 62.0%) and intensive care unit admission (2.8% vs. 6.3%) than males, all p < 0.001. Conclusion: Our results indicate important sex-dependent differences in the diagnosis, clinical manifestation, and treatment of patients with COVID-19. These results warrant further research to identify and close the gender gap in the ongoing pandemic.",2021,10.1089/jwh.2020.8721,,,,
Evolution of IgE responses to multiple allergen components throughout childhood,"BACKGROUND: There is a paucity of information about longitudinal patterns of IgE responses to allergenic proteins (components) from multiple sources. OBJECTIVES: This study sought to investigate temporal patterns of component-specific IgE responses from infancy to adolescence, and their relationship with allergic diseases. METHODS: In a population-based birth cohort, we measured IgE to 112 components at 6 follow-ups during childhood. We used a Bayesian method to discover cross-sectional sensitization patterns and their longitudinal trajectories, and we related these patterns to asthma and rhinitis in adolescence. RESULTS: We identified 1 sensitization cluster at age 1, 3 at age 3, 4 at ages 5 and 8, 5 at age 11, and 6 at age 16 years. ""Broad"" cluster was the only cluster present at every follow-up, comprising components from multiple sources. ""Dust mite"" cluster formed at age 3 years and remained unchanged to adolescence. At age 3 years, a single-component ""Grass"" cluster emerged, which at age 5 years absorbed additional grass components and Fel d 1 to form the ""Grass/cat"" cluster. Two new clusters formed at age 11 years: ""Cat"" cluster and ""PR-10/profilin"" (which divided at age 16 years into ""PR-10"" and ""Profilin""). The strongest contemporaneous associate of asthma at age 16 years was sensitization to dust mite cluster (odds ratio: 2.6; 95% CI: 1.2-6.1; P < .05), but the strongest early life predictor of subsequent asthma was sensitization to grass/cat cluster (odds ratio: 3.5; 95% CI: 1.6-7.4; P < .01). CONCLUSIONS: We describe the architecture of the evolution of IgE responses to multiple allergen components throughout childhood, which may facilitate development of better diagnostic and prognostic biomarkers for allergic diseases.",2018,10.1016/j.jaci.2017.11.064,,,,
Evolutionary image simplification for lung nodule classification with convolutional neural networks,"PURPOSE: Understanding decisions of deep learning techniques is important. Especially in the medical field, the reasons for a decision in a classification task are as crucial as the pure classification results. In this article, we propose a new approach to compute relevant parts of a medical image. Knowing the relevant parts makes it easier to understand decisions. METHODS: In our approach, a convolutional neural network is employed to learn structures of images of lung nodules. Then, an evolutionary algorithm is applied to compute a simplified version of an unknown image based on the learned structures by the convolutional neural network. In the simplified version, irrelevant parts are removed from the original image. RESULTS: In the results, we show simplified images which allow the observer to focus on the relevant parts. In these images, more than 50% of the pixels are simplified. The simplified pixels do not change the meaning of the images based on the learned structures by the convolutional neural network. An experimental analysis shows the potential of the approach. Besides the examples of simplified images, we analyze the run time development. CONCLUSIONS: Simplified images make it easier to focus on relevant parts and to find reasons for a decision. The combination of an evolutionary algorithm employing a learned convolutional neural network is well suited for the simplification task. From a research perspective, it is interesting which areas of the images are simplified and which parts are taken as relevant.",2018,10.1007/s11548-018-1794-7,,,,
Exhaled breath condensate metabolome clusters for endotype discovery in asthma,"BACKGROUND: Asthma is a complex, heterogeneous disorder with similar presenting symptoms but with varying underlying pathologies. Exhaled breath condensate (EBC) is a relatively unexplored matrix which reflects the signatures of respiratory epithelium, but is difficult to normalize for dilution. METHODS: Here we explored whether internally normalized global NMR spectrum patterns, combined with machine learning, could be useful for diagnostics or endotype discovery. Nuclear magnetic resonance (NMR) spectroscopy of EBC was performed in 89 asthmatic subjects from a prospective cohort and 20 healthy controls. A random forest classifier was built to differentiate between asthmatics and healthy controls. Clustering of the spectra was done using k-means to identify potential endotypes. RESULTS: NMR spectra of the EBC could differentiate between asthmatics and healthy controls with 80% sensitivity and 75% specificity. Unsupervised clustering within the asthma group resulted in three clusters (n = 41,11, and 9). Cluster 1 patients had lower long-term exacerbation scores, when compared with other two clusters. Cluster 3 patients had lower blood eosinophils and higher neutrophils, when compared with other two clusters with a strong family history of asthma. CONCLUSION: Asthma clusters derived from NMR spectra of EBC show important clinical and chemical differences, suggesting this as a useful tool in asthma endotype-discovery.",2017,10.1186/s12967-017-1365-7,,,,
Expert knowledge-infused deep learning for automatic lung nodule detection,"BACKGROUND: Computer aided detection (CADe) of pulmonary nodules from computed tomography (CT) is crucial for early diagnosis of lung cancer. Self-learned features obtained by training datasets via deep learning have facilitated CADe of the nodules. However, the complexity of CT lung images renders a challenge of extracting effective features by self-learning only. This condition is exacerbated for limited size of datasets. On the other hand, the engineered features have been widely studied. OBJECTIVE: We proposed a novel nodule CADe which aims to relieve the challenge by the use of available engineered features to prevent convolution neural networks (CNN) from overfitting under dataset limitation and reduce the running-time complexity of self-learning. METHODS: The CADe methodology infuses adequately the engineered features, particularly texture features, into the deep learning process. RESULTS: The methodology was validated on 208 patients with at least one juxta-pleural nodule from the public LIDC-IDRI database. Results demonstrated that the methodology achieves a sensitivity of 88% with 1.9 false positives per scan and a sensitivity of 94.01% with 4.01 false positives per scan. CONCLUSIONS: The methodology shows high performance compared with the state-of-the-art results, in terms of accuracy and efficiency, from both existing CNN-based approaches and engineered feature-based classifications.",2019,10.3233/xst-180426,cross-sectional,diagnosis,CXR,Lung
Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers,"PROBLEM: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. MOTIVATION: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. AIM: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. METHODOLOGY: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). MAIN RESULTS: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.",2021,10.3390/s21165657,cross-sectional,diagnosis,CT,Lung
Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning,"This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopted advanced deep network architectures and proposed a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conducted extensive sets of experiments on two CT image datasets, namely, the SARS-CoV-2 CT-scan and the COVID19-CT. The results show superior performances for our models compared with previous studies. Our best models achieved average accuracy, precision, sensitivity, specificity, and F1-score values of 99.4%, 99.6%, 99.8%, 99.6%, and 99.4% on the SARS-CoV-2 dataset, and 92.9%, 91.3%, 93.7%, 92.2%, and 92.5% on the COVID19-CT dataset, respectively. For better interpretability of the results, we applied visualization techniques to provide visual explanations for the models' predictions. Feature visualizations of the learned features show well-separated clusters representing CT images of COVID-19 and non-COVID-19 cases. Moreover, the visualizations indicate that our models are not only capable of identifying COVID-19 cases but also provide accurate localization of the COVID-19-associated regions, as indicated by well-trained radiologists.",2021,10.3390/s21020455,cross-sectional,diagnosis,CT,Lung
Explainable DCNN based chest X-ray image analysis and classification for COVID-19 pneumonia detection,"To speed up the discovery of COVID-19 disease mechanisms by X-ray images, this research developed a new diagnosis platform using a deep convolutional neural network (DCNN) that is able to assist radiologists with diagnosis by distinguishing COVID-19 pneumonia from non-COVID-19 pneumonia in patients based on chest X-ray classification and analysis. Such a tool can save time in interpreting chest X-rays and increase the accuracy and thereby enhance our medical capacity for the detection and diagnosis of COVID-19. The explainable method is also used in the DCNN to select instances of the X-ray dataset images to explain the behavior of training-learning models to achieve higher prediction accuracy. The average accuracy of our method is above 96%, which can replace manual reading and has the potential to be applied to large-scale rapid screening of COVID-9 for widely use cases.",2021,10.1038/s41598-021-95680-6,cross-sectional,diagnosis,CXR,Lung
Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays,"BACKGROUND AND OBJECTIVE: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. METHOD: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. RESULTS AND CONCLUSION: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.",2020,10.1016/j.cmpb.2020.105608,cross-sectional,diagnosis,CXR,Lung
Explaining Deep Features Using Radiologist-Defined Semantic Features and Traditional Quantitative Features,"Quantitative features are generated from a tumor phenotype by various data characterization, feature-extraction approaches and have been used successfully as a biomarker. These features give us information about a nodule, for example, nodule size, pixel intensity, histogram-based information, and texture information from wavelets or a convolution kernel. Semantic features, on the other hand, can be generated by an experienced radiologist and consist of the common characteristics of a tumor, for example, location of a tumor, fissure, or pleural wall attachment, presence of fibrosis or emphysema, concave cut on nodule surface. These features have been derived for lung nodules by our group. Semantic features have also shown promise in predicting malignancy. Deep features from images are generally extracted from the last layers before the classification layer of a convolutional neural network (CNN). By training with the use of different types of images, the CNN learns to recognize various patterns and textures. But when we extract deep features, there is no specific naming approach for them, other than denoting them by the feature column number (position of a neuron in a hidden layer). In this study, we tried to relate and explain deep features with respect to traditional quantitative features and semantic features. We discovered that 26 deep features from the Vgg-S neural network and 12 deep features from our trained CNN could be explained by semantic or traditional quantitative features. From this, we concluded that those deep features can have a recognizable definition via semantic or quantitative features.",2019,10.18383/j.tom.2018.00034,cross-sectional,diagnosis,CT,Lung
Exploiting Global Structure Information to Improve Medical Image Segmentation,"In this paper, we propose a method to enhance the performance of segmentation models for medical images. The method is based on convolutional neural networks that learn the global structure information, which corresponds to anatomical structures in medical images. Specifically, the proposed method is designed to learn the global boundary structures via an autoencoder and constrain a segmentation network through a loss function. In this manner, the segmentation model performs the prediction in the learned anatomical feature space. Unlike previous studies that considered anatomical priors by using a pre-trained autoencoder to train segmentation networks, we propose a single-stage approach in which the segmentation network and autoencoder are jointly learned. To verify the effectiveness of the proposed method, the segmentation performance is evaluated in terms of both the overlap and distance metrics on the lung area and spinal cord segmentation tasks. The experimental results demonstrate that the proposed method can enhance not only the segmentation performance but also the robustness against domain shifts.",2021,10.3390/s21093249,cross-sectional,diagnosis,CXR,Lung
Exploiting Multiple Optimizers with Transfer Learning Techniques for the Identification of COVID-19 Patients,"Due to the rapid spread of COVID-19 and its induced death worldwide, it is imperative to develop a reliable tool for the early detection of this disease. Chest X-ray is currently accepted to be one of the reliable means for such a detection purpose. However, most of the available methods utilize large training data, and there is a need for improvement in the detection accuracy due to the limited boundary segment of the acquired images for symptom identifications. In this study, a robust and efficient method based on transfer learning techniques is proposed to identify normal and COVID-19 patients by employing small training data. Transfer learning builds accurate models in a timesaving way. First, data augmentation was performed to help the network for memorization of image details. Next, five state-of-the-art transfer learning models, AlexNet, MobileNetv2, ShuffleNet, SqueezeNet, and Xception, with three optimizers, Adam, SGDM, and RMSProp, were implemented at various learning rates, 1e-4, 2e-4, 3e-4, and 4e-4, to reduce the probability of overfitting. All the experiments were performed on publicly available datasets with several analytical measurements attained after execution with a 10-fold cross-validation method. The results suggest that MobileNetv2 with Adam optimizer at a learning rate of 3e-4 provides an average accuracy, recall, precision, and F-score of 97%, 96.5%, 97.5%, and 97%, respectively, which are higher than those of all other combinations. The proposed method is competitive with the available literature, demonstrating that it could be used for the early detection of COVID-19 patients.",2020,10.1155/2020/8889412,cross-sectional,diagnosis,CXR,Lung
Exploiting Shared Knowledge From Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation,"The novel Coronavirus disease (COVID-19) is a highly contagious virus and has spread all over the world, posing an extremely serious threat to all countries. Automatic lung infection segmentation from computed tomography (CT) plays an important role in the quantitative analysis of COVID-19. However, the major challenge lies in the inadequacy of annotated COVID-19 datasets. Currently, there are several public non-COVID lung lesion segmentation datasets, providing the potential for generalizing useful information to the related COVID-19 segmentation task. In this paper, we propose a novel relation-driven collaborative learning model to exploit shared knowledge from non-COVID lesions for annotation-efficient COVID-19 CT lung infection segmentation. The model consists of a general encoder to capture general lung lesion features based on multiple non-COVID lesions, and a target encoder to focus on task-specific features based on COVID-19 infections. We develop a collaborative learning scheme to regularize feature-level relation consistency of given input and encourage the model to learn more general and discriminative representation of COVID-19 infections. Extensive experiments demonstrate that trained with limited COVID-19 data, exploiting shared knowledge from non-COVID lesions can further improve state-of-the-art performance with up to 3.0% in dice similarity coefficient and 4.2% in normalized surface dice. In addition, experimental results on large scale 2D dataset with CT slices show that our method significantly outperforms cutting-edge segmentation methods metrics. Our method promotes new insights into annotation-efficient deep learning and illustrates strong potential for real-world applications in the global fight against COVID-19 in the absence of sufficient high-quality annotations.",2021,10.1109/jbhi.2021.3106341,cross-sectional,diagnosis,CT,Lung
Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images,"Computer-aided diagnosis for the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the healthcare system. Chest X-ray (CXR) imaging has several advantages over other imaging and detection techniques. Numerous works have been reported on COVID-19 detection from a smaller set of original X-ray images. However, the effect of image enhancement and lung segmentation of a large dataset in COVID-19 detection was not reported in the literature. We have compiled a large X-ray dataset (COVQU) consisting of 18,479 CXR images with 8851 normal, 6012 non-COVID lung infections, and 3616 COVID-19 CXR images and their corresponding ground truth lung masks. To the best of our knowledge, this is the largest public COVID positive database and the lung masks. Five different image enhancement techniques: histogram equalization (HE), contrast limited adaptive histogram equalization (CLAHE), image complement, gamma correction, and balance contrast enhancement technique (BCET) were used to investigate the effect of image enhancement techniques on COVID-19 detection. A novel U-Net model was proposed and compared with the standard U-Net model for lung segmentation. Six different pre-trained Convolutional Neural Networks (CNNs) (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and ChexNet) and a shallow CNN model were investigated on the plain and segmented lung CXR images. The novel U-Net model showed an accuracy, Intersection over Union (IoU), and Dice coefficient of 98.63%, 94.3%, and 96.94%, respectively for lung segmentation. The gamma correction-based enhancement technique outperforms other techniques in detecting COVID-19 from the plain and the segmented lung CXR images. Classification performance from plain CXR images is slightly better than the segmented lung CXR images; however, the reliability of network performance is significantly improved for the segmented lung images, which was observed using the visualization technique. The accuracy, precision, sensitivity, F1-score, and specificity were 95.11%, 94.55%, 94.56%, 94.53%, and 95.59% respectively for the segmented lung images. The proposed approach with very reliable and comparable performance will boost the fast and robust COVID-19 detection using chest X-ray images.",2021,10.1016/j.compbiomed.2021.104319,cross-sectional,diagnosis,CXR,Lung
Extended application of a CT-based artificial intelligence prognostication model in patients with primary lung cancer undergoing stereotactic ablative radiotherapy,"BACKGROUND AND PURPOSE: To validate a computed tomography (CT)-based deep learning prognostication model, originally developed for a surgical cohort, in patients with primary lung cancer undergoing stereotactic ablative radiotherapy (SABR). MATERIALS AND METHODS: This retrospective study identified patients with clinical stage T1-2N0M0 lung cancer treated with SABR between 2013 and 2018. The outcomes were local recurrence-free survival (LRFS), disease-free survival (DFS), and overall survival (OS). The discrimination performance of the model, which extracted a quantitative score of cumulative risk for an adverse event up to 900 days, was evaluated using time-dependent receiver operating characteristic curve analysis. Multivariable Cox regression was performed to investigate the independent prognostic value of the model output adjusting for clinical factors including age, sex, smoking status, and clinical T category. RESULTS: In total, 135 patients (median age, 78 years; 101 men; 78 [57.8%] adenocarcinomas and 57 [42.2%] squamous cell carcinomas) were evaluated. Most patients (117/135) were treated with 48-60 Gy in four fractions. Median biologically effective dose was 150.0 Gy (interquartile range, 126.9, 150.0 Gy). For LRFS, the area under the curve (AUC) was 0.72 (95% confidence interval [CI]: 0.58, 0.87). The AUCs were 0.70 (95% CI: 0.60, 0.81) for DFS and 0.66 (95% CI: 0.54, 0.77) for OS. Model output was associated with LRFS (adjusted hazard ratio [HR], 1.043; 95% CI: 1.003, 1.085; P = 0.04), DFS (adjusted HR, 1.03; 95% CI: 1.01, 1.05; P = 0.008), and OS (adjusted HR, 1.025; 95% CI: 1.002, 1.047; P = 0.03). CONCLUSION: This study showed external validity and transportability of the CT-based deep learning prognostication model for radiotherapy candidates.",2021,10.1016/j.radonc.2021.10.022,,,,
External validation of a convolutional neural network artificial intelligence tool to predict malignancy in pulmonary nodules,"BACKGROUND: Estimation of the risk of malignancy in pulmonary nodules detected by CT is central in clinical management. The use of artificial intelligence (AI) offers an opportunity to improve risk prediction. Here we compare the performance of an AI algorithm, the lung cancer prediction convolutional neural network (LCP-CNN), with that of the Brock University model, recommended in UK guidelines. METHODS: A dataset of incidentally detected pulmonary nodules measuring 5-15 mm was collected retrospectively from three UK hospitals for use in a validation study. Ground truth diagnosis for each nodule was based on histology (required for any cancer), resolution, stability or (for pulmonary lymph nodes only) expert opinion. There were 1397 nodules in 1187 patients, of which 234 nodules in 229 (19.3%) patients were cancer. Model discrimination and performance statistics at predefined score thresholds were compared between the Brock model and the LCP-CNN. RESULTS: The area under the curve for LCP-CNN was 89.6% (95% CI 87.6 to 91.5), compared with 86.8% (95% CI 84.3 to 89.1) for the Brock model (p≤0.005). Using the LCP-CNN, we found that 24.5% of nodules scored below the lowest cancer nodule score, compared with 10.9% using the Brock score. Using the predefined thresholds, we found that the LCP-CNN gave one false negative (0.4% of cancers), whereas the Brock model gave six (2.5%), while specificity statistics were similar between the two models. CONCLUSION: The LCP-CNN score has better discrimination and allows a larger proportion of benign nodules to be identified without missing cancers than the Brock model. This has the potential to substantially reduce the proportion of surveillance CT scans required and thus save significant resources.",2020,10.1136/thoraxjnl-2019-214104,cross-sectional,diagnosis,CT,Lung
External validation of radiomics-based predictive models in low-dose CT screening for early lung cancer diagnosis,"PURPOSE: Low-dose CT screening allows early lung cancer detection, but is affected by frequent false positive results, inter/intra observer variation and uncertain diagnoses of lung nodules. Radiomics-based models have recently been introduced to overcome these issues, but limitations in demonstrating their generalizability on independent datasets are slowing their introduction to clinic. The aim of this study is to evaluate two radiomics-based models to classify malignant pulmonary nodules in low-dose CT screening, and to externally validate them on an independent cohort. The effect of a radiomics features harmonization technique is also investigated to evaluate its impact on the classification of lung nodules from a multicenter data. METHODS: Pulmonary nodules from two independent cohorts were considered in this study; the first cohort (110 subjects, 113 nodules) was used to train prediction models, and the second cohort (72 nodules) to externally validate them. Literature-based radiomics features were extracted and, after feature selection, used as predictive variables in models for malignancy identification. An in-house prediction model based on artificial neural network (ANN) was implemented and evaluated, along with an alternative model from the literature, based on a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). External validation was performed on the second cohort to evaluate models' generalization ability. Additionally, the impact of the Combat harmonization method was investigated to compensate for multicenter datasets variabilities. A new training of the models based on harmonized features was performed on the first cohort, then tested separately on the harmonized and non-harmonized features of the second cohort. RESULTS: Preliminary results showed a good accuracy of the investigated models in distinguishing benign from malignant pulmonary nodules with both sets of radiomics features (i.e., non-harmonized and harmonized). The performance of the models, quantified in terms of Area Under the Curve (AUC), was > 0.89 in the training set and > 0.82 in the external validation set for all the investigated scenarios, outperforming the clinical standard (AUC of 0.76). Slightly higher performance was observed for the SVM-LASSO model than the ANN in the external dataset, although they did not result significantly different. For both harmonized and non-harmonized features, no statistical difference was found between Receiver operating characteristic (ROC) curves related to training and test set for both models. CONCLUSIONS: Although no significant improvements were observed when applying the Combat harmonization method, both in-house and literature-based models were able to classify lung nodules with good generalization to an independent dataset, thus showing their potential as tools for clinical decision-making in lung cancer screening.",2020,10.1002/mp.14308,cross-sectional,diagnosis,CT,Lung
Extracting and Learning Fine-Grained Labels from Chest Radiographs,"Chest radiographs are the most common diagnostic exam in emergency rooms and intensive care units today. Recently, a number of researchers have begun working on large chest X-ray datasets to develop deep learning models for recognition of a handful of coarse finding classes such as opacities, masses and nodules. In this paper, we focus on extracting and learning fine-grained labels for chest X-ray images. Specifically we develop a new method of extracting fine-grained labels from radiology reports by combining vocabulary-driven concept extraction with phrasal grouping in dependency parse trees for association of modifiers with findings. A total of457finegrained labels depicting the largest spectrum of findings to date were selected and sufficiently large datasets acquired to train a new deep learning model designed for fine-grained classification. We show results that indicate a highly accurate label extraction process and a reliable learning of fine-grained labels. The resulting network, to our knowledge, is the first to recognize fine-grained descriptions offindings in images covering over nine modifiers including laterality, location, severity, size and appearance.",2020,,,,,
FaceMask: A New Image Dataset for the Automated Identification of People Wearing Masks in the Wild,"The rapid spread of the COVID-19 pandemic, in early 2020, has radically changed the lives of people. In our daily routine, the use of a face (surgical) mask is necessary, especially in public places, to prevent the spread of this disease. Furthermore, in crowded indoor areas, the automated recognition of people wearing a mask is a requisite for the assurance of public health. In this direction, image processing techniques, in combination with deep learning, provide effective ways to deal with this problem. However, it is a common phenomenon that well-established datasets containing images of people wearing masks are not publicly available. To overcome this obstacle and to assist the research progress in this field, we present a publicly available annotated image database containing images of people with and without a mask on their faces, in different environments and situations. Moreover, we tested the performance of deep learning detectors in images and videos on this dataset. The training and the evaluation were performed on different versions of the YOLO network using Darknet, which is a state-of-the-art real-time object detection system. Finally, different experiments and evaluations were carried out for each version of YOLO, and the results for each detector are presented.",2022,10.3390/s22030896,,,,
Factors determining generalization in deep learning models for scoring COVID-CT images,"The COVID-19 pandemic has inspired unprecedented data collection and computer vision modelling efforts worldwide, focused on the diagnosis of COVID-19 from medical images. However, these models have found limited, if any, clinical application due in part to unproven generalization to data sets beyond their source training corpus. This study investigates the generalizability of deep learning models using publicly available COVID-19 Computed Tomography data through cross dataset validation. The predictive ability of these models for COVID-19 severity is assessed using an independent dataset that is stratified for COVID-19 lung involvement. Each inter-dataset study is performed using histogram equalization, and contrast limited adaptive histogram equalization with and without a learning Gabor filter. We show that under certain conditions, deep learning models can generalize well to an external dataset with F1 scores up to 86%. The best performing model shows predictive accuracy of between 75% and 96% for lung involvement scoring against an external expertly stratified dataset. From these results we identify key factors promoting deep learning generalization, being primarily the uniform acquisition of training images, and secondly diversity in CT slice position.",2021,10.3934/mbe.2021456,cross-sectional,diagnosis,CT,Lung
False positive reduction in pulmonary nodule classification using 3D texture and edge feature in CT images,"BACKGROUND: Pulmonary nodule detection can significantly influence the early diagnosis of lung cancer while is confused by false positives. OBJECTIVE: In this study, we focus on the false positive reduction and present a method for accurate and rapid detection of pulmonary nodule from suspective regions with 3D texture and edge feature. METHODS: This work mainly consists of four modules. Firstly, small pulmonary nodule candidates are preprocessed by a reconstruction approach for enhancing 3D image feature. Secondly, a texture feature descriptor is proposed, named cross-scale local binary patterns (CS-LBP), to extract spatial texture information. Thirdly, we design a 3D edge feature descriptor named orthogonal edge orientation histogram (ORT-EOH) to obtain spatial edge information. Finally, hierarchical support vector machines (H-SVMs) is used to classify suspective regions as either nodules or non-nodules with joint CS-LBP and ORT-EOH feature vector. RESULTS: For the solitary solid nodule, ground-glass opacity, juxta-vascular nodule and juxta-pleural nodule, average sensitivity, average specificity and average accuracy of our method are 95.69%, 96.95% and 96.04%, respectively. The elapsed time in training and testing stage are 321.76 s and 5.69 s. CONCLUSIONS: Our proposed method has the best performance compared with other state-of-the-art methods and is shown the improved precision of pulmonary nodule detection with computationaly low cost.",2021,10.3233/thc-181565,cross-sectional,diagnosis,CT,Lung
False-Negative Results of Real-Time Reverse-Transcriptase Polymerase Chain Reaction for Severe Acute Respiratory Syndrome Coronavirus 2: Role of Deep-Learning-Based CT Diagnosis and Insights from Two Cases,"The epidemic of 2019 novel coronavirus, later named as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is still gradually spreading worldwide. The nucleic acid test or genetic sequencing serves as the gold standard method for confirmation of infection, yet several recent studies have reported false-negative results of real-time reverse-transcriptase polymerase chain reaction (rRT-PCR). Here, we report two representative false-negative cases and discuss the supplementary role of clinical data with rRT-PCR, including laboratory examination results and computed tomography features. Coinfection with SARS-COV-2 and other viruses has been discussed as well.",2020,10.3348/kjr.2020.0146,,,,
Fast and Accurate Detection of COVID-19 Along With 14 Other Chest Pathologies Using a Multi-Level Classification: Algorithm Development and Validation Study,"BACKGROUND: COVID-19 has spread very rapidly, and it is important to build a system that can detect it in order to help an overwhelmed health care system. Many research studies on chest diseases rely on the strengths of deep learning techniques. Although some of these studies used state-of-the-art techniques and were able to deliver promising results, these techniques are not very useful if they can detect only one type of disease without detecting the others. OBJECTIVE: The main objective of this study was to achieve a fast and more accurate diagnosis of COVID-19. This study proposes a diagnostic technique that classifies COVID-19 x-ray images from normal x-ray images and those specific to 14 other chest diseases. METHODS: In this paper, we propose a novel, multilevel pipeline, based on deep learning models, to detect COVID-19 along with other chest diseases based on x-ray images. This pipeline reduces the burden of a single network to classify a large number of classes. The deep learning models used in this study were pretrained on the ImageNet dataset, and transfer learning was used for fast training. The lungs and heart were segmented from the whole x-ray images and passed onto the first classifier that checks whether the x-ray is normal, COVID-19 affected, or characteristic of another chest disease. If it is neither a COVID-19 x-ray image nor a normal one, then the second classifier comes into action and classifies the image as one of the other 14 diseases. RESULTS: We show how our model uses state-of-the-art deep neural networks to achieve classification accuracy for COVID-19 along with 14 other chest diseases and normal cases based on x-ray images, which is competitive with currently used state-of-the-art models. Due to the lack of data in some classes such as COVID-19, we applied 10-fold cross-validation through the ResNet50 model. Our classification technique thus achieved an average training accuracy of 96.04% and test accuracy of 92.52% for the first level of classification (ie, 3 classes). For the second level of classification (ie, 14 classes), our technique achieved a maximum training accuracy of 88.52% and test accuracy of 66.634% by using ResNet50. We also found that when all the 16 classes were classified at once, the overall accuracy for COVID-19 detection decreased, which in the case of ResNet50 was 88.92% for training data and 71.905% for test data. CONCLUSIONS: Our proposed pipeline can detect COVID-19 with a higher accuracy along with detecting 14 other chest diseases based on x-ray images. This is achieved by dividing the classification task into multiple steps rather than classifying them collectively.",2021,10.2196/23693,cross-sectional,diagnosis,CXR,Lung
Fast and accurate reconstruction of human lung gas MRI with deep learning,"PURPOSE: To fast and accurately reconstruct human lung gas MRI from highly undersampled k-space using deep learning. METHODS: The scheme was comprised of coarse-to-fine nets (C-net and F-net). Zero-filling images from retrospectively undersampled k-space at an acceleration factor of 4 were used as input for C-net, and then output intermediate results which were fed into F-net. During training, a L2 loss function was adopted in C-net, while a function that united L2 loss with proton prior knowledge was used in F-net. The 871 hyperpolarized (129) Xe pulmonary ventilation images from 72 volunteers were randomly arranged as training (90%) and testing (10%) data. Ventilation defect percentage comparisons were implemented using a paired 2-tailed Student's t-test and correlation analysis. Furthermore, prospective acquisitions were demonstrated in 5 healthy subjects and 5 asymptomatic smokers. RESULTS: Each image with size of 96 × 84 could be reconstructed within 31 ms (mean absolute error was 4.35% and structural similarity was 0.7558). Compared with conventional compressed sensing MRI, the mean absolute error decreased by 17.92%, but the structural similarity increased by 6.33%. For ventilation defect percentage, there were no significant differences between the fully sampled and reconstructed images through the proposed algorithm (P = 0.932), but had significant correlations (r = 0.975; P < 0.001). The prospectively undersampled results validated a good agreement with fully sampled images, with no significant differences in ventilation defect percentage but significantly higher signal-to-noise ratio values. CONCLUSION: The proposed algorithm outperformed classical undersampling methods, paving the way for future use of deep learning in real-time and accurate reconstruction of gas MRI.",2019,10.1002/mrm.27889,prospective cohort,others,MRI,Lung
Fast and fully-automated detection and segmentation of pulmonary nodules in thoracic CT scans using deep convolutional neural networks,"Deep learning techniques have been extensively used in computerized pulmonary nodule analysis in recent years. Many reported studies still utilized hybrid methods for diagnosis, in which convolutional neural networks (CNNs) are used only as one part of the pipeline, and the whole system still needs either traditional image processing modules or human intervention to obtain final results. In this paper, we introduced a fast and fully-automated end-to-end system that can efficiently segment precise lung nodule contours from raw thoracic CT scans. Our proposed system has four major modules: candidate nodule detection with Faster regional-CNN (R-CNN), candidate merging, false positive (FP) reduction with CNN, and nodule segmentation with customized fully convolutional neural network (FCN). The entire system has no human interaction or database specific design. The average runtime is about 16 s per scan on a standard workstation. The nodule detection accuracy is 91.4% and 94.6% with an average of 1 and 4 false positives (FPs) per scan. The average dice coefficient of nodule segmentation compared to the groundtruth is 0.793.",2019,10.1016/j.compmedimag.2019.02.003,cross-sectional,diagnosis,CT,Lung
Fast automated detection of COVID-19 from medical images using convolutional neural networks,"Coronavirus disease 2019 (COVID-19) is a global pandemic posing significant health risks. The diagnostic test sensitivity of COVID-19 is limited due to irregularities in specimen handling. We propose a deep learning framework that identifies COVID-19 from medical images as an auxiliary testing method to improve diagnostic sensitivity. We use pseudo-coloring methods and a platform for annotating X-ray and computed tomography images to train the convolutional neural network, which achieves a performance similar to that of experts and provides high scores for multiple statistical indices (F1 scores > 96.72% (0.9307, 0.9890) and specificity >99.33% (0.9792, 1.0000)). Heatmaps are used to visualize the salient features extracted by the neural network. The neural network-based regression provides strong correlations between the lesion areas in the images and five clinical indicators, resulting in high accuracy of the classification framework. The proposed method represents a potential computer-aided diagnosis method for COVID-19 in clinical practice.",2021,10.1038/s42003-020-01535-7,cross-sectional,diagnosis,CT/CXR,Lung
FBSED based automatic diagnosis of COVID-19 using X-ray and CT images,"This work introduces the Fourier-Bessel series expansion-based decomposition (FBSED) method, which is an implementation of the wavelet packet decomposition approach in the Fourier-Bessel series expansion domain. The proposed method has been used for the diagnosis of pneumonia caused by the 2019 novel coronavirus disease (COVID-19) using chest X-ray image (CXI) and chest computer tomography image (CCTI). The FBSED method is used to decompose CXI and CCTI into sub-band images (SBIs). The SBIs are then used to train various pre-trained convolutional neural network (CNN) models separately using a transfer learning approach. The combination of SBI and CNN is termed as one channel. Deep features from each channel are fused to get a feature vector. Different classifiers are used to classify pneumonia caused by COVID-19 from other viral and bacterial pneumonia and healthy subjects with the extracted feature vector. The different combinations of channels have also been analyzed to make the process computationally efficient. For CXI and CCTI databases, the best performance has been obtained with only one and four channels, respectively. The proposed model was evaluated using 5-fold and 10-fold cross-validation processes. The average accuracy for the CXI database was 100% for both 5-fold and 10-fold cross-validation processes, and for the CCTI database, it is 97.6% for the 5-fold cross-validation process. Therefore, the proposed method may be used by radiologists to rapidly diagnose patients with COVID-19.",2021,10.1016/j.compbiomed.2021.104454,cross-sectional,diagnosis,CT/CXR,Lung
Feasibility of volatile organic compound in breath analysis in the follow-up of colorectal cancer: A pilot study,"BACKGROUND: Colorectal carcinoma (CRC) has a worldwide incidence of 1.4 million patients and a large share in cancer-related mortality. After curative treatment, the risk of recurrence is 30-65%. Early detection may result in curative treatment. However, current follow-up (FU) examinations have low sensitivity ranging from 49 to 85% and are associated with high costs. Therefore, the search for a new diagnostic tool is justified. Analysis of volatile organic compound in exhaled air through an electronic nose (eNose) is a promising new patient-friendly diagnostic tool. We studied whether the eNose under investigation, the Aeonose™, is able to detect local recurrence or metastases of CRC. METHODS: In this cross-sectional study we included 62 patients, all of whom underwent curative treatment for CRC in the past 5 years. Thirty-six of them had no metastases and 26 had extraluminal local recurrence or metastases of CRC, detected during FU. Breath testing was performed and machine learning was used to predict extraluminal recurrences or metastases, and based on the receiver operating characteristics (ROC)-curve both sensitivity and specificity were calculated. RESULTS: The eNose identified extra luminal local recurrences or metastases of CRC with a sensitivity and specificity of 0.88 (CI 0.69-0.97) and 0.75 (CI 0.57-0.87), respectively, with an overall accuracy of 0.81. DISCUSSION: This eNose may be a promising tool in detecting extraluminal local recurrences or metastases in the FU of curatively treated CRC. However, a well-designed prospective study is warranted to show its accuracy and predictive value before it can be used in clinical practice.",2020,10.1016/j.ejso.2020.07.028,,,,
Feasibility Study of a Generalized Framework for Developing Computer-Aided Detection Systems-a New Paradigm,"We propose a generalized framework for developing computer-aided detection (CADe) systems whose characteristics depend only on those of the training dataset. The purpose of this study is to show the feasibility of the framework. Two different CADe systems were experimentally developed by a prototype of the framework, but with different training datasets. The CADe systems include four components; preprocessing, candidate area extraction, candidate detection, and candidate classification. Four pretrained algorithms with dedicated optimization/setting methods corresponding to the respective components were prepared in advance. The pretrained algorithms were sequentially trained in the order of processing of the components. In this study, two different datasets, brain MRA with cerebral aneurysms and chest CT with lung nodules, were collected to develop two different types of CADe systems in the framework. The performances of the developed CADe systems were evaluated by threefold cross-validation. The CADe systems for detecting cerebral aneurysms in brain MRAs and for detecting lung nodules in chest CTs were successfully developed using the respective datasets. The framework was shown to be feasible by the successful development of the two different types of CADe systems. The feasibility of this framework shows promise for a new paradigm in the development of CADe systems: development of CADe systems without any lesion specific algorithm designing.",2017,10.1007/s10278-017-9968-3,,,,
Feature fusion for lung nodule classification,"PURPOSE: This article examines feature-based nodule description for the purpose of nodule classification in chest computed tomography scanning. METHODS: Three features based on (i) Gabor filter, (ii) multi-resolution local binary pattern (LBP) texture features and (iii) signed distance fused with LBP which generates a combinational shape and texture feature are utilized to provide feature descriptors of malignant and benign nodules and non-nodule regions of interest. Support vector machines (SVMs) and k-nearest neighbor (kNN) classifiers in serial and two-tier cascade frameworks are optimized and analyzed for optimal classification results of nodules. RESULTS: A total of 1191 nodule and non-nodule samples from the Lung Image Data Consortium database is used for analysis. Classification using SVM and kNN classifiers is examined. The classification results from the two-tier cascade SVM using Gabor features showed overall better results for identifying non-nodules, malignant and benign nodules with average area under the receiver operating characteristics (AUC-ROC) curves of 0.99 and average f1-score of 0.975 over the two tiers. CONCLUSION: In the results, higher overall AUCs and f1-scores were obtained for the non-nodules cases using any of the three features, showing the greatest distinguishability over nodules (benign/malignant). SVM and kNN classifiers were used for benign, malignant and non-nodule classification, where Gabor proved to be the most effective of the features for classification. The cascaded framework showed the greatest distinguishability between benign and malignant nodules.",2017,10.1007/s11548-017-1626-1,,,,
Feature-driven local cell graph (FLocK): New computational pathology-based descriptors for prognosis of lung cancer and HPV status of oropharyngeal cancers,"Local spatial arrangement of nuclei in histopathology images of different cancer subtypes has been shown to have prognostic value. In order to capture localized nuclear architectural information, local cell cluster graph-based measurements have been proposed. However, conventional ways of cell graph construction only utilize nuclear spatial proximity, and do not differentiate between different cell types while constructing the graph. In this paper, we present feature-driven local cell cluster graph (FLocK), a new approach to constructing local cell graphs by simultaneously considering spatial proximity and attributes of the individual nuclei (e.g. shape, size, texture). In addition, we have designed a new set of quantitative graph-derived metrics to be extracted from FLocKs, in turn capturing the interplay between different proximally located clusters of nuclei. We have evaluated the efficacy of FLocK features extracted from H&E stained tissue images in two clinical applications: to classify short-term vs. long-term survival among patients of early stage non-small cell lung cancer (ES-NSCLC), and also to predict human papillomavirus (HPV) status of oropharyngeal squamous cell carcinoma (OP-SCCs). In the classification of long-term vs. short-term survival among patients of ES-NSCLC (training cohort, n = 434), the top 10 discriminative FLocK features related to the variation of FLocK size and intersected FLocK distance were identified, via Minimum Redundancy and Maximum Relevance (MRMR) selection, in 100 runs of 10-fold cross-validation, and in conjunction with a linear discriminant classifier yielded a mean AUC of 0.68 for predicting survival in the training cohort. This is better than other state-of-art histomorphometric and deep learning classifiers (cell cluster graphs (AUC = 0.62), global cell graph (AUC = 0.56), nuclear shape (AUC = 0.54), nuclear orientation (AUC = 0.61), AlexNet (AUC = 0.55), ResNet (AUC = 0.56)). The FLocK-based classifier yielded an AUC of 0.70 in an independent testing cohort (n = 150). The patients identified as ""high-risk"" had significantly poorer overall survival in the testing cohort, with a hazard ratio (95% confidence interval) of 2.24 (1.24-4.05), p = 0.01144). In the classification of HPV status of OP-SCC, the top three FLocK features pertaining to the portion of intersected FLocKs were used to construct a classifier, which yielded an AUC of 0.80 in the training cohort (n = 50), and an accuracy of 0.78 in an independent testing cohort (n = 35). The combination of FLocK measurements with cell cluster graphs, nuclear orientation, and nuclear shape improved the training AUC to 0.87, 0.91 and 0.85, respectively. Deep learning approaches yielded marginally better performance than the FLocK-based classifier in this application, with AUC = 0.78 for AlexNet, AUC = 0.81 for ResNet, and AUC = 0.76 for FLocK-based classifier in the testing cohort. However, the combination of two hand-crafted features: FLocK and nuclear orientation yielded a better performance (AUC = 0.84). FLocK provides a unique and quantitative way to analyze histology images of solid tumors and interrogate tumor morphology from a different aspect than existing histomorphometrics. The source code can be accessed at https://github.com/hacylu/FLocK.",2021,10.1016/j.media.2020.101903,,,,
Feature-shared adaptive-boost deep learning for invasiveness classification of pulmonary subsolid nodules in CT images,"PURPOSE: In clinical practice, invasiveness is an important reference indicator for differentiating the malignant degree of subsolid pulmonary nodules. These nodules can be classified as atypical adenomatous hyperplasia (AAH), adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IAC). The automatic determination of a nodule's invasiveness based on chest CT scans can guide treatment planning. However, it is challenging, owing to the insufficiency of training data and their interclass similarity and intraclass variation. To address these challenges, we propose a two-stage deep learning strategy for this task: prior-feature learning followed by adaptive-boost deep learning. METHODS: The adaptive-boost deep learning is proposed to train a strong classifier for invasiveness classification of subsolid nodules in chest CT images, using multiple 3D convolutional neural network (CNN)-based weak classifiers. Because ensembles of multiple deep 3D CNN models have a huge number of parameters and require large computing resources along with more training and testing time, the prior-feature learning is proposed to reduce the computations by sharing the CNN layers between all weak classifiers. Using this strategy, all weak classifiers can be integrated into a single network. RESULTS: Tenfold cross validation of binary classification was conducted on a total of 1357 nodules, including 765 noninvasive (AAH and AIS) and 592 invasive nodules (MIA and IAC). Ablation experimental results indicated that the proposed binary classifier achieved an accuracy of 73.4\% ± 1.4 with an AUC of 81.3 \% ± 2.2 . These results are superior compared to those achieved by three experienced chest imaging specialists who achieved an accuracy of 69.1\% , 69.3\% , and 67.9\% , respectively. About 200 additional nodules were also collected. These nodules covered 50 cases for each category (AAH, AIS, MIA, and IAC, respectively). Both binary and multiple classifications were performed on these data and the results demonstrated that the proposed method definitely achieves better performance than the performance achieved by nonensemble deep learning methods. CONCLUSIONS: It can be concluded that the proposed adaptive-boost deep learning can significantly improve the performance of invasiveness classification of pulmonary subsolid nodules in CT images, while the prior-feature learning can significantly reduce the total size of deep models. The promising results on clinical data show that the trained models can be used as an effective lung cancer screening tool in hospitals. Moreover, the proposed strategy can be easily extended to other similar classification tasks in 3D medical images.",2020,10.1002/mp.14068,cross-sectional,diagnosis,CT,Lung
Federated learning for predicting clinical outcomes in patients with COVID-19,"Federated learning (FL) is a method used for training artificial intelligence models with data from multiple sources while maintaining data anonymity, thus removing many barriers to data sharing. Here we used data from 20 institutes across the globe to train a FL model, called EXAM (electronic medical record (EMR) chest X-ray AI model), that predicts the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs, laboratory data and chest X-rays. EXAM achieved an average area under the curve (AUC) >0.92 for predicting outcomes at 24 and 72 h from the time of initial presentation to the emergency room, and it provided 16% improvement in average AUC measured across all participating sites and an average increase in generalizability of 38% when compared with models trained at a single site using that site's data. For prediction of mechanical ventilation treatment or death at 24 h at the largest independent test site, EXAM achieved a sensitivity of 0.950 and specificity of 0.882. In this study, FL facilitated rapid data science collaboration without data exchange and generated a model that generalized across heterogeneous, unharmonized datasets for prediction of clinical outcomes in patients with COVID-19, setting the stage for the broader use of FL in healthcare.",2021,10.1038/s41591-021-01506-3,prospective cohort,prognosis,CT,Lung
Federated Semi-Supervised Multi-Task Learning to Detect COVID-19 and Lungs Segmentation Marking Using Chest Radiography Images and Raspberry Pi Devices: An Internet of Medical Things Application,"Internet of Medical Things (IoMT) provides an excellent opportunity to investigate better automatic medical decision support tools with the effective integration of various medical equipment and associated data. This study explores two such medical decision-making tasks, namely COVID-19 detection and lung area segmentation detection, using chest radiography images. We also explore different cutting-edge machine learning techniques, such as federated learning, semi-supervised learning, transfer learning, and multi-task learning to explore the issue. To analyze the applicability of computationally less capable edge devices in the IoMT system, we report the results using Raspberry Pi devices as accuracy, precision, recall, Fscore for COVID-19 detection, and average dice score for lung segmentation detection tasks. We also publish the results obtained through server-centric simulation for comparison. The results show that Raspberry Pi-centric devices provide better performance in lung segmentation detection, and server-centric experiments provide better results in COVID-19 detection. We also discuss the IoMT application-centric settings, utilizing medical data and decision support systems, and posit that such a system could benefit all the stakeholders in the IoMT domain.",2021,10.3390/s21155025,cross-sectional,diagnosis,CXR,Lung
FedSGDCOVID: Federated SGD COVID-19 Detection under Local Differential Privacy Using Chest X-ray Images and Symptom Information,"Coronavirus (COVID-19) has created an unprecedented global crisis because of its detrimental effect on the global economy and health. COVID-19 cases have been rapidly increasing, with no sign of stopping. As a result, test kits and accurate detection models are in short supply. Early identification of COVID-19 patients will help decrease the infection rate. Thus, developing an automatic algorithm that enables the early detection of COVID-19 is essential. Moreover, patient data are sensitive, and they must be protected to prevent malicious attackers from revealing information through model updates and reconstruction. In this study, we presented a higher privacy-preserving federated learning system for COVID-19 detection without sharing data among data owners. First, we constructed a federated learning system using chest X-ray images and symptom information. The purpose is to develop a decentralized model across multiple hospitals without sharing data. We found that adding the spatial pyramid pooling to a 2D convolutional neural network improves the accuracy of chest X-ray images. Second, we explored that the accuracy of federated learning for COVID-19 identification reduces significantly for non-independent and identically distributed (Non-IID) data. We then proposed a strategy to improve the model's accuracy on Non-IID data by increasing the total number of clients, parallelism (client-fraction), and computation per client. Finally, for our federated learning model, we applied a differential privacy stochastic gradient descent (DP-SGD) to improve the privacy of patient data. We also proposed a strategy to maintain the robustness of federated learning to ensure the security and accuracy of the model.",2022,10.3390/s22103728,cross-sectional,diagnosis,CXR,Lung
FissureNet: A Deep Learning Approach For Pulmonary Fissure Detection in CT Images,"Pulmonary fissure detection in computed tomography (CT) is a critical component for automatic lobar segmentation. The majority of fissure detection methods use feature descriptors that are hand-crafted, low-level, and have local spatial extent. The design of such feature detectors is typically targeted toward normal fissure anatomy, yielding low sensitivity to weak, and abnormal fissures that are common in clinical data sets. Furthermore, local features commonly suffer from low specificity, as the complex textures in the lung can be indistinguishable from the fissure when the global context is not considered. We propose a supervised discriminative learning framework for simultaneous feature extraction and classification. The proposed framework, called FissureNet, is a coarse-to-fine cascade of two convolutional neural networks. The coarse-to-fine strategy alleviates the challenges associated with training a network to segment a thin structure that represents a small fraction of the image voxels. FissureNet was evaluated on a cohort of 3706 subjects with inspiration and expiration 3DCT scans from the COPDGene clinical trial and a cohort of 20 subjects with 4DCT scans from a lung cancer clinical trial. On both data sets, FissureNet showed superior performance compared with a deep learning approach using the U-Net architecture and a Hessian-based fissure detection method in terms of area under the precision-recall curve (PR-AUC). The overall PR-AUC for FissureNet, U-Net, and Hessian on the COPDGene (lung cancer) data set was 0.980 (0.966), 0.963 (0.937), and 0.158 (0.182), respectively. On a subset of 30 COPDGene scans, FissureNet was compared with a recently proposed advanced fissure detection method called derivative of sticks (DoS) and showed superior performance with a PR-AUC of 0.991 compared with 0.668 for DoS.",2019,10.1109/tmi.2018.2858202,cross-sectional,diagnosis,CT,Lung
From community-acquired pneumonia to COVID-19: a deep learning-based method for quantitative analysis of COVID-19 on thick-section CT scans,"OBJECTIVE: To develop a fully automated AI system to quantitatively assess the disease severity and disease progression of COVID-19 using thick-section chest CT images. METHODS: In this retrospective study, an AI system was developed to automatically segment and quantify the COVID-19-infected lung regions on thick-section chest CT images. Five hundred thirty-one CT scans from 204 COVID-19 patients were collected from one appointed COVID-19 hospital. The automatically segmented lung abnormalities were compared with manual segmentation of two experienced radiologists using the Dice coefficient on a randomly selected subset (30 CT scans). Two imaging biomarkers were automatically computed, i.e., the portion of infection (POI) and the average infection HU (iHU), to assess disease severity and disease progression. The assessments were compared with patient status of diagnosis reports and key phrases extracted from radiology reports using the area under the receiver operating characteristic curve (AUC) and Cohen's kappa, respectively. RESULTS: The dice coefficient between the segmentation of the AI system and two experienced radiologists for the COVID-19-infected lung abnormalities was 0.74 ± 0.28 and 0.76 ± 0.29, respectively, which were close to the inter-observer agreement (0.79 ± 0.25). The computed two imaging biomarkers can distinguish between the severe and non-severe stages with an AUC of 0.97 (p value < 0.001). Very good agreement (κ = 0.8220) between the AI system and the radiologists was achieved on evaluating the changes in infection volumes. CONCLUSIONS: A deep learning-based AI system built on the thick-section CT imaging can accurately quantify the COVID-19-associated lung abnormalities and assess the disease severity and its progressions. KEY POINTS: • A deep learning-based AI system was able to accurately segment the infected lung regions by COVID-19 using the thick-section CT scans (Dice coefficient ≥ 0.74). • The computed imaging biomarkers were able to distinguish between the non-severe and severe COVID-19 stages (area under the receiver operating characteristic curve 0.97). • The infection volume changes computed by the AI system were able to assess the COVID-19 progression (Cohen's kappa 0.8220).",2020,10.1007/s00330-020-07042-x,cross-sectional,diagnosis,CT,Lung
Fully Automated MR Detection and Segmentation of Brain Metastases in Non-small Cell Lung Cancer Using Deep Learning,"BACKGROUND: Non-small cell lung cancer (NSCLC) is the most common tumor entity spreading to the brain and up to 50% of patients develop brain metastases (BMs). Detection of BMs on MRI is challenging with an inherent risk of missed diagnosis. PURPOSE: To train and evaluate a deep learning model (DLM) for fully automated detection and 3D segmentation of BMs in NSCLC on clinical routine MRI. STUDY TYPE: Retrospective. POPULATION: Ninety-eight NSCLC patients with 315 BMs on pretreatment MRI, divided into training (66 patients, 248 BMs) and independent test (17 patients, 67 BMs) and control (15 patients, 0 BMs) cohorts. FIELD STRENGTH/SEQUENCE: T(1) -/T(2) -weighted, T(1) -weighted contrast-enhanced (T(1) CE; gradient-echo and spin-echo sequences), and FLAIR at 1.0, 1.5, and 3.0 T from various vendors and study centers. ASSESSMENT: A 3D convolutional neural network (DeepMedic) was trained on the training cohort using 5-fold cross-validation and evaluated on the independent test and control sets. Three-dimensional voxel-wise manual segmentations of BMs by a neurosurgeon and a radiologist on T(1) CE served as the reference standard. STATISTICAL TESTS: Sensitivity (recall) and false positive (FP) findings per scan, dice similarity coefficient (DSC) to compare the spatial overlap between manual and automated segmentations, Pearson's correlation coefficient (r) to evaluate the relationship between quantitative volumetric measurements of segmentations, and Wilcoxon rank-sum test to compare the volumes of BMs. A P value <0.05 was considered statistically significant. RESULTS: In the test set, the DLM detected 57 of the 67 BMs (mean volume: 0.99 ± 4.24 cm(3) ), resulting in a sensitivity of 85.1%, while FP findings of 1.5 per scan were observed. Missed BMs had a significantly smaller volume (0.05 ± 0.04 cm(3) ) than detected BMs (0.96 ± 2.4 cm(3) ). Compared with the reference standard, automated segmentations achieved a median DSC of 0.72 and a good volumetric correlation (r = 0.95). In the control set, 1.8 FPs/scan were observed. DATA CONCLUSION: Deep learning provided a high detection sensitivity and good segmentation performance for BMs in NSCLC on heterogeneous scanner data while yielding a low number of FP findings. Level of Evidence 3 Technical Efficacy Stage 2.",2021,10.1002/jmri.27741,,,,
Fully automated unified prognosis of Covid-19 chest X-ray/CT scan images using Deep Covix-Net model,"SARS-COV2 (Covid-19) prevails in the form of multiple mutant variants causing pandemic situations around the world. Thus, medical diagnosis is not accurate. Although several clinical diagnostic methodologies have been introduced hitherto, chest X-ray and computed tomography (CT) imaging techniques complement the analytical methods (for instance, RT-PCR) to a certain extent. In this context, we demonstrate a novel framework by employing various image segmentation models to leverage the available image databases (9000 chest X-ray images and 6000 CT scan images). The proposed methodology is expected to assist in the prognosis of Covid-19-infected individuals through examination of chest X-rays and CT scans of images using the Deep Covix-Net model for identifying novel coronavirus-infected patients effectively and efficiently. The slice of the precision score is analysed in terms of performance metrics such as accuracy, the confusion matrix, and the receiver operating characteristic curve. The result leans on the database obtainable in the GitHub and Kaggle repository, conforming to their endorsed chest X-ray and CT images. The classification performances of various algorithms were examined for a test set with 1800 images. The proposed model achieved a 96.8% multiple-classification accuracy among Covid-19, normal, and pneumonia chest X-ray databases. Moreover, it attained a 97% accuracy among Covid-19 and normal CT scan images. Thus, the proposed mechanism achieves the rigorousness associated with the machine learning technique, providing rapid outcomes for both training and testing datasets.",2021,10.1016/j.compbiomed.2021.104729,cross-sectional,prognosis,CT/CXR,Lung
Fully automatic detection of lung nodules in CT images using a hybrid feature set,"PURPOSE: The aim of this study was to develop a novel technique for lung nodule detection using an optimized feature set. This feature set has been achieved after rigorous experimentation, which has helped in reducing the false positives significantly. METHOD: The proposed method starts with preprocessing, removing any present noise from input images, followed by lung segmentation using optimal thresholding. Then the image is enhanced using multiscale dot enhancement filtering prior to nodule detection and feature extraction. Finally, classification of lung nodules is achieved using Support Vector Machine (SVM) classifier. The feature set consists of intensity, shape (2D and 3D) and texture features, which have been selected to optimize the sensitivity and reduce false positives. In addition to SVM, some other supervised classifiers like K-Nearest-Neighbor (KNN), Decision Tree and Linear Discriminant Analysis (LDA) have also been used for performance comparison. The extracted features have also been compared class-wise to determine the most relevant features for lung nodule detection. The proposed system has been evaluated using 850 scans from Lung Image Database Consortium (LIDC) dataset and k-fold cross-validation scheme. RESULTS: The overall sensitivity has been improved compared to the previous methods and false positives per scan have been reduced significantly. The achieved sensitivities at detection and classification stages are 94.20% and 98.15%, respectively, with only 2.19 false positives per scan. CONCLUSIONS: It is very difficult to achieve high performance metrics using only a single feature class therefore hybrid approach in feature selection remains a better choice. Choosing right set of features can improve the overall accuracy of the system by improving the sensitivity and reducing false positives.",2017,10.1002/mp.12273,cross-sectional,diagnosis,CT,Lung
Fully automatic pipeline of convolutional neural networks and capsule networks to distinguish COVID-19 from community-acquired pneumonia via CT images,"BACKGROUND: Chest computed tomography (CT) is crucial in the diagnosis of coronavirus disease 2019 (COVID-19). However, the persistent pandemic and similar CT manifestations between COVID-19 and community-acquired pneumonia (CAP) raise methodological requirements. METHODS: A fully automatic pipeline of deep learning is proposed for distinguishing COVID-19 from CAP using CT images. Inspired by the diagnostic process of radiologists, the pipeline comprises four connected modules for lung segmentation, selection of slices with lesions, slice-level prediction, and patient-level prediction. The roles of the first and second modules and the effectiveness of the capsule network for slice-level prediction were investigated. A dataset of 326 CT scans was collected to train and test the pipeline. Another public dataset of 110 patients was used to evaluate the generalization capability. RESULTS: LinkNet exhibited the largest intersection over union (0.967) and Dice coefficient (0.983) for lung segmentation. For the selection of slices with lesions, the capsule network with the ResNet50 block achieved an accuracy of 92.5% and an area under the curve (AUC) of 0.933. The capsule network using the DenseNet121 block demonstrated better performance for slice-level prediction, with an accuracy of 97.1% and AUC of 0.992. For both datasets, the prediction accuracy of our pipeline was 100% at the patient level. CONCLUSIONS: The proposed fully automatic deep learning pipeline of deep learning can distinguish COVID-19 from CAP via CT images rapidly and accurately, thereby accelerating diagnosis and augmenting the performance of radiologists. This pipeline is convenient for use by radiologists and provides explainable predictions.",2022,10.1016/j.compbiomed.2021.105182,cross-sectional,diagnosis,CT,Lung
Fully convolutional network-based multi-output model for automatic segmentation of organs at risk in thorax,"PURPOSE: To propose a multi-output fully convolutional network (MOFCN) to segment bilateral lung, heart and spinal cord in the planning thoracic computed tomography (CT) slices automatically and simultaneously. METHODS: The MOFCN includes two components: one main backbone and three branches. The main backbone extracts the features about lung, heart and spinal cord. The extracted features are transferred to three branches which correspond to three organs respectively. The longest branch to segment spinal cord is nine layers, including input and output layers. The MOFCN was evaluated on 19,277 CT slices from 966 patients with cancer in the thorax. In these slices, the organs at risk (OARs) were delineated and validated by experienced radiation oncologists, and served as ground truth for training and evaluation. The data from 61 randomly chosen patients were used for training and validation. The remaining 905 patients' slices were used for testing. The metric used to evaluate the similarity between the auto-segmented organs and their ground truth was Dice. Besides, we compared the MOFCN with other published models. To assess the distinct output design and the impact of layer number and dilated convolution, we compared MOFCN with a multi-label learning model and its variants. By analyzing the not good performances, we suggested possible solutions. RESULTS: MOFCN achieved Dice of 0.95 ± 0.02 for lung, 0.91 ± 0.03 for heart and 0.87 ± 0.06 for spinal cord. Compared to other models, MOFCN could achieve a comparable accuracy with the least time cost. CONCLUSION: The results demonstrated the MOFCN's effectiveness. It uses less parameters to delineate three OARs simultaneously and automatically, and thus shows a relatively low requirement for hardware and has potential for broad application.",2021,10.1177/00368504211020161,cross-sectional,treatment,CT,NA
Fully Integrated Quantitative Multiparametric Analysis of Non-Small Cell Lung Cancer at 3-T PET/MRI: Toward One-Stop-Shop Tumor Biological Characterization at the Supervoxel Level,"INTRODUCTION: The aim of this study was to study the feasibility of a fully integrated multiparametric imaging framework to characterize non-small cell lung cancer (NSCLC) at 3-T PET/MRI. PATIENTS AND METHODS: An 18F-FDG PET/MRI multiparametric imaging framework was developed and prospectively applied to 11 biopsy-proven NSCLC patients. For each tumor, 12 parametric maps were generated, including PET full kinetic modeling, apparent diffusion coefficient, T1/T2 relaxation times, and DCE full kinetic modeling. Gaussian mixture model-based clustering was applied at the whole data set level to define supervoxels of similar multidimensional PET/MRI behaviors. Taking the multidimensional voxel behaviors as input and the supervoxel class as output, machine learning procedure was finally trained and validated voxelwise to reveal the dominant PET/MRI characteristics of these supervoxels at the whole data set and individual tumor levels. RESULTS: The Gaussian mixture model-based clustering clustering applied at the whole data set level (17,316 voxels) found 3 main multidimensional behaviors underpinned by the 12 PET/MRI quantitative parameters. Four dominant PET/MRI parameters of clinical relevance (PET: k2, k3 and DCE: ve, vp) predicted the overall supervoxel behavior with 97% of accuracy (SD, 0.7; 10-fold cross-validation). At the individual tumor level, these dimensionality-reduced supervoxel maps showed mean discrepancy of 16.7% compared with the original ones. CONCLUSIONS: One-stop-shop PET/MRI multiparametric quantitative analysis of NSCLC is clinically feasible. Both PET and MRI parameters are useful to characterize the behavior of tumors at the supervoxel level. In the era of precision medicine, the full capabilities of PET/MRI would give further insight of the characterization of NSCLC behavior, opening new avenues toward image-based personalized medicine in this field.",2021,10.1097/rlu.0000000000003680,cross-sectional,diagnosis,PET/MRI,Lung
Fully-Connected Neural Networks with Reduced Parameterization for Predicting Histological Types of Lung Cancer from Somatic Mutations,"Several challenges appear in the application of deep learning to genomic data. First, the dimensionality of input can be orders of magnitude greater than the number of samples, forcing the model to be prone to overfitting the training dataset. Second, each input variable's contribution to the prediction is usually difficult to interpret, owing to multiple nonlinear operations. Third, genetic data features sometimes have no innate structure. To alleviate these problems, we propose a modification to Diet Networks by adding element-wise input scaling. The original Diet Networks concept can considerably reduce the number of parameters of the fully-connected layers by taking the transposed data matrix as an input to its auxiliary network. The efficacy of the proposed architecture was evaluated on a binary classification task for lung cancer histology, that is, adenocarcinoma or squamous cell carcinoma, from a somatic mutation profile. The dataset consisted of 950 cases, and 5-fold cross-validation was performed for evaluating the model performance. The model achieved a prediction accuracy of around 80% and showed that our modification markedly stabilized the learning process. Also, latent representations acquired inside the model allowed us to interpret the relationship between somatic mutation sites for the prediction.",2020,10.3390/biom10091249,,,,
Functional connectivity and information flow of the respiratory neural network in chronic obstructive pulmonary disease,"Breathing involves a complex interplay between the brainstem automatic network and cortical voluntary command. How these brain regions communicate at rest or during inspiratory loading is unknown. This issue is crucial for several reasons: (i) increased respiratory loading is a major feature of several respiratory diseases, (ii) failure of the voluntary motor and cortical sensory processing drives is among the mechanisms that precede acute respiratory failure, (iii) several cerebral structures involved in responding to inspiratory loading participate in the perception of dyspnea, a distressing symptom in many disease. We studied functional connectivity and Granger causality of the respiratory network in controls and patients with chronic obstructive pulmonary disease (COPD), at rest and during inspiratory loading. Compared with those of controls, the motor cortex area of patients exhibited decreased connectivity with their contralateral counterparts and no connectivity with the brainstem. In the patients, the information flow was reversed at rest with the source of the network shifted from the medulla towards the motor cortex. During inspiratory loading, the system was overwhelmed and the motor cortex became the sink of the network. This major finding may help to understand why some patients with COPD are prone to acute respiratory failure. Network connectivity and causality were related to lung function and illness severity. We validated our connectivity and causality results with a mathematical model of neural network. Our findings suggest a new therapeutic strategy involving the modulation of brain activity to increase motor cortex functional connectivity and improve respiratory muscles performance in patients. Hum Brain Mapp 37:2736-2754, 2016. © 2016 The Authors Human Brain Mapping Published by Wiley Periodicals, Inc.",2016,10.1002/hbm.23205,,,,
Fused feature signatures to probe tumour radiogenomics relationships,"Radiogenomics relationships (RRs) aims to identify statistically significant correlations between medical image features and molecular characteristics from analysing tissue samples. Previous radiogenomics studies mainly relied on a single category of image feature extraction techniques (ETs); these are (i) handcrafted ETs that encompass visual imaging characteristics, curated from knowledge of human experts and, (ii) deep ETs that quantify abstract-level imaging characteristics from large data. Prior studies therefore failed to leverage the complementary information that are accessible from fusing the ETs. In this study, we propose a fused feature signature (FF(Sig)): a selection of image features from handcrafted and deep ETs (e.g., transfer learning and fine-tuning of deep learning models). We evaluated the FF(Sig)'s ability to better represent RRs compared to individual ET approaches with two public datasets: the first dataset was used to build the FF(Sig) using 89 patients with non-small cell lung cancer (NSCLC) comprising of gene expression data and CT images of the thorax and the upper abdomen for each patient; the second NSCLC dataset comprising of 117 patients with CT images and RNA-Seq data and was used as the validation set. Our results show that our FF(Sig) encoded complementary imaging characteristics of tumours and identified more RRs with a broader range of genes that are related to important biological functions such as tumourigenesis. We suggest that the FF(Sig) has the potential to identify important RRs that may assist cancer diagnosis and treatment in the future.",2022,10.1038/s41598-022-06085-y,cross-sectional,diagnosis,CT,Lung
Fusion of 3D lung CT and serum biomarkers for diagnosis of multiple pathological types on pulmonary nodules,"BACKGROUND AND OBJECTIVE: Current researches on pulmonary nodules mainly focused on the binary-classification of benign and malignant pulmonary nodules. However, in clinical applications, it is not enough to judge whether pulmonary nodules are benign or malignant. In this paper, we proposed a fusion model based on the Lung Information Dataset Containing 3D CT Images and Serum Biomarkers (LIDCCISB) we constructed to accurately diagnose the types of pulmonary nodules in squamous cell carcinoma, adenocarcinoma, inflammation and other benign diseases. METHODS: Using single modal information of lung 3D CT images and single modal information of Lung Tumor Biomarkers (LTBs) in LIDCCISB, a Multi-resolution 3D Multi-classification deep learning model (Mr-Mc) and a Multi-Layer Perceptron machine learning model (MLP) were constructed for diagnosing multiple pathological types of pulmonary nodules, respectively. To comprehensively use the double modal information of CT images and LTBs, we used transfer learning to fuse Mr-Mc and MLP, and constructed a multimodal information fusion model that could classify multiple pathological types of benign and malignant pulmonary nodules. RESULTS: Experiments showed that the constructed Mr-Mc model can achieve an average accuracy of 0.805 and MLP model can achieve an average accuracy of 0.887. The fusion model was verified on a dataset containing 64 samples, and achieved an average accuracy of 0.906. CONCLUSIONS: This is the first study to simultaneously use CT images and LTBs to diagnose multiple pathological types of benign and malignant pulmonary nodules, and experiments showed that our research was more advanced and more suitable for practical clinical applications.",2021,10.1016/j.cmpb.2021.106381,cross-sectional,diagnosis,CT,Lung
Fusion of FDG-PET Image and Clinical Features for Prediction of Lung Metastasis in Soft Tissue Sarcomas,"Extracting massive features from images to quantify tumors provides a new insight to solve the problem that tumor heterogeneity is difficult to assess quantitatively. However, quantification of tumors by single-mode methods often has defects such as difficulty in features extraction and high computational complexity. The multimodal approach has shown effective application prospects in solving these problems. In this paper, we propose a feature fusion method based on positron emission tomography (PET) images and clinical information, which is used to obtain features for lung metastasis prediction of soft tissue sarcomas (STSs). Random forest method was adopted to select effective features by eliminating irrelevant or redundant features, and then they were used for the prediction of the lung metastasis combined with back propagation (BP) neural network. The results show that the prediction ability of the proposed model using fusion features is better than that of the model using an image or clinical feature alone. Furthermore, a good performance can be obtained using 3 standard uptake value (SUV) features of PET image and 7 clinical features, and its average accuracy, sensitivity, and specificity on all the sets can reach 92%, 91%, and 92%, respectively. Therefore, the fusing features have the potential to predict lung metastasis for STSs.",2020,10.1155/2020/8153295,retrospective cohort,prognosis,PET,Lung
GAN-based disentanglement learning for chest X-ray rib suppression,"Clinical evidence has shown that rib-suppressed chest X-rays (CXRs) can improve the reliability of pulmonary disease diagnosis. However, previous approaches on generating rib-suppressed CXR face challenges in preserving details and eliminating rib residues. We hereby propose a GAN-based disentanglement learning framework called Rib Suppression GAN, or RSGAN, to perform rib suppression by utilizing the anatomical knowledge embedded in unpaired computed tomography (CT) images. In this approach, we employ a residual map to characterize the intensity difference between CXR and the corresponding rib-suppressed result. To predict the residual map in CXR domain, we disentangle the image into structure- and contrast-specific features and transfer the rib structural priors from digitally reconstructed radiographs (DRRs) computed by CT. Furthermore, we employ additional adaptive loss to suppress rib residue and preserve more details. We conduct extensive experiments based on 1673 CT volumes, and four benchmarking CXR datasets, totaling over 120K images, to demonstrate that (i) our proposed RSGAN achieves superior image quality compared to the state-of-the-art rib suppression methods; (ii) combining CXR with our rib-suppressed result leads to better performance in lung disease classification and tuberculosis area detection.",2022,10.1016/j.media.2022.102369,cross-sectional,others,CT/CXR,NA
Generalized chest CT and lab curves throughout the course of COVID-19,"A better understanding of temporal relationships between chest CT and labs may provide a reference for disease severity over the disease course. Generalized curves of lung opacity volume and density over time can be used as standardized references from well before symptoms develop to over a month after recovery, when residual lung opacities remain. 739 patients with COVID-19 underwent CT and RT-PCR in an outbreak setting between January 21st and April 12th, 2020. 29 of 739 patients had serial exams (121 CTs and 279 laboratory measurements) over 50 ± 16 days, with an average of 4.2 sequential CTs each. Sequential volumes of total lung, overall opacity and opacity subtypes (ground glass opacity [GGO] and consolidation) were extracted using deep learning and manual segmentation. Generalized temporal curves of CT and laboratory measurements were correlated. Lung opacities appeared 3.4 ± 2.2 days prior to symptom onset. Opacity peaked 1 day after symptom onset. GGO onset was earlier and resolved later than consolidation. Lactate dehydrogenase, and C-reactive protein peaked earlier than procalcitonin and leukopenia. The temporal relationships of quantitative CT features and clinical labs have distinctive patterns and peaks in relation to symptom onset, which may inform early clinical course in patients with mild COVID-19 pneumonia, or may shed light upon chronic lung effects or mechanisms of medical countermeasures in clinical trials.",2021,10.1038/s41598-021-85694-5,,,,
Generating synthetic contrast enhancement from non-contrast chest computed tomography using a generative adversarial network,"This study aimed to evaluate a deep learning model for generating synthetic contrast-enhanced CT (sCECT) from non-contrast chest CT (NCCT). A deep learning model was applied to generate sCECT from NCCT. We collected three separate data sets, the development set (n = 25) for model training and tuning, test set 1 (n = 25) for technical evaluation, and test set 2 (n = 12) for clinical utility evaluation. In test set 1, image similarity metrics were calculated. In test set 2, the lesion contrast-to-noise ratio of the mediastinal lymph nodes was measured, and an observer study was conducted to compare lesion conspicuity. Comparisons were performed using the paired t-test or Wilcoxon signed-rank test. In test set 1, sCECT showed a lower mean absolute error (41.72 vs 48.74; P < .001), higher peak signal-to-noise ratio (17.44 vs 15.97; P < .001), higher multiscale structural similarity index measurement (0.84 vs 0.81; P < .001), and lower learned perceptual image patch similarity metric (0.14 vs 0.15; P < .001) than NCCT. In test set 2, the contrast-to-noise ratio of the mediastinal lymph nodes was higher in the sCECT group than in the NCCT group (6.15 ± 5.18 vs 0.74 ± 0.69; P < .001). The observer study showed for all reviewers higher lesion conspicuity in NCCT with sCECT than in NCCT alone (P ≤ .001). Synthetic CECT generated from NCCT improves the depiction of mediastinal lymph nodes.",2021,10.1038/s41598-021-00058-3,cross-sectional,others,CT,NA
Generating synthetic CT from low-dose cone-beam CT by using generative adversarial networks for adaptive radiotherapy,"OBJECTIVE: To develop high-quality synthetic CT (sCT) generation method from low-dose cone-beam CT (CBCT) images by using attention-guided generative adversarial networks (AGGAN) and apply these images to dose calculations in radiotherapy. METHODS: The CBCT/planning CT images of 170 patients undergoing thoracic radiotherapy were used for training and testing. The CBCT images were scanned under a fast protocol with 50% less clinical projection frames compared with standard chest M20 protocol. Training with aligned paired images was performed using conditional adversarial networks (so-called pix2pix), and training with unpaired images was carried out with cycle-consistent adversarial networks (cycleGAN) and AGGAN, through which sCT images were generated. The image quality and Hounsfield unit (HU) value of the sCT images generated by the three neural networks were compared. The treatment plan was designed on CT and copied to sCT images to calculated dose distribution. RESULTS: The image quality of sCT images by all the three methods are significantly improved compared with original CBCT images. The AGGAN achieves the best image quality in the testing patients with the smallest mean absolute error (MAE, 43.5 ± 6.69), largest structural similarity (SSIM, 93.7 ± 3.88) and peak signal-to-noise ratio (PSNR, 29.5 ± 2.36). The sCT images generated by all the three methods showed superior dose calculation accuracy with higher gamma passing rates compared with original CBCT image. The AGGAN offered the highest gamma passing rates (91.4 ± 3.26) under the strictest criteria of 1 mm/1% compared with other methods. In the phantom study, the sCT images generated by AGGAN demonstrated the best image quality and the highest dose calculation accuracy. CONCLUSIONS: High-quality sCT images were generated from low-dose thoracic CBCT images by using the proposed AGGAN through unpaired CBCT and CT images. The dose distribution could be calculated accurately based on sCT images in radiotherapy.",2021,10.1186/s13014-021-01928-w,cross-sectional,others,CT,NA
Generation of virtual lung single-photon emission computed tomography/CT fusion images for functional avoidance radiotherapy planning using machine learning algorithms,"INTRODUCTION: Functional image-guided radiotherapy (RT) planning for normal lung avoidance has recently been introduced. Single-photon emission computed tomography (SPECT)/CT can help identify the functional areas of lungs, but it is associated with delayed treatment time, additional costs and unexpected radiation exposure. In this study, we propose a machine learning algorithm that can generate functional chest CT images using the conditional generative adversarial networks (cGANs). METHODS: We collected a total of 54 lung perfusion SPECT/CT image sets from lung cancer patients who had been treated at a single institution. CT-to-SPECT image pairs that contained no lung voxels or did not match anatomically (on account of the patient's breathing) were removed at the physician's discretion. After we excluded the inappropriate images, we selected 3054 CT-to-SPECT image pairs as the training set (49 patients) and the 400 testing sets (five patients). The model was trained using the cGAN algorithm. RESULTS: We firstly evaluated the model based on multiscale SSIM (MS-SSIM). With the 400 image pairs of the testing set, we obtained a lung SPECT/CT fusion image for which the MS-SSIM was 0.87 (0.60-0.99) compared with the original image. We next estimated a gamma index between the generated and the ground truth images, resulting in a mean passing rate of 97.7 ± 1.2% with a 2%/2 mm threshold. These results supported the potential to generate functional areas of the lung parenchyma directly from chest CT images using the machine learning algorithm. CONCLUSION: The results indicate that the cGAN model used here can generate functional areas from RT planning chest CT images. This could be used for functional image-guided RT planning, for example, to spare patients' lung function without additional imaging modalities and costs. Additional studies are needed with many more training and test sets.",2019,10.1111/1754-9485.12868,cross-sectional,others,CT,NA
Generative models improve radiomics performance in different tasks and different datasets: An experimental study,"PURPOSE: Radiomics is an active area of research focusing on high throughput feature extraction from medical images with a wide array of applications in clinical practice, such as clinical decision support in oncology. However, noise in low dose computed tomography (CT) scans can impair the accurate extraction of radiomic features. In this article, we investigate the possibility of using deep learning generative models to improve the performance of radiomics from low dose CTs. METHODS: We used two datasets of low dose CT scans - NSCLC Radiogenomics and LIDC-IDRI - as test datasets for two tasks - pre-treatment survival prediction and lung cancer diagnosis. We used encoder-decoder networks and conditional generative adversarial networks (CGANs) trained in a previous study as generative models to transform low dose CT images into full dose CT images. Radiomic features extracted from the original and improved CT scans were used to build two classifiers - a support vector machine (SVM) and a deep attention based multiple instance learning model - for survival prediction and lung cancer diagnosis respectively. Finally, we compared the performance of the models derived from the original and improved CT scans. RESULTS: Denoising with the encoder-decoder network and the CGAN improved the area under the curve (AUC) of survival prediction from 0.52 to 0.57 (p-value < 0.01). On the other hand, the encoder-decoder network and the CGAN improved the AUC of lung cancer diagnosis from 0.84 to 0.88 and 0.89 respectively (p-value < 0.01). Finally, there are no statistically significant improvements in AUC using encoder-decoder networks and CGAN (p-value = 0.34) when networks trained at 75 and 100 epochs. CONCLUSION: Generative models can improve the performance of low dose CT-based radiomics in different tasks. Hence, denoising using generative models seems to be a necessary pre-processing step for calculating radiomic features from low dose CTs.",2022,10.1016/j.ejmp.2022.04.008,,,,
Generative-based airway and vessel morphology quantification on chest CT images,"Accurately and precisely characterizing the morphology of small pulmonary structures from Computed Tomography (CT) images, such as airways and vessels, is becoming of great importance for diagnosis of pulmonary diseases. The smaller conducting airways are the major site of increased airflow resistance in chronic obstructive pulmonary disease (COPD), while accurately sizing vessels can help identify arterial and venous changes in lung regions that may determine future disorders. However, traditional methods are often limited due to image resolution and artifacts. We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius. CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth. For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods. Then, in-vivo validation is performed by analyzing the association between the percentage of the predicted forced expiratory volume in one second (FEV1%) and the value of the Pi10 parameter, two well-known measures of lung function and airway disease, for airways. For vessels, we assess the correlation between our estimate of the small-vessel blood volume and the lungs' diffusing capacity for carbon monoxide (DLCO). The results demonstrate that Convolutional Neural Networks (CNNs) provide a promising direction for accurately measuring vessels and airways on chest CT images with physiological correlates.",2020,10.1016/j.media.2020.101691,,,,
Global evolution of research on pulmonary nodules: a bibliometric analysis,"Aim: To provide a historical and global picture of research concerning lung nodules, compare the contributions of major countries and explore research trends over the past 10 years. Methods: A bibliometric analysis of publications from Scopus (1970-2020) and Web of Science (2011-2020). Results: Publications about pulmonary nodules showed an enormous growth trend from 1970 to 2020. There is a high level of collaboration among the 20 most productive countries and regions, with the USA located at the center of the collaboration network. The keywords 'deep learning', 'artificial intelligence' and 'machine learning' are current hotspots. Conclusions: Abundant research has focused on pulmonary nodules. Deep learning is emerging as a promising tool for lung cancer diagnosis and management.",2021,10.2217/fon-2020-0987,cross-sectional,others,CT,NA
Graph neural networks and deep reinforcement learning for simultaneous beam orientation and trajectory optimization of Cyberknife,"Objective. Despite the high-quality treatment, the long treatment time of the Cyberknife system is believed to be a drawback. The high flexibility of its robotic arm requires meticulous path-finding algorithms to deliver the prescribed dose in the shortest time.Approach. We proposed a Deep Q-learning based on Graph Neural Networks to find the subset of the beams and the order to traverse them. A complex reward function is defined to minimize the distance covered by the robotic arm while avoiding the selection of close beams. Individual beam scores are also generated based on their effect on the beam intensity and are incorporated in the reward function. Main results. The performance of the presented method is evaluated on three clinical cases suffering from lung cancer. Applying this approach leads to an average of 35% reduction in the treatment time while delivering the prescribed dose provided by the physicians.Significance. Shorter treatment times result in a better treatment experience for individual patients, reduces discomfort and the sides effects of inadvertent movements for them. Additionally, it creates the opportunity to treat a higher number of patients in a given time period at the radiation therapy centers.",2021,10.1088/1361-6560/ac2bb5,,,,
Graph of graphs analysis for multiplexed data with application to imaging mass cytometry,"Imaging Mass Cytometry (IMC) combines laser ablation and mass spectrometry to quantitate metal-conjugated primary antibodies incubated in intact tumor tissue slides. This strategy allows spatially-resolved multiplexing of dozens of simultaneous protein targets with 1μm resolution. Each slide is a spatial assay consisting of high-dimensional multivariate observations (m-dimensional feature space) collected at different spatial positions and capturing data from a single biological sample or even representative spots from multiple samples when using tissue microarrays. Often, each of these spatial assays could be characterized by several regions of interest (ROIs). To extract meaningful information from the multi-dimensional observations recorded at different ROIs across different assays, we propose to analyze such datasets using a two-step graph-based approach. We first construct for each ROI a graph representing the interactions between the m covariates and compute an m dimensional vector characterizing the steady state distribution among features. We then use all these m-dimensional vectors to construct a graph between the ROIs from all assays. This second graph is subjected to a nonlinear dimension reduction analysis, retrieving the intrinsic geometric representation of the ROIs. Such a representation provides the foundation for efficient and accurate organization of the different ROIs that correlates with their phenotypes. Theoretically, we show that when the ROIs have a particular bi-modal distribution, the new representation gives rise to a better distinction between the two modalities compared to the maximum a posteriori (MAP) estimator. We applied our method to predict the sensitivity to PD-1 axis blockers treatment of lung cancer subjects based on IMC data, achieving 97.3% average accuracy on two IMC datasets. This serves as empirical evidence that the graph of graphs approach enables us to integrate multiple ROIs and the intra-relationships between the features at each ROI, giving rise to an informative representation that is strongly associated with the phenotypic state of the entire image.",2021,10.1371/journal.pcbi.1008741,,,,
Graph refinement based airway extraction using mean-field networks and graph neural networks,"Graph refinement, or the task of obtaining subgraphs of interest from over-complete graphs, can have many varied applications. In this work, we extract trees or collection of sub-trees from image data by, first deriving a graph-based representation of the volumetric data and then, posing the tree extraction as a graph refinement task. We present two methods to perform graph refinement. First, we use mean-field approximation (MFA) to approximate the posterior density over the subgraphs from which the optimal subgraph of interest can be estimated. Mean field networks (MFNs) are used for inference based on the interpretation that iterations of MFA can be seen as feed-forward operations in a neural network. This allows us to learn the model parameters using gradient descent. Second, we present a supervised learning approach using graph neural networks (GNNs) which can be seen as generalisations of MFNs. Subgraphs are obtained by training a GNN-based graph refinement model to directly predict edge probabilities. We discuss connections between the two classes of methods and compare them for the task of extracting airways from 3D, low-dose, chest CT data. We show that both the MFN and GNN models show significant improvement when compared to one baseline method, that is similar to a top performing method in the EXACT'09 Challenge, and a 3D U-Net based airway segmentation model, in detecting more branches with fewer false positives.",2020,10.1016/j.media.2020.101751,cross-sectional,others,CT,NA
Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis,"Although convolutional neural networks have achieved tremendous success on histopathology image classification, they usually require large-scale clean annotated data and are sensitive to noisy labels. Unfortunately, labeling large-scale images is laborious, expensive and lowly reliable for pathologists. To address these problems, in this paper, we propose a novel self-ensembling based deep architecture to leverage the semantic information of annotated images and explore the information hidden in unlabeled data, and meanwhile being robust to noisy labels. Specifically, the proposed architecture first creates ensemble targets for feature and label predictions of training samples, by using exponential moving average (EMA) to aggregate feature and label predictions within multiple previous training epochs. Then, the ensemble targets within the same class are mapped into a cluster so that they are further enhanced. Next, a consistency cost is utilized to form consensus predictions under different configurations. Finally, we validate the proposed method with extensive experiments on lung and breast cancer datasets that contain thousands of images. It can achieve 90.5% and 89.5% image classification accuracy using only 20% labeled patients on the two datasets, respectively. This performance is comparable to that of the baseline method with all labeled patients. Experiments also demonstrate its robustness to small percentage of noisy labels.",2020,10.1016/j.media.2019.101624,,,,
GraphCovidNet: A graph neural network based model for detecting COVID-19 from CT scans and X-rays of chest,"COVID-19, a viral infection originated from Wuhan, China has spread across the world and it has currently affected over 115 million people. Although vaccination process has already started, reaching sufficient availability will take time. Considering the impact of this widespread disease, many research attempts have been made by the computer scientists to screen the COVID-19 from Chest X-Rays (CXRs) or Computed Tomography (CT) scans. To this end, we have proposed GraphCovidNet, a Graph Isomorphic Network (GIN) based model which is used to detect COVID-19 from CT-scans and CXRs of the affected patients. Our proposed model only accepts input data in the form of graph as we follow a GIN based architecture. Initially, pre-processing is performed to convert an image data into an undirected graph to consider only the edges instead of the whole image. Our proposed GraphCovidNet model is evaluated on four standard datasets: SARS-COV-2 Ct-Scan dataset, COVID-CT dataset, combination of covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset and CMSC-678-ML-Project dataset. The model shows an impressive accuracy of 99% for all the datasets and its prediction capability becomes 100% accurate for the binary classification problem of detecting COVID-19 scans. Source code of this work can be found at GitHub-link .",2021,10.1038/s41598-021-87523-1,,,,
Gross Tumor Volume Segmentation for Stage III NSCLC Radiotherapy Using 3D ResSE-Unet,"INTRODUCTION: Radiotherapy is one of the most effective ways to treat lung cancer. Accurately delineating the gross target volume is a key step in the radiotherapy process. In current clinical practice, the target area is still delineated manually by radiologists, which is time-consuming and laborious. However, these problems can be better solved by deep learning-assisted automatic segmentation methods. METHODS: In this paper, a 3D CNN model named 3D ResSE-Unet is proposed for gross tumor volume segmentation for stage III NSCLC radiotherapy. This model is based on 3D Unet and combines residual connection and channel attention mechanisms. Three-dimensional convolution operation and encoding-decoding structure are used to mine three-dimensional spatial information of tumors from computed tomography data. Inspired by ResNet and SE-Net, residual connection and channel attention mechanisms are used to improve segmentation performance. A total of 214 patients with stage III NSCLC were collected selectively and 148 cases were randomly selected as the training set, 30 cases as the validation set, and 36 cases as the testing set. The segmentation performance of models was evaluated by the testing set. In addition, the segmentation results of different depths of 3D Unet were analyzed. And the performance of 3D ResSE-Unet was compared with 3D Unet, 3D Res-Unet, and 3D SE-Unet. RESULTS: Compared with other depths, 3D Unet with four downsampling depths is more suitable for our work. Compared with 3D Unet, 3D Res-Unet, and 3D SE-Unet, 3D ResSE-Unet can obtain superior results. Its dice similarity coefficient, 95th-percentile of Hausdorff distance, and average surface distance can reach 0.7367, 21.39mm, 4.962mm, respectively. And the average time cost of 3D ResSE-Unet to segment a patient is only about 10s. CONCLUSION: The method proposed in this study provides a new tool for GTV auto-segmentation and may be useful for lung cancer radiotherapy.",2022,10.1177/15330338221090847,retrospective cohort,treatment,CT,Lung
H-SegNet: hybrid segmentation network for lung segmentation in chest radiographs using mask region-based convolutional neural network and adaptive closed polyline searching method,"Chest x-ray (CXR) is one of the most commonly used imaging techniques for the detection and diagnosis of pulmonary diseases. One critical component in many computer-aided systems, for either detection or diagnosis in digital CXR, is the accurate segmentation of the lung. Due to low-intensity contrast around lung boundary and large inter-subject variance, it has been challenging to segment lung from structural CXR images accurately. In this work, we propose an automatic Hybrid Segmentation Network (H-SegNet) for lung segmentation on CXR. The proposed H-SegNet consists of two key steps: (1) an image preprocessing step based on a deep learning model to automatically extract coarse lung contours; (2) a refinement step to fine-tune the coarse segmentation results based on an improved principal curve-based method coupled with an improved machine learning method. Experimental results on several public datasets show that the proposed method achieves superior segmentation results in lung CXRs, compared with several state-of-the-art methods.",2022,10.1088/1361-6560/ac5d74,,,,
Hahn-PCNN-CNN: an end-to-end multi-modal brain medical image fusion framework useful for clinical diagnosis,"BACKGROUND: In medical diagnosis of brain, the role of multi-modal medical image fusion is becoming more prominent. Among them, there is no lack of filtering layered fusion and newly emerging deep learning algorithms. The former has a fast fusion speed but the fusion image texture is blurred; the latter has a better fusion effect but requires higher machine computing capabilities. Therefore, how to find a balanced algorithm in terms of image quality, speed and computing power is still the focus of all scholars. METHODS: We built an end-to-end Hahn-PCNN-CNN. The network is composed of feature extraction module, feature fusion module and image reconstruction module. We selected 8000 multi-modal brain medical images downloaded from the Harvard Medical School website to train the feature extraction layer and image reconstruction layer to enhance the network's ability to reconstruct brain medical images. In the feature fusion module, we use the moments of the feature map combined with the pulse-coupled neural network to reduce the information loss caused by convolution in the previous fusion module and save time. RESULTS: We choose eight sets of registered multi-modal brain medical images in four diease to verify our model. The anatomical structure images are from MRI and the functional metabolism images are SPECT and 18F-FDG. At the same time, we also selected eight representative fusion models as comparative experiments. In terms of objective quality evaluation, we select six evaluation metrics in five categories to evaluate our model. CONCLUSIONS: The fusion image obtained by our model can retain the effective information in source images to the greatest extent. In terms of image fusion evaluation metrics, our model is superior to other comparison algorithms. In terms of time computational efficiency, our model also performs well. In terms of robustness, our model is very stable and can be generalized to multi-modal image fusion of other organs.",2021,10.1186/s12880-021-00642-z,,,,
Helping the Blind to Get through COVID-19: Social Distancing Assistant Using Real-Time Semantic Segmentation on RGB-D Video,"The current COVID-19 pandemic is having a major impact on our daily lives. Social distancing is one of the measures that has been implemented with the aim of slowing the spread of the disease, but it is difficult for blind people to comply with this. In this paper, we present a system that helps blind people to maintain physical distance to other persons using a combination of RGB and depth cameras. We use a real-time semantic segmentation algorithm on the RGB camera to detect where persons are and use the depth camera to assess the distance to them; then, we provide audio feedback through bone-conducting headphones if a person is closer than 1.5 m. Our system warns the user only if persons are nearby but does not react to non-person objects such as walls, trees or doors; thus, it is not intrusive, and it is possible to use it in combination with other assistive devices. We have tested our prototype system on one blind and four blindfolded persons, and found that the system is precise, easy to use, and amounts to low cognitive load.",2020,10.3390/s20185202,,,,
High precision localization of pulmonary nodules on chest CT utilizing axial slice number labels,"BACKGROUND: Reidentification of prior nodules for temporal comparison is an important but time-consuming step in lung cancer screening. We develop and evaluate an automated nodule detector that utilizes the axial-slice number of nodules found in radiology reports to generate high precision nodule predictions. METHODS: 888 CTs from Lung Nodule Analysis were used to train a 2-dimensional (2D) object detection neural network. A pipeline of 2D object detection, 3D unsupervised clustering, false positive reduction, and axial-slice numbers were used to generate nodule candidates. 47 CTs from the National Lung Cancer Screening Trial (NLST) were used for model evaluation. RESULTS: Our nodule detector achieved a precision of 0.962 at a recall of 0.573 on the NLST test set for any nodule. When adjusting for unintended nodule predictions, we achieved a precision of 0.931 at a recall 0.561, which corresponds to 0.06 false positives per CT. Error analysis revealed better detection of nodules with soft tissue attenuation compared to ground glass and undeterminable attenuation. Nodule margins, size, location, and patient demographics did not differ between correct and incorrect predictions. CONCLUSIONS: Utilization of axial-slice numbers from radiology reports allowed for development of a lung nodule detector with a low false positive rate compared to prior feature-engineering and machine learning approaches. This high precision nodule detector can reduce time spent on reidentification of prior nodules during lung cancer screening and can rapidly develop new institutional datasets to explore novel applications of computer vision in lung cancer imaging.",2021,10.1186/s12880-021-00594-4,cross-sectional,diagnosis,CT,Lung
High tumor cell platelet-derived growth factor receptor beta expression is associated with shorter survival in malignant pleural epithelioid mesothelioma,"Malignant pleural mesothelioma (MPM) has a rich stromal component containing mesenchymal fibroblasts. However, the properties and interplay of MPM tumor cells and their surrounding stromal fibroblasts are poorly characterized. Our objective was to spatially profile known mesenchymal markers in both tumor cells and associated fibroblasts and correlate their expression with patient survival. The primary study cohort consisted of 74 MPM patients, including 16 patients who survived at least 60 months. We analyzed location-specific tissue expression of seven fibroblast markers in clinical samples using multiplexed fluorescence immunohistochemistry (mfIHC) and digital image analysis. Effect on survival was assessed using Cox regression analyses. The outcome measurement was all-cause mortality. Univariate analysis revealed that high expression of secreted protein acidic and cysteine rich (SPARC) and fibroblast activation protein in stromal cells was associated with shorter survival. Importantly, high expression of platelet-derived growth factor receptor beta (PDGFRB) in tumor cells, but not in stromal cells, was associated with shorter survival (hazard ratio [HR] = 1.02, p < 0.001). A multivariable survival analysis adjusted for clinical parameters and stromal mfIHC markers revealed that tumor cell PDGFRB and stromal SPARC remained independently associated with survival (HR = 1.01, 95% confidence interval [CI] = 1.00-1.03 and HR = 1.05, 95% CI = 1.00-1.11, respectively). The prognostic effect of PDGFRB was validated with an artificial intelligence-based analysis method and further externally validated in another cohort of 117 MPM patients. In external validation, high tumor cell PDGFRB expression associated with shorter survival, especially in the epithelioid subtype. Our findings suggest PDGFRB and SPARC as potential markers for risk stratification and as targets for therapy.",2021,10.1002/cjp2.218,,,,
High-resolution CT image analysis based on 3D convolutional neural network can enhance the classification performance of radiologists in classifying pulmonary non-solid nodules,"OBJECTIVE: To investigate whether 3D convolutional neural network (CNN) is able to enhance the classification performance of radiologists in classifying pulmonary non-solid nodules (NSNs). MATERIALS AND METHODS: Data of patients with solitary NSNs and diagnosed as adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IAC) in pathological after surgical resection were analyzed retrospectively. Ultimately, 532 patients in our institution were included in the study: 427 cases (144 AIS, 167 MIA, 116 IAC) were assigned to training dataset and 105 cases (36 AIS, 41 MIA and 28 IAC) were assigned to validation dataset. For external validation, 177 patients (60 AIS, 69 MIA and 48 IAC) from another hospital were assigned to testing dataset. The clinical and morphological characteristics of NSNs were established as radiologists' model. The trained classification model based on 3D CNN was used to identify NSNs types automatically. The evaluation and comparison on classification performance of the two models and CNN + radiologists' model were performed via receiver operating curve (ROC) analysis and integrated discrimination improvement (IDI) index. The Akaike information criterion (AIC) was calculated to find the best-fit model. RESULTS: In external testing dataset, radiologists' model showed inferior classification performance than CNN model both in discriminating AIS from MIA-IAC and AIS-MIA from IAC (the area under the ROC curve (Az value), 0.693 vs 0.820, P = 0.011; 0.746 vs 0.833, P = 0.026, respectively). However, combining CNN significantly enhanced the classification performance of radiologists and exhibited higher Az values than CNN model alone (Az values, 0.893 vs 0.820, P < 0.001; 0.906 vs 0.833, P < 0.001, respectively). The IDI index further confirmed CNN's contribution to radiologists in classifying NSNs (IDI = 25.8 % (18.3-46.1 %), P < 0.001; IDI = 30.1 % (26.1-45.2 %), P < 0.001, respectively). The CNN + radiologists' model also provided the best fit over radiologists' model and CNN model alone (AIC value 63.3 % vs. 29.5 %, 49.5 %, P < 0.001; 69.2 % vs. 34.9 %, 53.6 %, P < 0.001, respectively). CONCLUSION: CNN successfully classified NSNs based on CT images and its classification performance were superior to radiologists' model. But the classification performance of radiologists can be significantly enhanced when combined with CNN in classifying NSNs.",2021,10.1016/j.ejrad.2021.109810,cross-sectional,diagnosis,CT,Lung
High-Throughput Virtual Screening and Validation of a SARS-CoV-2 Main Protease Noncovalent Inhibitor,"Despite the recent availability of vaccines against the acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the search for inhibitory therapeutic agents has assumed importance especially in the context of emerging new viral variants. In this paper, we describe the discovery of a novel noncovalent small-molecule inhibitor, MCULE-5948770040, that binds to and inhibits the SARS-Cov-2 main protease (M(pro)) by employing a scalable high-throughput virtual screening (HTVS) framework and a targeted compound library of over 6.5 million molecules that could be readily ordered and purchased. Our HTVS framework leverages the U.S. supercomputing infrastructure achieving nearly 91% resource utilization and nearly 126 million docking calculations per hour. Downstream biochemical assays validate this M(pro) inhibitor with an inhibition constant (K(i)) of 2.9 μM (95% CI 2.2, 4.0). Furthermore, using room-temperature X-ray crystallography, we show that MCULE-5948770040 binds to a cleft in the primary binding site of M(pro) forming stable hydrogen bond and hydrophobic interactions. We then used multiple μs-time scale molecular dynamics (MD) simulations and machine learning (ML) techniques to elucidate how the bound ligand alters the conformational states accessed by M(pro), involving motions both proximal and distal to the binding site. Together, our results demonstrate how MCULE-5948770040 inhibits M(pro) and offers a springboard for further therapeutic design.",2022,10.1021/acs.jcim.1c00851,,,,
Highly accurate model for prediction of lung nodule malignancy with CT scans,"Computed tomography (CT) examinations are commonly used to predict lung nodule malignancy in patients, which are shown to improve noninvasive early diagnosis of lung cancer. It remains challenging for computational approaches to achieve performance comparable to experienced radiologists. Here we present NoduleX, a systematic approach to predict lung nodule malignancy from CT data, based on deep learning convolutional neural networks (CNN). For training and validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort. All nodules were identified and classified by four experienced thoracic radiologists who participated in the LIDC project. NoduleX achieves high accuracy for nodule malignancy classification, with an AUC of ~0.99. This is commensurate with the analysis of the dataset by experienced radiologists. Our approach, NoduleX, provides an effective framework for highly accurate nodule malignancy prediction with the model trained on a large patient population. Our results are replicable with software available at http://bioinformatics.astate.edu/NoduleX .",2018,10.1038/s41598-018-27569-w,cross-sectional,diagnosis,CT,Lung
Histologic subtype classification of non-small cell lung cancer using PET/CT images,"PURPOSES: To evaluate the capability of PET/CT images for differentiating the histologic subtypes of non-small cell lung cancer (NSCLC) and to identify the optimal model from radiomics-based machine learning/deep learning algorithms. METHODS: In this study, 867 patients with adenocarcinoma (ADC) and 552 patients with squamous cell carcinoma (SCC) were retrospectively analysed. A stratified random sample of 283 patients (20%) was used as the testing set (173 ADC and 110 SCC); the remaining data were used as the training set. A total of 688 features were extracted from each outlined tumour region. Ten feature selection techniques, ten machine learning (ML) models and the VGG16 deep learning (DL) algorithm were evaluated to construct an optimal classification model for the differential diagnosis of ADC and SCC. Tenfold cross-validation and grid search technique were employed to evaluate and optimize the model hyperparameters on the training dataset. The area under the receiver operating characteristic curve (AUROC), accuracy, precision, sensitivity and specificity was used to evaluate the performance of the models on the test dataset. RESULTS: Fifty top-ranked subset features were selected by each feature selection technique for classification. The linear discriminant analysis (LDA) (AUROC, 0.863; accuracy, 0.794) and support vector machine (SVM) (AUROC, 0.863; accuracy, 0.792) classifiers, both of which coupled with the ℓ(2,1)NR feature selection method, achieved optimal performance. The random forest (RF) classifier (AUROC, 0.824; accuracy, 0.775) and ℓ(2,1)NR feature selection method (AUROC, 0.815; accuracy, 0.764) showed excellent average performance among the classifiers and feature selection methods employed in our study, respectively. Furthermore, the VGG16 DL algorithm (AUROC, 0.903; accuracy, 0.841) outperformed all conventional machine learning methods in combination with radiomics. CONCLUSION: Employing radiomic machine learning/deep learning algorithms could help radiologists to differentiate the histologic subtypes of NSCLC via PET/CT images.",2021,10.1007/s00259-020-04771-5,cross-sectional,diagnosis,CT,Lung
Histological Subtypes Classification of Lung Cancers on CT Images Using 3D Deep Learning and Radiomics,"RATIONALE AND OBJECTIVES: Histological subtypes of lung cancers are critical for clinical treatment decision. In this study, we attempt to use 3D deep learning and radiomics methods to automatically distinguish lung adenocarcinomas (ADC), squamous cell carcinomas (SCC), and small cell lung cancers (SCLC) respectively on Computed Tomography images, and then compare their performance. MATERIALS AND METHODS: 920 patients (mean age 61.2, range, 17-87; 340 Female and 580 Male) with lung cancer, including 554 patients with ADC, 175 patients with lung SCC and 191 patients with SCLC, were included in this retrospective study from January 2013 to August 2018. Histopathologic analysis was available for every patient. The classification models based on 3D deep learning (named the ProNet) and radiomics (named com_radNet) were designed to classify lung cancers into the three types mentioned above according to histopathologic results. The training, validation and testing cohorts counted 0.70, 0.15, and 0.15 of the whole datasets respectively. RESULTS: The ProNet model used to classify the three types of lung cancers achieved the F1-scores of 90.0%, 72.4%, 83.7% in ADC, SCC, and SCLC respectively, and the weighted average F1-score of 73.2%. For com_radNet, the F1-scores achieved 83.1%, 75.4%, 85.1% in ADC, SCC, and SCLC, and the weighted average F1-score was 72.2%. The area under the receiver operating characteristic curve of the ProNet model and com_radNet were 0.840 and 0.789, and the accuracy were 71.6% and 74.7% respectively. CONCLUSION: The ProNet and com_radNet models we developed can achieve high performance in distinguishing ADC, SCC, and SCLC and may be promising approaches for non-invasive predicting histological subtypes of lung cancers.",2021,10.1016/j.acra.2020.06.010,retrospective cohort,diagnosis,CT,Lung
Homological radiomics analysis for prognostic prediction in lung cancer patients,"PURPOSE: This study explored a novel homological analysis method for prognostic prediction in lung cancer patients. MATERIALS AND METHODS: The potential of homology-based radiomic features (HFs) was investigated by comparing HFs to conventional wavelet-based radiomic features (WFs) and combined radiomic features consisting of HFs and WFs (HWFs), using training (n = 135) and validation (n = 70) datasets, and Kaplan-Meier analysis. A total of 13,824 HFs were derived through homology-based texture analysis using Betti numbers, which represent the topologically invariant morphological characteristics of lung cancer. The prognostic potential of HFs was evaluated using statistically significant differences (p-values, log-rank test) to compare the survival curves of high- and low-risk patients. Those patients were stratified into high- and low-risk groups using the medians of the radiomic scores of signatures constructed with an elastic-net-regularized Cox proportional hazard model. Furthermore, deep learning (DL) based on AlexNet was utilized to compare HFs by stratifying patients into the two groups using a network that was pre-trained with over one million natural images from an ImageNet database. RESULTS: For the training dataset, the p-values between the two survival curves were 6.7 × 10(-6) (HF), 5.9 × 10(-3) (WF), 7.4 × 10(-6) (HWF), and 1.1 × 10(-3) (DL). The p-values for the validation dataset were 3.4 × 10(-5) (HF), 6.7 × 10(-1) (WF), 1.7 × 10(-7) (HWF), and 1.2 × 10(-1) (DL). CONCLUSION: This study demonstrates the excellent potential of HFs for prognostic prediction in lung cancer patients.",2020,10.1016/j.ejmp.2019.11.026,retrospective cohort,prognosis,CT,Lung
Homology-based radiomic features for prediction of the prognosis of lung cancer based on CT-based radiomics,"PURPOSE: Radiomics is a new technique that enables noninvasive prognostic prediction by extracting features from medical images. Homology is a concept used in many branches of algebra and topology that can quantify the contact degree. In the present study, we developed homology-based radiomic features to predict the prognosis of non-small-cell lung cancer (NSCLC) patients and then evaluated the accuracy of this prediction method. METHODS: Four datasets were used: two to provide training and test data and two for the selection of robust radiomic features. All the datasets were downloaded from The Cancer Imaging Archive (TCIA). In two-dimensional cases, the Betti numbers consist of two values: b(0) (zero-dimensional Betti number), which is the number of isolated components, and b(1) (one-dimensional Betti number), which is the number of one-dimensional or ""circular"" holes. For homology-based evaluation, computed tomography (CT) images must be converted to binarized images in which each pixel has two possible values: 0 or 1. All CT slices of the gross tumor volume were used for calculating the homology histogram. First, by changing the threshold of the CT value (range: -150 to 300 HU) for all its slices, we developed homology-based histograms for b(0) , b(1) , and b(1) /b(0) using binarized images. All histograms were then summed, and the summed histogram was normalized by the number of slices. 144 homology-based radiomic features were defined from the histogram. To compare the standard radiomic features, 107 radiomic features were calculated using the standard radiomics technique. To clarify the prognostic power, the relationship between the values of the homology-based radiomic features and overall survival was evaluated using LASSO Cox regression model and the Kaplan-Meier method. The retained features with nonzero coefficients calculated by the LASSO Cox regression model were used for fitting the regression model. Moreover, these features were then integrated into a radiomics signature. An individualized rad score was calculated from a linear combination of the selected features, which were weighted by their respective coefficients. RESULTS: When the patients in the training and test datasets were stratified into high-risk and low-risk groups according to the rad scores, the overall survival of the groups was significantly different. The C-index values for the homology-based features (rad score), standard features (rad score), and tumor size were 0.625, 0.603, and 0.607, respectively, for the training datasets and 0.689, 0.668, and 0.667 for the test datasets. This result showed that homology-based radiomic features had slightly higher prediction power than the standard radiomic features. CONCLUSIONS: Prediction performance using homology-based radiomic features had a comparable or slightly higher prediction power than standard radiomic features. These findings suggest that homology-based radiomic features may have great potential for improving the prognostic prediction accuracy of CT-based radiomics. In this result, it is noteworthy that there are some limitations.",2020,10.1002/mp.14104,retrospective cohort,prognosis,CT,Lung
HoTPiG: a novel graph-based 3-D image feature set and its applications to computer-assisted detection of cerebral aneurysms and lung nodules,"PURPOSE: A novel image feature set named histogram of triangular paths in graph (HoTPiG) is presented. The purpose of this study is to evaluate the feasibility of the proposed HoTPiG feature set through two clinical computer-aided detection tasks: nodule detection in lung CT images and aneurysm detection in head MR angiography images. METHODS: The HoTPiG feature set is calculated from an undirected graph structure derived from a binarized volume. The features are derived from a 3-D histogram in which each bin represents a triplet of shortest path distances between the target node and all possible node pairs near the target node. First, the vessel structure is extracted from CT/MR volumes. Then, a graph structure is extracted using an 18-neighbor rule. Using this graph, a HoTPiG feature vector is calculated at every foreground voxel. After explicit feature mapping with an exponential-χ(2) kernel, each voxel is judged by a linear support vector machine classifier. The proposed method was evaluated using 300 CT and 300 MR datasets. RESULTS: The proposed method successfully detected lung nodules and cerebral aneurysms. The sensitivity was about 80% when the number of false positives was three per case for both applications. CONCLUSIONS: The HoTPiG image feature set was presented, and its high general versatility was shown through two medical lesion detection applications.",2019,10.1007/s11548-019-01942-0,,,,
How many models/atlases are needed as priors for capturing anatomic population variations?,"Many medical image processing and analysis operations can benefit a great deal from prior information encoded in the form of models/atlases to capture variations over a population in form, shape, anatomic layout, and image appearance of objects. However, two fundamental questions have not been addressed in the literature: ""How many models/atlases are needed for optimally encoding prior information to address the differing body habitus factor in that population?"" and ""Images of how many subjects in the given population are needed to optimally harness prior information?"" We propose a method to seek answers to these questions. We assume that there is a well-defined body region of interest and a subject population under consideration, and that we are given a set of representative images of the body region for the population. After images are trimmed to the exact body region, a hierarchical agglomerative clustering algorithm partitions the set of images into a specified number of groups by using pairwise image (dis)similarity as a cost function. Optionally the images may be pre-registered among themselves prior to this partitioning operation. We define a measure called Residual Dissimilarity (RD) to determine the goodness of each partition. We then ascertain how RD varies as a function of the number of elements in the partition for finding the optimum number(s) of groups. Breakpoints in this function are taken as the recommended number of groups/models/atlases. Our results from analysis of sizeable CT data sets of adult patients from two body regions - thorax (346) and head and neck (298) - can be summarized as follows. (1) A minimum of 5 to 8 groups (or models/atlases) seems essential to properly capture information about differing anatomic forms and body habitus. (2) A minimum of 150 images from different subjects in a population seems essential to cover the anatomical variations for a given body region. (3) In grouping, body habitus variations seem to override differences due to other factors such as gender, with/without contrast enhancement in image acquisition, and presence of moderate pathology. This method may be helpful for constructing high quality models/atlases from a sufficiently large population of images and in optimally selecting the training image sets needed in deep learning strategies.",2019,10.1016/j.media.2019.101550,cross-sectional,others,CT,Lung
How should we model and evaluate breathing interplay effects in IMPT?,"Breathing interplay effects in Intensity Modulated Proton Therapy (IMPT) arise from the interaction between target motion and the scanning beam. Assessing the detrimental effect of interplay and the clinical robustness of several mitigation techniques requires statistical evaluation procedures that take into account the variability of breathing during dose delivery. In this study, we present such a statistical method to model intra-fraction respiratory motion based on breathing signals and assess clinical relevant aspects related to the practical evaluation of interplay in IMPT such as how to model irregular breathing, how small breathing changes affect the final dose distribution, and what is the statistical power (number of different scenarios) required for trustworthy quantification of interplay effects. First, two data-driven methodologies to generate artificial patient-specific breathing signals are compared: a simple sinusoidal model, and a precise probabilistic deep learning model generating very realistic samples of patient breathing. Second, we investigate the highly fluctuating relationship between interplay doses and breathing parameters, showing that small changes in breathing period result in large local variations in the dose. Our results indicate that using a limited number of samples to calculate interplay statistics introduces a bigger error than using simple sinusoidal models based on patient parameters or disregarding breathing hysteresis during the evaluation. We illustrate the power of the presented statistical method by analyzing interplay robustness of 4DCT and Internal Target Volume (ITV) treatment plans for a 8 lung cancer patients, showing that, unlike 4DCT plans, even 33 fraction ITV plans systematically fail to fulfill robustness requirements.",2021,10.1088/1361-6560/ac383f,,,,
How to Improve Compliance with Protective Health Measures during the COVID-19 Outbreak: Testing a Moderated Mediation Model and Machine Learning Algorithms,"In the wake of the sudden spread of COVID-19, a large amount of the Italian population practiced incongruous behaviors with the protective health measures. The present study aimed at examining psychological and psychosocial variables that could predict behavioral compliance. An online survey was administered from 18-22 March 2020 to 2766 participants. Paired sample t-tests were run to compare efficacy perception with behavioral compliance. Mediation and moderated mediation models were constructed to explore the association between perceived efficacy and compliance, mediated by self-efficacy and moderated by risk perception and civic attitudes. Machine learning algorithms were trained to predict which individuals would be more likely to comply with protective measures. Results indicated significantly lower scores in behavioral compliance than efficacy perception. Risk perception and civic attitudes as moderators rendered the mediating effect of self-efficacy insignificant. Perceived efficacy on the adoption of recommended behaviors varied in accordance with risk perception and civic engagement. The 14 collected variables, entered as predictors in machine learning models, produced an ROC area in the range of 0.82-0.91 classifying individuals as high versus low compliance. Overall, these findings could be helpful in guiding age-tailored information/advertising campaigns in countries affected by COVID-19 and directing further research on behavioral compliance.",2020,10.3390/ijerph17197252,,,,
Human-recognizable CT image features of subsolid lung nodules associated with diagnosis and classification by convolutional neural networks,"OBJECTIVES: The interpretability of convolutional neural networks (CNNs) for classifying subsolid nodules (SSNs) is insufficient for clinicians. Our purpose was to develop CNN models to classify SSNs on CT images and to investigate image features associated with the CNN classification. METHODS: CT images containing SSNs with a diameter of ≤ 3 cm were retrospectively collected. We trained and validated CNNs by a 5-fold cross-validation method for classifying SSNs into three categories (benign and preinvasive lesions [PL], minimally invasive adenocarcinoma [MIA], and invasive adenocarcinoma [IA]) that were histologically confirmed or followed up for 6.4 years. The mechanism of CNNs on human-recognizable CT image features was investigated and visualized by gradient-weighted class activation map (Grad-CAM), separated activation channels and areas, and DeepDream algorithm. RESULTS: The accuracy was 93% for classifying 586 SSNs from 569 patients into three categories (346 benign and PL, 144 MIA, and 96 IA in 5-fold cross-validation). The Grad-CAM successfully located the entire region of image features that determined the final classification. Activated areas in the benign and PL group were primarily smooth margins (p < 0.001) and ground-glass components (p = 0.033), whereas in the IA group, the activated areas were mainly part-solid (p < 0.001) and solid components (p < 0.001), lobulated shapes (p < 0.001), and air bronchograms (p < 0.001). However, the activated areas for MIA were variable. The DeepDream algorithm showed the image features in a human-recognizable pattern that the CNN learned from a training dataset. CONCLUSION: This study provides medical evidence to interpret the mechanism of CNNs that helps support the clinical application of artificial intelligence. KEY POINTS: • CNN achieved high accuracy (93%) in classifying subsolid nodules on CT images into three categories: benign and preinvasive lesions, MIA, and IA. • The gradient-weighted class activation map (Grad-CAM) located the entire region of image features that determined the final classification, and the visualization of the separated activated areas was consistent with radiologists' expertise for diagnosing subsolid nodules. • DeepDream showed the image features that CNN learned from a training dataset in a human-recognizable pattern.",2021,10.1007/s00330-021-07901-1,retrospective cohort,diagnosis,CT,Lung
Hybrid COVID-19 segmentation and recognition framework (HMB-HCF) using deep learning and genetic algorithms,"COVID-19 (Coronavirus) went through a rapid escalation until it became a pandemic disease. The normal and manual medical infection discovery may take few days and therefore computer science engineers can share in the development of the automatic diagnosis for fast detection of that disease. The study suggests a hybrid COVID-19 framework (named HMB-HCF) based on deep learning (DL), genetic algorithm (GA), weighted sum (WS), and majority voting principles in nine phases. Its segmentation phase suggests a lung segmentation algorithm using X-Ray images (named HMB-LSAXI) for extracting lungs. Its classification phase is built from a hybrid convolutional neural network (CNN) architecture using an abstractly-designed CNN (named HMB1-COVID19) and transfer learning (TL) pre-trained models (VGG16, VGG19, ResNet50, ResNet101, Xception, DenseNet121, DenseNet169, MobileNet, and MobileNetV2). The hybrid CNN architecture is used for learning, classification, and parameters optimization while GA is used to optimize the hyperparameters. This hybrid working mechanism is combined in an overall algorithm named HMB-DLGA. The study experiments implemented the WS approach to evaluate the models' performance using the loss, accuracy, F1-score, precision, recall, and area under curve (AUC) metrics with different pre-defined ratios. A collected, combined, and unified X-Ray dataset from 8 different public datasets was used alongside the regularization, dropout, and data augmentation techniques to limit the overall overfitting. The applied experiments reported state-of-the-art metrics. VGG16 reported 100% WS metric (i.e., 0.0097, 99.78%, 0.9984, 99.89%, 99.78%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the highest WS. It also reported a 99.92% WS metric (i.e., 0.0099, 99.84%, 0.9984, 99.84%, 99.84%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the last reported WS result. HMB-HCF was validated on 13 different public datasets to verify its generalization. The best-achieved metrics were compared with 13 related studies. These extensive experiments' target was the applicability verification and generalization.",2021,10.1016/j.artmed.2021.102156,cross-sectional,diagnosis,CXR,Lung
Hybrid Deep-Learning and Machine-Learning Models for Predicting COVID-19,"The COVID-19 pandemic has had a significant impact on public life and health worldwide, putting the world's healthcare systems at risk. The first step in stopping this outbreak is to detect the infection in its early stages, which will relieve the risk, control the outbreak's spread, and restore full functionality to the world's healthcare systems. Currently, PCR is the most prevalent diagnosis tool for COVID-19. However, chest X-ray images may play an essential role in detecting this disease, as they are successful for many other viral pneumonia diseases. Unfortunately, there are common features between COVID-19 and other viral pneumonia, and hence manual differentiation between them seems to be a critical problem and needs the aid of artificial intelligence. This research employs deep- and transfer-learning techniques to develop accurate, general, and robust models for detecting COVID-19. The developed models utilize either convolutional neural networks or transfer-learning models or hybridize them with powerful machine-learning techniques to exploit their full potential. For experimentation, we applied the proposed models to two data sets: the COVID-19 Radiography Database from Kaggle and a local data set from Asir Hospital, Abha, Saudi Arabia. The proposed models achieved promising results in detecting COVID-19 cases and discriminating them from normal and other viral pneumonia with excellent accuracy. The hybrid models extracted features from the flatten layer or the first hidden layer of the neural network and then fed these features into a classification algorithm. This approach enhanced the results further to full accuracy for binary COVID-19 classification and 97.8% for multiclass classification.",2021,10.1155/2021/9996737,cross-sectional,diagnosis,CXR,Lung
Hybrid detection of lung nodules on CT scan images,"PURPOSE: The diversity of lung nodules poses difficulty for the current computer-aided diagnostic (CAD) schemes for lung nodule detection on computed tomography (CT) scan images, especially in large-scale CT screening studies. We proposed a novel CAD scheme based on a hybrid method to address the challenges of detection in diverse lung nodules. METHODS: The hybrid method proposed in this paper integrates several existing and widely used algorithms in the field of nodule detection, including morphological operation, dot-enhancement based on Hessian matrix, fuzzy connectedness segmentation, local density maximum algorithm, geodesic distance map, and regression tree classification. All of the adopted algorithms were organized into tree structures with multi-nodes. Each node in the tree structure aimed to deal with one type of lung nodule. RESULTS: The method has been evaluated on 294 CT scans from the Lung Image Database Consortium (LIDC) dataset. The CT scans were randomly divided into two independent subsets: a training set (196 scans) and a test set (98 scans). In total, the 294 CT scans contained 631 lung nodules, which were annotated by at least two radiologists participating in the LIDC project. The sensitivity and false positive per scan for the training set were 87% and 2.61%. The sensitivity and false positive per scan for the testing set were 85.2% and 3.13%. CONCLUSIONS: The proposed hybrid method yielded high performance on the evaluation dataset and exhibits advantages over existing CAD schemes. We believe that the present method would be useful for a wide variety of CT imaging protocols used in both routine diagnosis and screening studies.",2015,10.1118/1.4927573,cross-sectional,diagnosis,CT,Lung
Hybrid ensemble model for differential diagnosis between COVID-19 and common viral pneumonia by chest X-ray radiograph,"BACKGROUND: Chest X-ray radiography (CXR) has been widely considered as an accessible, feasible, and convenient method to evaluate suspected patients' lung involvement during the COVID-19 pandemic. However, with the escalating number of suspected cases, traditional diagnosis via CXR fails to deliver results within a short period of time. Therefore, it is crucial to employ artificial intelligence (AI) to enhance CXRs for obtaining quick and accurate diagnoses. Previous studies have reported the feasibility of utilizing deep learning methods to screen for COVID-19 using CXR and CT results. However, these models only use a single deep learning network for chest radiograph detection; the accuracy of this approach required further improvement. METHODS: In this study, we propose a three-step hybrid ensemble model, including a feature extractor, a feature selector, and a classifier. First, a pre-trained AlexNet with an improved structure extracts the original image features. Then, the ReliefF algorithm is adopted to sort the extracted features, and a trial-and-error approach is used to select the n most important features to reduce the feature dimension. Finally, an SVM classifier provides classification results based on the n selected features. RESULTS: Compared to five existing models (InceptionV3: 97.916 ± 0.408%; SqueezeNet: 97.189 ± 0.526%; VGG19: 96.520 ± 1.220%; ResNet50: 97.476 ± 0.513%; ResNet101: 98.241 ± 0.209%), the proposed model demonstrated the best performance in terms of overall accuracy rate (98.642 ± 0.398%). Additionally, compared to the existing models, the proposed model demonstrates a considerable improvement in classification time efficiency (SqueezeNet: 6.602 ± 0.001s; InceptionV3: 12.376 ± 0.002s; ResNet50: 10.952 ± 0.001s; ResNet101: 18.040 ± 0.002s; VGG19: 16.632 ± 0.002s; proposed model: 5.917 ± 0.001s). CONCLUSION: The model proposed in this article is practical and effective, and can provide high-precision COVID-19 CXR detection. We demonstrated its suitability to aid medical professionals in distinguishing normal CXRs, viral pneumonia CXRs and COVID-19 CXRs efficiently on small sample sizes.",2021,10.1016/j.compbiomed.2021.104252,cross-sectional,diagnosis,CXR,Lung
Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images,"The novel Coronavirus disease (COVID-19), which first appeared at the end of December 2019, continues to spread rapidly in most countries of the world. Respiratory infections occur primarily in the majority of patients treated with COVID-19. In light of the growing number of COVID-19 cases, the need for diagnostic tools to identify COVID-19 infection at early stages is of vital importance. For decades, chest X-ray (CXR) technologies have proven their ability to accurately detect respiratory diseases. More recently, with the availability of COVID-19 CXR scans, deep learning algorithms have played a critical role in the healthcare arena by allowing radiologists to recognize COVID-19 patients from their CXR images. However, the majority of screening methods for COVID-19 reported in recent studies are based on 2D convolutional neural networks (CNNs). Although 3D CNNs are capable of capturing contextual information compared to their 2D counterparts, their use is limited due to their increased computational cost (i.e. requires much extra memory and much more computing power). In this study, a transfer learning-based hybrid 2D/3D CNN architecture for COVID-19 screening using CXRs has been developed. The proposed architecture consists of the incorporation of a pre-trained deep model (VGG16) and a shallow 3D CNN, combined with a depth-wise separable convolution layer and a spatial pyramid pooling module (SPP). Specifically, the depth-wise separable convolution helps to preserve the useful features while reducing the computational burden of the model. The SPP module is designed to extract multi-level representations from intermediate ones. Experimental results show that the proposed framework can achieve reasonable performances when evaluated on a collected dataset (3 classes to be predicted: COVID-19, Pneumonia, and Normal). Notably, it achieved a sensitivity of 98.33%, a specificity of 98.68% and an overall accuracy of 96.91.",2020,10.1007/s13246-020-00957-1,cross-sectional,diagnosis,CXR,Lung
Hypergraph learning for identification of COVID-19 with CT imaging,"The coronavirus disease, named COVID-19, has become the largest global public health crisis since it started in early 2020. CT imaging has been used as a complementary tool to assist early screening, especially for the rapid identification of COVID-19 cases from community acquired pneumonia (CAP) cases. The main challenge in early screening is how to model the confusing cases in the COVID-19 and CAP groups, with very similar clinical manifestations and imaging features. To tackle this challenge, we propose an Uncertainty Vertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP using CT images. In particular, multiple types of features (including regional features and radiomics features) are first extracted from CT image for each case. Then, the relationship among different cases is formulated by a hypergraph structure, with each case represented as a vertex in the hypergraph. The uncertainty of each vertex is further computed with an uncertainty score measurement and used as a weight in the hypergraph. Finally, a learning process of the vertex-weighted hypergraph is used to predict whether a new testing case belongs to COVID-19 or not. Experiments on a large multi-center pneumonia dataset, consisting of 2148 COVID-19 cases and 1182 CAP cases from five hospitals, are conducted to evaluate the prediction accuracy of the proposed method. Results demonstrate the effectiveness and robustness of our proposed method on the identification of COVID-19 in comparison to state-of-the-art methods.",2021,10.1016/j.media.2020.101910,cross-sectional,diagnosis,CXR,Lung
Identification of Benign and Malignant Lung Nodules in CT Images Based on Ensemble Learning Method,"BACKGROUND AND OBJECTIVE: Under the background of urgent need for computer-aided technology to provide physicians with objective decision support, aiming at reducing the false positive rate of nodule CT detection in pulmonary nodules detection and improving the accuracy of lung nodule recognition, this paper puts forward a method based on ensemble learning to distinguish between malignant and benign pulmonary nodules. METHODS: Firstly, trained on a public data set, a multi-layer feature fusion YOLOv3 network is used to detect lung nodules. Secondly, a CNN was trained to differentiate benign from malignant pulmonary nodules. Then, based on the idea of ensemble learning, the confidence probability of the above two models and the label of the training set are taken as data features to build a Logistic regression model. Finally, two test sets (public data set and private data set) were tested, and the confidence probability output by the two models was fused into the established logistic regression model to determine benign and malignant pulmonary nodules. RESULTS: The YOLOv3 network was trained to detect chest CT images of the test set. The number of pulmonary nodules detected in the public and private test sets was 356 and 314, respectively. The accuracy, sensitivity and specificity of the two test sets were 80.97%, 81.63%, 78.75% and 79.69%, 86.59%, 72.16%, respectively. With CNN training pulmonary nodules benign and malignant discriminant model analysis of two kinds of test set, the result of accuracy, sensitivity and specificity were 90.12%, 90.66%, 89.47% and 88.57%, 85.62%, 90.87%, respectively. Fused model based on YOLOv3 network and CNN is tested on two test sets, and the result of accuracy, sensitivity and specificity were 93.82%, 94.85%, 92.59% and 92.31%, 92.68%, 91.89%, respectively. CONCLUSION: The ensemble learning model is more effective than YOLOv3 network and CNN in removing false positives, and the accuracy of the ensemble. Learning model is higher than the other two networks in identifying pulmonary nodules.",2022,10.1007/s12539-021-00472-1,cross-sectional,diagnosis,CT,Lung
Identification of benign and malignant pulmonary nodules on chest CT using improved 3D U-Net deep learning framework,"PURPOSE: To accurately distinguish benign from malignant pulmonary nodules with CT based on partial structures of 3D U-Net integrated with Capsule Networks (CapNets) and provide a reference for the early diagnosis of lung cancer. METHOD: The dataset consisted of 1177 samples (benign/malignant: 414/763) from 997 patients provided by collaborating hospital. All nodules were biopsy or surgery proven, and pathologic results were regarded as the ""golden standard"". This study utilized partial U-Net to capture the low-level (edge, corner, etc.) information and CapNets to preserve high-level (semantic information) information of nodules. For CapNets, each capsule had a 4 × 4 matrix representing the pose and an activation probability representing the presence of an object. Furthermore, we chose accuracy (ACC), area under the curve (AUC), sensitivity (SE) and specificity (SP) to evaluate the generalization of the proposed architecture and compared its identification performance with 3D U-Net and experienced radiologists. RESULTS: The AUC of our architecture (0.84) was superior to that (0.81) of the original 3D U-Net (p = 0.04, DeLong's test). Moreover, ACC (84.5 %) and SE (92.9 %) of our model were clearly higher than radiologists' ACC (81.0 %) and SE (84.3 %) at the optimal operating point. However, SP (70 %) of our model was slightly lower than radiologists' SP (75 %), which might be the result of class imbalance with limited benign samples involved for algorithm training. CONCLUSIONS: Our architecture showed a high performance for identifying benign and malignant pulmonary nodules, indicating the improved model has a promising application in clinic.",2020,10.1016/j.ejrad.2020.109013,cross-sectional,diagnosis,CT,Lung
Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches,"BACKGROUND: The novel coronavirus disease 2019 (COVID-19) constitutes a public health emergency globally. The number of infected people and deaths are proliferating every day, which is putting tremendous pressure on our social and healthcare system. Rapid detection of COVID-19 cases is a significant step to fight against this virus as well as release pressure off the healthcare system. OBJECTIVE: One of the critical factors behind the rapid spread of COVID-19 pandemic is a lengthy clinical testing time. The imaging tool, such as Chest X-ray (CXR), can speed up the identification process. Therefore, our objective is to develop an automated CAD system for the detection of COVID-19 samples from healthy and pneumonia cases using CXR images. METHODS: Due to the scarcity of the COVID-19 benchmark dataset, we have employed deep transfer learning techniques, where we examined 15 different pre-trained CNN models to find the most suitable one for this task. RESULTS: A total of 860 images (260 COVID-19 cases, 300 healthy and 300 pneumonia cases) have been employed to investigate the performance of the proposed algorithm, where 70% images of each class are accepted for training, 15% is used for validation, and rest is for testing. It is observed that the VGG19 obtains the highest classification accuracy of 89.3% with an average precision, recall, and F1 score of 0.90, 0.89, 0.90, respectively. CONCLUSION: This study demonstrates the effectiveness of deep transfer learning techniques for the identification of COVID-19 cases using CXR images.",2020,10.3233/xst-200715,cross-sectional,diagnosis,CXR,Lung
Identification of Non-Small Cell Lung Cancer Sensitive to Systemic Cancer Therapies Using Radiomics,"PURPOSE: Using standard-of-care CT images obtained from patients with a diagnosis of non-small cell lung cancer (NSCLC), we defined radiomics signatures predicting the sensitivity of tumors to nivolumab, docetaxel, and gefitinib. EXPERIMENTAL DESIGN: Data were collected prospectively and analyzed retrospectively across multicenter clinical trials [nivolumab, n = 92, CheckMate017 (NCT01642004), CheckMate063 (NCT01721759); docetaxel, n = 50, CheckMate017; gefitinib, n = 46, (NCT00588445)]. Patients were randomized to training or validation cohorts using either a 4:1 ratio (nivolumab: 72T:20V) or a 2:1 ratio (docetaxel: 32T:18V; gefitinib: 31T:15V) to ensure an adequate sample size in the validation set. Radiomics signatures were derived from quantitative analysis of early tumor changes from baseline to first on-treatment assessment. For each patient, 1,160 radiomics features were extracted from the largest measurable lung lesion. Tumors were classified as treatment sensitive or insensitive; reference standard was median progression-free survival (NCT01642004, NCT01721759) or surgery (NCT00588445). Machine learning was implemented to select up to four features to develop a radiomics signature in the training datasets and applied to each patient in the validation datasets to classify treatment sensitivity. RESULTS: The radiomics signatures predicted treatment sensitivity in the validation dataset of each study group with AUC (95 confidence interval): nivolumab, 0.77 (0.55-1.00); docetaxel, 0.67 (0.37-0.96); and gefitinib, 0.82 (0.53-0.97). Using serial radiographic measurements, the magnitude of exponential increase in signature features deciphering tumor volume, invasion of tumor boundaries, or tumor spatial heterogeneity was associated with shorter overall survival. CONCLUSIONS: Radiomics signatures predicted tumor sensitivity to treatment in patients with NSCLC, offering an approach that could enhance clinical decision-making to continue systemic therapies and forecast overall survival.",2020,10.1158/1078-0432.Ccr-19-2942,retrospective cohort,prognosis,CT,Lung
Identification of pathological subtypes of early lung adenocarcinoma based on artificial intelligence parameters and CT signs,"OBJECTIVE: To explore the value of quantitative parameters of artificial intelligence (AI) and computed tomography (CT) signs in identifying pathological subtypes of lung adenocarcinoma appearing as ground-glass nodules (GGNs). METHODS: CT images of 224 GGNs from 210 individuals were collected retrospectively and classified into atypical adenomatous hyperplasia (AAH)/adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC) groups. AI was used to identify GGNs and to obtain quantitative parameters, and CT signs were recognized manually. The mixed predictive model based on logistic multivariate regression was built and evaluated. RESULTS: Of the 224 GGNs, 55, 93, and 76 were AAH/AIS, MIA, and IAC, respectively. In terms of AI parameters, from AAH/AIS to MIA, and IAC, there was a gradual increase in two-dimensional mean diameter, three-dimensional mean diameter, mean CT value, maximum CT value, and volume of GGNs (all P<0.0001). Except for the CT signs of the location, and the tumor-lung interface, there were significant differences among the three groups in the density, shape, vacuolar signs, air bronchogram, lobulation, spiculation, pleural indentation, and vascular convergence signs (all P<0.05). The areas under the curve (AUC) of predictive model 1 for identifying the AAH/AIS and MIA and model 2 for identifying MIA and IAC were 0.779 and 0.918, respectively, which were greater than the quantitative parameters independently (all P<0.05). CONCLUSION: AI parameters are valuable for identifying subtypes of early lung adenocarcinoma and have improved diagnostic efficacy when combined with CT signs.",2022,10.1042/bsr20212416,cross-sectional,diagnosis,CT,Lung
Identification of pulmonary nodules via CT images with hierarchical fully convolutional networks,"Lung cancer is one of the most diagnosable forms of cancer worldwide. The early diagnoses of pulmonary nodules in computed tomography (CT) chest scans are crucial for potential patients. Recent researches have showed that the methods based on deep learning have made a significant progress for the medical diagnoses. However, the achievements on identification of pulmonary nodules are not yet satisfactory enough to be adopted in clinical practice. It is largely caused by either the existence of many false positives or the heavy time of processing. With the development of fully convolutional networks (FCNs), in this study, we proposed a new method of identifying the pulmonary nodules. The method segments the suspected nodules from their environments and then removes the false positives. Especially, it optimizes the network architecture for the identification of nodules rapidly and accurately. In order to remove the false positives, the suspected nodules are reduced using the 2D models. Furthermore, according to the significant differences between nodules and non-nodules in 3D shapes, the false positives are eliminated by integrating into the 3D models and classified via 3D CNNs. The experiments on 1000 patients indicate that our proposed method achieved 97.78% sensitivity rate for segmentation and 90.1% accuracy rate for detection. The maximum response time was less than 30 s and the average time was about 15 s. Graphical Abstract This paper has proposed a new method of identifying the pulmonary nodules. The method segments the suspected nodules from CT images and removes the false positives. As shown in the above, the proposed approach consists of three stages. In stage I, raw data are filtered and normalized. The clean normalized data are then segmented in stage II to extract the suspected nodular lesions through 2D FCNs. Stage III is to remove some false positives generated at stage II via 3D CNNs and outputs the final results. The experiments on 1000 patients indicate that our proposed method has achieved 97.78% sensitivity rate for segmentation and 90.1% accuracy rate for detection. The maximum response time was less than 30 s and the average time was about 15 s.",2019,10.1007/s11517-019-01976-1,cross-sectional,diagnosis,CT,Lung
Identification of treatment error types for lung cancer patients using convolutional neural networks and EPID dosimetry,"BACKGROUND/PURPOSE: Electronic portal imaging device (EPID) dosimetry aims to detect treatment errors, potentially leading to treatment adaptation. Clinically used threshold classification methods for detecting errors lead to loss of information (from multi-dimensional EPID data to a few numbers) and cannot be used for identifying causes of errors. Advanced classification methods, such as deep learning, can use all available information. In this study, convolutional neural networks (CNNs) were trained to detect and identify error type and magnitude of simulated treatment errors in lung cancer patients. The purpose of this simulation study is to provide a proof-of-concept of CNNs for error identification using EPID dosimetry in an in vivo scenario. MATERIALS AND METHODS: Clinically realistic ranges of anatomical changes, positioning errors and mechanical errors were simulated for lung cancer patients. Predicted portal dose images (PDIs) containing errors were compared to error-free PDIs using the widely used gamma analysis. CNNs were trained to classify errors using 2D gamma maps. Three classification levels were assessed: Level 1 (main error type, e.g., anatomical change), Level 2 (error subtype, e.g., tumor regression) and Level 3 (error magnitude, e.g., >50% tumor regression). RESULTS: CNNs showed good performance for all classification levels (training/test accuracy 99.5%/96.1%, 92.5%/86.8%, 82.0%/72.9%). For Level 3, overfitting became more apparent. CONCLUSION: This simulation study indicates that deep learning is a promising powerful tool for identifying types and magnitude of treatment errors with EPID dosimetry, providing additional information not currently available from EPID dosimetry. This is a first step towards rapid, automated models for identification of treatment errors using EPID dosimetry.",2020,10.1016/j.radonc.2020.09.048,,,,
Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based Approach,"Coronavirus Disease (COVID19) is a fast-spreading infectious disease that is currently causing a healthcare crisis around the world. Due to the current limitations of the reverse transcription-polymerase chain reaction (RT-PCR) based tests for detecting COVID19, recently radiology imaging based ideas have been proposed by various works. In this work, various Deep CNN based approaches are explored for detecting the presence of COVID19 from chest CT images. A decision fusion based approach is also proposed, which combines predictions from multiple individual models, to produce a final prediction. Experimental results show that the proposed decision fusion based approach is able to achieve above 86% results across all the performance metrics under consideration, with average AUROC and F1-Score being 0.883 and 0.867, respectively. The experimental observations suggest the potential applicability of such Deep CNN based approach in real diagnostic scenarios, which could be of very high utility in terms of achieving fast testing for COVID19.",2020,10.1155/2020/8843664,cross-sectional,diagnosis,CT,Lung
Identifying EGFR mutations in lung adenocarcinoma by noninvasive imaging using radiomics features and random forest modeling,"OBJECTIVES: The tyrosine kinase inhibitor (TKI)-sensitive mutations of the epidermal growth factor receptor (EGFR) gene is essential in the treatment of lung adenocarcinoma. To overcome the difficulty of EGFR gene test in situations where surgery and biopsy samples are too risky to obtain, we tried a noninvasive imaging method using radiomics features and random forest models. METHODS: Five hundred three lung adenocarcinoma patients who received surgery-based treatment were included in this study. The diagnosis and EGFR gene test were based on resections. TKI-sensitive mutations were found in 60.8% of the patients. CT scans before any invasive operation were gathered and analyzed to extract quantitative radiomics features and build random forest classifiers to identify EGFR mutants from wild types. Clinical features (sex and smoking history) were added to the image-based model. The model was trained on a set of 345 patients and validated on an independent test group (n = 158) using the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity. RESULTS: The performance of the random forest model with 94 radiomics features reached an AUC of 0.802. Its AUC was further improved to 0.828 by adding sex and smoking history. The sensitivity and specificity are 60.6% and 85.1% at the best diagnostic decision point. CONCLUSION: Our results showed that radiomics could not only reflect the genetic differences among tumors but also have diagnostic value and the potential to be a diagnostic tool. KEY POINTS: • Radiomics provides a potential noninvasive method for the prediction of EGFR mutation status. • In situations where surgeries and biopsy are not available, CT image-based radiomics models could help to make treatment decisions. • The accuracy, sensitivity, and specificity still need to be improved before the image-based EGFR identifier could be used in clinics.",2019,10.1007/s00330-019-06024-y,retrospective cohort,prognosis,CT,Lung
Automatic segmentation and quantification of epicardial adipose tissue from coronary computed tomography angiography,"Epicardial adipose tissue (EAT) is a visceral fat deposit, that's known for its association with factors, such as obesity, diabetes mellitus, age, and hypertension. Segmentation of the EAT in a fast and reproducible way is important for the interpretation of its role as an independent risk marker intricate. However, EAT has a variable distribution, and various diseases may affect the volume of the EAT, which can increase the complexity of the already time-consuming manual segmentation work. We propose a 3D deep attention U-Net method to automatically segment the EAT from coronary computed tomography angiography (CCTA). Five-fold cross-validation and hold-out experiments were used to evaluate the proposed method through a retrospective investigation of 200 patients. The automatically segmented EAT volume was compared with physician-approved clinical contours. Quantitative metrics used were the Dice similarity coefficient (DSC), sensitivity, specificity, Jaccard index (JAC), Hausdorff distance (HD), mean surface distance (MSD), residual mean square distance (RMSD), and the center of mass distance (CMD). For cross-validation, the median DSC, sensitivity, and specificity were 92.7%, 91.1%, and 95.1%, respectively, with JAC, HD, CMD, MSD, and RMSD are 82.9% ± 8.8%, 3.77 ± 1.86 mm, 1.98 ± 1.50 mm, 0.37 ± 0.24 mm, and 0.65 ± 0.37 mm, respectively. For the hold-out test, the accuracy of the proposed method remained high. We developed a novel deep learning-based approach for the automated segmentation of the EAT on CCTA images. We demonstrated the high accuracy of the proposed learning-based segmentation method through comparison with ground truth contour of 200 clinical patient cases using 8 quantitative metrics, Pearson correlation, and Bland-Altman analysis. Our automatic EAT segmentation results show the potential of the proposed method to be used in computer-aided diagnosis of coronary artery diseases (CADs) in clinical settings.",2020,10.1088/1361-6560/ab8077,retrospective cohort,prognosis,CT,Pericardium
Deep learning segmentation and quantification method for assessing epicardial adipose tissue in CT calcium score scans,"Epicardial adipose tissue volume (EAT) has been linked to coronary artery disease and the risk of major adverse cardiac events. As manual quantification of EAT is time-consuming, requires specialized training, and is prone to human error, we developed a deep learning method (DeepFat) for the automatic assessment of EAT on non-contrast low-dose CT calcium score images. Our DeepFat intuitively segmented the tissue enclosed by the pericardial sac on axial slices, using two preprocessing steps. First, we applied a HU-attention-window with a window/level 350/40-HU to draw attention to the sac and reduce numerical errors. Second, we applied a novel look ahead slab-of-slices with bisection (""bisect"") in which we split the heart into halves and sequenced the lower half from bottom-to-middle and the upper half from top-to-middle, thereby presenting an always increasing curvature of the sac to the network. EAT volume was obtained by thresholding voxels within the sac in the fat window (- 190/- 30-HU). Compared to manual segmentation, our algorithm gave excellent results with volume Dice = 88.52% ± 3.3, slice Dice = 87.70% ± 7.5, EAT error = 0.5% ± 8.1, and R = 98.52% (p < 0.001). HU-attention-window and bisect improved Dice volume scores by 0.49% and 3.2% absolute, respectively. Variability between analysts was comparable to variability with DeepFat. Results compared favorably to those of previous publications.",2022,10.1038/s41598-022-06351-z,retrospective cohort,prognosis,CT,Pericardium
Automatic quantification of epicardial adipose tissue volume,"PURPOSE: Epicardial fat is the adipose tissue between the serosal pericardial wall layer and the visceral layer. It is distributed mainly around the atrioventricular groove, atrial septum, ventricular septum and coronary arteries. Studies have shown that the density, thickness, volume and other characteristics of epicardial adipose tissue (EAT) are independently correlated with a variety of cardiovascular diseases. Given this association, the accurate determination of EAT volume is an essential aim of future research. Therefore, the purpose of this study was to establish a framework for fully automatic EAT segmentation and quantification in coronary computed tomography angiography (CCTA) scans. METHODS: A set of 103 scans are randomly selected from our medical center. An automatic pipeline has been developed to segment and quantify the volume of EAT. First, a multi-slice deep neural network is used to simultaneously segment the pericardium in multiple adjacent slices. Then a deformable model is employed to reduce false positive and negative regions in the segmented binary pericardial images. Finally, the pericardium mask is used to define the region of interest (ROI) and the threshold method is utilized to extract the pixels ranging from -175 Hounsfield units (HU) to -15 HU for the segmentation of EAT. RESULTS: The Dice indices of the pericardial segmentation using the proposed method with respect to the manual delineation results of two radiology experts were 97.1% ± 0.7% and 96.9% ± 0.6%, respectively. The inter-observer variability was also assessed, resulting in a Dice index of 97.0% ± 0.7%. For the EAT segmentation results, the Dice indices between the proposed method and the two radiology experts were 93.4% ± 1.5% and 93.3% ± 1.3%, respectively, and the same measurement between the experts themselves was 93.6% ± 1.9%. The Pearson's correlation coefficients between the EAT volumes computed from the results of the proposed method and the manual delineation by the two experts were 1.00 and 0.99 and the same coefficients between the experts was 0.99. CONCLUSIONS: This work describes the development of a fully automatic EAT segmentation and quantification method from CCTA scans and the results compare favorably with the assessments of two independent experts. The proposed method is also packaged with a graphical user interface which can be found at https://github.com/MountainAndMorning/EATSeg.",2021,10.1002/mp.15012,cross-sectional,prognosis,CT,Pericard
"Metabolic syndrome, fatty liver, and artificial intelligence-based epicardial adipose tissue measures predict long-term risk of cardiac events: a prospective study","BACKGROUND: We sought to evaluate the association of metabolic syndrome (MetS) and computed tomography (CT)-derived cardiometabolic biomarkers (non-alcoholic fatty liver disease [NAFLD] and epicardial adipose tissue [EAT] measures) with long-term risk of major adverse cardiovascular events (MACE) in asymptomatic individuals. METHODS: This was a post-hoc analysis of the prospective EISNER (Early-Identification of Subclinical Atherosclerosis by Noninvasive Imaging Research) study of participants who underwent baseline coronary artery calcium (CAC) scoring CT and 14-year follow-up for MACE (myocardial infarction, late revascularization, or cardiac death). EAT volume (cm(3)) and attenuation (Hounsfield units [HU]) were quantified from CT using fully automated deep learning software (< 30 s per case). NAFLD was defined as liver-to-spleen attenuation ratio < 1.0 and/or average liver attenuation < 40 HU. RESULTS: In the final population of 2068 participants (59% males, 56 ± 9 years), those with MetS (n = 280;13.5%) had a greater prevalence of NAFLD (26.0% vs. 9.9%), higher EAT volume (114.1 cm(3) vs. 73.7 cm(3)), and lower EAT attenuation (-76.9 HU vs. -73.4 HU; all p < 0.001) compared to those without MetS. At 14 ± 3 years, MACE occurred in 223 (10.8%) participants. In multivariable Cox regression, MetS was associated with increased risk of MACE (HR 1.58 [95% CI 1.10-2.27], p = 0.01) independently of CAC score; however, not after adjustment for EAT measures (p = 0.27). In a separate Cox analysis, NAFLD predicted MACE (HR 1.78 [95% CI 1.21-2.61], p = 0.003) independently of MetS, CAC score, and EAT measures. Addition of EAT volume to current risk assessment tools resulted in significant net reclassification improvement for MACE (22% over ASCVD risk score; 17% over ASCVD risk score plus CAC score). CONCLUSIONS: MetS, NAFLD, and artificial intelligence-based EAT measures predict long-term MACE risk in asymptomatic individuals. Imaging biomarkers of cardiometabolic disease have the potential for integration into routine reporting of CAC scoring CT to enhance cardiovascular risk stratification. Trial registration NCT00927693.",2021,10.1186/s12933-021-01220-x,,,,
Identifying epidermal growth factor receptor mutation status in patients with lung adenocarcinoma by three-dimensional convolutional neural networks,"OBJECTIVE: Genetic phenotype plays a central role in making treatment decisions of lung adenocarcinoma, especially the tyrosine-kinase-inhibitors-sensitive mutations of the epidermal growth factor receptor (EGFR) gene. We constructed three-dimensional convolutional neural networks (CNN) to analyze underlying patterns in CT images that could indicate that EGFR gene mutation status but are invisible to human eyes. METHODS: From 2012 to 2015, 503 Chinese patients with lung adenocarcinoma that had underwent surgery were included. Pathological types and EGFR mutation status were tested from surgical resections. EGFR mutations (exon 19 deletion or exon 21 L858R) were found in 215/345 (62.3%) and 91/158 (57.6%) patients in the training and independent validation set, respectively. CT images were taken before any invasive operation. The patients were randomly chosen to train the CNNs or validate the CNNs' performance. The performance was quantified using area under receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy. RESULTS: The CNNs showed an AUC of 0.776 (range: 0.702-0.849, p< 0.0001) in the independent validation set and a fusion model of CNNs and clinical features (sex and smoking history) showed an AUC of 0.838 (range: 0.778-0.899, p< 0.0001), accuracy of 77.2%, sensitivity of 75.8% and specificity of 79.1% at the best diagnostic decision point. CONCLUSION: The CNN exhibits potential ability to identify EGFR mutation status in patients with lung adenocarcinoma which might help make clinical decisions. ADVANCES IN KNOWLEDGE: The CNN showed some diagnostic power and its performance could be further improved by increasing the training set, optimizing the network structure and training strategy. Medical image based CNN has the potential to reflect spatial heterogeneity.",2018,10.1259/bjr.20180334,cross-sectional,diagnosis,CT,Lung
Identifying Facemask-Wearing Condition Using Image Super-Resolution with Classification Network to Prevent COVID-19,"The rapid worldwide spread of Coronavirus Disease 2019 (COVID-19) has resulted in a global pandemic. Correct facemask wearing is valuable for infectious disease control, but the effectiveness of facemasks has been diminished, mostly due to improper wearing. However, there have not been any published reports on the automatic identification of facemask-wearing conditions. In this study, we develop a new facemask-wearing condition identification method by combining image super-resolution and classification networks (SRCNet), which quantifies a three-category classification problem based on unconstrained 2D facial images. The proposed algorithm contains four main steps: Image pre-processing, facial detection and cropping, image super-resolution, and facemask-wearing condition identification. Our method was trained and evaluated on the public dataset Medical Masks Dataset containing 3835 images with 671 images of no facemask-wearing, 134 images of incorrect facemask-wearing, and 3030 images of correct facemask-wearing. Finally, the proposed SRCNet achieved 98.70% accuracy and outperformed traditional end-to-end image classification methods using deep learning without image super-resolution by over 1.5% in kappa. Our findings indicate that the proposed SRCNet can achieve high-accuracy identification of facemask-wearing conditions, thus having potential applications in epidemic prevention involving COVID-19.",2020,10.3390/s20185236,,,,
Identifying pulmonary nodules or masses on chest radiography using deep learning: external validation and strategies to improve clinical practice,"AIM: To test the diagnostic performance of a deep learning-based system for the detection of clinically significant pulmonary nodules/masses on chest radiographs. MATERIALS AND METHODS: Using a retrospective study of 100 patients (47 with clinically significant pulmonary nodules/masses and 53 control subjects without pulmonary nodules), two radiologists verified clinically significantly pulmonary nodules/masses according to chest computed tomography (CT) findings. A computer-aided diagnosis (CAD) software using a deep-learning approach was used to detect pulmonary nodules/masses to determine the diagnostic performance in four algorithms (heat map, abnormal probability, nodule probability, and mass probability). RESULTS: A total of 100 cases were included in the analysis. Among the four algorithms, mass algorithm could achieve a 76.6% sensitivity (36/47, 11 false negative) and 88.68% specificity (47/53, six false-positive) in the detection of pulmonary nodules/masses at the optimal probability score cut-off of 0.2884. Compared to the other three algorithms, mass probability algorithm had best predictive ability for pulmonary nodule/mass detection at the optimal probability score cut-off of 0.2884 (AUC(Mass): 0.916 versus AUC(Heat map): 0.682, p<0.001; AUC(Mass): 0.916 versus AUC(Abnormal): 0.810, p=0.002; AUC(Mass): 0.916 versus AUC(Nodule): 0.813, p=0.014). CONCLUSION: In conclusion, the deep-learning based computer-aided diagnosis system will likely play a vital role in the early detection and diagnosis of pulmonary nodules/masses on chest radiographs. In future applications, these algorithms could support triage workflow via double reading to improve sensitivity and specificity during the diagnostic process.",2020,10.1016/j.crad.2019.08.005,cross-sectional,diagnosis,Radiograph,Lung
Identifying sarcopenia in advanced non-small cell lung cancer patients using skeletal muscle CT radiomics and machine learning,"BACKGROUND: Sarcopenia has been confirmed as a poor prognostic indicator of lung cancer. However, the lack of abdominal computed tomography (CT) hindered the application to assess the status of sarcopenia. The purpose of this study was to assess the ability of chest CT radiomics combined with machine learning classifiers to identify sarcopenia in advanced non-small cell lung cancer (NSCLC) patients. METHODS: This study retrospectively analyzed CT images of 99 patients with NSCLC. Skeletal muscle radiomics were extracted from a single axial slice of the chest CT scan at the 12th thoracic vertebrae level. In total, 854 radiomic and clinical features were obtained from each patient. Feature selection was conducted with FeatureSelector module, optimal key features were fed into the lightGBM classifier for model construction, and Bayesian optimization was adopted to tune hyperparameters. The model's performance was evaluated by specificity, sensitivity, accuracy, precision, F1-score, Matthew's correlation coefficient (MCC), Cohen's kappa coefficient (Kappa), and AUC. RESULTS: A total of 40 patients were found to have sarcopenia. Five optimal features were selected. In the base lightGBM model, the specificity, sensitivity, accuracy, precision, F1-score, AUC, MCC, Kappa of validation set were 0.889, 0.750, 0.833, 0.818, 0.783, 0.819, 0.649, 0.648, respectively. After Bayesian hyperparameter tuning, the optimized lightGBM model achieved better prediction performance, and the corresponding values were 0.944, 0.833, 0.900, 0.909, 0.870, 0.889, 0.791, 0.789, respectively. CONCLUSIONS: Chest CT-based radiomics has the potential to identify sarcopenia in NSCLC patients with the lightGBM classifier, and the optimal lightGBM model via Bayesian hyperparameter tuning demonstrated better performance. KEY POINTS: SIGNIFICANT FINDINGS OF THE STUDY: Our study demonstrates that chest CT-based radiomics combined with lightGBM classifier has the ability to identify sarcopenia in NSCLC patients. WHAT THIS STUDY ADDS: Skeletal muscle radiomics would be a potential biomarker for sarcopenia identity in NSCLC patients.",2020,10.1111/1759-7714.13598,cross-sectional,diagnosis,CT,Lung
Illness duration and symptom profile in symptomatic UK school-aged children tested for SARS-CoV-2,"BACKGROUND: In children, SARS-CoV-2 infection is usually asymptomatic or causes a mild illness of short duration. Persistent illness has been reported; however, its prevalence and characteristics are unclear. We aimed to determine illness duration and characteristics in symptomatic UK school-aged children tested for SARS-CoV-2 using data from the COVID Symptom Study, one of the largest UK citizen participatory epidemiological studies to date. METHODS: In this prospective cohort study, data from UK school-aged children (age 5-17 years) were reported by an adult proxy. Participants were voluntary, and used a mobile application (app) launched jointly by Zoe Limited and King's College London. Illness duration and symptom prevalence, duration, and burden were analysed for children testing positive for SARS-CoV-2 for whom illness duration could be determined, and were assessed overall and for younger (age 5-11 years) and older (age 12-17 years) groups. Children with longer than 1 week between symptomatic reports on the app were excluded from analysis. Data from symptomatic children testing negative for SARS-CoV-2, matched 1:1 for age, gender, and week of testing, were also assessed. FINDINGS: 258 790 children aged 5-17 years were reported by an adult proxy between March 24, 2020, and Feb 22, 2021, of whom 75 529 had valid test results for SARS-CoV-2. 1734 children (588 younger and 1146 older children) had a positive SARS-CoV-2 test result and calculable illness duration within the study timeframe (illness onset between Sept 1, 2020, and Jan 24, 2021). The most common symptoms were headache (1079 [62·2%] of 1734 children), and fatigue (954 [55·0%] of 1734 children). Median illness duration was 6 days (IQR 3-11) versus 3 days (2-7) in children testing negative, and was positively associated with age (Spearman's rank-order r(s) 0·19, p<0·0001). Median illness duration was longer for older children (7 days, IQR 3-12) than younger children (5 days, 2-9). 77 (4·4%) of 1734 children had illness duration of at least 28 days, more commonly in older than younger children (59 [5·1%] of 1146 older children vs 18 [3·1%] of 588 younger children; p=0·046). The commonest symptoms experienced by these children during the first 4 weeks of illness were fatigue (65 [84·4%] of 77), headache (60 [77·9%] of 77), and anosmia (60 [77·9%] of 77); however, after day 28 the symptom burden was low (median 2 symptoms, IQR 1-4) compared with the first week of illness (median 6 symptoms, 4-8). Only 25 (1·8%) of 1379 children experienced symptoms for at least 56 days. Few children (15 children, 0·9%) in the negatively tested cohort had symptoms for at least 28 days; however, these children experienced greater symptom burden throughout their illness (9 symptoms, IQR 7·7-11·0 vs 8, 6-9) and after day 28 (5 symptoms, IQR 1·5-6·5 vs 2, 1-4) than did children who tested positive for SARS-CoV-2. INTERPRETATION: Although COVID-19 in children is usually of short duration with low symptom burden, some children with COVID-19 experience prolonged illness duration. Reassuringly, symptom burden in these children did not increase with time, and most recovered by day 56. Some children who tested negative for SARS-CoV-2 also had persistent and burdensome illness. A holistic approach for all children with persistent illness during the pandemic is appropriate. FUNDING: Zoe Limited, UK Government Department of Health and Social Care, Wellcome Trust, UK Engineering and Physical Sciences Research Council, UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare, UK National Institute for Health Research, UK Medical Research Council, British Heart Foundation, and Alzheimer's Society.",2021,10.1016/s2352-4642(21)00198-x,,,,
Image quality of ultralow-dose chest CT using deep learning techniques: potential superiority of vendor-agnostic post-processing over vendor-specific techniques,"OBJECTIVE: To compare the image quality between the vendor-agnostic and vendor-specific algorithms on ultralow-dose chest CT. METHODS: Vendor-agnostic deep learning post-processing model (DLM), vendor-specific deep learning image reconstruction (DLIR, high level), and adaptive statistical iterative reconstruction (ASiR, 70%) algorithms were employed. One hundred consecutive ultralow-dose noncontrast CT scans (CTDI(vol); mean, 0.33 ± 0.056 mGy) were reconstructed with five algorithms: DLM-stnd (standard kernel), DLM-shrp (sharp kernel), DLIR, ASiR-stnd, and ASiR-shrp. Three thoracic radiologists blinded to the reconstruction algorithms reviewed five sets of 100 images and assessed subjective noise, spatial resolution, distortion artifact, and overall image quality. They selected the most preferred algorithm among five image sets for each case. Image noise and signal-to-noise ratio were measured. Edge-rise-distance was measured at a pulmonary vessel, i.e., the distance between two points where attenuation was 10% and 90% of maximal intravascular intensity. The skewness of attenuation was calculated in homogeneous areas. RESULTS: DLM-stnd, followed by DLIR, showed the best subjective noise on both lung and mediastinal windows, while DLIR yielded the least measured noise (ps < .0001). Compared to DLM-stnd, DLIR showed inferior subjective spatial resolution on lung window and higher edge-rise-distance (ps < .0001). Additionally, DLIR showed the most frequent distortion artifacts and deviated skewness (ps < .0001). DLM-stnd scored the best overall image quality, followed by DLM-shrp and DLIR (mean score 3.89 ± 0.19, 3.68 ± 0.24, and 3.53 ± 0.33; ps < .001). Two among three readers preferred DLM-stnd on both windows. CONCLUSION: Although DLIR provided the best quantitative noise profile, DLM-stnd showed the best overall image quality with fewer artifacts and was preferred by two among three readers. KEY POINTS: • A vendor-agnostic deep learning post-processing algorithm applied to ultralow-dose chest CT exhibited the best image quality compared to vendor-specific deep learning algorithm and ASiR techniques. • Two out of three readers preferred a vendor-agnostic deep learning post-processing algorithm in comparison to vendor-specific deep learning algorithm and ASiR techniques. • A vendor-specific deep learning reconstruction algorithm yielded the least image noise, but showed significantly more frequent specific distortion artifacts and increased skewness of attenuation compared to a vendor-agnostic algorithm.",2021,10.1007/s00330-020-07537-7,cross-sectional,informatics,CT,Lung
IMAL-Net: Interpretable multi-task attention learning network for invasive lung adenocarcinoma screening in CT images,"PURPOSE: Feature maps created from deep convolutional neural networks (DCNNs) have been widely used for visual explanation of DCNN-based classification tasks. However, many clinical applications such as benign-malignant classification of lung nodules normally require quantitative and objective interpretability, rather than just visualization. In this paper, we propose a novel interpretable multi-task attention learning network named IMAL-Net for early invasive adenocarcinoma screening in chest computed tomography images, which takes advantage of segmentation prior to assist interpretable classification. METHODS: Two sub-ResNets are firstly integrated together via a prior-attention mechanism for simultaneous nodule segmentation and invasiveness classification. Then, numerous radiomic features from the segmentation results are concatenated with high-level semantic features from the classification subnetwork by FC layers to achieve superior performance. Meanwhile, an end-to-end feature selection mechanism (named FSM) is designed to quantify crucial radiomic features greatly affecting the prediction of each sample, and thus it can provide clinically applicable interpretability to the prediction result. RESULTS: Nodule samples from a total of 1626 patients were collected from two grade-A hospitals for large-scale verification. Five-fold cross validation demonstrated that the proposed IMAL-Net can achieve an AUC score of 93.8% ± 1.1% and a recall score of 93.8% ± 2.8% for identification of invasive lung adenocarcinoma. CONCLUSIONS: It can be concluded that fusing semantic features and radiomic features can achieve obvious improvements in the invasiveness classification task. Moreover, by learning more fine-grained semantic features and highlighting the most important radiomics features, the proposed attention and FSM mechanisms not only can further improve the performance but also can be used for both visual explanations and objective analysis of the classification results.",2021,10.1002/mp.15293,cross-sectional,diagnosis,CT,Lung
"Immunohistochemical quantification of expression of a tight junction protein, claudin-7, in human lung cancer samples using digital image analysis method","BACKGROUND AND OBJECTIVES: Tight junction proteins are correlated with cancer development. As the pivotal proteins in epithelial cells, altered expression and distribution of different claudins have been reported in a wide variety of human malignancies. We have previously reported that claudin-7 was strongly expressed in benign bronchial epithelial cells at the cell-cell junction while expression of claudin-7 was either altered with discontinued weak expression or completely absent in lung cancers. Based on these results, we continued working on the expression pattern of claudin-7 and its relationship with lung cancer development. We herein proposed a new Digital Image Classification, Fragmentation index, Morphological analysis (DICFM) method for differentiating the normal lung tissues and lung cancer tissues based on the claudin-7 immunohistochemical staining. METHODS: Seventy-seven lung cancer samples were obtained from the Second Affiliated Hospital of Zhejiang University and claudin-7 immunohistochemical staining was performed. Based on C++ and Open Source Computer Vision Library (OpenCV, version 2.4.4), the DICFM processing module was developed. Intensity and fragmentation of claudin-7 expression, as well as the morphological parameters of nuclei were calculated. Evaluation of results was performed using Receiver Operator Characteristic (ROC) analysis. RESULTS: Agreement between these computational results and the results obtained by two pathologists was demonstrated. The intensity of claudin-7 expression was significantly decreased while the fragmentation was significantly increased in the lung cancer tissues compared to the normal lung tissues and the intensity was strongly positively associated with the differentiation of lung cancer cells. Moreover, the perimeters of the nuclei of lung cancer cells were significantly greater than that of the normal lung cells, while the parameters of area and circularity revealed no statistical significance. CONCLUSIONS: Taken together, our DICFM approach may be applied as an appropriate approach to quantify the immunohistochemical staining of claudin-7 on the cell membrane and claudin-7 may serve as a marker for identification of lung cancer.",2018,10.1016/j.cmpb.2017.12.014,,,,
Impact of Age and Sex on COVID-19 Severity Assessed From Radiologic and Clinical Findings,"BACKGROUND: Data on the epidemiological characteristics and clinical features of COVID-19 in patients of different ages and sex are limited. Existing studies have mainly focused on the pediatric and elderly population. OBJECTIVE: Assess whether age and sex interact with other risk factors to influence the severity of SARS-CoV-2 infection. MATERIAL AND METHODS: The study sample included all consecutive patients who satisfied the inclusion criteria and who were treated from 24 February to 1 July 2020 in Dubai Mediclinic Parkview (560 cases) and Al Ain Hospital (605 cases), United Arab Emirates. We compared disease severity estimated from the radiological findings among patients of different age groups and sex. To analyze factors associated with an increased risk of severe disease, we conducted uni- and multivariate regression analyses. Specifically, age, sex, laboratory findings, and personal risk factors were used to predict moderate and severe COVID-19 with conventional machine learning methods. RESULTS: Need for O (2) supplementation was positively correlated with age. Intensive care was required more often for men of all ages (p < 0.01). Males were more likely to have at least moderate disease severity (p = 0.0083). These findings were aligned with the results of biochemical findings and suggest a direct correlation between older age and male sex with a severe course of the disease. In young males (18-39 years), the percentage of the lung parenchyma covered with consolidation and the density characteristics of lesions were higher than those of other age groups; however, there was no marked sex difference in middle-aged (40-64 years) and older adults (≥65 years). From the univariate analysis, the risk of the non-mild COVID-19 was significantly higher (p < 0.05) in midlife adults and older adults compared to young adults. The multivariate analysis provided similar findings. CONCLUSION: Age and sex were important predictors of disease severity in the set of data typically collected on admission. Sexual dissimilarities reduced with age. Age disparities were more pronounced if studied with the clinical markers of disease severity than with the radiological markers. The impact of sex on the clinical markers was more evident than that of age in our study.",2021,10.3389/fcimb.2021.777070,,,,
Impact of Confounding Thoracic Tubes and Pleural Dehiscence Extent on Artificial Intelligence Pneumothorax Detection in Chest Radiographs,"OBJECTIVES: We hypothesized that published performances of algorithms for artificial intelligence (AI) pneumothorax (PTX) detection in chest radiographs (CXRs) do not sufficiently consider the influence of PTX size and confounding effects caused by thoracic tubes (TTs). Therefore, we established a radiologically annotated benchmarking cohort (n = 6446) allowing for a detailed subgroup analysis. MATERIALS AND METHODS: We retrospectively identified 6434 supine CXRs, among them 1652 PTX-positive cases and 4782 PTX-negative cases. Supine CXRs were radiologically annotated for PTX size, PTX location, and inserted TTs. The diagnostic performances of 2 AI algorithms (""AI_CheXNet"" [Rajpurkar et al], ""AI_1.5"" [Guendel et al]), both trained on publicly available datasets with labels obtained from automatic report interpretation, were quantified. The algorithms' discriminative power for PTX detection was quantified by the area under the receiver operating characteristics (AUROC), and significance analysis was based on the corresponding 95% confidence interval. A detailed subgroup analysis was performed to quantify the influence of PTX size and the confounding effects caused by inserted TTs. RESULTS: Algorithm performance was quantified as follows: overall performance with AUROCs of 0.704 (AI_1.5) / 0.765 (AI_CheXNet) for unilateral PTXs, AUROCs of 0.666 (AI_1.5) / 0.722 (AI_CheXNet) for unilateral PTXs smaller than 1 cm, and AUROCs of 0.735 (AI_1.5) / 0.818 (AI_CheXNet) for unilateral PTXs larger than 2 cm. Subgroup analysis identified TTs to be strong confounders that significantly influence algorithm performance: Discriminative power is completely eliminated by analyzing PTX-positive cases without TTs referenced to control PTX-negative cases with inserted TTs. Contrarily, AUROCs increased up to 0.875 (AI_CheXNet) for large PTX-positive cases with inserted TTs referenced to control cases without TTs. CONCLUSIONS: Our detailed subgroup analysis demonstrated that the performance of established AI algorithms for PTX detection trained on public datasets strongly depends on PTX size and is significantly biased by confounding image features, such as inserted TTS. Our established, clinically relevant and radiologically annotated benchmarking cohort might be of great benefit for ongoing algorithm development.",2020,10.1097/rli.0000000000000707,cross-sectional,diagnosis,Radiograph,Lung
Impact of dose reduction and iterative reconstruction algorithm on the detectability of pulmonary nodules by artificial intelligence,"PURPOSE: The purpose of this study was to assess whether the performances of an automated software for lung nodule detection with computed tomography (CT) are affected by radiation dose and the use of iterative reconstruction algorithm. MATERIALS AND METHODS: A chest phantom (Multipurpose Chest Phantom N1; Kyoto Kagaku Co. Ltd, Kyoto, Japan) with 15 pulmonary nodules was scanned with a total of five CT protocol settings with up to 20-fold dose reduction. All CT examinations were reconstructed with iterative reconstruction algorithms ADMIRE 3 and ADMIRE 5 and were then analyzed for the presence of pulmonary nodules with a fully automated computer aided detection software system (InferRead(TM) CT Lung, Infervision), which is based on deep neural networks. RESULTS: The sensitivity of fully automated pulmonary nodule detection for ground-glass nodules at standard dose CT was greater (70.0%; 14/20; 95% CI: 51.6-88.4%) than at 10-fold and 20-fold dose reduction (30.0%; 6/20; 95% CI: 0.0%-62.5%). There were less false positive findings when ADMIRE 5 reconstruction was used (4.0 ± 2.8 [SD]; range: 2-6) instead of ADMIRE 3 reconstruction (25.0 ± 15.6 [SD]; range: 14-36). There was no difference in the sensitivity of detection of solid and subsolid nodules between standard dose (100%; 95% CI: 100-100%) and 10- and 20-fold reduced dose CT (92.5%; 95% CI: 83.8-100.0%). Image noise was significantly greater with ADMIRE 3 (81 ± 2 [SD] [range: 79-84]; 104 ± 3 [SD] [range: 101-107]; 114 ± 5 [SD] [range: 110-119]; 193 ± 10 [SD] [range: 183-203]; 220 ± 16 [SD] [range: 210-238]) compared to ADMIRE 5 (44 ± 2 [SD] [range: 42-46]; 60 ± 2 [SD] [range: 57-61]; 66 ± 1 [SD] [range: 65-67]; 103 ± 4 [SD] [range: 98-106]; 110 ± 1 [SD] [range: 109-111]), respectively in each of the five CT protocols. CONCLUSION: This phantom study suggests that dose reduction and iterative reconstruction settings have an impact on detectability of pulmonary nodules by artificial intelligence software and we therefore encourage adaption of dose levels and reconstruction methods prior to widespread implementation of fully automatic nodule detection software for lung cancer screening purposes.",2022,10.1016/j.diii.2021.12.002,other,diagnosis,CT,Lung
Impact of feature harmonization on radiogenomics analysis: Prediction of EGFR and KRAS mutations from non-small cell lung cancer PET/CT images,"OBJECTIVE: To investigate the impact of harmonization on the performance of CT, PET, and fused PET/CT radiomic features toward the prediction of mutations status, for epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) genes in non-small cell lung cancer (NSCLC) patients. METHODS: Radiomic features were extracted from tumors delineated on CT, PET, and wavelet fused PET/CT images obtained from 136 histologically proven NSCLC patients. Univariate and multivariate predictive models were developed using radiomic features before and after ComBat harmonization to predict EGFR and KRAS mutation statuses. Multivariate models were built using minimum redundancy maximum relevance feature selection and random forest classifier. We utilized 70/30% splitting patient datasets for training/testing, respectively, and repeated the procedure 10 times. The area under the receiver operator characteristic curve (AUC), accuracy, sensitivity, and specificity were used to assess model performance. The performance of the models (univariate and multivariate), before and after ComBat harmonization was compared using statistical analyses. RESULTS: While the performance of most features in univariate modeling was significantly improved for EGFR prediction, most features did not show any significant difference in performance after harmonization in KRAS prediction. Average AUCs of all multivariate predictive models for both EGFR and KRAS were significantly improved (q-value < 0.05) following ComBat harmonization. The mean ranges of AUCs increased following harmonization from 0.87-0.90 to 0.92-0.94 for EGFR, and from 0.85-0.90 to 0.91-0.94 for KRAS. The highest performance was achieved by harmonized F_R0.66_W0.75 model with AUC of 0.94, and 0.93 for EGFR and KRAS, respectively. CONCLUSION: Our results demonstrated that regarding univariate modelling, while ComBat harmonization had generally a better impact on features for EGFR compared to KRAS status prediction, its effect is feature-dependent. Hence, no systematic effect was observed. Regarding the multivariate models, ComBat harmonization significantly improved the performance of all radiomics models toward more successful prediction of EGFR and KRAS mutation statuses in lung cancer patients. Thus, by eliminating the batch effect in multi-centric radiomic feature sets, harmonization is a promising tool for developing robust and reproducible radiomics using vast and variant datasets.",2022,10.1016/j.compbiomed.2022.105230,cross-sectional,diagnosis,PET/CT,Lung
Impact of GAN-based lesion-focused medical image super-resolution on the robustness of radiomic features,"Robust machine learning models based on radiomic features might allow for accurate diagnosis, prognosis, and medical decision-making. Unfortunately, the lack of standardized radiomic feature extraction has hampered their clinical use. Since the radiomic features tend to be affected by low voxel statistics in regions of interest, increasing the sample size would improve their robustness in clinical studies. Therefore, we propose a Generative Adversarial Network (GAN)-based lesion-focused framework for Computed Tomography (CT) image Super-Resolution (SR); for the lesion (i.e., cancer) patch-focused training, we incorporate Spatial Pyramid Pooling (SPP) into GAN-Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE). At [Formula: see text] SR, the proposed model achieved better perceptual quality with less blurring than the other considered state-of-the-art SR methods, while producing comparable results at [Formula: see text] SR. We also evaluated the robustness of our model's radiomic feature in terms of quantization on a different lung cancer CT dataset using Principal Component Analysis (PCA). Intriguingly, the most important radiomic features in our PCA-based analysis were the most robust features extracted on the GAN-super-resolved images. These achievements pave the way for the application of GAN-based image Super-Resolution techniques for studies of radiomics for robust biomarker discovery.",2021,10.1038/s41598-021-00898-z,cross-sectional,prognosis,CT,Lung
Impact of Lung Segmentation on the Diagnosis and Explanation of COVID-19 in Chest X-ray Images,"COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources.",2021,10.3390/s21217116,cross-sectional,diagnosis,Radiograph,Lung
Impact of pixel-based machine-learning techniques on automated frameworks for delineation of gross tumor volume regions for stereotactic body radiation therapy,"The aim of this study was to investigate the impact of pixel-based machine learning (ML) techniques, i.e., fuzzy-c-means clustering method (FCM), and the artificial neural network (ANN) and support vector machine (SVM), on an automated framework for delineation of gross tumor volume (GTV) regions of lung cancer for stereotactic body radiation therapy. The morphological and metabolic features for GTV regions, which were determined based on the knowledge of radiation oncologists, were fed on a pixel-by-pixel basis into the respective FCM, ANN, and SVM ML techniques. Then, the ML techniques were incorporated into the automated delineation framework of GTVs followed by an optimum contour selection (OCS) method, which we proposed in a previous study. The three-ML-based frameworks were evaluated for 16 lung cancer cases (six solid, four ground glass opacity (GGO), six part-solid GGO) with the datasets of planning computed tomography (CT) and (18)F-fluorodeoxyglucose (FDG) positron emission tomography (PET)/CT images using the three-dimensional Dice similarity coefficient (DSC). DSC denotes the degree of region similarity between the GTVs contoured by radiation oncologists and those estimated using the automated framework. The FCM-based framework achieved the highest DSCs of 0.79±0.06, whereas DSCs of the ANN-based and SVM-based frameworks were 0.76±0.14 and 0.73±0.14, respectively. The FCM-based framework provided the highest segmentation accuracy and precision without a learning process (lowest calculation cost). Therefore, the FCM-based framework can be useful for delineation of tumor regions in practical treatment planning.",2017,10.1016/j.ejmp.2017.08.012,cross-sectional,treatment,PET/CT,Lung
Impact of Vessel Suppressed-CT on Diagnostic Accuracy in Detection of Pulmonary Metastasis and Reading Time,"RATIONALE AND OBJECTIVES: To assess if vessel suppression (VS) improves nodule detection rate, interreader agreement, and reduces reading time in oncologic chest computed tomography (CT). MATERIAL AND METHODS: One-hundred consecutive oncologic patients (65 male; median age 60y) who underwent contrast-enhanced chest CT were retrospectively included. For all exams, additional VS series (ClearRead CT, Riverrain Technologies, Miamisburg) were reconstructed. Two groups of three radiologists each with matched experience were defined. Each group evaluated the SD-CT as well as VS-CT. Each reader marked the presence, size, and position of pulmonary nodules and documented reading time. In addition, for the VS-CT the presence of false positive nodules had to be stated. Cohen's Kappa (k) was used to calculate the interreader-agreement between groups. Reading time was compared using paired t test. RESULTS: Nodule detection rate was significantly higher in VS-CT compared to the SD-CT (+21%; p <0.001). Interreader-agreement was higher in the VS-CT (k = 0.431, moderate agreement) compared to SD-CT (k = 0.209, fair agreement). Almost all VS-CT series had false positive findings (97-99 out of 100). Average reading time was significantly shorter in the VS-CT compared to the SD-CT (154 ± 134vs. 194 ± 126; 21%, p<0.001). CONCLUSIONS: Vessel suppression increases nodule detection rate, improves interreader agreement, and reduces reading time in chest CT of oncologic patients. Due to false positive results a consensus reading with the SD-CT is essential.",2021,10.1016/j.acra.2020.01.014,cross-sectional,diagnosis,CT,Lung
Implementation of a Deep Learning-Based Computer-Aided Detection System for the Interpretation of Chest Radiographs in Patients Suspected for COVID-19,"OBJECTIVE: To describe the experience of implementing a deep learning-based computer-aided detection (CAD) system for the interpretation of chest X-ray radiographs (CXR) of suspected coronavirus disease (COVID-19) patients and investigate the diagnostic performance of CXR interpretation with CAD assistance. MATERIALS AND METHODS: In this single-center retrospective study, initial CXR of patients with suspected or confirmed COVID-19 were investigated. A commercialized deep learning-based CAD system that can identify various abnormalities on CXR was implemented for the interpretation of CXR in daily practice. The diagnostic performance of radiologists with CAD assistance were evaluated based on two different reference standards: 1) real-time reverse transcriptase-polymerase chain reaction (rRT-PCR) results for COVID-19 and 2) pulmonary abnormality suggesting pneumonia on chest CT. The turnaround times (TATs) of radiology reports for CXR and rRT-PCR results were also evaluated. RESULTS: Among 332 patients (male:female, 173:159; mean age, 57 years) with available rRT-PCR results, 16 patients (4.8%) were diagnosed with COVID-19. Using CXR, radiologists with CAD assistance identified rRT-PCR positive COVID-19 patients with sensitivity and specificity of 68.8% and 66.7%, respectively. Among 119 patients (male:female, 75:44; mean age, 69 years) with available chest CTs, radiologists assisted by CAD reported pneumonia on CXR with a sensitivity of 81.5% and a specificity of 72.3%. The TATs of CXR reports were significantly shorter than those of rRT-PCR results (median 51 vs. 507 minutes; p < 0.001). CONCLUSION: Radiologists with CAD assistance could identify patients with rRT-PCR-positive COVID-19 or pneumonia on CXR with a reasonably acceptable performance. In patients suspected with COVID-19, CXR had much faster TATs than rRT-PCRs.",2020,10.3348/kjr.2020.0536,cross-sectional,diagnosis,Radiograph,Lung
Implementation of an Artificial Intelligence-Based Double Read System in Capturing Pulmonary Nodule Discrepancy in CT Studies,"Studies show that up to 80% of all radiology errors are due to errors in perception. Early detection is critical for good outcomes in patients with primary lung cancer and lung metastasis. However, pulmonary nodules can be easily missed due to their small size. We prospectively applied a machine vision algorithm to CT studies containing lung parenchyma to detect pulmonary nodules, as well as a natural language processing algorithm to the text of the report to identify documentation of pulmonary nodules. Apparent discrepancies in perception - instances where a pulmonary nodule was not reported - were flagged for a secondary review by a radiologist. Four thousand and nine hundred studies were prospectively processed, of which 450 cases with potential discrepancies were detected. Preliminary manual analysis was performed to analyze the base error rate and to optimize thresholds for the machine vision and natural language processing algorithms, and 104 cases were flagged for final review. Of these 104 cases, 50 cases contained undocumented pulmonary nodules. Among these cases, 7 cases were classified as likely to be significant, where report addendums were done and the clinicians notified. We have successfully implemented an automated double read system to detect pulmonary nodule discrepancies, with minimal disruption to the radiology workflow and while keeping personal health information on-premises. This successful implementation demonstrates the viability of using an automated and secure radiology double-read system to improve patient safety in radiology workflows, either at a health system or an independent radiology practice.",2021,10.1067/j.cpradiol.2020.07.006,prospective cohort,diagnosis,CT,Lung
Implementation of convolutional neural network approach for COVID-19 disease detection,"In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.",2020,10.1152/physiolgenomics.00084.2020,cross-sectional,diagnosis,Radiograph,Lung
"Implementation of the Australian Computer-Assisted Theragnostics (AusCAT) network for radiation oncology data extraction, reporting and distributed learning","INTRODUCTION: There is significant potential to analyse and model routinely collected data for radiotherapy patients to provide evidence to support clinical decisions, particularly where clinical trials evidence is limited or non-existent. However, in practice there are administrative, ethical, technical, logistical and legislative barriers to having coordinated data analysis platforms across radiation oncology centres. METHODS: A distributed learning network of computer systems is presented, with software tools to extract and report on oncology data and to enable statistical model development. A distributed or federated learning approach keeps data in the local centre, but models are developed from the entire cohort. RESULTS: The feasibility of this approach is demonstrated across six Australian oncology centres, using routinely collected lung cancer data from oncology information systems. The infrastructure was used to validate and develop machine learning for model-based clinical decision support and for one centre to assess patient eligibility criteria for two major lung cancer radiotherapy clinical trials (RTOG-9410, RTOG-0617). External validation of a 2-year overall survival model for non-small cell lung cancer (NSCLC) gave an AUC of 0.65 and C-index of 0.62 across the network. For one centre, 65% of Stage III NSCLC patients did not meet eligibility criteria for either of the two practice-changing clinical trials, and these patients had poorer survival than eligible patients (10.6 m vs. 15.8 m, P = 0.024). CONCLUSION: Population-based studies on routine data are possible using a distributed learning approach. This has the potential for decision support models for patients for whom supporting clinical trial evidence is not applicable.",2021,10.1111/1754-9485.13287,,,,
Improved 3D tumour definition and quantification of uptake in simulated lung tumours using deep learning,"Objective.In clinical positron emission tomography (PET) imaging, quantification of radiotracer uptake in tumours is often performed using semi-quantitative measurements such as the standardised uptake value (SUV). For small objects, the accuracy of SUV estimates is limited by the noise properties of PET images and the partial volume effect. There is need for methods that provide more accurate and reproducible quantification of radiotracer uptake.Approach.In this work, we present a deep learning approach with the aim of improving quantification of lung tumour radiotracer uptake and tumour shape definition. A set of simulated tumours, assigned with 'ground truth' radiotracer distributions, are used to generate realistic PET raw data which are then reconstructed into PET images. In this work, the ground truth images are generated by placing simulated tumours characterised by different sizes and activity distributions in the left lung of an anthropomorphic phantom. These images are then used as input to an analytical simulator to simulate realistic raw PET data. The PET images reconstructed from the simulated raw data and the corresponding ground truth images are used to train a 3D convolutional neural network.Results.When tested on an unseen set of reconstructed PET phantom images, the network yields improved estimates of the corresponding ground truth. The same network is then applied to reconstructed PET data generated with different point spread functions. Overall the network is able to recover better defined tumour shapes and improved estimates of tumour maximum and median activities.Significance.Our results suggest that the proposed approach, trained on data simulated with one scanner geometry, has the potential to restore PET data acquired with different scanners.",2022,10.1088/1361-6560/ac65d6,cross-sectional,informatics,PET,Lung
Improved Classification of Lung Cancer Using Radial Basis Function Neural Network with Affine Transforms of Voss Representation,"Lung cancer is one of the diseases responsible for a large number of cancer related death cases worldwide. The recommended standard for screening and early detection of lung cancer is the low dose computed tomography. However, many patients diagnosed die within one year, which makes it essential to find alternative approaches for screening and early detection of lung cancer. We present computational methods that can be implemented in a functional multi-genomic system for classification, screening and early detection of lung cancer victims. Samples of top ten biomarker genes previously reported to have the highest frequency of lung cancer mutations and sequences of normal biomarker genes were respectively collected from the COSMIC and NCBI databases to validate the computational methods. Experiments were performed based on the combinations of Z-curve and tetrahedron affine transforms, Histogram of Oriented Gradient (HOG), Multilayer perceptron and Gaussian Radial Basis Function (RBF) neural networks to obtain an appropriate combination of computational methods to achieve improved classification of lung cancer biomarker genes. Results show that a combination of affine transforms of Voss representation, HOG genomic features and Gaussian RBF neural network perceptibly improves classification accuracy, specificity and sensitivity of lung cancer biomarker genes as well as achieving low mean square error.",2015,10.1371/journal.pone.0143542,,,,
Improved digital chest tomosynthesis image quality by use of a projection-based dual-energy virtual monochromatic convolutional neural network with super resolution,"We developed a novel dual-energy (DE) virtual monochromatic (VM) very-deep super-resolution (VDSR) method with an unsharp masking reconstruction algorithm (DE-VM-VDSR) that uses projection data to improve the nodule contrast and reduce ripple artifacts during chest digital tomosynthesis (DT). For estimating the residual errors from high-resolution and multiscale VM images from the projection space, the DE-VM-VDSR algorithm employs a training network (mini-batch stochastic gradient-descent algorithm with momentum) and a hybrid super-resolution (SR) image [simultaneous algebraic reconstruction technique (SART) total-variation (TV) first-iterative shrinkage-thresholding algorithm (FISTA); SART-TV-FISTA] that involves subjective reconstruction with bilateral filtering (BF) [DE-VM-VDSR with BF]. DE-DT imaging was accomplished by pulsed X-ray exposures rapidly switched between low (60 kV, 37 projection) and high (120 kV, 37 projection) tube-potential kVp by employing a 40° swing angle. This was followed by comparison of images obtained employing the conventional polychromatic filtered backprojection (FBP), SART, SART-TV-FISTA, and DE-VM-SART-TV-FISTA algorithms. The improvements in contrast, ripple artifacts, and resolution were compared using the signal-difference-to-noise ratio (SDNR), Gumbel distribution of the largest variations, radial modulation transfer function (radial MTF) for a chest phantom with simulated ground-glass opacity (GGO) nodules, and noise power spectrum (NPS) for uniform water phantom. The novel DE-VM-VDSR with BF improved the overall performance in terms of SDNR (DE-VM-VDSR with BF: 0.1603, without BF: 0.1517; FBP: 0.0521; SART: 0.0645; SART-TV-FISTA: 0.0984; and DE-VM-SART-TV-FISTA: 0.1004), obtained a Gumbel distribution that yielded good images showing the type of simulated GGO nodules used in the chest phantom, and reduced the ripple artifacts. The NPS of DE-VM-VDSR with BF showed the lowest noise characteristics in the high-frequency region (~0.8 cycles/mm). The DE-VM-VDSR without BF yielded an improved resolution relative to that of the conventional reconstruction algorithms for radial MTF analysis (0.2-0.3 cycles/mm). Finally, based on the overall image quality, DE-VM-VDSR with BF improved the contrast and reduced the high-frequency ripple artifacts and noise.",2020,10.1371/journal.pone.0244745,cross-sectional,diagnosis,digital tomosynthesis,Lung
Improved lung nodule diagnosis accuracy using lung CT images with uncertain class,"BACKGROUND AND OBJECTIVE: Among all malignant tumors, lung cancer ranks in the top in mortality rate. Pulmonary nodule is the early manifestation of lung cancer, and plays an important role in its discovery, diagnosis and treatment. The technology of medical imaging has encountered a rapid development in recent years, thus the amount of pulmonary nodules can be discovered are on the raise, which means even tiny or minor changes in lung can be recorded by the CT images. This paper proposes a pulmonary nodule computer aided diagnosis (CAD) based on semi-supervised extreme learning machine(SS-ELM). METHODS: First, the feature model based on the pulmonary nodules regions of lung CT images is established. After that, the same feature data sets have been put into ELM, support vector machine (SVM) methods, probabilistic neural network (PNN) and multilayer perceptron (MLP) so as to compare the performance of the methods. ELM turned out to have better performance in training time and testing accuracy compared with SVM, PNN and MLP. Then, we propose a pulmonary nodules computer aided diagnosis algorithm based on semi-supervised ELM (SS-ELM), which enables both certain class feature sets with labels and unlabeled feature sets to be input for training and computer aided diagnosing. This algorithm has provided a solution for the using of uncertain class data and improve the testing accuracy of benign and malignant diagnosis. RESULTS: 1018 sets of thoracic CT images from the Lung Database Consortium and Image Database Resource Initiative (LIDC-IDRI) have been used in experiment in order to test the effectiveness of the algorithm. Compared with ELM, the pulmonary nodules CAD based on SS-ELM has better testing accuracy performance. CONCLUSIONS: We have proposed a pulmonary nodule CAD system based on SS-ELM, which achieving better generalization performance at faster learning speed and higher testing accuracy than ELM, SVM, PNN and MLP. The SS-ELM based pulmonary nodules CAD has been proposed to solve the problem of uncertain class data using.",2018,10.1016/j.cmpb.2018.05.028,cross-sectional,diagnosis,CT,Lung
Improvement of fully automated airway segmentation on volumetric computed tomographic images using a 2.5 dimensional convolutional neural net,"We propose a novel airway segmentation method in volumetric chest computed tomography (CT) and evaluate its performance on multiple datasets. The segmentation is performed voxel-by-voxel by a 2.5D convolutional neural net (2.5D CNN) trained in a supervised manner. To enhance the accuracy of the segmented airway tree, we simultaneously took three adjacent slices in each of the orthogonal directions including axial, sagittal, and coronal and fine-tuned the parameters that influence the tree length and the number of leakage. The gold standard of airway segmentation was generated by a semi-automated method using AVIEW™. The 2.5D CNN was trained and evaluated on a subset of inspiratory thoracic CT scans taken from the Korean obstructive lung disease study, which includes normal subjects and chronic obstructive pulmonary disease patients. The reliability and further practicality of our proposed method was demonstrated in multiple datasets. In eight test datasets collected by the same imaging protocol, the percentage detected tree length, false positive rate, and Dice similarity coefficient of our method were 92.16%, 7.74%, and 0.8997 ± 0.0892, respectively. In 20 test datasets of the EXACT'09 challenge, the percentage detected tree length was 60.1% and the false positive rate was 4.56%. Our fully automated (end-to-end) segmentation method could be applied in radiologic practice.",2019,10.1016/j.media.2018.10.006,cross-sectional,informatics,CT,Lung
Improving accuracy and robustness of deep convolutional neural network based thoracic OAR segmentation,"Deep convolutional neural network (DCNN) has shown great success in various medical image segmentation tasks, including organ-at-risk (OAR) segmentation from computed tomography (CT) images. However, most studies use the dataset from the same source(s) for training and testing so that the ability of a trained DCNN to generalize to a different dataset is not well studied, as well as the strategy to address the issue of performance drop on a different dataset. In this study we investigated the performance of a well-trained DCNN model from a public dataset for thoracic OAR segmentation on a local dataset and explored the systematic differences between the datasets. We observed that a subtle shift of organs inside patient body due to the abdominal compression technique during image acquisition caused significantly worse performance on the local dataset. Furthermore, we developed an optimal strategy via incorporating different numbers of new cases from the local institution and using transfer learning to improve the accuracy and robustness of the trained DCNN model. We found that by adding as few as 10 cases from the local institution, the performance can reach the same level as in the original dataset. With transfer learning, the training time can be significantly shortened with slightly worse performance for heart segmentation.",2020,10.1088/1361-6560/ab7877,cross-sectional,informatics,CT,Heart
Improving Accuracy of Lung Nodule Classification Using Deep Learning with Focal Loss,"Early detection and classification of pulmonary nodules using computer-aided diagnosis (CAD) systems is useful in reducing mortality rates of lung cancer. In this paper, we propose a new deep learning method to improve classification accuracy of pulmonary nodules in computed tomography (CT) scans. Our method uses a novel 15-layer 2D deep convolutional neural network architecture for automatic feature extraction and classification of pulmonary candidates as nodule or nonnodule. Focal loss function is then applied to the training process to boost classification accuracy of the model. We evaluated our method on the LIDC/IDRI dataset extracted by the LUNA16 challenge. The experiments showed that our deep learning method with focal loss is a high-quality classifier with an accuracy of 97.2%, sensitivity of 96.0%, and specificity of 97.3%.",2019,10.1155/2019/5156416,cross-sectional,diagnosis,CT,Lung
Improving airway segmentation in computed tomography using leak detection with convolutional networks,"We propose a novel method to improve airway segmentation in thoracic computed tomography (CT) by detecting and removing leaks. Leak detection is formulated as a classification problem, in which a convolutional network (ConvNet) is trained in a supervised fashion to perform the classification task. In order to increase the segmented airway tree length, we take advantage of the fact that multiple segmentations can be extracted from a given airway segmentation algorithm by varying the parameters that influence the tree length and the amount of leaks. We propose a strategy in which the combination of these segmentations after removing leaks can increase the airway tree length while limiting the amount of leaks. This strategy therefore largely circumvents the need for parameter fine-tuning of a given airway segmentation algorithm. The ConvNet was trained and evaluated using a subset of inspiratory thoracic CT scans taken from the COPDGene study. Our method was validated on a separate independent set of the EXACT'09 challenge. We show that our method significantly improves the quality of a given leaky airway segmentation, achieving a higher sensitivity at a low false-positive rate compared to all the state-of-the-art methods that entered in EXACT09, and approaching the performance of the combination of all of them.",2017,10.1016/j.media.2016.11.001,cross-sectional,informatics,CT,Lung
Improving clinical disease subtyping and future events prediction through a chest CT-based deep learning approach,"PURPOSE: To develop and evaluate a deep learning (DL) approach to extract rich information from high-resolution computed tomography (HRCT) of patients with chronic obstructive pulmonary disease (COPD). METHODS: We develop a DL-based model to learn a compact representation of a subject, which is predictive of COPD physiologic severity and other outcomes. Our DL model learned: (a) to extract informative regional image features from HRCT; (b) to adaptively weight these features and form an aggregate patient representation; and finally, (c) to predict several COPD outcomes. The adaptive weights correspond to the regional lung contribution to the disease. We evaluate the model on 10 300 participants from the COPDGene cohort. RESULTS: Our model was strongly predictive of spirometric obstruction ( r2 = 0.67) and grouped 65.4% of subjects correctly and 89.1% within one stage of their GOLD severity stage. Our model achieved an accuracy of 41.7% and 52.8% in stratifying the population-based on centrilobular (5-grade) and paraseptal (3-grade) emphysema severity score, respectively. For predicting future exacerbation, combining subjects' representations from our model with their past exacerbation histories achieved an accuracy of 80.8% (area under the ROC curve of 0.73). For all-cause mortality, in Cox regression analysis, we outperformed the BODE index improving the concordance metric (ours: 0.61 vs BODE: 0.56). CONCLUSIONS: Our model independently predicted spirometric obstruction, emphysema severity, exacerbation risk, and mortality from CT imaging alone. This method has potential applicability in both research and clinical practice.",2021,10.1002/mp.14673,retrospective cohort,prognosis,CT,Lung
Improving Early Identification of Significant Weight Loss Using Clinical Decision Support System in Lung Cancer Radiation Therapy,"PURPOSE: Early identification of patients who may be at high risk of significant weight loss (SWL) is important for timely clinical intervention in lung cancer radiotherapy (RT). A clinical decision support system (CDSS) for SWL prediction was implemented within the routine clinical workflow and assessed on a prospective cohort of patients. MATERIALS AND METHODS: CDSS incorporated a machine learning prediction model on the basis of radiomics and dosiomics image features and was connected to a web-based dashboard for streamlined patient enrollment, feature extraction, SWL prediction, and physicians' evaluation processes. Patients with lung cancer (N = 37) treated with definitive RT without prior RT were prospectively enrolled in the study. Radiomics and dosiomics features were extracted from CT and 3D dose volume, and SWL probability (≥ 0.5 considered as SWL) was predicted. Two physicians predicted whether the patient would have SWL before and after reviewing the CDSS prediction. The physician's prediction performance without and with CDSS and prediction changes before and after using CDSS were compared. RESULTS: CDSS showed significantly better prediction accuracy than physicians (0.73 v 0.54) with higher specificity (0.81 v 0.50) but with lower sensitivity (0.55 v 0.64). Physicians changed their original prediction after reviewing CDSS prediction for four cases (three correctly and one incorrectly), for all of which CDSS prediction was correct. Physicians' prediction was improved with CDSS in accuracy (0.54-0.59), sensitivity (0.64-0.73), specificity (0.50-0.54), positive predictive value (0.35-0.40), and negative predictive value (0.76-0.82). CONCLUSION: Machine learning-based CDSS showed the potential to improve SWL prediction in lung cancer RT. More investigation on a larger patient cohort is needed to properly interpret CDSS prediction performance and its benefit in clinical decision making.",2021,10.1200/cci.20.00189,prospective cohort,diagnosis,CT,Lung
Improving motion-mask segmentation in thoracic CT with multiplanar U-nets,"PURPOSE: Motion-mask segmentation from thoracic computed tomography (CT) images is the process of extracting the region that encompasses lungs and viscera, where large displacements occur during breathing. It has been shown to help image registration between different respiratory phases. This registration step is, for example, useful for radiotherapy planning or calculating local lung ventilation. Knowing the location of motion discontinuity, that is, sliding motion near the pleura, allows a better control of the registration preventing unrealistic estimates. Nevertheless, existing methods for motion-mask segmentation are not robust enough to be used in clinical routine. This article shows that it is feasible to overcome this lack of robustness by using a lightweight deep-learning approach usable on a standard computer, and this even without data augmentation or advanced model design. METHODS: A convolutional neural-network architecture with three 2D U-nets for the three main orientations (sagittal, coronal, axial) was proposed. Predictions generated by the three U-nets were combined by majority voting to provide a single 3D segmentation of the motion mask. The networks were trained on a database of nonsmall cell lung cancer 4D CT images of 43 patients. Training and evaluation were done with a K-fold cross-validation strategy. Evaluation was based on a visual grading by two experts according to the appropriateness of the segmented motion mask for the registration task, and on a comparison with motion masks obtained by a baseline method using level sets. A second database (76 CT images of patients with early-stage COVID-19), unseen during training, was used to assess the generalizability of the trained neural network. RESULTS: The proposed approach outperformed the baseline method in terms of quality and robustness: the success rate increased from 53% to 79% without producing any failure. It also achieved a speed-up factor of 60 with GPU, or 17 with CPU. The memory footprint was low: less than 5 GB GPU RAM for training and less than 1 GB GPU RAM for inference. When evaluated on a dataset with images differing by several characteristics (CT device, pathology, and field of view), the proposed method improved the success rate from 53% to 83% . CONCLUSION: With 5-s processing time on a mid-range GPU and success rates around 80% , the proposed approach seems fast and robust enough to be routinely used in clinical practice. The success rate can be further improved by incorporating more diversity in training data via data augmentation and additional annotated images from different scanners and diseases. The code and trained model are publicly available.",2022,10.1002/mp.15347,cross-sectional,informatics,CT,Lung
Improving the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms,"OBJECTIVE: This study aims to develop and test a new computer-aided diagnosis (CAD) scheme of chest X-ray images to detect coronavirus (COVID-19) infected pneumonia. METHOD: CAD scheme first applies two image preprocessing steps to remove the majority of diaphragm regions, process the original image using a histogram equalization algorithm, and a bilateral low-pass filter. Then, the original image and two filtered images are used to form a pseudo color image. This image is fed into three input channels of a transfer learning-based convolutional neural network (CNN) model to classify chest X-ray images into 3 classes of COVID-19 infected pneumonia, other community-acquired no-COVID-19 infected pneumonia, and normal (non-pneumonia) cases. To build and test the CNN model, a publicly available dataset involving 8474 chest X-ray images is used, which includes 415, 5179 and 2,880 cases in three classes, respectively. Dataset is randomly divided into 3 subsets namely, training, validation, and testing with respect to the same frequency of cases in each class to train and test the CNN model. RESULTS: The CNN-based CAD scheme yields an overall accuracy of 94.5 % (2404/2544) with a 95 % confidence interval of [0.93,0.96] in classifying 3 classes. CAD also yields 98.4 % sensitivity (124/126) and 98.0 % specificity (2371/2418) in classifying cases with and without COVID-19 infection. However, without using two preprocessing steps, CAD yields a lower classification accuracy of 88.0 % (2239/2544). CONCLUSION: This study demonstrates that adding two image preprocessing steps and generating a pseudo color image plays an important role in developing a deep learning CAD scheme of chest X-ray images to improve accuracy in detecting COVID-19 infected pneumonia.",2020,10.1016/j.ijmedinf.2020.104284,cross-sectional,diagnosis,Radiograph,Lung
Improving the Subtype Classification of Non-small Cell Lung Cancer by Elastic Deformation Based Machine Learning,"Non-invasive image-based machine learning models have been used to classify subtypes of non-small cell lung cancer (NSCLC). However, the classification performance is limited by the dataset size, because insufficient data cannot fully represent the characteristics of the tumor lesions. In this work, a data augmentation method named elastic deformation is proposed to artificially enlarge the image dataset of NSCLC patients with two subtypes (squamous cell carcinoma and large cell carcinoma) of 3158 images. Elastic deformation effectively expanded the dataset by generating new images, in which tumor lesions go through elastic shape transformation. To evaluate the proposed method, two classification models were trained on the original and augmented dataset, respectively. Using augmented dataset for training significantly increased classification metrics including area under the curve (AUC) values of receiver operating characteristics (ROC) curves, accuracy, sensitivity, specificity, and f(1)-score, thus improved the NSCLC subtype classification performance. These results suggest that elastic deformation could be an effective data augmentation method for NSCLC tumor lesion images, and building classification models with the help of elastic deformation has the potential to serve for clinical lung cancer diagnosis and treatment design.",2021,10.1007/s10278-021-00455-0,cross-sectional,diagnosis,CT,Lung
INASNET: Automatic identification of coronavirus disease (COVID-19) based on chest X-ray using deep neural network,"Testing is one of the important methodologies used by various countries in order to fight against COVID-19 infection. The infection is considered as one of the deadliest ones although the mortality rate is not very high. COVID-19 infection is being caused by SARS-CoV2 which is termed as severe acute respiratory syndrome coronavirus 2 virus. To prevent the community, transfer among the masses, testing plays an important role. Efficient and quicker testing techniques helps in identification of infected person which makes it easier for to isolate the patient. Deep learning methods have proved their presence and effectiveness in medical image analysis and in the identification of some of the diseases like pneumonia. Authors have been proposed a deep learning mechanism and system to identify the COVID-19 infected patient on analyzing the X-ray images. Symptoms in the COVID-19 infection is well similar to the symptoms occurring in the influenza and pneumonia. The proposed model Inception Nasnet (INASNET) is being able to separate out and classify the X-ray images in the corresponding normal, COVID-19 infected or pneumonia infected classes. This testing method will be a boom for the doctors and for the state as it is a way cheaper method as compared to the other testing kits used by the healthcare workers for the diagnosis of the disease. Continuous analysis by convolutional neural network and regular evaluation will result in better accuracy and helps in eliminating the false-negative results. INASNET is based on the combined platform of InceptionNet and Neural network architecture search which will result in having higher and faster predictions. Regular testing, faster results, economically viable testing using X-ray images will help the front line workers to make a win over COVID-19.",2022,10.1016/j.isatra.2022.02.033,cross-sectional,diagnosis,CT,Lung
Incorporating automatically learned pulmonary nodule attributes into a convolutional neural network to improve accuracy of benign-malignant nodule classification,"Existing deep-learning-based pulmonary nodule classification models usually use images and benign-malignant labels as inputs for training. Image attributes of the nodules, as human-nameable high-level semantic labels, are rarely used to build a convolutional neural network (CNN). In this paper, a new method is proposed to combine the advantages of two classifications, which are pulmonary nodule benign-malignant classification and pulmonary nodule image attributes classification, into a deep learning network to improve the accuracy of pulmonary nodule classification. For this purpose, a unique 3D CNN is built to learn image attribute and benign-malignant classification simultaneously. A novel loss function is designed to balance the influence of two different kinds of classifications. The CNN is trained by a publicly available lung image database consortium (LIDC) dataset and is tested by a cross-validation method to predict the risk of a pulmonary nodule being malignant. This proposed method generates the accuracy of 91.47%, which is better than many existing models. Experimental findings show that if the CNN is built properly, the nodule attributes classification and benign-malignant classification can benefit from each other. By using nodule attribute learning as a control factor in a deep learning scheme, the accuracy of pulmonary nodule classification can be significantly improved by using a deep learning scheme.",2018,10.1088/1361-6560/aaf09f,cross-sectional,diagnosis,CT,Lung
Industry 4.0 technologies and their applications in fighting COVID-19 pandemic using deep learning techniques,"The disease known as COVID-19 has turned into a pandemic and spread all over the world. The fourth industrial revolution known as Industry 4.0 includes digitization, the Internet of Things, and artificial intelligence. Industry 4.0 has the potential to fulfil customized requirements during the COVID-19 emergency crises. The development of a prediction framework can help health authorities to react appropriately and rapidly. Clinical imaging like X-rays and computed tomography (CT) can play a significant part in the early diagnosis of COVID-19 patients that will help with appropriate treatment. The X-ray images could help in developing an automated system for the rapid identification of COVID-19 patients. This study makes use of a deep convolutional neural network (CNN) to extract significant features and discriminate X-ray images of infected patients from non-infected ones. Multiple image processing techniques are used to extract a region of interest (ROI) from the entire X-ray image. The ImageDataGenerator class is used to overcome the small dataset size and generate ten thousand augmented images. The performance of the proposed approach has been compared with state-of-the-art VGG16, AlexNet, and InceptionV3 models. Results demonstrate that the proposed CNN model outperforms other baseline models with high accuracy values: 97.68% for two classes, 89.85% for three classes, and 84.76% for four classes. This system allows COVID-19 patients to be processed by an automated screening system with minimal human contact.",2022,10.1016/j.compbiomed.2022.105418,cross-sectional,diagnosis,Radiograph,Lung
Inf-Net: Automatic COVID-19 Lung Infection Segmentation From CT Images,"Coronavirus Disease 2019 (COVID-19) spread globally in early 2020, causing the world to face an existential health crisis. Automated detection of lung infections from computed tomography (CT) images offers a great potential to augment the traditional healthcare strategy for tackling COVID-19. However, segmenting infected regions from CT slices faces several challenges, including high variation in infection characteristics, and low intensity contrast between infections and normal tissues. Further, collecting a large amount of data is impractical within a short time period, inhibiting the training of a deep model. To address these challenges, a novel COVID-19 Lung Infection Segmentation Deep Network (Inf-Net) is proposed to automatically identify infected regions from chest CT slices. In our Inf-Net, a parallel partial decoder is used to aggregate the high-level features and generate a global map. Then, the implicit reverse attention and explicit edge-attention are utilized to model the boundaries and enhance the representations. Moreover, to alleviate the shortage of labeled data, we present a semi-supervised segmentation framework based on a randomly selected propagation strategy, which only requires a few labeled images and leverages primarily unlabeled data. Our semi-supervised framework can improve the learning ability and achieve a higher performance. Extensive experiments on our COVID-SemiSeg and real CT volumes demonstrate that the proposed Inf-Net outperforms most cutting-edge segmentation models and advances the state-of-the-art performance.",2020,10.1109/tmi.2020.2996645,cross-sectional,informatics,CT,Lung
Infection Control for CT Equipment and Radiographers' Personal Protection During the Coronavirus Disease (COVID-19) Outbreak in China,"OBJECTIVE. Because CT plays an important role in diagnosis, isolation, treatment, and effective evaluation of coronavirus disease (COVID-19), infection prevention and control management of CT examination rooms is important. CONCLUSION. We describe modifications to the CT examination process, strict disinfection of examination rooms, arrangement of waiting areas, and efforts to increase radiographers' awareness of personal protection made at our institution during the COVID-19 outbreak. In addition, we discuss the potential of using artificial intelligence in imaging patients with contagious diseases.",2020,10.2214/ajr.20.23112,,,,
Influence of CT effective dose and convolution kernel on the detection of pulmonary nodules in different artificial intelligence software systems: A phantom study,"PURPOSE: To investigate the effective dose (E) and convolution kernel's effects on the detection of pulmonary nodules in different artificial intelligence (AI) software systems. METHODS: Simulated nodules of various sizes and densities in the Lungman phantom were CT scanned at different levels of E (3 - 5, 1 - 3, 0.5 - 1, and <0.5 mSv) and were reconstructed with different kernels (B30f, B60f, and B80f). The number of nodules and corresponding volumes in different images were detected by four AI software systems (A, B, C, and D). Sensitivity, false positives (FPs), false negatives (FNs), and relative volume error (RVE) were calculated and compared to the aspects of the E and convolution kernel. RESULTS: System B had the highest median sensitivity (100 %). The median FPs of systems B (1) and D (1) was lower than A (11.5) and C (5). System D had the smallest RVE (13.12 %). When the E was <0.5 mSv, system D's sensitivity decreased, while the FPs and FNs of systems A and B increased significantly (P < 0.05). When the kernel was changed from B80f to B30f, the FPs of system A decreased, while that of system C increased, and the RVE of systems A, B, and C increased (P < 0.05). CONCLUSION: AI software systems B and D have high detection efficiency under normal or low dose conditions and show better stability. However, the detection efficiency of systems A and C would be affected by the E or convolution kernel, but the E would not affect the volume measurement of four systems.",2020,10.1016/j.ejrad.2020.108928,cross-sectional,diagnosis,CT,Lung
Initial chest radiographs and artificial intelligence (AI) predict clinical outcomes in COVID-19 patients: analysis of 697 Italian patients,"OBJECTIVE: To evaluate whether the initial chest X-ray (CXR) severity assessed by an AI system may have prognostic utility in patients with COVID-19. METHODS: This retrospective single-center study included adult patients presenting to the emergency department (ED) between February 25 and April 9, 2020, with SARS-CoV-2 infection confirmed on real-time reverse transcriptase polymerase chain reaction (RT-PCR). Initial CXRs obtained on ED presentation were evaluated by a deep learning artificial intelligence (AI) system and compared with the Radiographic Assessment of Lung Edema (RALE) score, calculated by two experienced radiologists. Death and critical COVID-19 (admission to intensive care unit (ICU) or deaths occurring before ICU admission) were identified as clinical outcomes. Independent predictors of adverse outcomes were evaluated by multivariate analyses. RESULTS: Six hundred ninety-seven 697 patients were included in the study: 465 males (66.7%), median age of 62 years (IQR 52-75). Multivariate analyses adjusting for demographics and comorbidities showed that an AI system-based score ≥ 30 on the initial CXR was an independent predictor both for mortality (HR 2.60 (95% CI 1.69 - 3.99; p < 0.001)) and critical COVID-19 (HR 3.40 (95% CI 2.35-4.94; p < 0.001)). Other independent predictors were RALE score, older age, male sex, coronary artery disease, COPD, and neurodegenerative disease. CONCLUSION: AI- and radiologist-assessed disease severity scores on CXRs obtained on ED presentation were independent and comparable predictors of adverse outcomes in patients with COVID-19. TRIAL REGISTRATION: ClinicalTrials.gov NCT04318366 ( https://clinicaltrials.gov/ct2/show/NCT04318366 ). KEY POINTS: • AI system-based score ≥ 30 and a RALE score ≥ 12 at CXRs performed at ED presentation are independent and comparable predictors of death and/or ICU admission in COVID-19 patients. • Other independent predictors are older age, male sex, coronary artery disease, COPD, and neurodegenerative disease. • The comparable performance of the AI system in relation to a radiologist-assessed score in predicting adverse outcomes may represent a game-changer in resource-constrained settings.",2021,10.1007/s00330-020-07269-8,cross-sectional,diagnosis,Radiograph,Lung
INSIDEnet: Interpretable NonexpanSIve Data-Efficient network for denoising in grating interferometry breast CT,"PURPOSE: Breast cancer is the most common malignancy in women. Unfortunately, current breast imaging techniques all suffer from certain limitations: they are either not fully three dimensional, have an insufficient resolution or low soft-tissue contrast. Grating interferometry breast computed tomography (GI-BCT) is a promising X-ray phase contrast modality that could overcome these limitations by offering high soft-tissue contrast and excellent three-dimensional resolution. To enable the transition of this technology to clinical practice, dedicated data-processing algorithms must be developed in order to effectively retrieve the signals of interest from the measured raw data. METHODS: This article proposes a novel denoising algorithm that can cope with the high-noise amplitudes and heteroscedasticity which arise in GI-BCT when operated in a low-dose regime to effectively regularize the ill-conditioned GI-BCT inverse problem. We present a data-driven algorithm called INSIDEnet, which combines different ideas such as multiscale image processing, transform-domain filtering, transform learning, and explicit orthogonality to build an Interpretable NonexpanSIve Data-Efficient network (INSIDEnet). RESULTS: We apply the method to simulated breast phantom datasets and to real data acquired on a GI-BCT prototype and show that the proposed algorithm outperforms traditional state-of-the-art filters and is competitive with deep neural networks. The strong inductive bias given by the proposed model's architecture allows to reliably train the algorithm with very limited data while providing high model interpretability, thus offering a great advantage over classical convolutional neural networks (CNNs). CONCLUSIONS: The proposed INSIDEnet is highly data-efficient, interpretable, and outperforms state-of-the-art CNNs when trained on very limited training data. We expect the proposed method to become an important tool as part of a dedicated plug-and-play GI-BCT reconstruction framework, needed to translate this promising technology to the clinics.",2022,10.1002/mp.15595,,,,
Integrated Clinical and CT Based Artificial Intelligence Nomogram for Predicting Severity and Need for Ventilator Support in COVID-19 Patients: A Multi-Site Study,"Almost 25% of COVID-19 patients end up in ICU needing critical mechanical ventilation support. There is currently no validated objective way to predict which patients will end up needing ventilator support, when the disease is mild and not progressed. N = 869 patients from two sites (D(1): N = 822, D(2): N = 47) with baseline clinical characteristics and chest CT scans were considered for this study. The entire dataset was randomly divided into 70% training, D(1)(train) (N = 606) and 30% test-set (D(test): D(1)(test) (N = 216) + D(2) (N = 47)). An expert radiologist delineated ground-glass-opacities (GGOs) and consolidation regions on a subset of D(1)(train), (D(1)(train_sub), N = 88). These regions were automatically segmented and used along with their corresponding CT volumes to train an imaging AI predictor (AIP) on D(1)(train) to predict the need of mechanical ventilators for COVID-19 patients. Finally, top five prognostic clinical factors selected using univariate analysis were integrated with AIP to construct an integrated clinical and AI imaging nomogram (ClAIN). Univariate analysis identified lactate dehydrogenase, prothrombin time, aspartate aminotransferase, %lymphocytes, albumin as top five prognostic clinical features. AIP yielded an AUC of 0.81 on D(test) and was independently prognostic irrespective of other clinical parameters on multivariable analysis (p<0.001). ClAIN improved the performance over AIP yielding an AUC of 0.84 (p = 0.04) on D(test). ClAIN outperformed AIP in predicting which COVID-19 patients ended up needing a ventilator. Our results across multiple sites suggest that ClAIN could help identify COVID-19 with severe disease more precisely and likely to end up on a life-saving mechanical ventilation.",2021,10.1109/jbhi.2021.3103389,cross-sectional,prognosis,CT,Lung
Integrating Domain Knowledge Into Deep Networks for Lung Ultrasound With Applications to COVID-19,"Lung ultrasound (LUS) is a cheap, safe and non-invasive imaging modality that can be performed at patient bed-side. However, to date LUS is not widely adopted due to lack of trained personnel required for interpreting the acquired LUS frames. In this work we propose a framework for training deep artificial neural networks for interpreting LUS, which may promote broader use of LUS. When using LUS to evaluate a patient's condition, both anatomical phenomena (e.g., the pleural line, presence of consolidations), as well as sonographic artifacts (such as A- and B-lines) are of importance. In our framework, we integrate domain knowledge into deep neural networks by inputting anatomical features and LUS artifacts in the form of additional channels containing pleural and vertical artifacts masks along with the raw LUS frames. By explicitly supplying this domain knowledge, standard off-the-shelf neural networks can be rapidly and efficiently finetuned to accomplish various tasks on LUS data, such as frame classification or semantic segmentation. Our framework allows for a unified treatment of LUS frames captured by either convex or linear probes. We evaluated our proposed framework on the task of COVID-19 severity assessment using the ICLUS dataset. In particular, we finetuned simple image classification models to predict per-frame COVID-19 severity score. We also trained a semantic segmentation model to predict per-pixel COVID-19 severity annotations. Using the combined raw LUS frames and the detected lines for both tasks, our off-the-shelf models performed better than complicated models specifically designed for these tasks, exemplifying the efficacy of our framework.",2022,10.1109/tmi.2021.3117246,cross-sectional,diagnosis,Ultrasound,Lung
Integrating Multiomics Information in Deep Learning Architectures for Joint Actuarial Outcome Prediction in Non-Small Cell Lung Cancer Patients After Radiation Therapy,"PURPOSE: Novel actuarial deep learning neural network (ADNN) architectures are proposed for joint prediction of radiation therapy outcomes-radiation pneumonitis (RP) and local control (LC)-in stage III non-small cell lung cancer (NSCLC) patients. Unlike normal tissue complication probability/tumor control probability models that use dosimetric information solely, our proposed models consider complex interactions among multiomics information including positron emission tomography (PET) radiomics, cytokines, and miRNAs. Additional time-to-event information is also used in the actuarial prediction. METHODS AND MATERIALS: Three architectures were investigated: ADNN-DVH considered dosimetric information only; ADNN-com integrated multiomics information; and ADNN-com-joint combined RP2 (RP grade ≥2) and LC prediction. In these architectures, differential dose-volume histograms (DVHs) were fed into 1D convolutional neural networks (CNN) for extracting reduced representations. Variational encoders were used to learn representations of imaging and biological data. Reduced representations were fed into Surv-Nets to predict time-to-event probabilities for RP2 and LC independently and jointly by incorporating time information into designated loss functions. RESULTS: Models were evaluated on 117 retrospective patients and were independently tested on 25 newly accrued patients prospectively. A multi-institutional RTOG0617 data set of 327 patients was used for external validation. ADNN-DVH yielded cross-validated c-indexes (95% confidence intervals) of 0.660 (0.630-0.690) for RP2 prediction and 0.727 (0.700-0.753) for LC prediction, outperforming a generalized Lyman model for RP2 (0.613 [0.583-0.643]) and a generalized log-logistic model for LC (0.569 [0.545-0.594]). The independent internal test and external validation yielded similar results. ADNN-com achieved an even better performance than ADNN-DVH on both cross-validation and independent internal test. Furthermore, ADNN-com-joint, which yielded performance similar to ADNN-com, realized joint prediction with c-indexes of 0.705 (0.676-0.734) for RP2 and 0.740 (0.714-0.765) for LC and achieved an area under a free-response receiving operator characteristic curve (AU-FROC) of 0.729 (0.697-0.773) for the joint prediction of RP2 and LC. CONCLUSION: Novel deep learning architectures that integrate multiomics information outperformed traditional normal tissue complication probability/tumor control probability models in actuarial prediction of RP2 and LC.",2021,10.1016/j.ijrobp.2021.01.042,prospective cohort,prognosis,PET,Lung
"Integration of CNN, CBMIR, and Visualization Techniques for Diagnosis and Quantification of Covid-19 Disease","Diagnosis techniques based on medical image modalities have higher sensitivities compared to conventional RT-PCT tests. We propose two methods for diagnosing COVID-19 disease using X-ray images and differentiating it from viral pneumonia. The diagnosis section is based on deep neural networks, and the discriminating uses an image retrieval approach. Both units were trained by healthy, pneumonia, and COVID-19 images. In COVID-19 patients, the maximum intensity projection of the lung CT is visualized to a physician, and the CT Involvement Score is calculated. The performance of the CNN and image retrieval algorithms were improved by transfer learning and hashing functions. We achieved an accuracy of 97% and an overall prec@10 of 87%, respectively, concerning the CNN and the retrieval methods.",2021,10.1109/jbhi.2021.3067333,cross-sectional,diagnosis,Radiograph,Lung
Integration of convolutional neural networks for pulmonary nodule malignancy assessment in a lung cancer classification pipeline,"The early identification of malignant pulmonary nodules is critical for a better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules; METHODS: In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset; RESULTS: Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score; CONCLUSIONS: Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer.",2020,10.1016/j.cmpb.2019.105172,cross-sectional,diagnosis,CT,Lung
Integration of Deep Learning Radiomics and Counts of Circulating Tumor Cells Improves Prediction of Outcomes of Early Stage NSCLC Patients Treated With Stereotactic Body Radiation Therapy,"PURPOSE: We develop a deep learning (DL) radiomics model and integrate it with circulating tumor cell (CTC) counts as a clinically useful prognostic marker for predicting recurrence outcomes of early-stage (ES) non-small cell lung cancer (NSCLC) patients treated with stereotactic body radiation therapy (SBRT). METHODS AND MATERIALS: A cohort of 421 NSCLC patients was used to train a DL model for gleaning informative imaging features from computed tomography (CT) data. The learned imaging features were optimized on a cohort of 98 ES-NSCLC patients treated with SBRT for predicting individual patient recurrence risks by building DL models on CT data and clinical measures. These DL models were validated on the third cohort of 60 ES-NSCLC patients treated with SBRT to predict recurrent risks and stratify patients into subgroups with distinct outcomes in conjunction with CTC counts. RESULTS: The DL model obtained a concordance-index of 0.880 (95% confidence interval, 0.879-0.881). Patient subgroups with low and high DL risk scores had significantly different recurrence outcomes (P = 3.5e-04). The integration of DL risk scores and CTC measures identified 4 subgroups of patients with significantly different risks of recurrence (χ(2) = 20.11, P = 1.6e-04). Patients with positive CTC measures were associated with increased risks of recurrence that were significantly different from patients with negative CTC measures (P = 0.0447). CONCLUSIONS: In this first-ever study integrating DL radiomics models and CTC counts, our results suggested that this integration improves patient stratification compared with either imagining data or CTC measures alone in predicting recurrence outcomes for patients treated with SBRT for ES-NSCLC.",2022,10.1016/j.ijrobp.2021.11.006,retrospective cohort,prognosis,CT,Lung
Integrative analysis for COVID-19 patient outcome prediction,"While image analysis of chest computed tomography (CT) for COVID-19 diagnosis has been intensively studied, little work has been performed for image-based patient outcome prediction. Management of high-risk patients with early intervention is a key to lower the fatality rate of COVID-19 pneumonia, as a majority of patients recover naturally. Therefore, an accurate prediction of disease progression with baseline imaging at the time of the initial presentation can help in patient management. In lieu of only size and volume information of pulmonary abnormalities and features through deep learning based image segmentation, here we combine radiomics of lung opacities and non-imaging features from demographic data, vital signs, and laboratory findings to predict need for intensive care unit (ICU) admission. To our knowledge, this is the first study that uses holistic information of a patient including both imaging and non-imaging data for outcome prediction. The proposed methods were thoroughly evaluated on datasets separately collected from three hospitals, one in the United States, one in Iran, and another in Italy, with a total 295 patients with reverse transcription polymerase chain reaction (RT-PCR) assay positive COVID-19 pneumonia. Our experimental results demonstrate that adding non-imaging features can significantly improve the performance of prediction to achieve AUC up to 0.884 and sensitivity as high as 96.1%, which can be valuable to provide clinical decision support in managing COVID-19 patients. Our methods may also be applied to other lung diseases including but not limited to community acquired pneumonia. The source code of our work is available at https://github.com/DIAL-RPI/COVID19-ICUPrediction.",2021,10.1016/j.media.2020.101844,cross-sectional,prognosis,CT,Lung
Integrative analysis of imaging and transcriptomic data of the immune landscape associated with tumor metabolism in lung adenocarcinoma: Clinical and prognostic implications,"Although metabolic modulation in the tumor microenvironment (TME) is one of the key mechanisms of cancer immune escape, there is a lack of understanding of the comprehensive immune landscape of the TME and its association with tumor metabolism based on clinical evidence. We aimed to investigate the relationship between the immune landscape in the TME and tumor glucose metabolism in lung adenocarcinoma. Methods: Using RNA sequencing and image data, we developed a transcriptome-based tumor metabolism estimation model. The comprehensive TME cell types enrichment scores and overall immune cell enrichment (ImmuneScore) were estimated. Subjects were clustered by cellular heterogeneity in the TME and the clusters were characterized by tumor glucose metabolism and immune cell composition. Moreover, the prognostic value of ImmuneScore, tumor metabolism and the cell type-based clusters was also evaluated. Results: Four clusters were identified based on the cellular heterogeneity in the TME. They showed distinct immune cell composition, different tumor metabolism, and close relationship with overall survival. A cluster with high regulatory T cells showed relative hypermetabolism and poor prognosis. Another cluster with high mast cells and CD4+ central memory T cells showed relative hypometabolism and favorable prognosis. A cluster with high ImmuneScore showed favorable prognosis. Multivariate Cox analysis demonstrated that ImmuneScore was a predictive prognostic factor independent of other clinical features. Conclusions: Our results showed the association between predicted tumor metabolism and immune cell composition in the TME. Our studies also suggest that tumor glucose metabolism and immune cell infiltration in the TME can be clinically applicable for developing a prognostic stratification model.",2018,10.7150/thno.23767,,,,
Integrative Predictive Models of Computed Tomography Texture Parameters and Hematological Parameters for Lymph Node Metastasis in Lung Adenocarcinomas,"OBJECTIVES: The aims of the study were to integrate characteristics of computed tomography (CT), texture, and hematological parameters and to establish predictive models for lymph node (LN) metastasis in lung adenocarcinoma. METHODS: A total of 207 lung adenocarcinoma cases with confirmed postoperative pathology and preoperative CT scans between February 2017 and April 2019 were included in this retrospective study. All patients were divided into training and 2 validation cohorts chronologically in the ratio of 3:1:1. The χ2 test or Fisher exact test were used for categorical variables. The Shapiro-Wilk test and Mann-Whitney U test were used for continuous variables. Logistic regression and machine learning algorithm models based on CT characteristics, texture, and hematological parameters were used to predict LN metastasis. The performance of the multivariate models was evaluated using a receiver operating characteristic curve; prediction performance was evaluated in the validation cohorts. Decision curve analysis confirmed its clinical utility. RESULTS: Logistic regression analysis demonstrated that pleural thickening (P = 0.013), percentile 25th (P = 0.033), entropy gray-level co-occurrence matrix 10 (P = 0.019), red blood cell distribution width (P = 0.012), and lymphocyte-to-monocyte ratio (P = 0.049) were independent risk factors associated with LN metastasis. The area under the curve of the predictive model established using the previously mentioned 5 independent risk factors was 0.929 in the receiver operating characteristic analysis. The highest area under the curve was obtained in the training cohort (0.777 using Naive Bayes algorithm). CONCLUSIONS: Integrative predictive models of CT characteristics, texture, and hematological parameters could predict LN metastasis in lung adenocarcinomas. These findings may provide a reference for clinical decision making.",2022,10.1097/rct.0000000000001264,case control,diagnosis,CT,Lung
Intelligent Monitoring of Care Status for COPD Patients Based on Deep Learning,"To discuss the application method and effect of COPD patients in deep learning in intelligent monitoring, two groups were used under a reasonable selection of antibiotics specifically including reasonable and effective oxygen administration, atomization, sputum discharge treatment, psychotherapy, and rehabilitation training and treatment. Results were indicated, and there were significant differences between the lung function evaluation index and the two groups. Its intelligent monitoring mode was 97.5% and 80.0%, while the red blood cell ratio, arterial oxygen partial pressure (PaO(2)), pulse blood oxygen saturation (SpO(2)), arterial carbon dioxide partial pressure (PaCO(2)), and symptom improvement were better than artificial and were statistically significant (P < 0.05). Therefore, the training of the anti-inspiratory muscle can effectively improve the lung function and dyspnea symptoms of COPD patients at the stable stage, thus greatly improving their respiratory function and ensuring the quality of life of patients, which is worthy of clinical application.",2021,10.1155/2021/5690442,,,,
Interactive CT-video registration for the continuous guidance of bronchoscopy,"Bronchoscopy is a major step in lung cancer staging. To perform bronchoscopy, the physician uses a procedure plan, derived from a patient's 3D computed-tomography (CT) chest scan, to navigate the bronchoscope through the lung airways. Unfortunately, physicians vary greatly in their ability to perform bronchoscopy. As a result, image-guided bronchoscopy systems, drawing upon the concept of CT-based virtual bronchoscopy (VB), have been proposed. These systems attempt to register the bronchoscope's live position within the chest to a CT-based virtual chest space. Recent methods, which register the bronchoscopic video to CT-based endoluminal airway renderings, show promise but do not enable continuous real-time guidance. We present a CT-video registration method inspired by computer-vision innovations in the fields of image alignment and image-based rendering. In particular, motivated by the Lucas-Kanade algorithm, we propose an inverse-compositional framework built around a gradient-based optimization procedure. We next propose an implementation of the framework suitable for image-guided bronchoscopy. Laboratory tests, involving both single frames and continuous video sequences, demonstrate the robustness and accuracy of the method. Benchmark timing tests indicate that the method can run continuously at 300 frames/s, well beyond the real-time bronchoscopic video rate of 30 frames/s. This compares extremely favorably to the ≥ 1 s/frame speeds of other methods and indicates the method's potential for real-time continuous registration. A human phantom study confirms the method's efficacy for real-time guidance in a controlled setting, and, hence, points the way toward the first interactive CT-video registration approach for image-guided bronchoscopy. Along this line, we demonstrate the method's efficacy in a complete guidance system by presenting a clinical study involving lung cancer patients.",2013,10.1109/tmi.2013.2252361,cross-sectional,informatics,CT,Lung
Interest of the cellular population data analysis as an aid in the early diagnosis of SARS-CoV-2 infection,"INTRODUCTION: Coronavirus disease 2019 (COVID-19) is characterized by a high contagiousness requiring isolation measures. At this time, diagnosis is based on the positivity of specific RT-PCR and/or chest computed tomography scan, which are time-consuming and may delay diagnosis. Complete blood count (CBC) can potentially contribute to the diagnosis of COVID-19. We studied whether the analysis of cellular population data (CPD), provided as part of CBC-Diff analysis by the DxH 800 analyzers (Beckman Coulter), can help to identify SARS-CoV-2 infection. METHODS: Cellular population data of the different leukocyte subpopulations were analyzed in 137 controls, 322 patients with proven COVID-19 (COVID+), and 285 patients for whom investigations were negative for SARS-CoV-2 infection (COVID-). When CPD of COVID+ were different from controls and COVID- patients, we used receiver operating characteristic analysis to test the discriminating capacity of the individual parameters. Using a random forest classifier, we developed the algorithm based on the combination of 4 monocyte CPD to discriminate COVID+ from COVID- patients. This algorithm was tested prospectively in a series of 222 patients referred to the emergency unit. RESULTS: Among the 222 patients, 86 were diagnosed as COVID-19 and 60.5% were correctly identified using the discriminating protocol. Among the 136 COVID- patients, 10.3% were misclassified (specificity 89.7%, sensitivity 60.5%). False negatives were observed mainly in patients with a low inflammatory state whereas false positives were mainly seen in patients with sepsis. CONCLUSION: Consideration of CPD could constitute a first step and potentially aid in the early diagnosis of COVID-19.",2021,10.1111/ijlh.13312,,,,
Interpretative computer-aided lung cancer diagnosis: From radiology analysis to malignancy evaluation,"BACKGROUND AND OBJECTIVE: Computer-aided diagnosis (CAD) systems promote accurate diagnosis and reduce the burden of radiologists. A CAD system for lung cancer diagnosis includes nodule candidate detection and nodule malignancy evaluation. Recently, deep learning-based pulmonary nodule detection has reached satisfactory performance ready for clinical application. However, deep learning-based nodule malignancy evaluation depends on heuristic inference from low-dose computed tomography (LDCT) volume to malignant probability, and lacks clinical cognition. METHODS: In this paper, we propose a joint radiology analysis and malignancy evaluation network called R2MNet to evaluate pulmonary nodule malignancy via the analysis of radiological characteristics. Radiological features are extracted as channel descriptor to highlight specific regions of the input volume that are critical for nodule malignancy evaluation. In addition, for model explanations, we propose channel-dependent activation mapping (CDAM) to visualize features and shed light on the decision process of deep neural networks (DNNs). RESULTS: Experimental results on the lung image database consortium image collection (LIDC-IDRI) dataset demonstrate that the proposed method achieved an area under curve (AUC) of 96.27% and 97.52% on nodule radiology analysis and nodule malignancy evaluation, respectively. In addition, explanations of CDAM features proved that the shape and density of nodule regions are two critical factors that influence a nodule to be inferred as malignant. This process conforms to the diagnosis cognition of experienced radiologists. CONCLUSION: The network inference process conforms to the diagnostic procedure of radiologists and increases the confidence of evaluation results by incorporating radiology analysis with nodule malignancy evaluation. Besides, model interpretation with CDAM features shed light on the focus regions of DNNs during the estimation of nodule malignancy probabilities.",2021,10.1016/j.cmpb.2021.106363,cross-sectional,diagnosis,CT,Lung
Intra-tumoural heterogeneity characterization through texture and colour analysis for differentiation of non-small cell lung carcinoma subtypes,"Radiomics has shown potential in disease diagnosis, but its feasibility for non-small cell lung carcinoma (NSCLC) subtype classification is unclear. This study aims to explore the diagnosis value of texture and colour features from positron emission tomography computed tomography (PET-CT) images in differentiation of NSCLC subtypes: adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Two patient cohorts were retrospectively collected into a dataset of 341 (18)F-labeled 2-deoxy-2fluoro-d-glucose ([(18)F] FDG) PET-CT images of NSCLC tumours (125 ADC, 174 SqCC, and 42 cases with unknown subtype). Quantification of texture and colour features was performed using freehand regions of interest. The relation between extracted features and commonly used parameters such as age, gender, tumour size, and standard uptake value (SUVmax) was explored. To classify NSCLC subtypes, support vector machine algorithm was applied on these features and the classification performance was evaluated by receiver operating characteristic curve analysis. There was a significant difference between ADC and SqCC subtypes in texture and colour features (P < 0.05); this showed that imaging features were significantly correlated to both SUVmax and tumour diameter (P < 0.05). When evaluating classification performance, features combining texture and colour showed an AUC of 0.89 (95% CI, 0.78-1.00), colour features showed an AUC of 0.85 (95% CI, 0.71-0.99), and texture features showed an AUC of 0.68 (95% CI, 0.48-0.88). DeLong's test showed that AUC was higher for features combining texture and colour than that for texture features only (P = 0.010), but not significantly different from that for colour features only (P = 0.328). HSV colour features showed a similar performance to RGB colour features (P = 0.473). The colour features are promising in the refinement of NSCLC subtype differentiation, and features combining texture and colour of PET-CT images could result in better classification performance.",2018,10.1088/1361-6560/aad648,cross-sectional,diagnosis,PET,Lung
Intravital Imaging of Tumor Cell Motility in the Tumor Microenvironment Context,"Cancer cell motility and invasion are key features of metastatic tumors. Both are highly linked to tumor microenvironmental parameters, such as collagen architecture or macrophage density. However, due to the genetic, epigenetic and microenvironmental heterogeneities, only a small portion of tumor cells in the primary tumor are motile and furthermore, only a small portion of those will metastasize. This creates a challenge in predicting metastatic fate of single cells based on the phenotype they exhibit in the primary tumor. To overcome this challenge, tumor cell subpopulations need to be monitored at several timescales, mapping their phenotype in primary tumor as well as their potential homing to the secondary tumor site. Additionally, to address the spatial heterogeneity of the tumor microenvironment and how it relates to tumor cell phenotypes, large numbers of images need to be obtained from the same tumor. Finally, as the microenvironment complexity results in nonlinear relationships between tumor cell phenotype and its surroundings, advanced statistical models are required to interpret the imaging data. Toward improving our understanding of the relationship between cancer cell motility, the tumor microenvironment context and successful metastasis, we have developed several intravital approaches for continuous and longitudinal imaging, as well as data classification via support vector machine (SVM) algorithm. We also describe methods that extend the capabilities of intravital imaging by postsacrificial microscopy of the lung as well as correlative immunofluorescence in the primary tumor.",2018,10.1007/978-1-4939-7701-7_14,,,,
Introducing frequency representation into convolution neural networks for medical image segmentation via twin-Kernel Fourier convolution,"BACKGROUND AND OBJECTIVE: For medical image segmentation, deep learning-based methods have achieved state-of-the-art performance. However, the powerful spectral representation in the field of image processing is rarely considered in these models. METHODS: In this work, we propose to introduce frequency representation into convolution neural networks (CNNs) and design a novel model, tKFC-Net, to combine powerful feature representation in both frequency and spatial domains. Through the Fast Fourier Transform (FFT) operation, frequency representation is employed on pooling, upsampling, and convolution without any adjustments to the network architecture. Furthermore, we replace original convolution with twin-Kernel Fourier Convolution (t-KFC), a new designed convolution layer, to specify the convolution kernels for particular functions and extract features from different frequency components. RESULTS: We experimentally show that our method has an edge over other models in the task of medical image segmentation. Evaluated on four datasets-skin lesion segmentation (ISIC 2018), retinal blood vessel segmentation (DRIVE), lung segmentation (COVID-19-CT-Seg), and brain tumor segmentation (BraTS 2019), the proposed model achieves outstanding results: the metric F1-Score is 0.878 for ISIC 2018, 0.8185 for DRIVE, 0.9830 for COVID-19-CT-Seg, and 0.8457 for BraTS 2019. CONCLUSION: The introduction of spectral representation retains spectral features which result in more accurate segmentation. The proposed method is orthogonal to other topology improvement methods and very convenient to be combined.",2021,10.1016/j.cmpb.2021.106110,cross-sectional,informatics,CT,Lung
Introducing the GEV Activation Function for Highly Unbalanced Data to Develop COVID-19 Diagnostic Models,"Fast and accurate diagnosis is essential for the efficient and effective control of the COVID-19 pandemic that is currently disrupting the whole world. Despite the prevalence of the COVID-19 outbreak, relatively few diagnostic images are openly available to develop automatic diagnosis algorithms. Traditional deep learning methods often struggle when data is highly unbalanced with many cases in one class and only a few cases in another; new methods must be developed to overcome this challenge. We propose a novel activation function based on the generalized extreme value (GEV) distribution from extreme value theory, which improves performance over the traditional sigmoid activation function when one class significantly outweighs the other. We demonstrate the proposed activation function on a publicly available dataset and externally validate on a dataset consisting of 1,909 healthy chest X-rays and 84 COVID-19 X-rays. The proposed method achieves an improved area under the receiver operating characteristic (DeLong's p-value < 0.05) compared to the sigmoid activation. Our method is also demonstrated on a dataset of healthy and pneumonia vs. COVID-19 X-rays and a set of computerized tomography images, achieving improved sensitivity. The proposed GEV activation function significantly improves upon the previously used sigmoid activation for binary classification. This new paradigm is expected to play a significant role in the fight against COVID-19 and other diseases, with relatively few training cases available.",2020,10.1109/jbhi.2020.3012383,cross-sectional,diagnosis,Radiograph,Lung
Investigating phenotypes of pulmonary COVID-19 recovery: A longitudinal observational prospective multicenter trial,"BACKGROUND: The optimal procedures to prevent, identify, monitor, and treat long-term pulmonary sequelae of COVID-19 are elusive. Here, we characterized the kinetics of respiratory and symptom recovery following COVID-19. METHODS: We conducted a longitudinal, multicenter observational study in ambulatory and hospitalized COVID-19 patients recruited in early 2020 (n = 145). Pulmonary computed tomography (CT) and lung function (LF) readouts, symptom prevalence, and clinical and laboratory parameters were collected during acute COVID-19 and at 60, 100, and 180 days follow-up visits. Recovery kinetics and risk factors were investigated by logistic regression. Classification of clinical features and participants was accomplished by unsupervised and semi-supervised multiparameter clustering and machine learning. RESULTS: At the 6-month follow-up, 49% of participants reported persistent symptoms. The frequency of structural lung CT abnormalities ranged from 18% in the mild outpatient cases to 76% in the intensive care unit (ICU) convalescents. Prevalence of impaired LF ranged from 14% in the mild outpatient cases to 50% in the ICU survivors. Incomplete radiological lung recovery was associated with increased anti-S1/S2 antibody titer, IL-6, and CRP levels at the early follow-up. We demonstrated that the risk of perturbed pulmonary recovery could be robustly estimated at early follow-up by clustering and machine learning classifiers employing solely non-CT and non-LF parameters. CONCLUSIONS: The severity of acute COVID-19 and protracted systemic inflammation is strongly linked to persistent structural and functional lung abnormality. Automated screening of multiparameter health record data may assist in the prediction of incomplete pulmonary recovery and optimize COVID-19 follow-up management. FUNDING: The State of Tyrol (GZ 71934), Boehringer Ingelheim/Investigator initiated study (IIS 1199-0424). CLINICAL TRIAL NUMBER: ClinicalTrials.gov: NCT04416100.",2022,10.7554/eLife.72500,prospective cohort,prognosis,CT,Lung
Investigating recurrent neural networks for OCT A-scan based tissue analysis,"OBJECTIVES: Optical Coherence Tomography (OCT) has been proposed as a high resolution image modality to guide transbronchial biopsies. In this study we address the question, whether individual A-scans obtained in needle direction can contribute to the identification of pulmonary nodules. METHODS: OCT A-scans from freshly resected human lung tissue specimen were recorded through a customized needle with an embedded optical fiber. Bidirectional Long Short Term Memory networks (BLSTMs) were trained on randomly distributed training and test sets of the acquired A-scans. Patient specific training and different pre-processing steps were evaluated. RESULTS: Classification rates from 67.5% up to 76% were archived for different training scenarios. Sensitivity and specificity were highest for a patient specific training with 0.87 and 0.85. Low pass filtering decreased the accuracy from 73.2% on a reference distribution to 62.2% for higher cutoff frequencies and to 56% for lower cutoff frequencies. CONCLUSION: The results indicate that a grey value based classification is feasible and may provide additional information for diagnosis and navigation. Furthermore, the experiments show patient specific signal properties and indicate that the lower and upper parts of the frequency spectrum contribute to the classification.",2014,10.3414/me13-01-0135,,,,
Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images,"Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods.",2021,10.1121/10.0007272,case control,diagnosis,Ultrasound,Lung
"Investigation of Low-Dose CT Lung Cancer Screening Scan ""Over-Range"" Issue Using Machine Learning Methods","Low-dose computed tomography (CT) lung cancer screening is recommended by the US Preventive Services Task Force for high lung cancer-risk populations. In this study, we investigated an important factor affecting the CT dose-the scan length, for this CT exam. A neural network model based on the ""UNET"" framework was established to segment the lung region in the CT scout images. It was trained initially with 247 chest X-ray images and then with 40 CT scout images. The mean Intersection over Union (IOU) and Dice coefficient were reported to be 0.954 and 0.976, respectively. Lung scan boundaries were determined from this segmentation and compared with the boundaries marked by an expert for 150 validation images, resulting an average 4.7% difference. Seven hundred seventy CT low-dose lung screening exams were retrospectively analyzed with the validated model. The average ""desired"" scan length was 252 mm with a standard deviation of 28 mm. The average ""over-range"" was 58.5 mm or 24%. The upper boundary (superior) on average had an ""over-range"" of 17 mm, and the lower boundary (inferior) on average had an ""over-range"" of 41 mm. Further analysis of this data showed that the extent of ""over-range"" was independent of acquisition date, acquisition time, acquisition station, and patient age, but dependent on technologist and patient weight. We concluded that this machine learning method could effectively support quality control on the scan length for CT low-dose screening scans, enabling the eliminations of unnecessary patient dose.",2019,10.1007/s10278-019-00233-z,cross-sectional,informatics,CT,Lung
Investigation of pulmonary nodule classification using multi-scale residual network enhanced with 3DGAN-synthesized volumes,"It is often difficult to distinguish between benign and malignant pulmonary nodules using only image diagnosis. A biopsy is performed when malignancy is suspected based on CT examination. However, biopsies are highly invasive, and patients with benign nodules may undergo unnecessary procedures. In this study, we performed automated classification of pulmonary nodules using a three-dimensional convolutional neural network (3DCNN). In addition, to increase the number of training data, we utilized generative adversarial networks (GANs), a deep learning technique used as a data augmentation method. In this approach, three-dimensional regions of different sizes centered on pulmonary nodules were extracted from CT images, and a large number of pseudo-pulmonary nodules were synthesized using 3DGAN. The 3DCNN has a multi-scale structure in which multiple nodules in each region are inputted and integrated into the final layer. During the training of multi-scale 3DCNN, pre-training was first performed using 3DGAN-synthesized nodules, and the pulmonary nodules were then comprehensively classified by fine-tuning the pre-trained model using real nodules. Using an evaluation process that involved 60 confirmed cases of pathological diagnosis based on biopsies, the sensitivity was determined to be 90.9% and specificity was 74.1%. The classification accuracy was improved compared to the case of training with only real nodules without pre-training. The 2DCNN results of our previous study were slightly better than the 3DCNN results. However, it was shown that even though 3DCNN is difficult to train with limited data such as in the case of medical images, classification accuracy can be improved by GAN.",2020,10.1007/s12194-020-00564-5,case control,diagnosis,CT,Lung
IoT with cloud based lung cancer diagnosis model using optimal support vector machine,"In the last decade, exponential growth of Internet of Things (IoT) and cloud computing takes the healthcare services to the next level. At the same time, lung cancer is identified as a dangerous disease which increases the global mortality rate annually. Presently, support vector machine (SVM) is the effective image classification tool especially in medical imaging. Feature selection and parameter optimization are the effective ways to improve the results of SVM and are conventionally resolved individually. This paper presents an optimal SVM for lung image classification where the parameters of SVM are optimized and feature selection takes place by modified grey wolf optimization algorithm combined with genetic algorithm (GWO-GA). The experimentation part takes place on three dimensions: test for parameter optimization, feature selection, and optimal SVM. For assessing the performance of the presented approach, a benchmark image database is employed which comprises of 50 low-dosage and stored lung CT images. The presented method exhibits its superior results on all the applied test images under several aspects. In addition, it achieves average classification accuracy of 93.54 which is significantly higher than the compared methods.",2020,10.1007/s10729-019-09489-x,cross-sectional,diagnosis,CT,Lung
Issues associated with deploying CNN transfer learning to detect COVID-19 from chest X-rays,"Covid-19 first occurred in Wuhan, China in December 2019. Subsequently, the virus spread throughout the world and as of June 2020 the total number of confirmed cases are above 4.7 million with over 315,000 deaths. Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process. The aim of this work is to conduct a critical analysis to investigate the applicability of convolutional neural networks (CNNs) for the purpose of COVID-19 detection in chest X-ray images and highlight the issues of using CNN directly on the whole image. To accomplish this task, we use 12-off-the-shelf CNN architectures in transfer learning mode on 3 publicly available chest X-ray databases together with proposing a shallow CNN architecture in which we train it from scratch. Chest X-ray images are fed into CNN models without any preprocessing to replicate researches used chest X-rays in this manner. Then a qualitative investigation performed to inspect the decisions made by CNNs using a technique known as class activation maps (CAM). Using CAMs, one can map the activations contributed to the decision of CNNs back to the original image to visualize the most discriminating region(s) on the input image. We conclude that CNN decisions should not be taken into consideration, despite their high classification accuracy, until clinicians can visually inspect and approve the region(s) of the input image used by CNNs that lead to its prediction.",2020,10.1007/s13246-020-00934-8,cross-sectional,diagnosis,Radiograph,Lung
Iterative Low-Dose CT Reconstruction With Priors Trained by Artificial Neural Network,"Dose reduction in computed tomography (CT) is essential for decreasing radiation risk in clinical applications. Iterative reconstruction algorithms are one of the most promising way to compensate for the increased noise due to reduction of photon flux. Most iterative reconstruction algorithms incorporate manually designed prior functions of the reconstructed image to suppress noises while maintaining structures of the image. These priors basically rely on smoothness constraints and cannot exploit more complex features of the image. The recent development of artificial neural networks and machine learning enabled learning of more complex features of image, which has the potential to improve reconstruction quality. In this letter, K-sparse auto encoder was used for unsupervised feature learning. A manifold was learned from normal-dose images and the distance between the reconstructed image and the manifold was minimized along with data fidelity during reconstruction. Experiments on 2016 Low-dose CT Grand Challenge were used for the method verification, and results demonstrated the noise reduction and detail preservation abilities of the proposed method.",2017,10.1109/tmi.2017.2753138,cross-sectional,informatics,CT,Lung
JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation,"Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.5% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.",2021,10.1109/tip.2021.3058783,cross-sectional,diagnosis,CT,Lung
Joint Learning of 3D Lesion Segmentation and Classification for Explainable COVID-19 Diagnosis,"Given the outbreak of COVID-19 pandemic and the shortage of medical resource, extensive deep learning models have been proposed for automatic COVID-19 diagnosis, based on 3D computed tomography (CT) scans. However, the existing models independently process the 3D lesion segmentation and disease classification, ignoring the inherent correlation between these two tasks. In this paper, we propose a joint deep learning model of 3D lesion segmentation and classification for diagnosing COVID-19, called DeepSC-COVID, as the first attempt in this direction. Specifically, we establish a large-scale CT database containing 1,805 3D CT scans with fine-grained lesion annotations, and reveal 4 findings about lesion difference between COVID-19 and community acquired pneumonia (CAP). Inspired by our findings, DeepSC-COVID is designed with 3 subnets: a cross-task feature subnet for feature extraction, a 3D lesion subnet for lesion segmentation, and a classification subnet for disease diagnosis. Besides, the task-aware loss is proposed for learning the task interaction across the 3D lesion and classification subnets. Different from all existing models for COVID-19 diagnosis, our model is interpretable with fine-grained 3D lesion distribution. Finally, extensive experimental results show that the joint learning framework in our model significantly improves the performance of 3D lesion segmentation and disease classification in both efficiency and efficacy.",2021,10.1109/tmi.2021.3079709,cross-sectional,diagnosis,CT,Lung
Knowledge-Based Analysis for Mortality Prediction From CT Images,"Low-Dose CT (LDCT) can significantly improve the accuracy of lung cancer diagnosis and thus reduce cancer deaths compared to chest X-ray. The lung cancer risk population is also at high risk of other deadly diseases, for instance, cardiovascular diseases. Therefore, predicting the all-cause mortality risks of this population is of great importance. This paper introduces a knowledge-based analytical method using deep convolutional neural network (CNN) for all-cause mortality prediction. The underlying approach combines structural image features extracted from CNNs, based on LDCT volume at different scales, and clinical knowledge obtained from quantitative measurements, to predict the mortality risk of lung cancer screening subjects. The proposed method is referred as Knowledge-based Analysis of Mortality Prediction Network (KAMP-Net). It constitutes a collaborative framework that utilizes both imaging features and anatomical information, instead of completely relying on automatic feature extraction. Our work demonstrates the feasibility of incorporating quantitative clinical measurements to assist CNNs in all-cause mortality prediction from chest LDCT images. The results of this study confirm that radiologist defined features can complement CNNs in performance improvement. The experiments demonstrate that KAMP-Net can achieve a superior performance when compared to other methods.",2020,10.1109/jbhi.2019.2946066,retrospective cohort,prognosis,CT,Lung
Knowledge-based and deep learning-based automated chest wall segmentation in magnetic resonance images of extremely dense breasts,"PURPOSE: Segmentation of the chest wall, is an important component of methods for automated analysis of breast magnetic resonance imaging (MRI). Methods reported to date show promising results but have difficulties delineating the muscle border correctly in breasts with a large proportion of fibroglandular tissue (i.e., dense breasts). Knowledge-based methods (KBMs) as well as methods based on deep learning have been proposed, but a systematic comparison of these approaches within one cohort of images is currently lacking. Therefore, we developed a KBM and a deep learning method for segmentation of the chest wall in MRI of dense breasts and compared their performances. METHODS: Two automated methods were developed, an optimized KBM incorporating heuristics aimed at shape, location, and gradient features, and a deep learning-based method (DLM) using a dilated convolution neural network. A data set of 115 T1-weighted MR images was randomly selected from MR images of women with extremely dense breasts (ACR BI-RADS category 4) participating in a screening trial of women (mean age 56.6 yr, range 49.5-75.2 yr) with dense breasts. Manual segmentations of the chest wall, acquired under supervision of an experienced breast radiologist, were available for all data sets. Both methods were optimized using the same randomly selected 36 MRI data sets from a total of 115 data sets. Each MR data set consisted of 179 transversal images with voxel size 0.64 mm(3) × 0.64 mm(3) × 1.00 mm(3) . In the remaining 79 data sets, the results of both segmentation methods were qualitatively evaluated. A radiologist reviewed the segmentation results of both methods in all transversal images (n = 14 141) and determined whether the result would impact the ability to accurately determine the volume of fibroglandular and fatty tissue and whether segmentations masked breast regions that might harbor lesions. When no relevant deviation was detected, the result was considered successful. In addition, all segmentations were quantitatively assessed using the Dice similarity coefficient (DSC) and Hausdorff distance (HD), 95th percentile of the Hausdorff distance (HD95), false positive fraction (FPF), and false negative fraction (FNF) metrics. RESULTS: According to the radiologist's evaluation, the DLM had a significantly higher success rate than the KBM (81.6% vs 78.4%, P < 0.01). The success rate was further improved to 92.1% by combining both methods. Similarly, the DLM had significantly lower values for FNF (0.003 ± 0.003 vs 0.009 ± 0.011, P < 0.01) and HD95 (2.58 ± 1.78 mm vs 3.37 ± 2.11, P < 0.01). However, the KBM resulted in a significantly lower FPF than the DLM (0.018 ± 0.009 vs 0.030 ± 0.009, P < 0.01).There was no significant difference between the KBM and DLM in terms of DSC (0.982 ± 0.006 vs 0.984 ± 0.008, P = 0.08) or HD (24.14 ± 20.69 mm vs 12.81 ± 27.28 mm, P = 0.05). CONCLUSION: Both optimized knowledge-based and DLM showed good results to segment the pectoral muscle in women with dense breasts. Qualitatively assessed, the DLM was the most robust method. A quantitative comparison, however, did not indicate a preference for one method over the other.",2019,10.1002/mp.13699,,,,
Knowledge-based Collaborative Deep Learning for Benign-Malignant Lung Nodule Classification on Chest CT,"The accurate identification of malignant lung nodules on chest CT is critical for the early detection of lung cancer, which also offers patients the best chance of cure. Deep learning methods have recently been successfully introduced to computer vision problems, although substantial challenges remain in the detection of malignant nodules due to the lack of large training data sets. In this paper, we propose a multi-view knowledge-based collaborative (MV-KBC) deep model to separate malignant from benign nodules using limited chest CT data. Our model learns 3-D lung nodule characteristics by decomposing a 3-D nodule into nine fixed views. For each view, we construct a knowledge-based collaborative (KBC) submodel, where three types of image patches are designed to fine-tune three pre-trained ResNet-50 networks that characterize the nodules' overall appearance, voxel, and shape heterogeneity, respectively. We jointly use the nine KBC submodels to classify lung nodules with an adaptive weighting scheme learned during the error back propagation, which enables the MV-KBC model to be trained in an end-to-end manner. The penalty loss function is used for better reduction of the false negative rate with a minimal effect on the overall performance of the MV-KBC model. We tested our method on the benchmark LIDC-IDRI data set and compared it to the five state-of-the-art classification approaches. Our results show that the MV-KBC model achieved an accuracy of 91.60% for lung nodule classification with an AUC of 95.70%. These results are markedly superior to the state-of-the-art approaches.",2019,10.1109/tmi.2018.2876510,cross-sectional,diagnosis,CT,Lung
Label Co-Occurrence Learning With Graph Convolutional Networks for Multi-Label Chest X-Ray Image Classification,"Existing multi-label medical image learning tasks generally contain rich relationship information among pathologies such as label co-occurrence and interdependency, which is of great importance for assisting in clinical diagnosis and can be represented as the graph-structured data. However, most state-of-the-art works only focus on regression from the input to the binary labels, failing to make full use of such valuable graph-structured information due to the complexity of graph data. In this paper, we propose a novel label co-occurrence learning framework based on Graph Convolution Networks (GCNs) to explicitly explore the dependencies between pathologies for the multi-label chest X-ray (CXR) image classification task, which we term the ""CheXGCN"". Specifically, the proposed CheXGCN consists of two modules, i.e., the image feature embedding (IFE) module and label co-occurrence learning (LCL) module. Thanks to the LCL model, the relationship between pathologies is generalized into a set of classifier scores by introducing the word embedding of pathologies and multi-layer graph information propagation. During end-to-end training, it can be flexibly integrated into the IFE module and then adaptively recalibrate multi-label outputs with these scores. Extensive experiments on the ChestX-Ray14 and CheXpert datasets have demonstrated the effectiveness of CheXGCN as compared with the state-of-the-art baselines.",2020,10.1109/jbhi.2020.2967084,retrospective cohort,diagnosis,Radiograph,Lung
Large scale validation of the M5L lung CAD on heterogeneous CT datasets,"PURPOSE: M5L, a fully automated computer-aided detection (CAD) system for the detection and segmentation of lung nodules in thoracic computed tomography (CT), is presented and validated on several image datasets. METHODS: M5L is the combination of two independent subsystems, based on the Channeler Ant Model as a segmentation tool [lung channeler ant model (lungCAM)] and on the voxel-based neural approach. The lungCAM was upgraded with a scan equalization module and a new procedure to recover the nodules connected to other lung structures; its classification module, which makes use of a feed-forward neural network, is based of a small number of features (13), so as to minimize the risk of lacking generalization, which could be possible given the large difference between the size of the training and testing datasets, which contain 94 and 1019 CTs, respectively. The lungCAM (standalone) and M5L (combined) performance was extensively tested on 1043 CT scans from three independent datasets, including a detailed analysis of the full Lung Image Database Consortium/Image Database Resource Initiative database, which is not yet found in literature. RESULTS: The lungCAM and M5L performance is consistent across the databases, with a sensitivity of about 70% and 80%, respectively, at eight false positive findings per scan, despite the variable annotation criteria and acquisition and reconstruction conditions. A reduced sensitivity is found for subtle nodules and ground glass opacities (GGO) structures. A comparison with other CAD systems is also presented. CONCLUSIONS: The M5L performance on a large and heterogeneous dataset is stable and satisfactory, although the development of a dedicated module for GGOs detection could further improve it, as well as an iterative optimization of the training procedure. The main aim of the present study was accomplished: M5L results do not deteriorate when increasing the dataset size, making it a candidate for supporting radiologists on large scale screenings and clinical programs.",2015,10.1118/1.4907970,,,,
Latent traits of lung tissue patterns in former smokers derived by dual channel deep learning in computed tomography images,"Chronic obstructive pulmonary disease (COPD) is a heterogeneous disease and the traditional variables extracted from computed tomography (CT) images may not be sufficient to describe all the topological features of lung tissues in COPD patients. We employed an unsupervised three-dimensional (3D) convolutional autoencoder (CAE)-feature constructor (FC) deep learning network to learn from CT data and derive tissue pattern-clusters jointly. We then applied exploratory factor analysis (EFA) to discover the unobserved latent traits (factors) among pattern-clusters. CT images at total lung capacity (TLC) and residual volume (RV) of 541 former smokers and 59 healthy non-smokers from the cohort of the SubPopulations and Intermediate Outcome Measures in the COPD Study (SPIROMICS) were analyzed. TLC and RV images were registered to calculate the Jacobian (determinant) values for all the voxels in TLC images. 3D Regions of interest (ROIs) with two data channels of CT intensity and Jacobian value were randomly extracted from training images and were fed to the 3D CAE-FC model. 80 pattern-clusters and 7 factors were identified. Factor scores computed for individual subjects were able to predict spirometry-measured pulmonary functions. Two factors which correlated with various emphysema subtypes, parametric response mapping (PRM) metrics, airway variants, and airway tree to lung volume ratio were discriminants of patients across all severity stages. Our findings suggest the potential of developing factor-based surrogate markers for new COPD phenotypes.",2021,10.1038/s41598-021-84547-5,retrospective cohort,prognosis,CT,Lung
LBP-based information assisted intelligent system for COVID-19 identification,"A real-time COVID-19 detection system is an utmost requirement of the present situation. This article presents a chest X-ray image-based automated COVID-19 detection system which can be employed with the RT-PCR test to improve the diagnosis rate. In the proposed approach, the textural features are extracted from the chest X-ray images and local binary pattern (LBP) based images. Further, the image-based and LBP image-based features are jointly investigated. Thereafter, highly discriminatory features are provided to the classifier for developing an automated model for COVID-19 identification. The performance of the proposed approach is investigated over 2905 chest X-ray images of normal, pneumonia, and COVID-19 infected persons on various class combinations to analyze the robustness. The developed method achieves 97.97% accuracy (acc) and 99.88% sensitivity (sen) for classifying COVID-19 X-ray images against pneumonia infected and normal person's X-ray images. It attains 98.91% acc and 99.33% sen for COVID-19 X-ray against the normal X-ray classification. This method can be employed to assist the radiologists during mass screening for fast, accurate, and contact-free COVID-19 diagnosis.",2021,10.1016/j.compbiomed.2021.104453,cross-sectional,diagnosis,Radiograph,Lung
Learning from imbalanced COVID-19 chest X-ray (CXR) medical imaging data,"The trendy task of digital medical image analysis has been continually evolving. It has been an area of prominent and growing importance from both research and deployment perspectives. Nonetheless, it is necessary to realize that the use of algorithms, methodology, as well as the source of medical image data, must be strictly scrutinized. As the COVID-19 pandemic has been gripping much of the world recently, there has been much efforts gone into developing affordable testing for the masses, and it has been shown that the established and widely available chest X-rays (CXR) images may be used as a screening criteria for assistive diagnosis purpose. Thanks to the dedicated work by various individuals and organizations, publicly available CXR of COVID-19 subjects are available for analytic usage. We have also provided a publicly available CXR dataset on the Kaggle platform. As a case study, this paper presents a systematic approach to learn from a typically imbalanced set of CXR images, which consists of a limited number of publicly available COVID-19 images. Our results show that we are able to outperform the top finishers in a related Kaggle multi-class CXR challenge. The proposed methodology should be able to help guide medical personnel in obtaining a robust diagnosis model to discern COVID-19 from other conditions confidently.",2022,10.1016/j.ymeth.2021.06.002,cross-sectional,diagnosis,Radiograph,Lung
Learning to Quantify Emphysema Extent: What Labels Do We Need?,"Accurate assessment of pulmonary emphysema is crucial to assess disease severity and subtype, to monitor disease progression, and to predict lung cancer risk. However, visual assessment is time-consuming and subject to substantial inter-rater variability while standard densitometry approaches to quantify emphysema remain inferior to visual scoring. We explore if machine learning methods that learn from a large dataset of visually assessed CT scans can provide accurate estimates of emphysema extent and if methods that learn from emphysema extent scoring can outperform algorithms that learn only from emphysema presence scoring. Four Multiple Instance Learning classifiers, trained on emphysema presence labels, and five Learning with Label Proportions classifiers, trained on emphysema extent labels, are compared. Performance is evaluated on 600 low-dose CT scans from the Danish Lung Cancer Screening Trial and we find that learning from emphysema presence labels, which are much easier to obtain, gives equally good performance to learning from emphysema extent labels. The best performing Multiple Instance Learning and Learning with Label Proportions classifiers, achieve intra-class correlation coefficients around 0.90 and average overall agreement with raters of 78% and 79% compared to an inter-rater agreement of 83%.",2020,10.1109/jbhi.2019.2932145,cross-sectional,diagnosis,CT,Lung
Learning-to-augment strategy using noisy and denoised data: Improving generalizability of deep CNN for the detection of COVID-19 in X-ray images,"Chest X-ray images are used in deep convolutional neural networks for the detection of COVID-19, the greatest human challenge of the 21st century. Robustness to noise and improvement of generalization are the major challenges in designing these networks. In this paper, we introduce a strategy for data augmentation using the determination of the type and value of noise density to improve the robustness and generalization of deep CNNs for COVID-19 detection. Firstly, we present a learning-to-augment approach that generates new noisy variants of the original image data with optimized noise density. We apply a Bayesian optimization technique to control and choose the optimal noise type and its parameters. Secondly, we propose a novel data augmentation strategy, based on denoised X-ray images, that uses the distance between denoised and original pixels to generate new data. We develop an autoencoder model to create new data using denoised images corrupted by the Gaussian and impulse noise. A database of chest X-ray images, containing COVID-19 positive, healthy, and non-COVID pneumonia cases, is used to fine-tune the pre-trained networks (AlexNet, ShuffleNet, ResNet18, and GoogleNet). The proposed method performs better results compared to the state-of-the-art learning to augment strategies in terms of sensitivity (0.808), specificity (0.915), and F-Measure (0.737). The source code of the proposed method is available at https://github.com/mohamadmomeny/Learning-to-augment-strategy.",2021,10.1016/j.compbiomed.2021.104704,cross-sectional,diagnosis,Radiograph,Lung
Lightweight deep learning models for detecting COVID-19 from chest X-ray images,"Deep learning methods have already enjoyed an unprecedented success in medical imaging problems. Similar success has been evidenced when it comes to the detection of COVID-19 from medical images, therefore deep learning approaches are considered good candidates for detecting this disease, in collaboration with radiologists and/or physicians. In this paper, we propose a new approach to detect COVID-19 via exploiting a conditional generative adversarial network to generate synthetic images for augmenting the limited amount of data available. Additionally, we propose two deep learning models following a lightweight architecture, commensurating with the overall amount of data available. Our experiments focused on both binary classification for COVID-19 vs Normal cases and multi-classification that includes a third class for bacterial pneumonia. Our models achieved a competitive performance compared to other studies in literature and also a ResNet8 model. Our best performing binary model achieved 98.7% accuracy, 100% sensitivity and 98.3% specificity, while our three-class model achieved 98.3% accuracy, 99.3% sensitivity and 98.1% specificity. Moreover, via adopting a testing protocol proposed in literature, our models proved to be more robust and reliable in COVID-19 detection than a baseline ResNet8, making them good candidates for detecting COVID-19 from posteroanterior chest X-ray images.",2021,10.1016/j.compbiomed.2020.104181,cross-sectional,diagnosis,Radiograph,Lung
LNDb challenge on automatic lung cancer patient management,"Lung cancer is the deadliest type of cancer worldwide and late detection is the major factor for the low survival rate of patients. Low dose computed tomography has been suggested as a potential screening tool but manual screening is costly and time-consuming. This has fuelled the development of automatic methods for the detection, segmentation and characterisation of pulmonary nodules. In spite of promising results, the application of automatic methods to clinical routine is not straightforward and only a limited number of studies have addressed the problem in a holistic way. With the goal of advancing the state of the art, the Lung Nodule Database (LNDb) Challenge on automatic lung cancer patient management was organized. The LNDb Challenge addressed lung nodule detection, segmentation and characterization as well as prediction of patient follow-up according to the 2017 Fleischner society pulmonary nodule guidelines. 294 CT scans were thus collected retrospectively at the Centro Hospitalar e Universitrio de So Joo in Porto, Portugal and each CT was annotated by at least one radiologist. Annotations comprised nodule centroids, segmentations and subjective characterization. 58 CTs and the corresponding annotations were withheld as a separate test set. A total of 947 users registered for the challenge and 11 successful submissions for at least one of the sub-challenges were received. For patient follow-up prediction, a maximum quadratic weighted Cohen's kappa of 0.580 was obtained. In terms of nodule detection, a sensitivity below 0.4 (and 0.7) at 1 false positive per scan was obtained for nodules identified by at least one (and two) radiologist(s). For nodule segmentation, a maximum Jaccard score of 0.567 was obtained, surpassing the interobserver variability. In terms of nodule texture characterization, a maximum quadratic weighted Cohen's kappa of 0.733 was obtained, with part solid nodules being particularly challenging to classify correctly. Detailed analysis of the proposed methods and the differences in performance allow to identify the major challenges remaining and future directions - data collection, augmentation/generation and evaluation of under-represented classes, the incorporation of scan-level information for better decision-making and the development of tools and challenges with clinical-oriented goals. The LNDb Challenge and associated data remain publicly available so that future methods can be tested and benchmarked, promoting the development of new algorithms in lung cancer medical image analysis and patient follow-up recommendation.",2021,10.1016/j.media.2021.102027,,,,
Lobar Emphysema Distribution Is Associated With 5-Year Radiological Disease Progression,"BACKGROUND: Emphysema has considerable variability in its regional distribution. Craniocaudal emphysema distribution is an important predictor of the response to lung volume reduction. However, there is little consensus regarding how to define upper lobe-predominant and lower lobe-predominant emphysema subtypes. Consequently, the clinical and genetic associations with these subtypes are poorly characterized. METHODS: We sought to identify subgroups characterized by upper-lobe or lower-lobe emphysema predominance and comparable amounts of total emphysema by analyzing data from 9,210 smokers without alpha-1-antitrypsin deficiency in the Genetic Epidemiology of COPD (COPDGene) cohort. CT densitometric emphysema was measured in each lung lobe. Random forest clustering was applied to lobar emphysema variables after regressing out the effects of total emphysema. Clusters were tested for association with clinical and imaging outcomes at baseline and at 5-year follow-up. Their associations with genetic variants were also compared. RESULTS: Three clusters were identified: minimal emphysema (n = 1,312), upper lobe-predominant emphysema (n = 905), and lower lobe-predominant emphysema (n = 796). Despite a similar amount of total emphysema, the lower-lobe group had more severe airflow obstruction at baseline and higher rates of metabolic syndrome compared with subjects with upper-lobe predominance. The group with upper-lobe predominance had greater 5-year progression of emphysema, gas trapping, and dyspnea. Differential associations with known COPD genetic risk variants were noted. CONCLUSIONS: Subgroups of smokers defined by upper-lobe or lower-lobe emphysema predominance exhibit different functional and radiological disease progression rates, and the upper-lobe predominant subtype shows evidence of association with known COPD genetic risk variants. These subgroups may be useful in the development of personalized treatments for COPD.",2018,10.1016/j.chest.2017.09.022,retrospective cohort,prognosis,CT,Lung
Localized thin-section CT with radiomics feature extraction and machine learning to classify early-detected pulmonary nodules from lung cancer screening,"Lung cancer screening aims to detect small pulmonary nodules and decrease the mortality rate of those affected. However, studies from large-scale clinical trials of lung cancer screening have shown that the false-positive rate is high and positive predictive value is low. To address these problems, a technical approach is greatly needed for accurate malignancy differentiation among these early-detected nodules. We studied the clinical feasibility of an additional protocol of localized thin-section CT for further assessment on recalled patients from lung cancer screening tests. Our approach of localized thin-section CT was integrated with radiomics features extraction and machine learning classification which was supervised by pathological diagnosis. Localized thin-section CT images of 122 nodules were retrospectively reviewed and 374 radiomics features were extracted. In this study, 48 nodules were benign and 74 malignant. There were nine patients with multiple nodules and four with synchronous multiple malignant nodules. Different machine learning classifiers with a stratified ten-fold cross-validation were used and repeated 100 times to evaluate classification accuracy. Of the image features extracted from the thin-section CT images, 238 (64%) were useful in differentiating between benign and malignant nodules. These useful features include CT density (p = 0.002 518), sigma (p = 0.002 781), uniformity (p = 0.032 41), and entropy (p = 0.006 685). The highest classification accuracy was 79% by the logistic classifier. The performance metrics of this logistic classification model was 0.80 for the positive predictive value, 0.36 for the false-positive rate, and 0.80 for the area under the receiver operating characteristic curve. Our approach of direct risk classification supervised by the pathological diagnosis with localized thin-section CT and radiomics feature extraction may support clinical physicians in determining truly malignant nodules and therefore reduce problems in lung cancer screening.",2018,10.1088/1361-6560/aaafab,case control,diagnosis,CT,Lung
Localized-atlas-based segmentation of breast MRI in a decision-making framework,"Breast-region segmentation is an important step for density estimation and Computer-Aided Diagnosis (CAD) systems in Magnetic Resonance Imaging (MRI). Detection of breast-chest wall boundary is often a difficult task due to similarity between gray-level values of fibroglandular tissue and pectoral muscle. This paper proposes a robust breast-region segmentation method which is applicable for both complex cases with fibroglandular tissue connected to the pectoral muscle, and simple cases with high contrast boundaries. We present a decision-making framework based on geometric features and support vector machine (SVM) to classify breasts in two main groups, complex and simple. For complex cases, breast segmentation is done using a combination of intensity-based and atlas-based techniques; however, only intensity-based operation is employed for simple cases. A novel atlas-based method, that is called localized-atlas, accomplishes the processes of atlas construction and registration based on the region of interest (ROI). Atlas-based segmentation is performed by relying on the chest wall template. Our approach is validated using a dataset of 210 cases. Based on similarity between automatic and manual segmentation results, the proposed method achieves Dice similarity coefficient, Jaccard coefficient, total overlap, false negative, and false positive values of 96.3, 92.9, 97.4, 2.61 and 4.77%, respectively. The localization error of the breast-chest wall boundary is 1.97 mm, in terms of averaged deviation distance. The achieved results prove that the suggested framework performs the breast segmentation with negligible errors and efficient computational time for different breasts from the viewpoints of size, shape, and density pattern.",2017,10.1007/s13246-016-0513-3,,,,
Long-term follow-up of persistent pulmonary pure ground-glass nodules with deep learning-assisted nodule segmentation,"OBJECTIVE: To investigate the natural history of persistent pulmonary pure ground-glass nodules (pGGNs) with deep learning-assisted nodule segmentation. METHODS: Between January 2007 and October 2018, 110 pGGNs from 110 patients with 573 follow-up CT scans were included in this retrospective study. pGGN automatic segmentation was performed on initial and all follow-up CT scans using the Dr. Wise system based on convolution neural networks. Subsequently, pGGN diameter, density, volume, mass, volume doubling time (VDT), and mass doubling time (MDT) were calculated automatically. Enrolled pGGNs were categorized into growth, 52 (47.3%), and non-growth, 58 (52.7%), groups according to volume growth. Kaplan-Meier analyses with the log-rank test and Cox proportional hazards regression analysis were conducted to analyze the cumulative percentages of pGGN growth and identify risk factors for growth. RESULTS: The mean follow-up period of the enrolled pGGNs was 48.7 ± 23.8 months. The median VDT of the 52 pGGNs having grown was 1448 (range, 339-8640) days, and their median MDT was 1332 (range, 290-38,912) days. The 12-month, 24.7-month, and 60.8-month cumulative percentages of pGGN growth were 10%, 25.5%, and 51.1%, respectively, and they significantly differed among the initial diameter, volume, and mass subgroups (all p < 0.001). The growth pattern of pGGNs may conform to the exponential model. Lobulated sign (p = 0.044), initial mean diameter (p < 0.001), volume (p = 0.003), and mass (p = 0.023) predicted pGGN growth. CONCLUSIONS: Persistent pGGNs showed an indolent course. Deep learning can assist in accurately elucidating the natural history of pGGNs. pGGNs with lobulated sign and larger initial diameter, volume, and mass are more likely to grow. KEY POINTS: • The pure ground-glass nodule (pGGN) segmentation accuracy of the Dr. Wise system based on convolution neural networks (CNNs) was 96.5% (573/594). • The median volume doubling time (VDT) of 52 pure ground-glass nodules (pGGNs) having grown was 1448 days (range, 339-8640 days), and their median mass doubling time (MDT) was 1332 days (range, 290-38,912 days). The mean time to growth in volume was 854 ± 675 days (range, 116-2856 days). • The 12-month, 24.7-month, and 60.8-month cumulative percentages of pGGN growth were 10%, 25.5%, and 51.1%, respectively, and they significantly differed among the initial diameter, volume, and mass subgroups (all p values < 0.001). The growth pattern of pure ground-glass nodules may conform to exponential model.",2020,10.1007/s00330-019-06344-z,retrospective cohort,diagnosis,CT,Lung
Longitudinal changes in skeletal muscle mass in patients with advanced squamous cell lung cancer,"BACKGROUND: Skeletal muscle depletion (sarcopenia) is associated with poor prognosis in patients with lung cancer. We analyzed changes in skeletal muscle area using serial computed tomography (CT) until the death of patients with advanced squamous cell lung cancer (SQCLC). METHODS: This retrospective study comprised 70 consecutive patients who underwent palliative chemotherapy for SQCLC. The cross-sectional area of the skeletal muscle at the level of the first lumbar vertebra (L1) was measured using chest CT. An artificial intelligence algorithm was developed and used for the serial assessment of the muscle area. Sarcopenia was defined as an L1 skeletal muscle index <46 cm(2) /m(2) in men and < 29 cm(2) /m(2) in women. RESULTS: The median age was 69 years; 62 patients (89%) had metastatic disease at the time of initial diagnosis. Sarcopenia was present in 58 patients (82.9%) at baseline; all patients experienced net muscle loss over the disease trajectory. The median overall survival was 8.7 (95% confidence interval 5.9-11.5) months. The mean percentage loss of skeletal muscle between the first and last CT was 16.5 ± 11.0%. Skeletal muscle loss accelerated over time and was the highest in the last 3 months of life (p < 0.001). Patients losing skeletal muscle rapidly (upper tertile, >3.24 cm(2) /month) had shorter overall survival than patients losing skeletal muscle slowly (median, 5.7 vs. 12.0 months, p < 0.001). CONCLUSIONS: Patients with advanced SQCLC lose a significant amount of skeletal muscle until death. The rate of muscle area reduction is faster at the end of life.",2021,10.1111/1759-7714.13958,,,,
Longitudinal trajectories of severe wheeze exacerbations from infancy to school age and their association with early-life risk factors and late asthma outcomes,"INTRODUCTION: Exacerbation-prone asthma subtype has been reported in studies using data-driven methodologies. However, patterns of severe exacerbations have not been studied. OBJECTIVE: To investigate longitudinal trajectories of severe wheeze exacerbations from infancy to school age. METHODS: We applied longitudinal k-means clustering to derive exacerbation trajectories among 887 participants from a population-based birth cohort with severe wheeze exacerbations confirmed in healthcare records. We examined early-life risk factors of the derived trajectories, and their asthma-related outcomes and lung function in adolescence. RESULTS: 498/887 children (56%) had physician-confirmed wheeze by age 8 years, of whom 160 had at least one severe exacerbation. A two-cluster model provided the optimal solution for severe exacerbation trajectories among these 160 children: ""Infrequent exacerbations (IE)"" (n = 150, 93.7%) and ""Early-onset frequent exacerbations (FE)"" (n = 10, 6.3%). Shorter duration of breastfeeding was the strongest early-life risk factor for FE (weeks, median [IQR]: FE, 0 [0-1.75] vs. IE, 6 [0-20], P < .001). Specific airway resistance (sR(aw) ) was significantly higher in FE compared with IE trajectory throughout childhood. We then compared children in the two exacerbation trajectories with those who have never wheezed (NW, n = 389) or have wheezed but had no severe exacerbations (WNE, n = 338). At age 8 years, FEV(1) /FVC was significantly lower and FeNO significantly higher among FE children compared with all other groups. By adolescence (age 16), subjects in FE trajectory were significantly more likely to have current asthma (67% FE vs. 30% IE vs. 13% WNE, P < .001) and use inhaled corticosteroids (77% FE vs. 15% IE vs. 18% WNE, P < .001). Lung function was significantly diminished in the FE trajectory (FEV(1) /FVC, mean [95%CI]: 89.9% [89.3-90.5] vs. 88.1% [87.3-88.8] vs. 85.1% [83.4-86.7] vs. 74.7% [61.5-87.8], NW, WNE, IE, FE respectively, P < .001). CONCLUSION: We have identified two distinct trajectories of severe exacerbations during childhood with different early-life risk factors and asthma-related outcomes in adolescence.",2020,10.1111/cea.13553,,,,
Low-dose CT image and projection dataset,"PURPOSE: To describe a large, publicly available dataset comprising computed tomography (CT) projection data from patient exams, both at routine clinical doses and simulated lower doses. ACQUISITION AND VALIDATION METHODS: The library was developed under local ethics committee approval. Projection and image data from 299 clinically performed patient CT exams were archived for three types of clinical exams: noncontrast head CT scans acquired for acute cognitive or motor deficit, low-dose noncontrast chest scans acquired to screen high-risk patients for pulmonary nodules, and contrast-enhanced CT scans of the abdomen acquired to look for metastatic liver lesions. Scans were performed on CT systems from two different CT manufacturers using routine clinical protocols. Projection data were validated by reconstructing the data using several different reconstruction algorithms and through use of the data in the 2016 Low Dose CT Grand Challenge. Reduced dose projection data were simulated for each scan using a validated noise-insertion method. Radiologists marked location and diagnosis for detected pathologies. Reference truth was obtained from the patient medical record, either from histology or subsequent imaging. DATA FORMAT AND USAGE NOTES: Projection datasets were converted into the previously developed DICOM-CT-PD format, which is an extended DICOM format created to store CT projections and acquisition geometry in a nonproprietary format. Image data are stored in the standard DICOM image format and clinical data in a spreadsheet. Materials are provided to help investigators use the DICOM-CT-PD files, including a dictionary file, data reader, and user manual. The library is publicly available from The Cancer Imaging Archive (https://doi.org/10.7937/9npb-2637). POTENTIAL APPLICATIONS: This CT data library will facilitate the development and validation of new CT reconstruction and/or denoising algorithms, including those associated with machine learning or artificial intelligence. The provided clinical information allows evaluation of task-based diagnostic performance.",2021,10.1002/mp.14594,,,,
Low-Dose Lung CT Image Restoration Using Adaptive Prior Features From Full-Dose Training Database,"The valuable structure features in full-dose computed tomography (FdCT) scans can be exploited as prior knowledge for low-dose CT (LdCT) imaging. However, lacking the capability to represent local characteristics of interested structures of the LdCT image adaptively may result in poor preservation of details/textures in LdCT image. This paper aims to explore a novel prior knowledge retrieval and representation paradigm, called adaptive prior features assisted restoration algorithm, for the purpose of better restoration of the low-dose lung CT images by capturing local features from FdCT scans adaptively. The innovation lies in the construction of an offline training database and the online patch-search scheme integrated with the principal components analysis (PCA). Specifically, the offline training database is composed of 3-D patch samples extracted from existing full-dose lung scans. For online patch-search, 3-D patches with structure similar to the noisy target patch are first selected from the database as the training samples. Then, PCA is applied on the training samples to retrieve their local prior principal features adaptively. By employing the principal features to decompose the noisy target patch and using an adaptive coefficient shrinkage technique for inverse transformation, the noise of the target patch can be efficiently removed and the detailed texture can be well preserved. The effectiveness of the proposed algorithm was validated by CT scans of patients with lung cancer. The results show that it can achieve a noticeable gain over some state-of-the-art methods in terms of noise suppression and details/textures preservation.",2017,10.1109/tmi.2017.2757035,cross-sectional,informatics,CT,Lung
Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches,"Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this papet, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3D convolutional neural network and transfer learning. Motivated by the radiologists' interpretations of the scans, we then show how to incorporate task-dependent feature representations into a CAD system via a graph-regularized sparse multi-task learning framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion approaches in computer vision, we propose to use proportion-support vector machine for characterizing tumors. We also seek the answer to the fundamental question about the goodness of ""deep features"" for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems.",2019,10.1109/tmi.2019.2894349,cross-sectional,diagnosis,CT,Lung
Lung Cancer and Granuloma Identification Using a Deep Learning Model to Extract 3-Dimensional Radiomics Features in CT Imaging,"BACKGROUND: We aimed to evaluate a deep learning (DL) model combining perinodular and intranodular radiomics features and clinical features for preoperative differentiation of solitary granuloma nodules (GNs) from solid lung cancer nodules in patients with spiculation, lobulation, or pleural indentation on CT. PATIENTS AND METHODS: We retrospectively recruited 915 patients with solitary solid pulmonary nodules and suspicious signs of malignancy. Data including clinical characteristics and subjective CT findings were obtained. A 3-dimensional U-Net-based DL model was used for tumor segmentation and extraction of 3-dimensional radiomics features. We used the Maximum Relevance and Minimum Redundancy (mRMR) algorithm and the eXtreme Gradient Boosting (XGBoost) algorithm to select the intranodular, perinodular, and gross nodular radiomics features. We propose a medical image DL (IDL) model, a clinical image DL (CIDL) model, a radiomics DL (RDL) model, and a clinical image radiomics DL (CIRDL) model to preoperatively differentiate GNs from solid lung cancer. Five-fold cross-validation was used to select and evaluate the models. The prediction performance of the models was evaluated using receiver operating characteristic and calibration curves. RESULTS: The CIRDL model achieved the best performance in differentiating between GNs and solid lung cancer (area under the curve [AUC] = 0.9069), which was significantly higher compared with the IDL (AUC = 0.8322), CIDL (AUC = 0.8652), intra-RDL (AUC = 0.8583), peri-RDL (AUC = 0.8259), and gross-RDL (AUC = 0.8705) models. CONCLUSION: The proposed CIRDL model is a noninvasive diagnostic tool to differentiate between granuloma nodules and solid lung cancer nodules and reduce the need for invasive diagnostic and surgical procedures.",2021,10.1016/j.cllc.2021.02.004,cross-sectional,diagnosis,CT,Lung
Lung Cancer Classification Employing Proposed Real Coded Genetic Algorithm Based Radial Basis Function Neural Network Classifier,"A proposed real coded genetic algorithm based radial basis function neural network classifier is employed to perform effective classification of healthy and cancer affected lung images. Real Coded Genetic Algorithm (RCGA) is proposed to overcome the Hamming Cliff problem encountered with the Binary Coded Genetic Algorithm (BCGA). Radial Basis Function Neural Network (RBFNN) classifier is chosen as a classifier model because of its Gaussian Kernel function and its effective learning process to avoid local and global minima problem and enable faster convergence. This paper specifically focused on tuning the weights and bias of RBFNN classifier employing the proposed RCGA. The operators used in RCGA enable the algorithm flow to compute weights and bias value so that minimum Mean Square Error (MSE) is obtained. With both the lung healthy and cancer images from Lung Image Database Consortium (LIDC) database and Real time database, it is noted that the proposed RCGA based RBFNN classifier has performed effective classification of the healthy lung tissues and that of the cancer affected lung nodules. The classification accuracy computed using the proposed approach is noted to be higher in comparison with that of the classifiers proposed earlier in the literatures.",2016,10.1155/2016/7493535,,,,
Lung cancer classification using neural networks for CT images,"Early detection of cancer is the most promising way to enhance a patient's chance for survival. This paper presents a computer aided classification method in computed tomography (CT) images of lungs developed using artificial neural network. The entire lung is segmented from the CT images and the parameters are calculated from the segmented image. The statistical parameters like mean, standard deviation, skewness, kurtosis, fifth central moment and sixth central moment are used for classification. The classification process is done by feed forward and feed forward back propagation neural networks. Compared to feed forward networks the feed forward back propagation network gives better classification. The parameter skewness gives the maximum classification accuracy. Among the already available thirteen training functions of back propagation neural network, the Traingdx function gives the maximum classification accuracy of 91.1%. Two new training functions are proposed in this paper. The results show that the proposed training function 1 gives an accuracy of 93.3%, specificity of 100% and sensitivity of 91.4% and a mean square error of 0.998. The proposed training function 2 gives a classification accuracy of 93.3% and minimum mean square error of 0.0942.",2014,10.1016/j.cmpb.2013.10.011,cross-sectional,diagnosis,CT,Lung
Lung Cancer Detection Based on Kernel PCA-Convolution Neural Network Feature Extraction and Classification by Fast Deep Belief Neural Network in Disease Management Using Multimedia Data Sources,"In lung cancer, tumor histology is a significant predictor of treatment response and prognosis. Although tissue samples for pathologist view are the most pertinent approach for histology classification, current advances in DL for medical image analysis point to the importance of radiologic data in further characterization of disease characteristics as well as risk stratification. Cancer is a complex global health problem that has seen an increase in death rates in recent years. Progress in cancer disease detection based on subset traits has enabled awareness of significant as well as exact disease diagnosis, thanks to the rapid flowering of high-throughput technology as well as numerous ML techniques that have emerged in recent years. As a result, advanced ML approaches that can successfully distinguish lung cancer patients from healthy people are of major importance. This paper proposed lung tumor detection based on histopathological image analysis using deep learning architectures. Here, the input image is taken as a histopathological image, and it has also been processed for removing noise, image resizing, and enhancing the image. Then the image features are extracted using Kernel PCA integrated with a convolutional neural network (KPCA-CNN), in which KPCA has been used in the feature extraction layer of CNN. The classification of extracted features has been put into effect using a Fast Deep Belief Neural Network (FDBNN). Finally, the classified output will give the tumorous cell and nontumorous cell of the lung from the input histopathological image. The experimental analysis has been carried out for various histopathological image datasets, and the obtained parameters are accuracy, precision, recall, and F-measure. Confusion matrix gives the actual class and predicted class of tumor in an input image. From the comparative analysis, the proposed technique obtains enhanced output in detecting the tumor once compared with an existing methodology for the various datasets.",2022,10.1155/2022/3149406,,,,
Lung Cancer Detection Using Fuzzy Auto-Seed Cluster Means Morphological Segmentation and SVM Classifier,"An effective fuzzy auto-seed cluster means morphological algorithm developed in this work to segment the lung nodules from the consecutive slices of Computer Tomography (CT) images to detect the lung cancer. The initial cluster values were chosen automatically by averaging the minimum and maximum pixel values in each row of an image. The area and eccentricity features were used to eliminate the line like structure and very tiny clusters less than 3 mm in size. The change in centroid analysis was carried out to eliminate the blood vessels. The tissue clusters whose centroid varies much in consecutive slices must be blood vessels. After eliminating the blood vessels, the co-occurrence matrix based texture features contrast, homogeneity and auto correlation were computed on the remaining nodules from the consecutive CT slices to discriminate the calcifications. The extracted centroid shift and texture features were used as the inputs to the Support Vector Machine (SVM) kernel classifier in order to classify the real malignant nodules. This work was carried out on 56 malignant (cancerous) cases and 50 normal cases (with lung infections), which had a total of 56 malignant nodules and 745 benign nodules. Out of these, 60 % of subjects (34 cancerous & 30 non-cancerous) were used for training. The remaining 40 % subjects (22 cancerous & 20 non-cancerous) were used for testing. This work produced a good sensitivity, specificity and accuracy of 100 %, 93 % and 94 %, respectively. The False Positive (FP) per patient was calculated as 0.38.",2016,10.1007/s10916-016-0539-9,retrospective cohort,diagnosis,CT,Lung
Lung Cancer Detection using Probabilistic Neural Network with modified Crow-Search Algorithm,"Objective: Lung cancer is a type of malignancy that occurs most commonly among men and the third most common type of malignancy among women. The timely recognition of lung cancer is necessary for decreasing the effect of death rate worldwide. Since the symptoms of lung cancer are identified only at an advanced stage, it is essential to predict the disease at its earlier stage using any medical imaging techniques. This work aims to propose a classification methodology for lung cancer automatically at the initial stage. Methods: The work adopts computed tomography (CT) imaging modality of lungs for the examination and probabilistic neural network (PNN) for the classification task. After pre-processing of the input lung images, feature extraction for the work is carried out based on the Gray-Level Co-Occurrence Matrix (GLCM) and chaotic crow search algorithm (CCSA) based feature selection is proposed. Results: Specificity, Sensitivity, Positive and Negative Predictive Values, Accuracy are the computation metrics used. The results indicate that the CCSA based feature selection effectively provides an accuracy of 90%. Conclusion: The strategy for the selection of appropriate extracted features is employed to improve the efficiency of classification and the work shows that the PNN with CCSA based feature selection gives an improved classification than without using CCSA for feature selection.",2019,10.31557/apjcp.2019.20.7.2159,cross-sectional,diagnosis,CT,Lung
Lung Cancer Diagnosis Based on an ANN Optimized by Improved TEO Algorithm,"A quarter of all cancer deaths are due to lung cancer. Studies show that early diagnosis and treatment of this disease are the most effective way to increase patient life expectancy. In this paper, automatic and optimized computer-aided detection is proposed for lung cancer. The method first applies a preprocessing step for normalizing and denoising the input images. Afterward, Kapur entropy maximization is performed along with mathematical morphology to lung area segmentation. Afterward, 19 GLCM features are extracted from the segmented images for the final evaluations. The higher priority images are then selected for decreasing the system complexity. The feature selection is based on a new optimization design, called Improved Thermal Exchange Optimization (ITEO), which is designed to improve the accuracy and convergence abilities. The images are finally classified into healthy or cancerous cases based on an optimized artificial neural network by ITEO. Simulation is compared with some well-known approaches and the results showed the superiority of the suggested method. The results showed that the proposed method with 92.27% accuracy provides the highest value among the compared methods.",2021,10.1155/2021/6078524,cross-sectional,diagnosis,CT,Lung
Lung cancer histology classification from CT images based on radiomics and deep learning models,"Adenocarcinoma (AC) and squamous cell carcinoma (SCC) are frequent reported cases of non-small cell lung cancer (NSCLC), responsible for a large fraction of cancer deaths worldwide. In this study, we aim to investigate the potential of NSCLC histology classification into AC and SCC by applying different feature extraction and classification techniques on pre-treatment CT images. The employed image dataset (102 patients) was taken from the publicly available cancer imaging archive collection (TCIA). We investigated four different families of techniques: (a) radiomics with two classifiers (kNN and SVM), (b) four state-of-the-art convolutional neural networks (CNNs) with transfer learning and fine tuning (Alexnet, ResNet101, Inceptionv3 and InceptionResnetv2), (c) a CNN combined with a long short-term memory (LSTM) network to fuse information about the spatial coherency of tumor's CT slices, and (d) combinatorial models (LSTM + CNN + radiomics). In addition, the CT images were independently evaluated by two expert radiologists. Our results showed that the best CNN was Inception (accuracy = 0.67, auc = 0.74). LSTM + Inception yielded superior performance than all other methods (accuracy = 0.74, auc = 0.78). Moreover, LSTM + Inception outperformed experts by 7-25% (p < 0.05). The proposed methodology does not require detailed segmentation of the tumor region and it may be used in conjunction with radiological findings to improve clinical decision-making. Lung cancer histology classification from CT images based on CNN + LSTM.",2021,10.1007/s11517-020-02302-w,cross-sectional,diagnosis,CT,Lung
Lung cancer prediction by Deep Learning to identify benign lung nodules,"INTRODUCTION: Deep Learning has been proposed as promising tool to classify malignant nodules. Our aim was to retrospectively validate our Lung Cancer Prediction Convolutional Neural Network (LCP-CNN), which was trained on US screening data, on an independent dataset of indeterminate nodules in an European multicentre trial, to rule out benign nodules maintaining a high lung cancer sensitivity. METHODS: The LCP-CNN has been trained to generate a malignancy score for each nodule using CT data from the U.S. National Lung Screening Trial (NLST), and validated on CT scans containing 2106 nodules (205 lung cancers) detected in patients from from the Early Lung Cancer Diagnosis Using Artificial Intelligence and Big Data (LUCINDA) study, recruited from three tertiary referral centers in the UK, Germany and Netherlands. We pre-defined a benign nodule rule-out test, to identify benign nodules whilst maintaining a high sensitivity, by calculating thresholds on the malignancy score that achieve at least 99 % sensitivity on the NLST data. Overall performance per validation site was evaluated using Area-Under-the-ROC-Curve analysis (AUC). RESULTS: The overall AUC across the European centers was 94.5 % (95 %CI 92.6-96.1). With a high sensitivity of 99.0 %, malignancy could be ruled out in 22.1 % of the nodules, enabling 18.5 % of the patients to avoid follow-up scans. The two false-negative results both represented small typical carcinoids. CONCLUSION: The LCP-CNN, trained on participants with lung nodules from the US NLST dataset, showed excellent performance on identification of benign lung nodules in a multi-center external dataset, ruling out malignancy with high accuracy in about one fifth of the patients with 5-15 mm nodules.",2021,10.1016/j.lungcan.2021.01.027,cross-sectional,diagnosis,CT,Lung
Lung cancer scRNA-seq and lipidomics reveal aberrant lipid metabolism for early-stage diagnosis,"Lung cancer is the leading cause of cancer mortality, and early detection is key to improving survival. However, there are no reliable blood-based tests currently available for early-stage lung cancer diagnosis. Here, we performed single-cell RNA sequencing of different early-stage lung cancers and found that lipid metabolism was broadly dysregulated in different cell types, with glycerophospholipid metabolism as the most altered lipid metabolism-related pathway. Untargeted lipidomics was carried out in an exploratory cohort of 311 participants. Through support vector machine algorithm-based and mass spectrum-based feature selection, we identified nine lipids (lysophosphatidylcholines 16:0, 18:0, and 20:4; phosphatidylcholines 16:0-18:1, 16:0-18:2, 18:0-18:1, 18:0-18:2, and 16:0-22:6; and triglycerides 16:0-18:1-18:1) as the features most important for early-stage cancer detection. Using these nine features, we developed a liquid chromatography-mass spectrometry (MS)-based targeted assay using multiple reaction monitoring. This target assay achieved 100.00% specificity on an independent validation cohort. In a hospital-based lung cancer screening cohort of 1036 participants examined by low-dose computed tomography and a prospective clinical cohort containing 109 participants, the assay reached more than 90.00% sensitivity and 92.00% specificity. Accordingly, matrix-assisted laser desorption/ionization MS imaging confirmed that the selected lipids were differentially expressed in early-stage lung cancer tissues in situ. This method, designated as Lung Cancer Artificial Intelligence Detector, may be useful for early detection of lung cancer or large-scale screening of high-risk populations for cancer prevention.",2022,10.1126/scitranslmed.abk2756,,,,
Lung cancer staging: a physiological update,"The tumour-node metastasis (TNM) classification system is anatomically based. We investigated whether the addition of simple physiological variables, age and body mass index (BMI), would affect survival curves, i.e. a composite anatomical and physiological staging system. We retrospectively analysed a prospectively validated thoracic surgery database (n = 1981). Cox multivariate analysis was performed to determine possible significant factors. Kaplan-Meier survival curves were constructed with combined anatomical and physiological factors. Cox multivariate analysis revealed age (P < 0.001) and BMI (P = 0.01) as significant factors affecting survival. Receiver operating curve analysis determined cut-off levels for age of 67 and BMI of 27.6. A composite anatomical and physiological survival curve based on TNM for BMI > 27.6 and age < 67 was produced. Age and BMI criteria resulted in significantly different survival curves, for stage I (P < 0.0001) and stage II (P = 0.0032), but not for stage III (P = 0.06). Neural network analysis confirmed the importance of BMI and age above cancer stage with regard to long-term survival. Combining age < 67, BMI > 27.6 and TNM anatomical classification results in very different estimated survival curves from the usual TNM system. Patients from stages I, II and III may have survival equivalent to a stage higher or lower depending on their age and BMI.",2012,10.1093/icvts/ivr164,,,,
Lung cancer subtype classification using histopathological images based on weakly supervised multi-instance learning,"Objective.Subtype classification plays a guiding role in the clinical diagnosis and treatment of non-small-cell lung cancer (NSCLC). However, due to the gigapixel of whole slide images (WSIs) and the absence of definitive morphological features, most automatic subtype classification methods for NSCLC require manually delineating the regions of interest (ROIs) on WSIs.Approach.In this paper, a weakly supervised framework is proposed for accurate subtype classification while freeing pathologists from pixel-level annotation. With respect to the characteristics of histopathological images, we design a two-stage structure with ROI localization and subtype classification. We first develop a method called multi-resolution expectation-maximization convolutional neural network (MR-EM-CNN) to locate ROIs for subsequent subtype classification. The EM algorithm is introduced to select the discriminative image patches for training a patch-wise network, with only WSI-wise labels available. A multi-resolution mechanism is designed for fine localization, similar to the coarse-to-fine process of manual pathological analysis. In the second stage, we build a novel hierarchical attention multi-scale network (HMS) for subtype classification. HMS can capture multi-scale features flexibly driven by the attention module and implement hierarchical features interaction.Results.Experimental results on the 1002-patient Cancer Genome Atlas dataset achieved an AUC of 0.9602 in the ROI localization and an AUC of 0.9671 for subtype classification.Significance.The proposed method shows superiority compared with other algorithms in the subtype classification of NSCLC. The proposed framework can also be extended to other classification tasks with WSIs.",2021,10.1088/1361-6560/ac3b32,,,,
Lung CT Segmentation to Identify Consolidations and Ground Glass Areas for Quantitative Assesment of SARS-CoV Pneumonia,"Segmentation is a complex task, faced by radiologists and researchers as radiomics and machine learning grow in potentiality. The process can either be automatic, semi-automatic, or manual, the first often not being sufficiently precise or easily reproducible, and the last being excessively time consuming when involving large districts with high-resolution acquisitions. A high-resolution CT of the chest is composed of hundreds of images, and this makes the manual approach excessively time consuming. Furthermore, the parenchymal alterations require an expert evaluation to be discerned from the normal appearance; thus, a semi-automatic approach to the segmentation process is, to the best of our knowledge, the most suitable when segmenting pneumonias, especially when their features are still unknown. For the studies conducted in our institute on the imaging of COVID-19, we adopted 3D Slicer, a freeware software produced by the Harvard University, and combined the threshold with the paint brush instruments to achieve fast and precise segmentation of aerated lung, ground glass opacities, and consolidations. When facing complex cases, this method still requires a considerable amount of time for proper manual adjustments, but provides an extremely efficient mean to define segments to use for further analysis, such as the calculation of the percentage of the affected lung parenchyma or texture analysis of the ground glass areas.",2020,10.3791/61737,,,,
Lung detection and severity prediction of pneumonia patients based on COVID-19 DET-PRE network,"BACKGROUND: The sudden outbreak of COVID-19 pneumonia has brought a heavy disaster to individuals globally. Facing this new virus, the clinicians have no automatic tools to assess the severity of pneumonia patients. METHODS: In the current work, a COVID-19 DET-PRE network with two pipelines was proposed. Firstly, the lungs in X-rays were detected and segmented through the improved YOLOv3 Dense network to remove redundant features. Then, the VGG16 classifier was pre-trained on the source domain, and the severity of the disease was predicted on the target domain by means of transfer learning. RESULTS: The experiment results demonstrated that the COVID-19 DET-PRE network can effectively detect the lungs from X-rays and accurately predict the severity of the disease. The mean average precisions (mAPs) of lung detection in patients with mild and severe illness were 0.976 and 0.983 respectively. Moreover, the accuracy of severity prediction of COVID-19 pneumonia can reach 86.1%. CONCLUSIONS: The proposed neural network has high accuracy, which is suitable for the clinical diagnosis of COVID-19 pneumonia.",2022,10.1080/17434440.2022.2014319,cross-sectional,prognosis,Radiograph,Lung
Lung Disease Classification in CXR Images Using Hybrid Inception-ResNet-v2 Model and Edge Computing,"Chest X-ray (CXR) imaging is one of the most widely used and economical tests to diagnose a wide range of diseases. However, even for expert radiologists, it is a challenge to accurately diagnose diseases from CXR samples. Furthermore, there remains an acute shortage of trained radiologists worldwide. In the present study, a range of machine learning (ML), deep learning (DL), and transfer learning (TL) approaches have been evaluated to classify diseases in an openly available CXR image dataset. A combination of the synthetic minority over-sampling technique (SMOTE) and weighted class balancing is used to alleviate the effects of class imbalance. A hybrid Inception-ResNet-v2 transfer learning model coupled with data augmentation and image enhancement gives the best accuracy. The model is deployed in an edge environment using Amazon IoT Core to automate the task of disease detection in CXR images with three categories, namely pneumonia, COVID-19, and normal. Comparative analysis has been given in various metrics such as precision, recall, accuracy, AUC-ROC score, etc. The proposed technique gives an average accuracy of 98.66%. The accuracies of other TL models, namely SqueezeNet, VGG19, ResNet50, and MobileNetV2 are 97.33%, 91.66%, 90.33%, and 76.00%, respectively. Further, a DL model, trained from scratch, gives an accuracy of 92.43%. Two feature-based ML classification techniques, namely support vector machine with local binary pattern (SVM + LBP) and decision tree with histogram of oriented gradients (DT + HOG) yield an accuracy of 87.98% and 86.87%, respectively.",2022,10.1155/2022/9036457,cross-sectional,diagnosis,Radiograph,Lung
Lung Infection Segmentation for COVID-19 Pneumonia Based on a Cascade Convolutional Network from CT Images,"The COVID-19 pandemic is a global, national, and local public health concern which has caused a significant outbreak in all countries and regions for both males and females around the world. Automated detection of lung infections and their boundaries from medical images offers a great potential to augment the patient treatment healthcare strategies for tackling COVID-19 and its impacts. Detecting this disease from lung CT scan images is perhaps one of the fastest ways to diagnose patients. However, finding the presence of infected tissues and segment them from CT slices faces numerous challenges, including similar adjacent tissues, vague boundary, and erratic infections. To eliminate these obstacles, we propose a two-route convolutional neural network (CNN) by extracting global and local features for detecting and classifying COVID-19 infection from CT images. Each pixel from the image is classified into the normal and infected tissues. For improving the classification accuracy, we used two different strategies including fuzzy c-means clustering and local directional pattern (LDN) encoding methods to represent the input image differently. This allows us to find more complex pattern from the image. To overcome the overfitting problems due to small samples, an augmentation approach is utilized. The results demonstrated that the proposed framework achieved precision 96%, recall 97%, F score, average surface distance (ASD) of 2.8 ± 0.3 mm, and volume overlap error (VOE) of 5.6 ± 1.2%.",2021,10.1155/2021/5544742,cross-sectional,informatics,CT,Lung
"Lung involvement in macrophage activation syndrome and severe COVID-19: results from a cross-sectional study to assess clinical, laboratory and artificial intelligence-radiological differences","OBJECTIVES: To evaluate the clinical pictures, laboratory tests and imaging of patients with lung involvement, either from severe COVID-19 or macrophage activation syndrome (MAS), in order to assess how similar these two diseases are. METHODS: The present work has been designed as a cross-sectional single-centre study to compare characteristics of patients with lung involvement either from MAS or severe COVID-19. Chest CT scans were assessed by using an artificial intelligence (AI)-based software. RESULTS: Ten patients with MAS and 47 patients with severe COVID-19 with lung involvement were assessed. Although all patients showed fever and dyspnoea, patients with MAS were characterised by thrombocytopaenia, whereas patients with severe COVID-19 were characterised by lymphopaenia and neutrophilia. Higher values of H-score characterised patients with MAS when compared with severe COVID-19. AI-reconstructed images of chest CT scan showed that apical, basal, peripheral and bilateral distributions of ground-glass opacities (GGOs), as well as apical consolidations, were more represented in severe COVID-19 than in MAS. C reactive protein directly correlated with GGOs extension in both diseases. Furthermore, lymphopaenia inversely correlated with GGOs extension in severe COVID-19. CONCLUSIONS: Our data could suggest laboratory and radiological differences between MAS and severe COVID-19, paving the way for further hypotheses to be investigated in future confirmatory studies.",2020,10.1136/annrheumdis-2020-218048,,,,
Lung Lesion Detection in CT Scan Images Using the Fuzzy Local Information Cluster Means (FLICM) Automatic Segmentation Algorithm and Back Propagation Network Classification,"Lung cancer is a frequently lethal disease often causing death of human beings at an early age because of uncontrolled cell growth in the lung tissues. The diagnostic methods available are less than effective for detection of cancer. Therefore an automatic lesion segmentation method with computed tomography (CT) scans has been developed. However it is very difficult to perform automatic identification and segmentation of lung tumours with good accuracy because of the existence of variation in lesions. This paper describes the application of a robust lesion detection and segmentation technique to segment every individual cell from pathological images to extract the essential features. The proposed technique based on the FLICM (Fuzzy Local Information Cluster Means) algorithm used for segmentation, with reduced false positives in detecting lung cancers. The back propagation network used to classify cancer cells is based on computer aided diagnosis (CAD).",2017,10.22034/apjcp.2017.18.12.3395,cross-sectional,diagnosis,CT,Lung
Lung Lesion Localization of COVID-19 From Chest CT Image: A Novel Weakly Supervised Learning Method,"Chest computed tomography (CT) image data is necessary for early diagnosis, treatment, and prognosis of Coronavirus Disease 2019 (COVID-19). Artificial intelligence has been tried to help clinicians in improving the diagnostic accuracy and working efficiency of CT. Whereas, existing supervised approaches on CT image of COVID-19 pneumonia require voxel-based annotations for training, which take a lot of time and effort. This paper proposed a weakly-supervised method for COVID-19 lesion localization based on generative adversarial network (GAN) with image-level labels only. We first introduced a GAN-based framework to generate normal-looking CT slices from CT slices with COVID-19 lesions. We then developed a novel feature match strategy to improve the reality of generated images by guiding the generator to capture the complex texture of chest CT images. Finally, the localization map of lesions can be easily obtained by subtracting the output image from its corresponding input image. By adding a classifier branch to the GAN-based framework to classify localization maps, we can further develop a diagnosis system with improved classification accuracy. Three CT datasets from hospitals of Sao Paulo, Italian Society of Medical and Interventional Radiology, and China Medical University about COVID-19 were collected in this article for evaluation. Our weakly supervised learning method obtained AUC of 0.883, dice coefficient of 0.575, accuracy of 0.884, sensitivity of 0.647, specificity of 0.929, and F1-score of 0.640, which exceeded other widely used weakly supervised object localization methods by a significant margin. We also compared the proposed method with fully supervised learning methods in COVID-19 lesion segmentation task, the proposed weakly supervised method still leads to a competitive result with dice coefficient of 0.575. Furthermore, we also analyzed the association between illness severity and visual score, we found that the common severity cohort had the largest sample size as well as the highest visual score which suggests our method can help rapid diagnosis of COVID-19 patients, especially in massive common severity cohort. In conclusion, we proposed this novel method can serve as an accurate and efficient tool to alleviate the bottleneck of expert annotation cost and advance the progress of computer-aided COVID-19 diagnosis.",2021,10.1109/jbhi.2021.3067465,cross-sectional,diagnosis,CT,Lung
"Lung Nodule Classification Using Biomarkers, Volumetric Radiomics, and 3D CNNs","We present a hybrid algorithm to estimate lung nodule malignancy that combines imaging biomarkers from Radiologist's annotation with image classification of CT scans. Our algorithm employs a 3D Convolutional Neural Network (CNN) as well as a Random Forest in order to combine CT imagery with biomarker annotation and volumetric radiomic features. We analyze and compare the performance of the algorithm using only imagery, only biomarkers, combined imagery + biomarkers, combined imagery + volumetric radiomic features, and finally the combination of imagery + biomarkers + volumetric features in order to classify the suspicion level of nodule malignancy. The National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to train and evaluate the classification task. We show that the incorporation of semi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the available training sample size of the LIDC-IDRI, thereby further improving the accuracy of malignancy estimation of most of the models tested although there is no significant improvement with the use of KNN semi-supervised learning if image classification with CNNs and volumetric features is combined with descriptive biomarkers. Unexpectedly, we also show that a model using image biomarkers alone is more accurate than one that combines biomarkers with volumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the possibility that this result may be influenced by cognitive bias in LIDC-IDRI because malignancy estimates were recorded by the same radiologist panel as biomarkers, as well as future work to incorporate pathology information over a subset of study participants.",2021,10.1007/s10278-020-00417-y,cross-sectional,diagnosis,CT,Lung
Lung nodule classification using deep feature fusion in chest radiography,"Lung nodules are small, round, or oval-shaped masses of tissue in the lung region. Early diagnosis and treatment of lung nodules can significantly improve the quality of patients' lives. Because of their small size and the interlaced nature of chest anatomy, detection of lung nodules using different medical imaging techniques becomes challenging. Recently, several methods for computer aided diagnosis (CAD) were proposed to improve the detection of lung nodules with good performances. However, the current methods are unable to achieve high sensitivity and high specificity. In this paper, we propose using deep feature fusion from the non-medical training and hand-crafted features to reduce the false positive results. Based on our experimentation of the public dataset, our results show that, the deep fusion feature can achieve promising results in terms of sensitivity and specificity (69.3% and 96.2%) at 1.19 false positive per image, which is better than the single hand-crafted features (62% and 95.4%) at 1.45 false positive per image. As it stands, fusion features that were used to classify our candidate nodules have resulted in a more promising outcome as compared to the single features from deep learning features and the hand-crafted features. This will improve the current CAD method based on the use of deep feature fusion to more effectively diagnose the presence of lung nodules.",2017,10.1016/j.compmedimag.2016.11.004,cross-sectional,diagnosis,Radiograph,Lung
Lung nodule classification using deep Local-Global networks,"PURPOSE: Lung nodules have very diverse shapes and sizes, which makes classifying them as benign/malignant a challenging problem. In this paper, we propose a novel method to predict the malignancy of nodules that have the capability to analyze the shape and size of a nodule using a global feature extractor, as well as the density and structure of the nodule using a local feature extractor. METHODS: We propose to use Residual Blocks with a 3 × 3 kernel size for local feature extraction and Non-Local Blocks to extract the global features. The Non-Local Block has the ability to extract global features without using a huge number of parameters. The key idea behind the Non-Local Block is to apply matrix multiplications between features on the same feature maps. RESULTS: We trained and validated the proposed method on the LIDC-IDRI dataset which contains 1018 computed tomography scans. We followed a rigorous procedure for experimental setup, namely tenfold cross-validation, and ignored the nodules that had been annotated by < 3 radiologists. The proposed method achieved state-of-the-art results with AUC = 95.62%, while significantly outperforming other baseline methods. CONCLUSIONS: Our proposed deep Local-Global network has the capability to accurately extract both local and global features. Our new method outperforms state-of-the-art architecture including Densenet and Resnet with transfer learning.",2019,10.1007/s11548-019-01981-7,cross-sectional,diagnosis,CT,Lung
Lung nodule classification with multilevel patch-based context analysis,"In this paper, we propose a novel classification method for the four types of lung nodules, i.e., well-circumscribed, vascularized, juxta-pleural, and pleural-tail, in low dose computed tomography scans. The proposed method is based on contextual analysis by combining the lung nodule and surrounding anatomical structures, and has three main stages: an adaptive patch-based division is used to construct concentric multilevel partition; then, a new feature set is designed to incorporate intensity, texture, and gradient information for image patch feature description, and then a contextual latent semantic analysis-based classifier is designed to calculate the probabilistic estimations for the relevant images. Our proposed method was evaluated on a publicly available dataset and clearly demonstrated promising classification performance.",2014,10.1109/tbme.2013.2295593,cross-sectional,diagnosis,CT,Lung
Lung Nodule Detectability of Artificial Intelligence-assisted CT Image Reading in Lung Cancer Screening,"BACKGROUND: Artificial Intelligence (AI)-based automatic lung nodule detection system improves the detection rate of nodules. It is important to evaluate the clinical value of the AI system by comparing AI-assisted nodule detection with actual radiology reports. OBJECTIVE: To compare the detection rate of lung nodules between the actual radiology reports and AI-assisted reading in lung cancer CT screening. METHODS: Participants in chest CT screening from November to December 2019 were retrospectively included. In the real-world radiologist observation, 14 residents and 15 radiologists participated in finalizing radiology reports. In AI-assisted reading, one resident and one radiologist reevaluated all subjects with the assistance of an AI system to locate and measure the detected lung nodules. A reading panel determined the type and number of detected lung nodules between these two methods. RESULTS: In 860 participants (57±7 years), the reading panel confirmed 250 patients with >1 solid nodule, while radiologists observed 131, lower than 247 by AI-assisted reading (p<0.001). The panel confirmed 111 patients with >1 non-solid nodule, whereas radiologist observation identified 28, lower than 110 by AI-assisted reading (p<0.001). The accuracy and sensitivity of radiologist observation for solid nodules were 86.2% and 52.4%, lower than 99.1% and 98.8% by AI-assisted reading, respectively. These metrics were 90.4% and 25.2% for non-solid nodules, lower than 98.8% and 99.1% by AI-assisted reading, respectively. CONCLUSION: Comparing with the actual radiology reports, AI-assisted reading greatly improves the accuracy and sensitivity of nodule detection in chest CT, which benefits lung nodule detection, especially for non-solid nodules.",2022,10.2174/1573405617666210806125953,cross-sectional,diagnosis,CT,Lung
Lung Nodule Detection based on Ensemble of Hand Crafted and Deep Features,"Lung cancer is considered as a deadliest disease worldwide due to which 1.76 million deaths occurred in the year 2018. Keeping in view its dreadful effect on humans, cancer detection at a premature stage is a more significant requirement to reduce the probability of mortality rate. This manuscript depicts an approach of finding lung nodule at an initial stage that comprises of three major phases: (1) lung nodule segmentation using Otsu threshold followed by morphological operation; (2) extraction of geometrical, texture and deep learning features for selecting optimal features; (3) The optimal features are fused serially for classification of lung nodule into two categories that is malignant and benign. The lung image database consortium image database resource initiative (LIDC-IDRI) is used for experimentation. The experimental outcomes show better performance of presented approach as compared with the existing methods.",2019,10.1007/s10916-019-1455-6,cross-sectional,diagnosis,CT,Lung
Lung Nodule Detection based on Faster R-CNN Framework,"BACKGROUND: Lung cancer is a worldwide high-risk disease, and lung nodules are the main manifestation of early lung cancer. Automatic detection of lung nodules reduces the workload of radiologists, the rate of misdiagnosis and missed diagnosis. For this purpose, we propose a Faster R-CNN algorithm for the detection of these lung nodules. METHOD: Faster R-CNN algorithm can detect lung nodules, and the training set is used to prove the feasibility of this technique. In theory, parameter optimization can improve network structure, as well as detection accuracy. RESULT: Through experiments, the best parameters are that the basic learning rate is 0.001, step size is 70,000, attenuation coefficient is 0.1, the value of Dropout is 0.5, and the value of Batch Size is 64. Compared with other networks for detecting lung nodules, the optimized and improved algorithm proposed in this paper generally improves detection accuracy by more than 20% when compared with the other traditional algorithms. CONCLUSION: Our experimental results have proved that the method of detecting lung nodules based on Faster R-CNN algorithm has good accuracy and therefore, presents potential clinical value in lung disease diagnosis. This method can further assist radiologists, and also for researchers in the design and development of the detection system for lung nodules.",2021,10.1016/j.cmpb.2020.105866,cross-sectional,diagnosis,CT,Lung
Lung nodule detection in chest X-rays using synthetic ground-truth data comparing CNN-based diagnosis to human performance,"We present a method to generate synthetic thorax radiographs with realistic nodules from CT scans, and a perfect ground truth knowledge. We evaluated the detection performance of nine radiologists and two convolutional neural networks in a reader study. Nodules were artificially inserted into the lung of a CT volume and synthetic radiographs were obtained by forward-projecting the volume. Hence, our framework allowed for a detailed evaluation of CAD systems' and radiologists' performance due to the availability of accurate ground-truth labels for nodules from synthetic data. Radiographs for network training (U-Net and RetinaNet) were generated from 855 CT scans of a public dataset. For the reader study, 201 radiographs were generated from 21 nodule-free CT scans with altering nodule positions, sizes and nodule counts of inserted nodules. Average true positive detections by nine radiologists were 248.8 nodules, 51.7 false positive predicted nodules and 121.2 false negative predicted nodules. The best performing CAD system achieved 268 true positives, 66 false positives and 102 false negatives. Corresponding weighted alternative free response operating characteristic figure-of-merits (wAFROC FOM) for the radiologists range from 0.54 to 0.87 compared to a value of 0.81 (CI 0.75-0.87) for the best performing CNN. The CNN did not perform significantly better against the combined average of the 9 readers (p = 0.49). Paramediastinal nodules accounted for most false positive and false negative detections by readers, which can be explained by the presence of more tissue in this area.",2021,10.1038/s41598-021-94750-z,cross-sectional,diagnosis,Radiograph,Lung
Lung Nodule Detection in CT Images Using a Raw Patch-Based Convolutional Neural Network,"Remarkable progress has been made in image classification and segmentation, due to the recent study of deep convolutional neural networks (CNNs). To solve the similar problem of diagnostic lung nodule detection in low-dose computed tomography (CT) scans, we propose a new Computer-Aided Detection (CAD) system using CNNs and CT image segmentation techniques. Unlike former studies focusing on the classification of malignant nodule types or relying on prior image processing, in this work, we put raw CT image patches directly in CNNs to reduce the complexity of the system. Specifically, we split each CT image into several patches, which are divided into 6 types consisting of 3 nodule types and 3 non-nodule types. We compare the performance of ResNet with different CNNs architectures on CT images from a publicly available dataset named the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). Results show that our best model reaches a high detection sensitivity of 92.8% with 8 false positives per scan (FPs/scan). Compared with related work, our work obtains a state-of-the-art effect.",2019,10.1007/s10278-019-00221-3,cross-sectional,diagnosis,CT,Lung
Lung Nodule Detection using Convolutional Neural Networks with Transfer Learning on CT Images,"AIM AND OBJECTIVE: Lung nodule detection is critical in improving the five-year survival rate and reducing mortality for patients with lung cancer. Numerous methods based on Convolutional Neural Networks (CNNs) have been proposed for lung nodule detection in Computed Tomography (CT) images. With the collaborative development of computer hardware technology, the detection accuracy and efficiency can still be improved. MATERIALS AND METHODS: In this study, an automatic lung nodule detection method using CNNs with transfer learning is presented. We first compared three of the state-of-the-art convolutional neural network (CNN) models, namely, VGG16, VGG19 and ResNet50, to determine the most suitable model for lung nodule detection. We then utilized two different training strategies, namely, freezing layers and fine-tuning, to illustrate the effectiveness of transfer learning. Furthermore, the hyper-parameters of the CNN model such as optimizer, batch size and epoch were optimized. RESULTS: Evaluated on the Lung Nodule Analysis 2016 (LUNA16) challenge, promising results with an accuracy of 96.86%, a precision of 91.10%, a sensitivity of 90.78%, a specificity of 98.13%, and an AUC of 99.37% were achieved. CONCLUSION: Compared with other works, state-of-the-art specificity is obtained, which demonstrates that the proposed method is effective and applicable to lung nodule detection.",2021,10.2174/1386207323666200714002459,cross-sectional,diagnosis,CT,Lung
Lung Nodule Malignancy Prediction in Sequential CT Scans: Summary of ISBI 2018 Challenge,"Lung cancer is by far the leading cause of cancer death in the US. Recent studies have demonstrated the effectiveness of screening using low dose CT (LDCT) in reducing lung cancer related mortality. While lung nodules are detected with a high rate of sensitivity, this exam has a low specificity rate and it is still difficult to separate benign and malignant lesions. The ISBI 2018 Lung Nodule Malignancy Prediction Challenge, developed by a team from the Quantitative Imaging Network of the National Cancer Institute, was focused on the prediction of lung nodule malignancy from two sequential LDCT screening exams using automated (non-manual) algorithms. We curated a cohort of 100 subjects who participated in the National Lung Screening Trial and had established pathological diagnoses. Data from 30 subjects were randomly selected for training and the remaining was used for testing. Participants were evaluated based on the area under the receiver operating characteristic curve (AUC) of nodule-wise malignancy scores generated by their algorithms on the test set. The challenge had 17 participants, with 11 teams submitting reports with method description, mandated by the challenge rules. Participants used quantitative methods, resulting in a reporting test AUC ranging from 0.698 to 0.913. The top five contestants used deep learning approaches, reporting an AUC between 0.87 - 0.91. The team's predictor did not achieve significant differences from each other nor from a volume change estimate (p =.05 with Bonferroni-Holm's correction).",2021,10.1109/tmi.2021.3097665,,,,
Lung nodule segmentation using Salp Shuffled Shepherd Optimization Algorithm-based Generative Adversarial Network,"Lung nodule segmentation is an exciting area of research for the effective detection of lung cancer. One of the significant challenges in detecting lung cancer is Accuracy, which is affected due to the visual deviations and heterogeneity in the lung nodules. Hence, to improve the segmentation process's Accuracy, a Salp Shuffled Shepherd Optimization Algorithm-based Generative Adversarial Network (SSSOA-based GAN) model is developed in this research for lung nodule segmentation. The SSSOA is the hybrid optimization algorithm developed by integrating the Salp Swarm Algorithm (SSA) and shuffled shepherd optimization algorithm (SSOA). The artefacts in the input Computed Tomography (CT) image are removed by performing pre-processing with the help of a Gaussian filter. The pre-processed image is subjected to lung lobe segmentation, which is done with the help of deep joint segmentation for segmenting the appropriate regions. The lung nodule segmentation is performed using the GAN. The GAN is trained using the SSSOA to effectively segment the lung nodule from the lung lobe image. The metrics, such as Dice Coefficient, Accuracy, and Jaccard Similarity, are used to evaluate the performance. The developed SSSOA-based GAN method obtained a maximum Accuracy of 0.9387, a maximum Dice Coefficient of 0.7986, and a maximum Jaccard Similarity of 0.8026, respectively, compared with the existing lung nodule segmentation method.",2021,10.1016/j.compbiomed.2021.104811,cross-sectional,diagnosis,CT,Lung
Lung Nodule Sizes Are Encoded When Scaling CT Image for CNN's,"Noninvasive diagnosis of lung cancer in early stages is one task where radiomics helps. Clinical practice shows that the size of a nodule has high predictive power for malignancy. In the literature, convolutional neural networks (CNNs) have become widely used in medical image analysis. We study the ability of a CNN to capture nodule size in computed tomography images after images are resized for CNN input. For our experiments, we used the National Lung Screening Trial data set. Nodules were labeled into 2 categories (small/large) based on the original size of a nodule. After all extracted patches were re-sampled into 100-by-100-pixel images, a CNN was able to successfully classify test nodules into small- and large-size groups with high accuracy. To show the generality of our discovery, we repeated size classification experiments using Common Objects in Context (COCO) data set. From the data set, we selected 3 categories of images, namely, bears, cats, and dogs. For all 3 categories a 5- × 2-fold cross-validation was performed to put them into small and large classes. The average area under receiver operating curve is 0.954, 0.952, and 0.979 for the bear, cat, and dog categories, respectively. Thus, camera image rescaling also enables a CNN to discover the size of an object. The source code for experiments with the COCO data set is publicly available in Github (https://github.com/VisionAI-USF/COCO_Size_Decoding/).",2020,10.18383/j.tom.2019.00024,,,,
Lung tumor segmentation in 4D CT images using motion convolutional neural networks,"PURPOSE: Manual delineation on all breathing phases of lung cancer 4D CT image datasets can be challenging, exhaustive, and prone to subjective errors because of both the large number of images in the datasets and variations in the spatial location of tumors secondary to respiratory motion. The purpose of this work is to present a new deep learning-based framework for fast and accurate segmentation of lung tumors on 4D CT image sets. METHODS: The proposed DL framework leverages motion region convolutional neural network (R-CNN). Through integration of global and local motion estimation network architectures, the network can learn both major and minor changes caused by tumor motion. Our network design first extracts tumor motion information by feeding 4D CT images with consecutive phases into an integrated backbone network architecture, locating volume-of-interest (VOIs) via a regional proposal network and removing irrelevant information via a regional convolutional neural network. Extracted motion information is then advanced into the subsequent global and local motion head network architecture to predict corresponding deformation vector fields (DVFs) and further adjust tumor VOIs. Binary masks of tumors are then segmented within adjusted VOIs via a mask head. A self-attention strategy is incorporated in the mask head network to remove any noisy features that might impact segmentation performance. We performed two sets of experiments. In the first experiment, a five-fold cross-validation on 20 4D CT datasets, each consisting of 10 breathing phases (i.e., 200 3D image volumes in total). The network performance was also evaluated on an additional unseen 200 3D images volumes from 20 hold-out 4D CT datasets. In the second experiment, we trained another model with 40 patients' 4D CT datasets from experiment 1 and evaluated on additional unseen nine patients' 4D CT datasets. The Dice similarity coefficient (DSC), center of mass distance (CMD), 95th percentile Hausdorff distance (HD(95) ), mean surface distance (MSD), and volume difference (VD) between the manual and segmented tumor contour were computed to evaluate tumor detection and segmentation accuracy. The performance of our method was quantitatively evaluated against four different methods (VoxelMorph, U-Net, network without global and local networks, and network without attention gate strategy) across all evaluation metrics through a paired t-test. RESULTS: The proposed fully automated DL method yielded good overall agreement with the ground truth for contoured tumor volume and segmentation accuracy. Our model yielded significantly better values of evaluation metrics (p < 0.05) than all four competing methods in both experiments. On hold-out datasets of experiment 1 and 2, our method yielded DSC of 0.86 and 0.90 compared to 0.82 and 0.87, 0.75 and 0.83, 081 and 0.89, and 0.81 and 0.89 yielded by VoxelMorph, U-Net, network without global and local networks, and networks without attention gate strategy. Tumor VD between ground truth and our method was the smallest with the value of 0.50 compared to 0.99, 1.01, 0.92, and 0.93 for between ground truth and VoxelMorph, U-Net, network without global and local networks, and networks without attention gate strategy, respectively. CONCLUSIONS: Our proposed DL framework of tumor segmentation on lung cancer 4D CT datasets demonstrates a significant promise for fully automated delineation. The promising results of this work provide impetus for its integration into the 4D CT treatment planning workflow to improve the accuracy and efficiency of lung radiotherapy.",2021,10.1002/mp.15204,cross-sectional,diagnosis,CT,Lung
Lung ultrasound education: simulation and hands-on,"COVID-19 can cause damage to the lung, which can result in progressive respiratory failure and potential death. Chest radiography and CT are the imaging tools used to diagnose and monitor patients with COVID-19. Lung ultrasound (LUS) during COVID-19 is being used in some areas to aid decision-making and improve patient care. However, its increased use could help improve existing practice for patients with suspected COVID-19, or other lung disease. A limitation of LUS is that it requires practitioners with sufficient competence to ensure timely, safe, and diagnostic clinical/imaging assessments. This commentary discusses the role and governance of LUS during and beyond the COVID-19 pandemic, and how increased education and training in this discipline can be undertaken given the restrictions in imaging highly infectious patients. The use of simulation, although numerical methods or dedicated scan trainers, and machine learning algorithms could further improve the accuracy of LUS, whilst helping to reduce its learning curve for greater uptake in clinical practice.",2021,10.1259/bjr.20200755,,,,
LungNet: A hybrid deep-CNN model for lung cancer diagnosis using CT and wearable sensor-based medical IoT data,"Lung cancer, also known as pulmonary cancer, is one of the deadliest cancers, but yet curable if detected at the early stage. At present, the ambiguous features of the lung cancer nodule make the computer-aided automatic diagnosis a challenging task. To alleviate this, we present LungNet, a novel hybrid deep-convolutional neural network-based model, trained with CT scan and wearable sensor-based medical IoT (MIoT) data. LungNet consists of a unique 22-layers Convolutional Neural Network (CNN), which combines latent features that are learned from CT scan images and MIoT data to enhance the diagnostic accuracy of the system. Operated from a centralized server, the network has been trained with a balanced dataset having 525,000 images that can classify lung cancer into five classes with high accuracy (96.81%) and low false positive rate (3.35%), outperforming similar CNN-based classifiers. Moreover, it classifies the stage-1 and stage-2 lung cancers into 1A, 1B, 2A and 2B sub-classes with 91.6% accuracy and false positive rate of 7.25%. High predictive capability accompanied with sub-stage classification renders LungNet as a promising prospect in developing CNN-based automatic lung cancer diagnosis systems.",2021,10.1016/j.compbiomed.2021.104961,cross-sectional,diagnosis,CT,Lung
Lungs nodule detection framework from computed tomography images using support vector machine,"The emergence of cloud infrastructure has the potential to provide significant benefits in a variety of areas in the medical imaging field. The driving force behind the extensive use of cloud infrastructure for medical image processing is the exponential increase in the size of computed tomography (CT) and magnetic resonance imaging (MRI) data. The size of a single CT/MRI image has increased manifold since the inception of these imagery techniques. This demand for the introduction of effective and efficient frameworks for extracting relevant and most suitable information (features) from these sizeable images. As early detection of lungs cancer can significantly increase the chances of survival of a lung scanner patient, an effective and efficient nodule detection system can play a vital role. In this article, we have proposed a novel classification framework for lungs nodule classification with less false positive rates (FPRs), high accuracy, sensitivity rate, less computationally expensive and uses a small set of features while preserving edge and texture information. The proposed framework comprises multiple phases that include image contrast enhancement, segmentation, feature extraction, followed by an employment of these features for training and testing of a selected classifier. Image preprocessing and feature selection being the primary steps-playing their vital role in achieving improved classification accuracy. We have empirically tested the efficacy of our technique by utilizing the well-known Lungs Image Consortium Database dataset. The results prove that the technique is highly effective for reducing FPRs with an impressive sensitivity rate of 97.45%.",2019,10.1002/jemt.23275,cross-sectional,diagnosis,CT,Lung
M (3)Lung-Sys: A Deep Learning System for Multi-Class Lung Pneumonia Screening From CT Imaging,"To counter the outbreak of COVID-19, the accurate diagnosis of suspected cases plays a crucial role in timely quarantine, medical treatment, and preventing the spread of the pandemic. Considering the limited training cases and resources (e.g, time and budget), we propose a Multi-task Multi-slice Deep Learning System (M (3)Lung-Sys) for multi-class lung pneumonia screening from CT imaging, which only consists of two 2D CNN networks, i.e., slice- and patient-level classification networks. The former aims to seek the feature representations from abundant CT slices instead of limited CT volumes, and for the overall pneumonia screening, the latter one could recover the temporal information by feature refinement and aggregation between different slices. In addition to distinguish COVID-19 from Healthy, H1N1, and CAP cases, our M (3)Lung-Sys also be able to locate the areas of relevant lesions, without any pixel-level annotation. To further demonstrate the effectiveness of our model, we conduct extensive experiments on a chest CT imaging dataset with a total of 734 patients (251 healthy people, 245 COVID-19 patients, 105 H1N1 patients, and 133 CAP patients). The quantitative results with plenty of metrics indicate the superiority of our proposed model on both slice- and patient-level classification tasks. More importantly, the generated lesion location maps make our system interpretable and more valuable to clinicians.",2020,10.1109/jbhi.2020.3030853,cross-sectional,diagnosis,CT,Lung
Machine Learning Algorithms Utilizing Functional Respiratory Imaging May Predict COPD Exacerbations,"RATIONALE AND OBJECTIVES: Acute chronic obstructive pulmonary disease exacerbations (AECOPD) have a significant negative impact on the quality of life and accelerate progression of the disease. Functional respiratory imaging (FRI) has the potential to better characterize this disease. The purpose of this study was to identify FRI parameters specific to AECOPD and assess their ability to predict future AECOPD, by use of machine learning algorithms, enabling a better understanding and quantification of disease manifestation and progression. MATERIALS AND METHODS: A multicenter cohort of 62 patients with COPD was analyzed. FRI obtained from baseline high resolution CT data (unenhanced and volume gated), clinical, and pulmonary function test were analyzed and incorporated into machine learning algorithms. RESULTS: A total of 11 baseline FRI parameters could significantly distinguish ( p < 0.05) the development of AECOPD from a stable period. In contrast, no baseline clinical or pulmonary function test parameters allowed significant classification. Furthermore, using Support Vector Machines, an accuracy of 80.65% and positive predictive value of 82.35% could be obtained by combining baseline FRI features such as total specific image-based airway volume and total specific image-based airway resistance, measured at functional residual capacity. Patients who developed an AECOPD, showed significantly smaller airway volumes and (hence) significantly higher airway resistances at baseline. CONCLUSION: This study indicates that FRI is a sensitive tool (PPV 82.35%) for predicting future AECOPD on a patient specific level in contrast to classical clinical parameters.",2019,10.1016/j.acra.2018.10.022,retrospective cohort,prognosis,CT,Lung
Machine learning and bioinformatics analysis revealed classification and potential treatment strategy in stage 3-4 NSCLC patients,"BACKGROUND: Precision medicine has increased the accuracy of cancer diagnosis and treatment, especially in the era of cancer immunotherapy. Despite recent advances in cancer immunotherapy, the overall survival rate of advanced NSCLC patients remains low. A better classification in advanced NSCLC is important for developing more effective treatments. METHOD: The calculation of abundances of tumor-infiltrating immune cells (TIICs) was conducted using Cell-type Identification By Estimating Relative Subsets Of RNA Transcripts (CIBERSORT), xCell (xCELL), Tumor IMmune Estimation Resource (TIMER), Estimate the Proportion of Immune and Cancer cells (EPIC), and Microenvironment Cell Populations-counter (MCP-counter). K-means clustering was used to classify patients, and four machine learning methods (SVM, Randomforest, Adaboost, Xgboost) were used to build the classifiers. Multi-omics datasets (including transcriptomics, DNA methylation, copy number alterations, miRNA profile) and ICI immunotherapy treatment cohorts were obtained from various databases. The drug sensitivity data were derived from PRISM and CTRP databases. RESULTS: In this study, patients with stage 3-4 NSCLC were divided into three clusters according to the abundance of TIICs, and we established classifiers to distinguish these clusters based on different machine learning algorithms (including SVM, RF, Xgboost, and Adaboost). Patients in cluster-2 were found to have a survival advantage and might have a favorable response to immunotherapy. We then constructed an immune-related Poor Prognosis Signature which could successfully predict the advanced NSCLC patient survival, and through epigenetic analysis, we found 3 key molecules (HSPA8, CREB1, RAP1A) which might serve as potential therapeutic targets in cluster-1. In the end, after screening of drug sensitivity data derived from CTRP and PRISM databases, we identified several compounds which might serve as medication for different clusters. CONCLUSIONS: Our study has not only depicted the landscape of different clusters of stage 3-4 NSCLC but presented a treatment strategy for patients with advanced NSCLC.",2022,10.1186/s12920-022-01184-1,,,,
Machine Learning and Prediction of All-Cause Mortality in COPD,"BACKGROUND: COPD is a leading cause of mortality. RESEARCH QUESTION: We hypothesized that applying machine learning to clinical and quantitative CT imaging features would improve mortality prediction in COPD. STUDY DESIGN AND METHODS: We selected 30 clinical, spirometric, and imaging features as inputs for a random survival forest. We used top features in a Cox regression to create a machine learning mortality prediction (MLMP) in COPD model and also assessed the performance of other statistical and machine learning models. We trained the models in subjects with moderate to severe COPD from a subset of subjects in Genetic Epidemiology of COPD (COPDGene) and tested prediction performance in the remainder of individuals with moderate to severe COPD in COPDGene and Evaluation of COPD Longitudinally to Identify Predictive Surrogate Endpoints (ECLIPSE). We compared our model with the BMI, airflow obstruction, dyspnea, exercise capacity (BODE) index; BODE modifications; and the age, dyspnea, and airflow obstruction index. RESULTS: We included 2,632 participants from COPDGene and 1,268 participants from ECLIPSE. The top predictors of mortality were 6-min walk distance, FEV(1) % predicted, and age. The top imaging predictor was pulmonary artery-to-aorta ratio. The MLMP-COPD model resulted in a C index ≥ 0.7 in both COPDGene and ECLIPSE (6.4- and 7.2-year median follow-ups, respectively), significantly better than all tested mortality indexes (P < .05). The MLMP-COPD model had fewer predictors but similar performance to that of other models. The group with the highest BODE scores (7-10) had 64% mortality, whereas the highest mortality group defined by the MLMP-COPD model had 77% mortality (P = .012). INTERPRETATION: An MLMP-COPD model outperformed four existing models for predicting all-cause mortality across two COPD cohorts. Performance of machine learning was similar to that of traditional statistical methods. The model is available online at: https://cdnm.shinyapps.io/cgmortalityapp/.",2020,10.1016/j.chest.2020.02.079,,,,
Machine learning application for the prediction of SARS-CoV-2 infection using blood tests and chest radiograph,"Triaging and prioritising patients for RT-PCR test had been essential in the management of COVID-19 in resource-scarce countries. In this study, we applied machine learning (ML) to the task of detection of SARS-CoV-2 infection using basic laboratory markers. We performed the statistical analysis and trained an ML model on a retrospective cohort of 5148 patients from 24 hospitals in Hong Kong to classify COVID-19 and other aetiology of pneumonia. We validated the model on three temporal validation sets from different waves of infection in Hong Kong. For predicting SARS-CoV-2 infection, the ML model achieved high AUCs and specificity but low sensitivity in all three validation sets (AUC: 89.9-95.8%; Sensitivity: 55.5-77.8%; Specificity: 91.5-98.3%). When used in adjunction with radiologist interpretations of chest radiographs, the sensitivity was over 90% while keeping moderate specificity. Our study showed that machine learning model based on readily available laboratory markers could achieve high accuracy in predicting SARS-CoV-2 infection.",2021,10.1038/s41598-021-93719-2,,,,
"Machine learning applied on chest x-ray can aid in the diagnosis of COVID-19: a first experience from Lombardy, Italy","BACKGROUND: We aimed to train and test a deep learning classifier to support the diagnosis of coronavirus disease 2019 (COVID-19) using chest x-ray (CXR) on a cohort of subjects from two hospitals in Lombardy, Italy. METHODS: We used for training and validation an ensemble of ten convolutional neural networks (CNNs) with mainly bedside CXRs of 250 COVID-19 and 250 non-COVID-19 subjects from two hospitals (Centres 1 and 2). We then tested such system on bedside CXRs of an independent group of 110 patients (74 COVID-19, 36 non-COVID-19) from one of the two hospitals. A retrospective reading was performed by two radiologists in the absence of any clinical information, with the aim to differentiate COVID-19 from non-COVID-19 patients. Real-time polymerase chain reaction served as the reference standard. RESULTS: At 10-fold cross-validation, our deep learning model classified COVID-19 and non-COVID-19 patients with 0.78 sensitivity (95% confidence interval [CI] 0.74-0.81), 0.82 specificity (95% CI 0.78-0.85), and 0.89 area under the curve (AUC) (95% CI 0.86-0.91). For the independent dataset, deep learning showed 0.80 sensitivity (95% CI 0.72-0.86) (59/74), 0.81 specificity (29/36) (95% CI 0.73-0.87), and 0.81 AUC (95% CI 0.73-0.87). Radiologists' reading obtained 0.63 sensitivity (95% CI 0.52-0.74) and 0.78 specificity (95% CI 0.61-0.90) in Centre 1 and 0.64 sensitivity (95% CI 0.52-0.74) and 0.86 specificity (95% CI 0.71-0.95) in Centre 2. CONCLUSIONS: This preliminary experience based on ten CNNs trained on a limited training dataset shows an interesting potential of deep learning for COVID-19 diagnosis. Such tool is in training with new CXRs to further increase its performance.",2021,10.1186/s41747-020-00203-z,cross-sectional,diagnosis,Radiograph,Lung
Machine learning applied to near-infrared spectra for clinical pleural effusion classification,"Lung cancer patients with malignant pleural effusions (MPE) have a particular poor prognosis. It is crucial to distinguish MPE from benign pleural effusion (BPE). The present study aims to develop a rapid, convenient and economical diagnostic method based on FTIR near-infrared spectroscopy (NIRS) combined with machine learning strategy for clinical pleural effusion classification. NIRS spectra were recorded for 47 MPE samples and 35 BPE samples. The sample data were randomly divided into train set (n = 62) and test set (n = 20). Partial least squares, random forest, support vector machine (SVM), and gradient boosting machine models were trained, and subsequent predictive performance were predicted on the test set. Besides the whole spectra used in modeling, selected features using SVM recursive feature elimination algorithm were also investigated in modeling. Among those models, NIRS combined with SVM showed the best predictive performance (accuracy: 1.0, kappa: 1.0, and AUC(ROC): 1.0). SVM with the top 50 feature wavenumbers also displayed a high predictive performance (accuracy: 0.95, kappa: 0.89, AUC(ROC): 0.99). Our study revealed that the combination of NIRS and machine learning is an innovative, rapid, and convenient method for clinical pleural effusion classification, and worth further evaluation.",2021,10.1038/s41598-021-87736-4,cross-sectional,diagnosis,near-infrared spectroscopy (NIRS),Lung
Machine learning approach for distinguishing malignant and benign lung nodules utilizing standardized perinodular parenchymal features from CT,"PURPOSE: Computed tomography (CT) is an effective method for detecting and characterizing lung nodules in vivo. With the growing use of chest CT, the detection frequency of lung nodules is increasing. Noninvasive methods to distinguish malignant from benign nodules have the potential to decrease the clinical burden, risk, and cost involved in follow-up procedures on the large number of false-positive lesions detected. This study examined the benefit of including perinodular parenchymal features in machine learning (ML) tools for pulmonary nodule assessment. METHODS: Lung nodule cases with pathology confirmed diagnosis (74 malignant, 289 benign) were used to extract quantitative imaging characteristics from computed tomography scans of the nodule and perinodular parenchyma tissue. A ML tool development pipeline was employed using k-medoids clustering and information theory to determine efficient predictor sets for different amounts of parenchyma inclusion and build an artificial neural network classifier. The resulting ML tool was validated using an independent cohort (50 malignant, 50 benign). RESULTS: The inclusion of parenchymal imaging features improved the performance of the ML tool over exclusively nodular features (P < 0.01). The best performing ML tool included features derived from nodule diameter-based surrounding parenchyma tissue quartile bands. We demonstrate similar high-performance values on the independent validation cohort (AUC-ROC = 0.965). A comparison using the independent validation cohort with the Fleischner pulmonary nodule follow-up guidelines demonstrated a theoretical reduction in recommended follow-up imaging and procedures. CONCLUSIONS: Radiomic features extracted from the parenchyma surrounding lung nodules contain valid signals with spatial relevance for the task of lung cancer risk classification. Through standardization of feature extraction regions from the parenchyma, ML tool validation performance of 100% sensitivity and 96% specificity was achieved.",2019,10.1002/mp.13592,cross-sectional,diagnosis,CT,Lung
Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort,"OBJECTIVES: To investigate machine learning classifiers and interpretable models using chest CT for detection of COVID-19 and differentiation from other pneumonias, interstitial lung disease (ILD) and normal CTs. METHODS: Our retrospective multi-institutional study obtained 2446 chest CTs from 16 institutions (including 1161 COVID-19 patients). Training/validation/testing cohorts included 1011/50/100 COVID-19, 388/16/33 ILD, 189/16/33 other pneumonias, and 559/17/34 normal (no pathologies) CTs. A metric-based approach for the classification of COVID-19 used interpretable features, relying on logistic regression and random forests. A deep learning-based classifier differentiated COVID-19 via 3D features extracted directly from CT attenuation and probability distribution of airspace opacities. RESULTS: Most discriminative features of COVID-19 are the percentage of airspace opacity and peripheral and basal predominant opacities, concordant with the typical characterization of COVID-19 in the literature. Unsupervised hierarchical clustering compares feature distribution across COVID-19 and control cohorts. The metrics-based classifier achieved AUC = 0.83, sensitivity = 0.74, and specificity = 0.79 versus respectively 0.93, 0.90, and 0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19 pneumonia with manifestations that overlap with COVID-19, as well as mild COVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for other pneumonias, and 94% for no pathologies, which demonstrates the robustness of our method against different compositions of control groups. CONCLUSIONS: Our new method accurately discriminates COVID-19 from other types of pneumonia, ILD, and CTs with no pathologies, using quantitative imaging features derived from chest CT, while balancing interpretability of results and classification performance and, therefore, may be useful to facilitate diagnosis of COVID-19. KEY POINTS: • Unsupervised clustering reveals the key tomographic features including percent airspace opacity and peripheral and basal opacities most typical of COVID-19 relative to control groups. • COVID-19-positive CTs were compared with COVID-19-negative chest CTs (including a balanced distribution of non-COVID-19 pneumonia, ILD, and no pathologies). Classification accuracies for COVID-19, pneumonia, ILD, and CT scans with no pathologies are respectively 90%, 64%, 91%, and 94%. • Our deep learning (DL)-based classification method demonstrates an AUC of 0.93 (sensitivity 90%, specificity 83%). Machine learning methods applied to quantitative chest CT metrics can therefore improve diagnostic accuracy in suspected COVID-19, particularly in resource-constrained environments.",2021,10.1007/s00330-021-07937-3,cross-sectional,diagnosis,CT,Lung
Machine Learning Based Clinical Decision Support System for Early COVID-19 Mortality Prediction,"The coronavirus disease 2019 (COVID-19), caused by the virus SARS-CoV-2, is an acute respiratory disease that has been classified as a pandemic by the World Health Organization (WHO). The sudden spike in the number of infections and high mortality rates have put immense pressure on the public healthcare systems. Hence, it is crucial to identify the key factors for mortality prediction to optimize patient treatment strategy. Different routine blood test results are widely available compared to other forms of data like X-rays, CT-scans, and ultrasounds for mortality prediction. This study proposes machine learning (ML) methods based on blood tests data to predict COVID-19 mortality risk. A powerful combination of five features: neutrophils, lymphocytes, lactate dehydrogenase (LDH), high-sensitivity C-reactive protein (hs-CRP), and age helps to predict mortality with 96% accuracy. Various ML models (neural networks, logistic regression, XGBoost, random forests, SVM, and decision trees) have been trained and performance compared to determine the model that achieves consistently high accuracy across the days that span the disease. The best performing method using XGBoost feature importance and neural network classification, predicts with an accuracy of 90% as early as 16 days before the outcome. Robust testing with three cases based on days to outcome confirms the strong predictive performance and practicality of the proposed model. A detailed analysis and identification of trends was performed using these key biomarkers to provide useful insights for intuitive application. This study provide solutions that would help accelerate the decision-making process in healthcare systems for focused medical treatments in an accurate, early, and reliable manner.",2021,10.3389/fpubh.2021.626697,,,,
Machine learning based differentiation of glioblastoma from brain metastasis using MRI derived radiomics,"Few studies have addressed radiomics based differentiation of Glioblastoma (GBM) and intracranial metastatic disease (IMD). However, the effect of different tumor masks, comparison of single versus multiparametric MRI (mp-MRI) or select combination of sequences remains undefined. We cross-compared multiple radiomics based machine learning (ML) models using mp-MRI to determine optimized configurations. Our retrospective study included 60 GBM and 60 IMD patients. Forty-five combinations of ML models and feature reduction strategies were assessed for features extracted from whole tumor and edema masks using mp-MRI [T1W, T2W, T1-contrast enhanced (T1-CE), ADC, FLAIR], individual MRI sequences and combined T1-CE and FLAIR sequences. Model performance was assessed using receiver operating characteristic curve. For mp-MRI, the best model was LASSO model fit using full feature set (AUC 0.953). FLAIR was the best individual sequence (LASSO-full feature set, AUC 0.951). For combined T1-CE/FLAIR sequence, adaBoost-full feature set was the best performer (AUC 0.951). No significant difference was seen between top models across all scenarios, including models using FLAIR only, mp-MRI and combined T1-CE/FLAIR sequence. Top features were extracted from both the whole tumor and edema masks. Shape sphericity is an important discriminating feature.",2021,10.1038/s41598-021-90032-w,,,,
Machine learning based on clinical characteristics and chest CT quantitative measurements for prediction of adverse clinical outcomes in hospitalized patients with COVID-19,"OBJECTIVES: To develop and validate a machine learning model for the prediction of adverse outcomes in hospitalized patients with COVID-19. METHODS: We included 424 patients with non-severe COVID-19 on admission from January 17, 2020, to February 17, 2020, in the primary cohort of this retrospective multicenter study. The extent of lung involvement was quantified on chest CT images by a deep learning-based framework. The composite endpoint was the occurrence of severe or critical COVID-19 or death during hospitalization. The optimal machine learning classifier and feature subset were selected for model construction. The performance was further tested in an external validation cohort consisting of 98 patients. RESULTS: There was no significant difference in the prevalence of adverse outcomes (8.7% vs. 8.2%, p = 0.858) between the primary and validation cohorts. The machine learning method extreme gradient boosting (XGBoost) and optimal feature subset including lactic dehydrogenase (LDH), presence of comorbidity, CT lesion ratio (lesion%), and hypersensitive cardiac troponin I (hs-cTnI) were selected for model construction. The XGBoost classifier based on the optimal feature subset performed well for the prediction of developing adverse outcomes in the primary and validation cohorts, with AUCs of 0.959 (95% confidence interval [CI]: 0.936-0.976) and 0.953 (95% CI: 0.891-0.986), respectively. Furthermore, the XGBoost classifier also showed clinical usefulness. CONCLUSIONS: We presented a machine learning model that could be effectively used as a predictor of adverse outcomes in hospitalized patients with COVID-19, opening up the possibility for patient stratification and treatment allocation. KEY POINTS: • Developing an individually prognostic model for COVID-19 has the potential to allow efficient allocation of medical resources. • We proposed a deep learning-based framework for accurate lung involvement quantification on chest CT images. • Machine learning based on clinical and CT variables can facilitate the prediction of adverse outcomes of COVID-19.",2021,10.1007/s00330-021-07957-z,cross-sectional,prognosis,CT,Lung
Machine learning based on clinico-biological features integrated (18)F-FDG PET/CT radiomics for distinguishing squamous cell carcinoma from adenocarcinoma of lung,"PURPOSE: To develop and validate a clinico-biological features and (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) radiomic-based nomogram via machine learning for the pretherapy prediction of discriminating between adenocarcinoma (ADC) and squamous cell carcinoma (SCC) in non-small cell lung cancer (NSCLC). METHODS: A total of 315 NSCLC patients confirmed by postoperative pathology between January 2017 and June 2019 were retrospectively analyzed and randomly divided into the training (n = 220) and validation (n = 95) sets. Preoperative clinical factors, serum tumor markers, and PET, and CT radiomic features were analyzed. Prediction models were developed using the least absolute shrinkage and selection operator (LASSO) regression analysis. The performance of the models was evaluated and compared by the area under receiver-operator characteristic (ROC) curve (AUC) and DeLong test. The clinical utility of the models was determined via decision curve analysis (DCA). Then, a nomogram was developed based on the model with the best predictive efficiency and clinical utility and was validated using the calibration plots. RESULTS: In total, 122 SCC and 193 ADC patients were enrolled in this study. Four independent prediction models were separately developed to differentiate SCC from ADC using clinical factors-tumor markers, PET radiomics, CT radiomics, and their combination. The DeLong test and DCA showed that the Combined Model, consisting of 2 clinical factors, 2 tumor markers, 7 PET radiomics, and 3 CT radiomic parameters, held the highest predictive efficiency and clinical utility in predicting the NSCLC subtypes compared with the use of these parameters alone in both the training and validation sets (AUCs (95% CIs) = 0.932 (0.900-0.964), 0.901 (0.840-0.957), respectively) (p < 0.05). A quantitative nomogram was subsequently constructed using the independently risk factors from the Combined Model. The calibration curves indicated a good consistency between the actual observations and nomogram predictions. CONCLUSION: This study presents an integrated clinico-biologico-radiological nomogram that can be accurately and noninvasively used for the individualized differentiation SCC from ADC in NSCLC, thereby assisting in clinical decision making for precision treatment.",2021,10.1007/s00259-020-05065-6,cross-sectional,diagnosis,CT,Lung
Machine learning helps identifying volume-confounding effects in radiomics,Highlighting the risk of biases in radiomics-based models will help improve their quality and increase usage as decision support systems in the clinic. In this study we use machine learning-based methods to identify the presence of volume-confounding effects in radiomics features. Methods 841 radiomics features were extracted from two retrospective publicly available datasets of lung and head neck cancers using open source software. Unsupervised hierarchical clustering and principal component analysis (PCA) identified relations between radiomics and clinical outcomes (overall survival). Bootstrapping techniques with logistic regression verified features' prognostic power and robustness. Results Over 80% of the features had large pairwise correlations. Nearly 30% of the features presented strong correlations with tumor volume. Using volume-independent features for clustering and PCA did not allow risk stratification of patients. Clinical predictors outperformed radiomics features in bootstrapping and logistic regression. Conclusions The adoption of safeguards in radiomics is imperative to improve the quality of radiomics studies. We proposed machine learning (ML) - based methods for robust radiomics signatures development.,2020,10.1016/j.ejmp.2020.02.010,,,,
Machine learning in the prediction of cardiac epicardial and mediastinal fat volumes,"We propose a methodology to predict the cardiac epicardial and mediastinal fat volumes in computed tomography images using regression algorithms. The obtained results indicate that it is feasible to predict these fats with a high degree of correlation, thus alleviating the requirement for manual or automatic segmentation of both fat volumes. Instead, segmenting just one of them suffices, while the volume of the other may be predicted fairly precisely. The correlation coefficient obtained by the Rotation Forest algorithm using MLP Regressor for predicting the mediastinal fat based on the epicardial fat was 0.9876, with a relative absolute error of 14.4% and a root relative squared error of 15.7%. The best correlation coefficient obtained in the prediction of the epicardial fat based on the mediastinal was 0.9683 with a relative absolute error of 19.6% and a relative squared error of 24.9%. Moreover, we analysed the feasibility of using linear regressors, which provide an intuitive interpretation of the underlying approximations. In this case, the obtained correlation coefficient was 0.9534 for predicting the mediastinal fat based on the epicardial, with a relative absolute error of 31.6% and a root relative squared error of 30.1%. On the prediction of the epicardial fat based on the mediastinal fat, the correlation coefficient was 0.8531, with a relative absolute error of 50.43% and a root relative squared error of 52.06%. In summary, it is possible to speed up general medical analyses and some segmentation and quantification methods that are currently employed in the state-of-the-art by using this prediction approach, which consequently reduces costs and therefore enables preventive treatments that may lead to a reduction of health problems.",2017,10.1016/j.compbiomed.2017.02.010,cross-sectional,informatics,CT,Heart
Machine Learning methods for Quantitative Radiomic Biomarkers,"Radiomics extracts and mines large number of medical imaging features quantifying tumor phenotypic characteristics. Highly accurate and reliable machine-learning approaches can drive the success of radiomic applications in clinical care. In this radiomic study, fourteen feature selection methods and twelve classification methods were examined in terms of their performance and stability for predicting overall survival. A total of 440 radiomic features were extracted from pre-treatment computed tomography (CT) images of 464 lung cancer patients. To ensure the unbiased evaluation of different machine-learning methods, publicly available implementations along with reported parameter configurations were used. Furthermore, we used two independent radiomic cohorts for training (n = 310 patients) and validation (n = 154 patients). We identified that Wilcoxon test based feature selection method WLCX (stability = 0.84 ± 0.05, AUC = 0.65 ± 0.02) and a classification method random forest RF (RSD = 3.52%, AUC = 0.66 ± 0.03) had highest prognostic performance with high stability against data perturbation. Our variability analysis indicated that the choice of classification method is the most dominant source of performance variation (34.21% of total variance). Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice.",2015,10.1038/srep13087,cross-sectional,diagnosis,CT,Lung
Machine Learning Models for Sarcopenia Identification Based on Radiomic Features of Muscles in Computed Tomography,"The diagnosis of sarcopenia requires accurate muscle quantification. As an alternative to manual muscle mass measurement through computed tomography (CT), artificial intelligence can be leveraged for the automation of these measurements. Although generally difficult to identify with the naked eye, the radiomic features in CT images are informative. In this study, the radiomic features were extracted from L3 CT images of the entire muscle area and partial areas of the erector spinae collected from non-small cell lung carcinoma (NSCLC) patients. The first-order statistics and gray-level co-occurrence, gray-level size zone, gray-level run length, neighboring gray-tone difference, and gray-level dependence matrices were the radiomic features analyzed. The identification performances of the following machine learning models were evaluated: logistic regression, support vector machine (SVM), random forest, and extreme gradient boosting (XGB). Sex, coarseness, skewness, and cluster prominence were selected as the relevant features effectively identifying sarcopenia. The XGB model demonstrated the best performance for the entire muscle, whereas the SVM was the worst-performing model. Overall, the models demonstrated improved performance for the entire muscle compared to the erector spinae. Although further validation is required, the radiomic features presented here could become reliable indicators for quantifying the phenomena observed in the muscles of NSCLC patients, thus facilitating the diagnosis of sarcopenia.",2021,10.3390/ijerph18168710,,,,
Machine learning predictive model for severe COVID-19,"To develop a modified predictive model for severe COVID-19 in people infected with Sars-Cov-2. We developed the predictive model for severe patients of COVID-19 based on the clinical date from the Tumor Center of Union Hospital affiliated with Tongji Medical College, China. A total of 151 cases from Jan. 26 to Mar. 20, 2020, were included. Then we followed 5 steps to predict and evaluate the model: data preprocessing, data splitting, feature selection, model building, prevention of overfitting, and Evaluation, and combined with artificial neural network algorithms. We processed the results in the 5 steps. In feature selection, ALB showed a strong negative correlation (r = 0.771, P < 0.001) whereas GLB (r = 0.661, P < 0.001) and BUN (r = 0.714, P < 0.001) showed a strong positive correlation with severity of COVID-19. TensorFlow was subsequently applied to develop a neural network model. The model achieved good prediction performance, with an area under the curve value of 0.953(0.889-0.982). Our results showed its outstanding performance in prediction. GLB and BUN may be two risk factors for severe COVID-19. Our findings could be of great benefit in the future treatment of patients with COVID-19 and will help to improve the quality of care in the long term. This model has great significance to rationalize early clinical interventions and improve the cure rate.",2021,10.1016/j.meegid.2021.104737,cross-sectional,prognosis,Radiograph,Lung
Machine Learning Radiomics Model for Early Identification of Small-Cell Lung Cancer on Computed Tomography Scans,"PURPOSE: Small-cell lung cancer (SCLC) is the deadliest form of lung cancer, partly because of its short doubling time. Delays in imaging identification and diagnosis of nodules create a risk for stage migration. The purpose of our study was to determine if a machine learning radiomics model can detect SCLC on computed tomography (CT) among all nodules at least 1 cm in size. MATERIALS AND METHODS: Computed tomography scans from a single institution were selected and resampled to 1 × 1 × 1 mm. Studies were divided into SCLC and other scans comprising benign, adenocarcinoma, and squamous cell carcinoma that were segregated into group A (noncontrast scans) and group B (contrast-enhanced scans). Four machine learning classification models, support vector classifier, random forest (RF), XGBoost, and logistic regression, were used to generate radiomic models using 59 quantitative first-order and texture Imaging Biomarker Standardization Initiative compliant PyRadiomics features, which were found to be robust between two segmenters with minimum Redundancy Maximum Relevance feature selection within each leave-one-out-cross-validation to avoid overfitting. The performance was evaluated using a receiver operating characteristic curve. A final model was created using the RF classifier and aggregate minimum Redundancy Maximum Relevance to determine feature importance. RESULTS: A total of 103 studies were included in the analysis. The area under the receiver operating characteristic curve for RF, support vector classifier, XGBoost, and logistic regression was 0.81, 0.77, 0.84, and 0.84 in group A, and 0.88, 0.87, 0.85, and 0.81 in group B, respectively. Nine radiomic features in group A and 14 radiomic features in group B were predictive of SCLC. Six radiomic features overlapped between groups A and B. CONCLUSION: A machine learning radiomics model may help differentiate SCLC from other lung lesions.",2021,10.1200/cci.21.00021,cross-sectional,diagnosis,CT,Lung
Machine Learning Techniques Applied to Dose Prediction in Computed Tomography Tests,"Increasingly more patients exposed to radiation from computed axial tomography (CT) will have a greater risk of developing tumors or cancer that are caused by cell mutation in the future. A minor dose level would decrease the number of these possible cases. However, this framework can result in medical specialists (radiologists) not being able to detect anomalies or lesions. This work explores a way of addressing these concerns, achieving the reduction of unnecessary radiation without compromising the diagnosis. We contribute with a novel methodology in the CT area to predict the precise radiation that a patient should be given to accomplish this goal. Specifically, from a real dataset composed of the dose data of over fifty thousand patients that have been classified into standardized protocols (skull, abdomen, thorax, pelvis, etc.), we eliminate atypical information (outliers), to later generate regression curves employing diverse well-known Machine Learning techniques. As a result, we have chosen the best analytical technique per protocol; a selection that was thoroughly carried out according to traditional dosimetry parameters to accurately quantify the dose level that the radiologist should apply in each CT test.",2019,10.3390/s19235116,cross-sectional,informatics,CT,Lung
Machine learning to distinguish lymphangioleiomyomatosis from other diffuse cystic lung diseases,"Patients with lymphangioleiomyomatosis (LAM) frequently experience delays in diagnosis, owing partly to the delayed characterization of imaging findings. This project aimed to develop a machine learning model to distinguish LAM from other diffuse cystic lung diseases (DCLDs). Computed tomography scans from patients with confirmed DCLDs were acquired from registry datasets and a recurrent convolutional neural network was trained for their classification. The final model provided sensitivity and specificity of 85% and 92%, respectively, for LAM, similar to the historical metrics of 88% and 97%, respectively, by experts. The proof-of-concept work holds promise as a clinically useful tool to assist in recognizing LAM.",2022,10.1016/j.resinv.2022.01.001,cross-sectional,diagnosis,CT,Lung
Machine learning to predict lung nodule biopsy method using CT image features: A pilot study,"Computed tomography (CT)-based screening on lung cancer mortality is poised to make lung nodule management a growing public health problem. Biopsy and pathologic analysis of suspicious nodules is necessary to ensure accurate diagnosis and appropriate intervention. Biopsy techniques vary as do the specialists that perform them and the ways lung nodule patients are referred and triaged. The largest dichotomy is between minimally invasive biopsy (MIB) and surgical biopsy (SB). Cases of unsuccessful MIB preceding a SB can result in considerable delay in definitive care with potentially an adverse impact on prognosis besides potentially avoidable healthcare expenditures. An automated method that predicts the optimal biopsy method for a given lung nodule could save time and healthcare costs by facilitating referral and triage patterns. To our knowledge, no such method has been published. Here, we used CT image features and radiologist-annotated semantic features to predict successful MIB in a way that has not been described before. Using data from the Lung Image Database Consortium image collection (LIDC-IDRI), we trained a logistic regression model to determine whether a MIB or SB procedure was used to diagnose lung cancer in a patient presenting with lung nodules. We found that in successful MIB cases, the nodules were significantly larger and more spiculated. Our model illustrates that using robust machine learning tools on easily accessible semantic and image data can predict whether a patient's nodule is best biopsied by MIB or SB. Pending further validation and optimization, clinicians could use our publicly accessible model to aid clinical decision-making.",2019,10.1016/j.compmedimag.2018.10.006,,,,
Machine Learning to Predict Mortality and Critical Events in a Cohort of Patients With COVID-19 in New York City: Model Development and Validation,"BACKGROUND: COVID-19 has infected millions of people worldwide and is responsible for several hundred thousand fatalities. The COVID-19 pandemic has necessitated thoughtful resource allocation and early identification of high-risk patients. However, effective methods to meet these needs are lacking. OBJECTIVE: The aims of this study were to analyze the electronic health records (EHRs) of patients who tested positive for COVID-19 and were admitted to hospitals in the Mount Sinai Health System in New York City; to develop machine learning models for making predictions about the hospital course of the patients over clinically meaningful time horizons based on patient characteristics at admission; and to assess the performance of these models at multiple hospitals and time points. METHODS: We used Extreme Gradient Boosting (XGBoost) and baseline comparator models to predict in-hospital mortality and critical events at time windows of 3, 5, 7, and 10 days from admission. Our study population included harmonized EHR data from five hospitals in New York City for 4098 COVID-19-positive patients admitted from March 15 to May 22, 2020. The models were first trained on patients from a single hospital (n=1514) before or on May 1, externally validated on patients from four other hospitals (n=2201) before or on May 1, and prospectively validated on all patients after May 1 (n=383). Finally, we established model interpretability to identify and rank variables that drive model predictions. RESULTS: Upon cross-validation, the XGBoost classifier outperformed baseline models, with an area under the receiver operating characteristic curve (AUC-ROC) for mortality of 0.89 at 3 days, 0.85 at 5 and 7 days, and 0.84 at 10 days. XGBoost also performed well for critical event prediction, with an AUC-ROC of 0.80 at 3 days, 0.79 at 5 days, 0.80 at 7 days, and 0.81 at 10 days. In external validation, XGBoost achieved an AUC-ROC of 0.88 at 3 days, 0.86 at 5 days, 0.86 at 7 days, and 0.84 at 10 days for mortality prediction. Similarly, the unimputed XGBoost model achieved an AUC-ROC of 0.78 at 3 days, 0.79 at 5 days, 0.80 at 7 days, and 0.81 at 10 days. Trends in performance on prospective validation sets were similar. At 7 days, acute kidney injury on admission, elevated LDH, tachypnea, and hyperglycemia were the strongest drivers of critical event prediction, while higher age, anion gap, and C-reactive protein were the strongest drivers of mortality prediction. CONCLUSIONS: We externally and prospectively trained and validated machine learning models for mortality and critical events for patients with COVID-19 at different time horizons. These models identified at-risk patients and uncovered underlying relationships that predicted outcomes.",2020,10.2196/24018,,,,
Machine learning-based CT radiomics features for the prediction of pulmonary metastasis in osteosarcoma,"OBJECTIVE: This study aims to build machine learning-based CT radiomic features to predict patients developing metastasis after osteosarcoma diagnosis. METHODS AND MATERIALS: This retrospective study has included 81 patients with a histopathological diagnosis of osteosarcoma. The entire dataset was divided randomly into training (60%) and test sets (40%). A data augmentation technique for the minority class was performed in the training set, along with feature's selection and model's training. The radiomic features were extracted from CT's image of the local osteosarcoma. Three frequently used machine learning models tried to predict patients with lung metastases (MT) and those without lung metastases (non-MT). According to the higher area under the curve (AUC), the best classifier was chosen and applied in the testing set with unseen data to provide an unbiased evaluation of the final model. RESULTS: The best classifier for predicting MT and non-MT groups used a Random Forest algorithm. The AUC and accuracy results of the test set were bulky (accuracy of 73% [ 95% coefficient interval (CI): 54%; 87%] and AUC of 0.79 [95% CI: 0.62; 0.96]). Features that fitted the model (radiomics signature) derived from Laplacian of Gaussian and wavelet filters. CONCLUSIONS: Machine learning-based CT radiomics approach can provide a non-invasive method with a fair predictive accuracy of the risk of developing pulmonary metastasis in osteosarcoma patients. ADVANCES IN KNOWLEDGE: Models based on CT radiomic analysis help assess the risk of developing pulmonary metastases in patients with osteosarcoma, allowing further studies for those with a worse prognosis.",2021,10.1259/bjr.20201391,,,,
Machine learning-based CT radiomics model distinguishes COVID-19 from non-COVID-19 pneumonia,"BACKGROUND: To develop a machine learning-based CT radiomics model is critical for the accurate diagnosis of the rapid spreading coronavirus disease 2019 (COVID-19). METHODS: In this retrospective study, a total of 326 chest CT exams from 134 patients (63 confirmed COVID-19 patients and 71 non-COVID-19 patients) were collected from January 20 to February 8, 2020. A semi-automatic segmentation procedure was used to delineate the volume of interest (VOI), and radiomic features were extracted. The Support Vector Machine (SVM) model was built on the combination of 4 groups of features, including radiomic features, traditional radiological features, quantifying features, and clinical features. By repeating cross-validation procedure, the performance on the time-independent testing cohort was evaluated by the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. RESULTS: For the SVM model built on the combination of 4 groups of features (integrated model), the per-exam AUC was 0.925 (95% CI 0.856 to 0.994) for differentiating COVID-19 on the testing cohort, and the sensitivity and specificity were 0.816 (95% CI 0.651 to 0.917) and 0.923 (95% CI 0.621 to 0.996), respectively. As for the SVM models built on radiomic features, radiological features, quantifying features, and clinical features, individually, the AUC on the testing cohort reached 0.765, 0.818, 0.607, and 0.739, respectively, significantly lower than the integrated model, except for the radiomic model. CONCLUSION: The machine learning-based CT radiomics models may accurately classify COVID-19, helping clinicians and radiologists to identify COVID-19 positive cases.",2021,10.1186/s12879-021-06614-6,cross-sectional,diagnosis,CT,Lung
Machine learning-based cytokine microarray digital immunoassay analysis,"Serial measurement of a large panel of protein biomarkers near the bedside could provide a promising pathway to transform the critical care of acutely ill patients. However, attaining the combination of high sensitivity and multiplexity with a short assay turnaround poses a formidable technological challenge. Here, the authors develop a rapid, accurate, and highly multiplexed microfluidic digital immunoassay by incorporating machine learning-based autonomous image analysis. The assay has achieved 12-plexed biomarker detection in sample volume <15 μL at concentrations < 5 pg/mL while only requiring a 5-min assay incubation, allowing for all processes from sampling to result to be completed within 40 min. The assay procedure applies both a spatial-spectral microfluidic encoding scheme and an image data analysis algorithm based on machine learning with a convolutional neural network (CNN) for pre-equilibrated single-molecule protein digital counting. This unique approach remarkably reduces errors facing the high-capacity multiplexing of digital immunoassay at low protein concentrations. Longitudinal data obtained for a panel of 12 serum cytokines in human patients receiving chimeric antigen receptor-T (CAR-T) cell therapy reveals the powerful biomarker profiling capability. The assay could also be deployed for near-real-time immune status monitoring of critically ill COVID-19 patients developing cytokine storm syndrome.",2021,10.1016/j.bios.2021.113088,,,,
Machine learning-based diagnostic method of pre-therapeutic (18)F-FDG PET/CT for evaluating mediastinal lymph nodes in non-small cell lung cancer,"OBJECTIVES: We aimed to find the best machine learning (ML) model using (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) for evaluating metastatic mediastinal lymph nodes (MedLNs) in non-small cell lung cancer, and compare the diagnostic results with those of nuclear medicine physicians. METHODS: A total of 1329 MedLNs were reviewed. Boosted decision tree, logistic regression, support vector machine, neural network, and decision forest models were compared. The diagnostic performance of the best ML model was compared with that of physicians. The ML method was divided into ML with quantitative variables only (MLq) and adding clinical information (MLc). We performed an analysis based on the (18)F-FDG-avidity of the MedLNs. RESULTS: The boosted decision tree model obtained higher sensitivity and negative predictive values but lower specificity and positive predictive values than the physicians. There was no significant difference between the accuracy of the physicians and MLq (79.8% vs. 76.8%, p = 0.067). The accuracy of MLc was significantly higher than that of the physicians (81.0% vs. 76.8%, p = 0.009). In MedLNs with low (18)F-FDG-avidity, ML had significantly higher accuracy than the physicians (70.0% vs. 63.3%, p = 0.018). CONCLUSION: Although there was no significant difference in accuracy between the MLq and physicians, the diagnostic performance of MLc was better than that of MLq or of the physicians. The ML method appeared to be useful for evaluating low metabolic MedLNs. Therefore, adding clinical information to the quantitative variables from (18)F-FDG PET/CT can improve the diagnostic results of ML. KEY POINTS: • Machine learning using two-class boosted decision tree model revealed the highest value of area under curve, and it showed higher sensitivity and negative predictive values but lower specificity and positive predictive values than nuclear medicine physicians. • The diagnostic results from machine learning method after adding clinical information to the quantitative variables improved accuracy significantly than nuclear medicine physicians. • Machine learning could improve the diagnostic significance of metastatic mediastinal lymph nodes, especially in mediastinal lymph nodes with low 18F-FDG-avidity.",2021,10.1007/s00330-020-07523-z,cross-sectional,diagnosis,CT,Lung
Machine Learning-Based Prediction of COVID-19 Severity and Progression to Critical Illness Using CT Imaging and Clinical Data,"OBJECTIVE: To develop a machine learning (ML) pipeline based on radiomics to predict Coronavirus Disease 2019 (COVID-19) severity and the future deterioration to critical illness using CT and clinical variables. MATERIALS AND METHODS: Clinical data were collected from 981 patients from a multi-institutional international cohort with real-time polymerase chain reaction-confirmed COVID-19. Radiomics features were extracted from chest CT of the patients. The data of the cohort were randomly divided into training, validation, and test sets using a 7:1:2 ratio. A ML pipeline consisting of a model to predict severity and time-to-event model to predict progression to critical illness were trained on radiomics features and clinical variables. The receiver operating characteristic area under the curve (ROC-AUC), concordance index (C-index), and time-dependent ROC-AUC were calculated to determine model performance, which was compared with consensus CT severity scores obtained by visual interpretation by radiologists. RESULTS: Among 981 patients with confirmed COVID-19, 274 patients developed critical illness. Radiomics features and clinical variables resulted in the best performance for the prediction of disease severity with a highest test ROC-AUC of 0.76 compared with 0.70 (0.76 vs. 0.70, p = 0.023) for visual CT severity score and clinical variables. The progression prediction model achieved a test C-index of 0.868 when it was based on the combination of CT radiomics and clinical variables compared with 0.767 when based on CT radiomics features alone (p < 0.001), 0.847 when based on clinical variables alone (p = 0.110), and 0.860 when based on the combination of visual CT severity scores and clinical variables (p = 0.549). Furthermore, the model based on the combination of CT radiomics and clinical variables achieved time-dependent ROC-AUCs of 0.897, 0.933, and 0.927 for the prediction of progression risks at 3, 5 and 7 days, respectively. CONCLUSION: CT radiomics features combined with clinical variables were predictive of COVID-19 severity and progression to critical illness with fairly high accuracy.",2021,10.3348/kjr.2020.1104,retrospective cohort,prognosis,CT,Lung
Machine learning-based prognostic modeling using clinical data and quantitative radiomic features from chest CT images in COVID-19 patients,"OBJECTIVE: To develop prognostic models for survival (alive or deceased status) prediction of COVID-19 patients using clinical data (demographics and history, laboratory tests, visual scoring by radiologists) and lung/lesion radiomic features extracted from chest CT images. METHODS: Overall, 152 patients were enrolled in this study protocol. These were divided into 106 training/validation and 46 test datasets (untouched during training), respectively. Radiomic features were extracted from the segmented lungs and infectious lesions separately from chest CT images. Clinical data, including patients' history and demographics, laboratory tests and radiological scores were also collected. Univariate analysis was first performed (q-value reported after false discovery rate (FDR) correction) to determine the most predictive features among all imaging and clinical data. Prognostic modeling of survival was performed using radiomic features and clinical data, separately or in combination. Maximum relevance minimum redundancy (MRMR) and XGBoost were used for feature selection and classification. The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC), sensitivity, specificity, and accuracy were used to assess the prognostic performance of the models on the test datasets. RESULTS: For clinical data, cancer comorbidity (q-value < 0.01), consciousness level (q-value < 0.05) and radiological score involved zone (q-value < 0.02) were found to have high correlated features with outcome. Oxygen saturation (AUC = 0.73, q-value < 0.01) and Blood Urea Nitrogen (AUC = 0.72, q-value = 0.72) were identified as high clinical features. For lung radiomic features, SAHGLE (AUC = 0.70) and HGLZE (AUC = 0.67) from GLSZM were identified as most prognostic features. Amongst lesion radiomic features, RLNU from GLRLM (AUC = 0.73), HGLZE from GLSZM (AUC = 0.73) had the highest performance. In multivariate analysis, combining lung, lesion and clinical features was determined to provide the most accurate prognostic model (AUC = 0.95 ± 0.029 (95%CI: 0.95-0.96), accuracy = 0.88 ± 0.046 (95% CI: 0.88-0.89), sensitivity = 0.88 ± 0.066 (95% CI = 0.87-0.9) and specificity = 0.89 ± 0.07 (95% CI = 0.87-0.9)). CONCLUSION: Combination of radiomic features and clinical data can effectively predict outcome in COVID-19 patients. The developed model has significant potential for improved management of COVID-19 patients.",2021,10.1016/j.compbiomed.2021.104304,retrospective cohort,prognosis,CT,Lung
Machine Learning-Based Radiomics for Prediction of Epidermal Growth Factor Receptor Mutations in Lung Adenocarcinoma,"Identifying an epidermal growth factor receptor (EGFR) mutation is important because EGFR tyrosine kinase inhibitors are the first-line treatment of choice for patients with EGFR mutation-positive lung adenocarcinomas (LUAC). This study is aimed at developing and validating a radiomics-based machine learning (ML) approach to identify EGFR mutations in patients with LUAC. We retrospectively collected data from 201 patients with positive EGFR mutation LUAC (140 in the training cohort and 61 in the validation cohort). We extracted 1316 radiomics features from preprocessed CT images and selected 14 radiomics features and 1 clinical feature which were most relevant to mutations through filter method. Subsequently, we built models using 7 ML approaches and established the receiver operating characteristic (ROC) curve to assess the discriminating performance of these models. In terms of predicting EGFR mutation, the model derived from radiomics features and combined models (radiomics features and relevant clinical factors) had an AUC of 0.79 (95% confidence interval (CI): 0.77-0.82), 0.86 (0.87-0.88), respectively. Our study offers a radiomics-based ML model using filter methods to detect the EGFR mutation in patients with LUAC. This convenient and low-cost method may be of help to noninvasively identify patients before obtaining tumor sample for molecule testing.",2022,10.1155/2022/2056837,cross-sectional,diagnosis,CT,Lung
Machine Learning-Based Radiomics Signatures for EGFR and KRAS Mutations Prediction in Non-Small-Cell Lung Cancer,"Early identification of epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene homolog (KRAS) mutations is crucial for selecting a therapeutic strategy for patients with non-small-cell lung cancer (NSCLC). We proposed a machine learning-based model for feature selection and prediction of EGFR and KRAS mutations in patients with NSCLC by including the least number of the most semantic radiomics features. We included a cohort of 161 patients from 211 patients with NSCLC from The Cancer Imaging Archive (TCIA) and analyzed 161 low-dose computed tomography (LDCT) images for detecting EGFR and KRAS mutations. A total of 851 radiomics features, which were classified into 9 categories, were obtained through manual segmentation and radiomics feature extraction from LDCT. We evaluated our models using a validation set consisting of 18 patients derived from the same TCIA dataset. The results showed that the genetic algorithm plus XGBoost classifier exhibited the most favorable performance, with an accuracy of 0.836 and 0.86 for detecting EGFR and KRAS mutations, respectively. We demonstrated that a noninvasive machine learning-based model including the least number of the most semantic radiomics signatures could robustly predict EGFR and KRAS mutations in patients with NSCLC.",2021,10.3390/ijms22179254,cross-sectional,diagnosis,CT,Lung
Machine learning-based radiomics strategy for prediction of cell proliferation in non-small cell lung cancer,"PURPOSE: To explore the feasibility and performance of machine learning-based radiomics classifier to predict the cell proliferation(Ki-67)in non-small cell lung cancer (NSCLC). METHODS: 245 histopathological confirmed NSCLC patients who underwent CT scans were retrospectively included. The Ki-67 proliferation index (Ki-67 PI) were measured within 2 weeks after CT scans. A lesion volume of interest (VOI) was manually delineated and radiomics features were extracted by MaZda software from CT images. A random forest feature selection algorithm (RFFS) was used to reduce features. Six kinds of machine learning methods were used to establish radiomics classifiers, subjective imaging feature classifiers and combined classifiers, respectively. The performance of these classifiers was evaluated by the receiver operating characteristic curve (ROC) and compared with Delong test. RESULTS: 103 radiomics features were extracted and 20 optimal features were selected using RFFS. Among the radiomics classifiers established by six machine learning methods, random forest-based radiomics classifier achieved the best performance (AUC = 0.776) in predicting the Ki-67 expression level with sensitivity and specificity of 0.726 and 0.661, which was better than that of subjective imaging classifiers (AUC = 0.625, P < 0.05). However, the combined classifiers did not improve the predictive performance (AUC = 0.780, P > 0.05), with sensitivity and specificity of 0.752 and 0.633. CONCLUSIONS: The machine learning-based CT radiomics classifier in NSCLC can facilitate the prediction of the expression level of Ki-67 and provide a novel non-invasive strategy for assessing the cell proliferation.",2019,10.1016/j.ejrad.2019.06.025,retrospective cohort,prognosis,CT,Lung
Malignant-benign classification of pulmonary nodules based on random forest aided by clustering analysis,"To help the radiologists better differentiate the benign from malignant pulmonary nodules on CT images, a novel classification scheme was proposed to improve the performance of benign and malignant classifier of pulmonary nodules. First, the pulmonary nodules were segmented with the references to the results from four radiologists. Then, some basic features of the segmented nodules such as the shape, gray and texture are given by calculation. Finally, malignant-benign classification of pulmonary nodules is performed by using random forest (RF) with the aid of clustering analysis. The data with a set of 952 nodules have been collected from lung image database consortium (LIDC). The effect of proposed classification scheme was verified by three experiments, in which the variant composite rank of malignancy were got from four radiologists (experiment 1: rank of malignancy '1', '2' as benign and '4', '5' as malignant; experiment 2: rank of malignancy '1', '2', '3' as benign and '4', '5' as malignant; experiment 3: rank of malignancy '1', '2' as benign and '3', '4', '5' as malignant) and the corresponding ([Formula: see text]) (area under the receiver operating characteristic curve) are 0.9702, 0.9190 and 0.8662, respectively. It can be drawn that the method in this work can greatly improve the accuracy of the classification of benign and malignant pulmonary nodules based on CT images.",2019,10.1088/1361-6560/aafab0,cross-sectional,diagnosis,CT,Lung
MAMA Net: Multi-Scale Attention Memory Autoencoder Network for Anomaly Detection,"Anomaly detection refers to the identification of cases that do not conform to the expected pattern, which takes a key role in diverse research areas and application domains. Most of existing methods can be summarized as anomaly object detection-based and reconstruction error-based techniques. However, due to the bottleneck of defining encompasses of real-world high-diversity outliers and inaccessible inference process, individually, most of them have not derived groundbreaking progress. To deal with those imperfectness, and motivated by memory-based decision-making and visual attention mechanism as a filter to select environmental information in human vision perceptual system, in this paper, we propose a Multi-scale Attention Memory with hash addressing Autoencoder network (MAMA Net) for anomaly detection. First, to overcome a battery of problems result from the restricted stationary receptive field of convolution operator, we coin the multi-scale global spatial attention block which can be straightforwardly plugged into any networks as sampling, upsampling and downsampling function. On account of its efficient features representation ability, networks can achieve competitive results with only several level blocks. Second, it's observed that traditional autoencoder can only learn an ambiguous model that also reconstructs anomalies ""well"" due to lack of constraints in training and inference process. To mitigate this challenge, we design a hash addressing memory module that proves abnormalities to produce higher reconstruction error for classification. In addition, we couple the mean square error (MSE) with Wasserstein loss to improve the encoding data distribution. Experiments on various datasets, including two different COVID-19 datasets and one brain MRI (RIDER) dataset prove the robustness and excellent generalization of the proposed MAMA Net.",2021,10.1109/tmi.2020.3045295,cross-sectional,diagnosis,Radiograph,Lung
Managing tumor changes during radiotherapy using a deep learning model,"PURPOSE: We propose a treatment planning framework that accounts for weekly lung tumor shrinkage using cone beam computed tomography (CBCT) images with a deep learning-based model. METHODS: Sixteen patients with non-small-cell lung cancer (NSCLC) were selected with one planning CT and six weekly CBCTs each. A deep learning-based model was applied to predict the weekly deformation of the primary tumor based on the spatial and temporal features extracted from previous weekly CBCTs. Starting from Week 3, the tumor contour at Week N was predicted by the model based on the input from all the previous weeks (1, 2 … N - 1), and was evaluated against the manually contoured tumor using Dice coefficient (DSC), precision, average surface distance (ASD), and Hausdorff distance (HD). Information about the predicted tumor was then entered into the treatment planning system and the plan was re-optimized every week. The objectives were to maximize the dose coverage in the target region while minimizing the toxicity to the surrounding healthy tissue. Dosimetric evaluation of the target and organs at risk (heart, lung, esophagus, and spinal cord) was performed on four cases, comparing between a conventional plan (ignoring tumor shrinkage) and the shrinkage-based plan. RESULTS: he primary tumor volumes decreased on average by 38% ± 26% during six weeks of treatment. DSCs and ASD between the predicted tumor and the actual tumor for Weeks 3, 4, 5, 6 were 0.81, 0.82, 0.79, 0.78 and 1.49, 1.59, 1.92, 2.12 mm, respectively, which were significantly superior to the score of 0.70, 0.68, 0.66, 0.63 and 2.81, 3.22, 3.69, 3.63 mm between the rigidly transferred tumors ignoring shrinkage and the actual tumor. While target coverage metrics were maintained for the re-optimized plans, lung mean dose dropped by 2.85, 0.46, 2.39, and 1.48 Gy for four sample cases when compared to the original plan. Doses in other organs such as esophagus were also reduced for some cases. CONCLUSION: We developed a deep learning-based model for tumor shrinkage prediction. This model used CBCTs and contours from previous weeks as input and produced reasonable tumor contours with a high prediction accuracy (DSC, precision, HD, and ASD). The proposed framework maintained target coverage while reducing dose in the lungs and esophagus.",2021,10.1002/mp.14925,retrospective cohort,treatment,Cone Beam CT (CBCT),Lung
MANORAA: A machine learning platform to guide protein-ligand design by anchors and influential distances,"The MANORAA platform uses structure-based approaches to provide information on drug design originally derived from mapping tens of thousands of amino acids on a grid. In-depth analyses of the pockets, frequently occurring atoms, influential distances, and active-site boundaries are used for the analysis of active sites. The algorithms derived provide model equations that can predict whether changes in distances, such as contraction or expansion, will result in improved binding affinity. The algorithm is confirmed using kinetic studies of dihydrofolate reductase (DHFR), together with two DHFR-TS crystal structures. Empirical analyses of 881 crystal structures involving 180 ligands are used to interpret protein-ligand binding affinities. MANORAA links to major biological databases for web-based analysis of drug design. The frequency of atoms inside the main protease structures, including those from SARS-CoV-2, shows how the rigid part of the ligand can be used as a probe for molecular design (http://manoraa.org).",2022,10.1016/j.str.2021.09.004,,,,
Marginal radiomics features as imaging biomarkers for pathological invasion in lung adenocarcinoma,"OBJECTIVES: Lung adenocarcinomas which manifest as ground-glass nodules (GGNs) have different degrees of pathological invasion and differentiating among them is critical for treatment. Our goal was to evaluate the addition of marginal features to a baseline radiomics model on computed tomography (CT) images to predict the degree of pathologic invasiveness. METHODS: We identified 236 patients from two cohorts (training, n = 189; validation, n = 47) who underwent surgery for GGNs. All GGNs were pathologically confirmed as adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IA). The regions of interest were semi-automatically annotated and 40 radiomics features were computed. We selected features using L1-norm regularization to build the baseline radiomics model. Additional marginal features were developed using the cumulative distribution function (CDF) of intratumoral intensities. An improved model was built combining the baseline model with CDF features. Three classifiers were tested for both models. RESULTS: The baseline radiomics model included five features and resulted in an average area under the curve (AUC) of 0.8419 (training) and 0.9142 (validation) for the three classifiers. The second model, with the additional marginal features, resulted in AUCs of 0.8560 (training) and 0.9581 (validation). All three classifiers performed better with the added features. The support vector machine showed the most performance improvement (AUC improvement = 0.0790) and the best performance was achieved by the logistic classifier (validation AUC = 0.9825). CONCLUSION: Our novel marginal features, when combined with a baseline radiomics model, can help differentiate IA from AIS and MIA on preoperative CT scans. KEY POINTS: • Our novel marginal features could improve the existing radiomics model to predict the degree of pathologic invasiveness in lung adenocarcinoma.",2020,10.1007/s00330-019-06581-2,retrospective cohort,prognosis,CT,Lung
Marker-free automated histopathological annotation of lung tumour subtypes by FTIR imaging,"By integration of FTIR imaging and a novel trained random forest classifier, lung tumour classes and subtypes of adenocarcinoma are identified in fresh-frozen tissue slides automated and marker-free. The tissue slices are collected under standard operation procedures within our consortium and characterized by current gold standards in histopathology. In addition, meta data of the patients are taken. The improved standards on sample collection and characterization results in higher accuracy and reproducibility as compared to former studies and allows here for the first time the identification of adenocarcinoma subtypes by this approach. The differentiation of subtypes is especially important for prognosis and therapeutic decision.",2015,10.1039/c4an01978d,,,,
MD-NDNet: a multi-dimensional convolutional neural network for false-positive reduction in pulmonary nodule detection,"Pulmonary nodule false-positive reduction is of great significance for automated nodule detection in clinical diagnosis of low-dose computed tomography (LDCT) lung cancer screening. Due to individual intra-nodule variations and visual similarities between true nodules and false positives as soft tissues in LDCT images, the current clinical practices remain subject to shortcomings of potential high-risk and time-consumption issues. In this paper, we propose a multi-dimensional nodule detection network (MD-NDNet) for automatic nodule false-positive reduction using deep convolutional neural network (DCNNs). The underlying method collaboratively integrates multi-dimensional nodule information to complementarily and comprehensively extract nodule inter-plane volumetric correlation features using three-dimensional CNNs (3D CNNs) and spatial nodule correlation features from sagittal, coronal, and axial planes using two-dimensional CNNs (2D CNNs) with attention module. To incorporate different sizes and shapes of nodule candidates, a multi-scale ensemble strategy is employed for probability aggregation with weights. The proposed method is evaluated on the LUNA16 challenge dataset in ISBI 2016 with ten-fold cross-validation. Experiment results show that the proposed framework achieves classification performance with a CPM score of 0.9008. All of these indicate that our method enables an efficient, accurate and reliable pulmonary nodule detection for clinical diagnosis.",2020,10.1088/1361-6560/aba87c,cross-sectional,diagnosis,CT,Lung
MEA-Net: multilayer edge attention network for medical image segmentation,"Medical image segmentation is a fundamental step in medical analysis and diagnosis. In recent years, deep learning networks have been used for precise segmentation. Numerous improved encoder-decoder structures have been proposed for various segmentation tasks. However, high-level features have gained more research attention than the abundant low-level features in the early stages of segmentation. Consequently, the learning of edge feature maps has been limited, which can lead to ambiguous boundaries of the predicted results. Inspired by the encoder-decoder network and attention mechanism, this study investigates a novel multilayer edge attention network (MEA-Net) to fully utilize the edge information in the encoding stages. MEA-Net comprises three major components: a feature encoder module, a feature decoder module, and an edge module. An edge feature extraction module in the edge module is designed to produce edge feature maps by a sequence of convolution operations so as to integrate the inconsistent edge information from different encoding stages. A multilayer attention guidance module is designed to use each attention feature map to filter edge information and select important and useful features. Through experiments, MEA-Net is evaluated on four medical image datasets, including tongue images, retinal vessel images, lung images, and clinical images. The evaluation values of the Accuracy of four medical image datasets are 0.9957, 0.9736, 0.9942, and 0.9993, respectively. The values of the Dice coefficient are 0.9902, 0.8377, 0.9885, and 0.9704, respectively. Experimental results demonstrate that the network being studied outperforms current state-of-the-art methods in terms of the five commonly used evaluation metrics. The proposed MEA-Net can be used for the early diagnosis of relevant diseases. In addition, clinicians can obtain more accurate clinical information from segmented medical images.",2022,10.1038/s41598-022-11852-y,case control,informatics,CT,Lung
Measurement of SARS-CoV-2 Antibody Titers Improves the Prediction Accuracy of COVID-19 Maximum Severity by Machine Learning in Non-Vaccinated Patients,"Numerous studies have suggested that the titers of antibodies against SARS-CoV-2 are associated with the COVID-19 severity, however, the types of antibodies associated with the disease maximum severity and the timing at which the associations are best observed, especially within one week after symptom onset, remain controversial. We attempted to elucidate the antibody responses against SARS-CoV-2 that are associated with the maximum severity of COVID-19 in the early phase of the disease, and to investigate whether antibody testing might contribute to prediction of the disease maximum severity in COVID-19 patients. We classified the patients into four groups according to the disease maximum severity (severity group 1 (did not require oxygen supplementation), severity group 2a (required oxygen supplementation at low flow rates), severity group 2b (required oxygen supplementation at relatively high flow rates), and severity group 3 (required mechanical ventilatory support)), and serially measured the titers of IgM, IgG, and IgA against the nucleocapsid protein, spike protein, and receptor-binding domain of SARS-CoV-2 until day 12 after symptom onset. The titers of all the measured antibody responses were higher in severity group 2b and 3, especially severity group 2b, as early as at one week after symptom onset. Addition of data obtained from antibody testing improved the ability of analysis models constructed using a machine learning technique to distinguish severity group 2b and 3 from severity group 1 and 2a. These models constructed with non-vaccinated COVID-19 patients could not be applied to the cases of breakthrough infections. These results suggest that antibody testing might help physicians identify non-vaccinated COVID-19 patients who are likely to require admission to an intensive care unit.",2022,10.3389/fimmu.2022.811952,,,,
Mediastinal lymph node detection and station mapping on chest CT using spatial priors and random forest,"PURPOSE: To develop an automated system for mediastinal lymph node detection and station mapping for chest CT. METHODS: The contextual organs, trachea, lungs, and spine are first automatically identified to locate the region of interest (ROI) (mediastinum). The authors employ shape features derived from Hessian analysis, local object scale, and circular transformation that are computed per voxel in the ROI. Eight more anatomical structures are simultaneously segmented by multiatlas label fusion. Spatial priors are defined as the relative multidimensional distance vectors corresponding to each structure. Intensity, shape, and spatial prior features are integrated and parsed by a random forest classifier for lymph node detection. The detected candidates are then segmented by the following curve evolution process. Texture features are computed on the segmented lymph nodes and a support vector machine committee is used for final classification. For lymph node station labeling, based on the segmentation results of the above anatomical structures, the textual definitions of mediastinal lymph node map according to the International Association for the Study of Lung Cancer are converted into patient-specific color-coded CT image, where the lymph node station can be automatically assigned for each detected node. RESULTS: The chest CT volumes from 70 patients with 316 enlarged mediastinal lymph nodes are used for validation. For lymph node detection, their system achieves 88% sensitivity at eight false positives per patient. For lymph node station labeling, 84.5% of lymph nodes are correctly assigned to their stations. CONCLUSIONS: Multiple-channel shape, intensity, and spatial prior features aggregated by a random forest classifier improve mediastinal lymph node detection on chest CT. Using the location information of segmented anatomic structures from the multiatlas formulation enables accurate identification of lymph node stations.",2016,10.1118/1.4954009,cross-sectional,diagnosis,CT,Lymph nodes
Medical Image Classification Algorithm Based on Visual Attention Mechanism-MCNN,"Due to the complexity of medical images, traditional medical image classification methods have been unable to meet the actual application needs. In recent years, the rapid development of deep learning theory has provided a technical approach for solving medical image classification. However, deep learning has the following problems in the application of medical image classification. First, it is impossible to construct a deep learning model with excellent performance according to the characteristics of medical images. Second, the current deep learning network structure and training strategies are less adaptable to medical images. Therefore, this paper first introduces the visual attention mechanism into the deep learning model so that the information can be extracted more effectively according to the problem of medical images, and the reasoning is realized at a finer granularity. It can increase the interpretability of the model. Additionally, to solve the problem of matching the deep learning network structure and training strategy to medical images, this paper will construct a novel multiscale convolutional neural network model that can automatically extract high-level discriminative appearance features from the original image, and the loss function uses the Mahalanobis distance optimization model to obtain a better training strategy, which can improve the robust performance of the network model. The medical image classification task is completed by the above method. Based on the above ideas, this paper proposes a medical classification algorithm based on a visual attention mechanism-multiscale convolutional neural network. The lung nodules and breast cancer images were classified by the method in this paper. The experimental results show that the accuracy of medical image classification in this paper is not only higher than that of traditional machine learning methods but also improved compared with other deep learning methods, and the method has good stability and robustness.",2021,10.1155/2021/6280690,,,,
Membrane-Based In-Gel Loop-Mediated Isothermal Amplification (mgLAMP) System for SARS-CoV-2 Quantification in Environmental Waters,"Since the COVID-19 pandemic is expected to become endemic, quantification of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) in ambient waters is critical for environmental surveillance and for early detection of outbreaks. Herein, we report the development of a membrane-based in-gel loop-mediated isothermal amplification (mgLAMP) system that is designed for the rapid point-of-use quantification of SARS-CoV-2 particles in environmental waters. The mgLAMP system integrates the viral concentration, in-assay viral lysis, and on-membrane hydrogel-based RT-LAMP quantification using enhanced fluorescence detection with a target-specific probe. With a sample-to-result time of less than 1 h, mgLAMP successfully detected SARS-CoV-2 below 0.96 copies/mL in Milli-Q water. In surface water, the lowest detected SARS-CoV-2 concentration was 93 copies/mL for mgLAMP, while the reverse transcription quantitative polymerase chain reaction (RT-qPCR) with optimal pretreatment was inhibited at 930 copies/mL. A 3D-printed portable device is designed to integrate heated incubation and fluorescence illumination for the simultaneous analysis of nine mgLAMP assays. Smartphone-based imaging and machine learning-based image processing are used for the interpretation of results. In this report, we demonstrate that mgLAMP is a promising method for large-scale environmental surveillance of SARS-CoV-2 without the need for specialized equipment, highly trained personnel, and labor-intensive procedures.",2022,10.1021/acs.est.1c04623,,,,
MHA-CoroCapsule: Multi-Head Attention Routing-Based Capsule Network for COVID-19 Chest X-Ray Image Classification,"The outbreak of COVID-19 threatens the lives and property safety of countless people and brings a tremendous pressure to health care systems worldwide. The principal challenge in the fight against this disease is the lack of efficient detection methods. AI-assisted diagnosis based on deep learning can detect COVID-19 cases for chest X-ray images automatically, and also improve the accuracy and efficiency of doctors' diagnosis. However, large scale annotation of chest X-ray images is difficult because of limited resources and heavy burden on the medical system. To meet the challenge, we propose a capsule network model with multi-head attention routing algorithm, called MHA-CoroCapsule, to provide fast and accurate diagnostics for COVID-19 diseases from chest X-ray images. The MHA-CoroCapsule consists of convolutional layers, two capsule layers, and a non-iterative, parameterized multi-head attention routing algorithm is used to quantify the relationship between the two capsule layers. The experiments are performed on a combined dataset constituted by two publicly available datasets including normal, non-COVID pneumonia and COVID-19 images. The model achieves the accuracy of 97.28%, recall of 97.36%, and precision of 97.38% even with a limited number of samples. The experimental results demonstrate that, contrary to the transfer learning and deep feature extraction approaches, the proposed MHA-CoroCapsule has an encouraging performance with fewer trainable parameters and does not require pretraining and plenty of training samples.",2022,10.1109/tmi.2021.3134270,cross-sectional,diagnosis,Radiograph,Lung
Microscopic handcrafted features selection from computed tomography scans for early stage lungs cancer diagnosis using hybrid classifiers,"Lung's cancer is the leading cause of cancer-related deaths worldwide. Recently cancer mortality rate and incidence increased exponentially. Many patients with lung cancer are diagnosed late, so the survival rate is shallow. Machine learning approaches have been widely used to increase the effectiveness of cancer detection at an early stage. Even while these methods are efficient in detecting specific forms of cancer, there is no known technique that could be used universally and consistently to identify new malignancies. As a result, cancer diagnosis via machine learning algorithms is still fresh area of research. Computed tomography (CT) images are frequently employed for early cancer detection and diagnosis because they contain significant information. In this research, an automated lung cancer detection and classification framework is proposed which consists of preprocessing, three patches local binary pattern feature encoding, local binary pattern, histogram of oriented gradients features are extracted and fused. The fast learning network (FLN) is a novel machine-learning technique that is fast to train and economical in terms of processing resources. However, the FLN's internal power parameters (weight and basis) are randomly initialized, resulting it an unstable algorithm. Therefore, to enhance accuracy, FLN is hybrid with K-nearest neighbors to classify texture and appearance-based features of lung chest CT scans from Kaggle dataset into cancerous and non-cancerous images. The proposed model performance is evaluated using accuracy, sensitivity, specificity on the Kaggle benchmark dataset that is found comparable in state of the art using simple machine learning strategies. RESEARCH HIGHLIGHTS: Fast learning network and K-nearest neighbor hybrid classifier proposed first time for lung cancer classification using handcrafted features including three patches local binary pattern, local binary pattern, and histogram of oriented gradients. Promising results obtained from novel simple combination.",2022,10.1002/jemt.24075,cross-sectional,diagnosis,CT,Lung
Microscopic segmentation and classification of COVID-19 infection with ensemble convolutional neural network,"The detection of biological RNA from sputum has a comparatively poor positive rate in the initial/early stages of discovering COVID-19, as per the World Health Organization. It has a different morphological structure as compared to healthy images, manifested by computer tomography (CT). COVID-19 diagnosis at an early stage can aid in the timely cure of patients, lowering the mortality rate. In this reported research, three-phase model is proposed for COVID-19 detection. In Phase I, noise is removed from CT images using a denoise convolutional neural network (DnCNN). In the Phase II, the actual lesion region is segmented from the enhanced CT images by using deeplabv3 and ResNet-18. In Phase III, segmented images are passed to the stack sparse autoencoder (SSAE) deep learning model having two stack auto-encoders (SAE) with the selected hidden layers. The designed SSAE model is based on both SAE and softmax layers for COVID19 classification. The proposed method is evaluated on actual patient data of Pakistan Ordinance Factories and other public benchmark data sets with different scanners/mediums. The proposed method achieved global segmentation accuracy of 0.96 and 0.97 for classification.",2022,10.1002/jemt.23913,cross-sectional,diagnosis,CT,Lung
"Microscopy-based assay for semi-quantitative detection of SARS-CoV-2 specific antibodies in human sera: A semi-quantitative, high throughput, microscopy-based assay expands existing approaches to measure SARS-CoV-2 specific antibody levels in human sera","Emergence of the novel pathogenic coronavirus SARS-CoV-2 and its rapid pandemic spread presents challenges that demand immediate attention. Here, we describe the development of a semi-quantitative high-content microscopy-based assay for detection of three major classes (IgG, IgA, and IgM) of SARS-CoV-2 specific antibodies in human samples. The possibility to detect antibodies against the entire viral proteome together with a robust semi-automated image analysis workflow resulted in specific, sensitive and unbiased assay that complements the portfolio of SARS-CoV-2 serological assays. Sensitive, specific and quantitative serological assays are urgently needed for a better understanding of humoral immune response against the virus as a basis for developing public health strategies to control viral spread. The procedure described here has been used for clinical studies and provides a general framework for the application of quantitative high-throughput microscopy to rapidly develop serological assays for emerging virus infections.",2021,10.1002/bies.202000257,,,,
Mini-COVIDNet: Efficient Lightweight Deep Neural Network for Ultrasound Based Point-of-Care Detection of COVID-19,"Lung ultrasound (US) imaging has the potential to be an effective point-of-care test for detection of COVID-19, due to its ease of operation with minimal personal protection equipment along with easy disinfection. The current state-of-the-art deep learning models for detection of COVID-19 are heavy models that may not be easy to deploy in commonly utilized mobile platforms in point-of-care testing. In this work, we develop a lightweight mobile friendly efficient deep learning model for detection of COVID-19 using lung US images. Three different classes including COVID-19, pneumonia, and healthy were included in this task. The developed network, named as Mini-COVIDNet, was bench-marked with other lightweight neural network models along with state-of-the-art heavy model. It was shown that the proposed network can achieve the highest accuracy of 83.2% and requires a training time of only 24 min. The proposed Mini-COVIDNet has 4.39 times less number of parameters in the network compared to its next best performing network and requires a memory of only 51.29 MB, making the point-of-care detection of COVID-19 using lung US imaging plausible on a mobile platform. Deployment of these lightweight networks on embedded platforms shows that the proposed Mini-COVIDNet is highly versatile and provides optimal performance in terms of being accurate as well as having latency in the same order as other lightweight networks. The developed lightweight models are available at https://github.com/navchetan-awasthi/Mini-COVIDNet.",2021,10.1109/tuffc.2021.3068190,cross-sectional,diagnosis,Ultrasound,Lung
Mining whole-lung information by artificial intelligence for predicting EGFR genotype and targeted therapy response in lung cancer: a multicohort study,"BACKGROUND: Epidermal growth factor receptor (EGFR) genotype is crucial for treatment decision making in lung cancer, but it can be affected by tumour heterogeneity and invasive biopsy during gene sequencing. Importantly, not all patients with an EGFR mutation have good prognosis with EGFR-tyrosine kinase inhibitors (TKIs), indicating the necessity of stratifying for EGFR-mutant genotype. In this study, we proposed a fully automated artificial intelligence system (FAIS) that mines whole-lung information from CT images to predict EGFR genotype and prognosis with EGFR-TKI treatment. METHODS: We included 18 232 patients with lung cancer with CT imaging and EGFR gene sequencing from nine cohorts in China and the USA, including a prospective cohort in an Asian population (n=891) and The Cancer Imaging Archive cohort in a White population. These cohorts were divided into thick CT group and thin CT group. The FAIS was built for predicting EGFR genotype and progression-free survival of patients receiving EGFR-TKIs, and it was evaluated by area under the curve (AUC) and Kaplan-Meier analysis. We further built two tumour-based deep learning models as comparison with the FAIS, and we explored the value of combining FAIS and clinical factors (the FAIS-C model). Additionally, we included 891 patients with 56-panel next-generation sequencing and 87 patients with RNA sequencing data to explore the biological mechanisms of FAIS. FINDINGS: FAIS achieved AUCs ranging from 0·748 to 0·813 in the six retrospective and prospective testing cohorts, outperforming the commonly used tumour-based deep learning model. Genotype predicted by the FAIS-C model was significantly associated with prognosis to EGFR-TKIs treatment (log-rank p<0·05), an important complement to gene sequencing. Moreover, we found 29 prognostic deep learning features in FAIS that were able to identify patients with an EGFR mutation at high risk of TKI resistance. These features showed strong associations with multiple genotypes (p<0·05, t test or Wilcoxon test) and gene pathways linked to drug resistance and cancer progression mechanisms. INTERPRETATION: FAIS provides a non-invasive method to detect EGFR genotype and identify patients with an EGFR mutation at high risk of TKI resistance. The superior performance of FAIS over tumour-based deep learning methods suggests that genotype and prognostic information could be obtained from the whole lung instead of only tumour tissues. FUNDING: National Natural Science Foundation of China.",2022,10.1016/s2589-7500(22)00024-3,cross-sectional,diagnosis,CT,Lung
Mix Contrast for COVID-19 Mild-to-Critical Prediction,"OBJECTIVE: In a few patients with mild COVID-19, there is a possibility of the infection becoming severe or critical in the future. This work aims to identify high-risk patients who have a high probability of changing from mild to critical COVID-19 (only account for 5% of cases). METHODS: Using traditional convolutional neural networks for classification may not be suitable to identify this 5% of high risk patients from an entire dataset due to the highly imbalanced label distribution. To address this problem, we propose a Mix Contrast model, which matches original features with mixed features for contrastive learning. Three modules are proposed for training the model: 1) a cumulative learning strategy for synthesizing the mixed feature; 2) a commutative feature combination module for learning the commutative law of feature concatenation; 3) a united pairwise loss assigning adaptive weights for sample pairs with different class anchors based on their current optimization status. RESULTS: We collect a multi-center computed tomography dataset including 918 confirmed COVID-19 patients from four hospitals and evaluate the proposed method on both the COVID-19 mild-to-critical prediction and COVID-19 diagnosis tasks. For mild-to-critical prediction, the experimental results show a recall of 0.80 and a specificity of 0.815. For diagnosis, the model shows comparable results with deep neural networks using a large dataset. Our method demonstrates improvements when the amount of training data is small or imbalanced. SIGNIFICANCE: Identifying mild-to-critical COVID-19 patients is important for early prevention and personalized treatment planning.",2021,10.1109/tbme.2021.3085576,retrospective cohort,prognosis,CT,Lung
Mix-and-Interpolate: A Training Strategy to Deal With Source-Biased Medical Data,"Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the data source bias problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",2022,10.1109/jbhi.2021.3119325,cross-sectional,diagnosis,CT,Lung
Modality alignment contrastive learning for severity assessment of COVID-19 from lung ultrasound and clinical information,"The outbreak of COVID-19 around the world has caused great pressure to the health care system, and many efforts have been devoted to artificial intelligence (AI)-based analysis of CT and chest X-ray images to help alleviate the shortage of radiologists and improve the diagnosis efficiency. However, only a few works focus on AI-based lung ultrasound (LUS) analysis in spite of its significant role in COVID-19. In this work, we aim to propose a novel method for severity assessment of COVID-19 patients from LUS and clinical information. Great challenges exist regarding the heterogeneous data, multi-modality information, and highly nonlinear mapping. To overcome these challenges, we first propose a dual-level supervised multiple instance learning module (DSA-MIL) to effectively combine the zone-level representations into patient-level representations. Then a novel modality alignment contrastive learning module (MA-CLR) is presented to combine representations of the two modalities, LUS and clinical information, by matching the two spaces while keeping the discriminative features. To train the nonlinear mapping, a staged representation transfer (SRT) strategy is introduced to maximumly leverage the semantic and discriminative information from the training data. We trained the model with LUS data of 233 patients, and validated it with 80 patients. Our method can effectively combine the two modalities and achieve accuracy of 75.0% for 4-level patient severity assessment, and 87.5% for the binary severe/non-severe identification. Besides, our method also provides interpretation of the severity assessment by grading each of the lung zone (with accuracy of 85.28%) and identifying the pathological patterns of each lung zone. Our method has a great potential in real clinical practice for COVID-19 patients, especially for pregnant women and children, in aspects of progress monitoring, prognosis stratification, and patient management.",2021,10.1016/j.media.2021.101975,retrospective cohort,diagnosis,CT,Lung
Model-Agnostic Method for Thoracic Wall Segmentation in Fetal Ultrasound Videos,"The application of segmentation methods to medical imaging has the potential to create novel diagnostic support models. With respect to fetal ultrasound, the thoracic wall is a key structure on the assessment of the chest region for examiners to recognize the relative orientation and size of structures inside the thorax, which are critical components in neonatal prognosis. In this study, to improve the segmentation performance of the thoracic wall in fetal ultrasound videos, we proposed a novel model-agnostic method using deep learning techniques: the Multi-Frame + Cylinder method (MFCY). The Multi-frame method (MF) uses time-series information of ultrasound videos, and the Cylinder method (CY) utilizes the shape of the thoracic wall. To evaluate the achieved improvement, we performed segmentation using five-fold cross-validation on 538 ultrasound frames in the four-chamber view (4CV) of 256 normal cases using U-net and DeepLabv3+. MFCY increased the mean values of the intersection over union (IoU) of thoracic wall segmentation from 0.448 to 0.493 for U-net and from 0.417 to 0.470 for DeepLabv3+. These results demonstrated that MFCY improved the segmentation performance of the thoracic wall in fetal ultrasound videos without altering the network structure. MFCY is expected to facilitate the development of diagnostic support models in fetal ultrasound by providing further accurate segmentation of the thoracic wall.",2020,10.3390/biom10121691,,,,
Models of Artificial Intelligence-Assisted Diagnosis of Lung Cancer Pathology Based on Deep Learning Algorithms,"In this article, in order to explore the application of a diagnosis system for lung cancer, we use an auxiliary diagnostic system to predict and diagnose the good and evil attributes of chest CT pulmonary nodules. This research improves the new diagnosis method based on the convolutional neural network (CNN) and the recurrent neural network (RNN) and combines the dual effects of the two algorithms to process the classification of benign and malignant nodules. By collecting H-E-stained pathological slices of 652 patients' lung lesions from two hospitals between January 2018 and January 2019, the output results of the improved 3D U-net system and the consistent results of two-person reading were compared. This article analyzes the sensitivity, specificity, positive flammability rate, and negative flammability rate of different lung nodule detection methods. In addition, the artificial intelligence system's and the radiologist's judgment results of benign and malignant pulmonary nodules are used to draw ROC curves for further analysis. The improved model has an accuracy rate of 92.3% for predicting malignant lung nodules and an accuracy rate of 82.8% for benign lung nodules. The new diagnostic method using the convolutional neural network and the recurrent neural network can be very effective for improving the accuracy of predicting lung cancer diagnosis. It can play a very effective role in the disease prediction of lung cancer patients, thereby improving the treatment effect.",2022,10.1155/2022/3972298,cross-sectional,diagnosis,CT,Lung
MR-Forest: A Deep Decision Framework for False Positive Reduction in Pulmonary Nodule Detection,"With the development of deep learning methods such as convolutional neural network (CNN), the accuracy of automated pulmonary nodule detection has been greatly improved. However, the high computational and storage costs of the large-scale network have been a potential concern for the future widespread clinical application. In this paper, an alternative Multi-ringed (MR)-Forest framework, against the resource-consuming neural networks (NN)-based architectures, has been proposed for false positive reduction in pulmonary nodule detection, which consists of three steps. First, a novel multi-ringed scanning method is used to extract the order ring facets (ORFs) from the surface voxels of the volumetric nodule models; Second, Mesh-LBP and mapping deformation are employed to estimate the texture and shape features. By sliding and resampling the multi-ringed ORFs, feature volumes with different lengths are generated. Finally, the outputs of multi-level are cascaded to predict the candidate class. On 1034 scans merging the dataset from the Affiliated Hospital of Liaoning University of Traditional Chinese Medicine (AH-LUTCM) and the LUNA16 Challenge dataset, our framework performs enough competitiveness than state-of-the-art in false positive reduction task (CPM score of 0.865). Experimental results demonstrate that MR-Forest is a successful solution to satisfy both resource-consuming and effectiveness for automated pulmonary nodule detection. The proposed MR-forest is a general architecture for 3D target detection, it can be easily extended in many other medical imaging analysis tasks, where the growth trend of the targeting object is approximated as a spheroidal expansion.",2020,10.1109/jbhi.2019.2947506,cross-sectional,diagnosis,CT,Lung
MRI Image Segmentation Model with Support Vector Machine Algorithm in Diagnosis of Solitary Pulmonary Nodule,"This study focused on the application value of MRI images processed by a Support Vector Machine (SVM) algorithm-based model in diagnosis of benign and malignant solitary pulmonary nodule (SPN). The SVM algorithm was constrained by a self-paced regularization item and gradient value to establish the MRI image segmentation model (SVM-L) for lung. Its performance was compared factoring into the Dice index (DI), sensitivity (SE), specificity (SP), and Mean Square Error (MSE). 28 SPN patients who underwent the parallel MRI examination were selected as research subjects and were divided into the benign group (11 patients) and malignant group (17 patients) according to different plans for diagnosis and treatment. The apparent diffusion coefficient (ADC) at different b values was analyzed, and the steepest slope (SS) and washout ratio (WR) values in the two groups were calculated. The result showed that the MSE, DI, SE, SP values, and operation time of the SVM-L model were (0.41 ± 0.02), (0.84 ± 0.13), (0.89 ± 0.04), (0.993 ± 0.004), and (30.69 ± 2.60)s, respectively, apparently superior to those of the other algorithms, but there were no statistic differences (P > 0.05) in the WR value between the two groups of patients. The SS values of the time-signal curve in the benign and malignant groups were (2.52 ± 0.69) %/s and (3.34 ± 00.41) %/s, respectively. Obviously, the SS value of the benign group was significantly lower than that of the malignant group (P < 0.01). The ADC value with different b values in the benign group was significantly lower than that of the malignant group (P < 0.01). It suggested that the SVM-L model significantly improved the quality of lung MRI images and increased the accuracy to differentiate benign and malignant SPN, providing reference for the diagnosis and treatment of SPN patients.",2021,10.1155/2021/9668836,case control,diagnosis,MRI,Lung
MRI-based radiomics analysis for predicting the EGFR mutation based on thoracic spinal metastases in lung adenocarcinoma patients,"PURPOSE: This study aims to develop and evaluate multi-parametric MRI-based radiomics for preoperative identification of epidermal growth factor receptor (EGFR) mutation, which is important in treatment planning for patients with thoracic spinal metastases from primary lung adenocarcinoma. METHODS: A total of 110 patients were enrolled between January 2016 and March 2019 as a primary cohort. A time-independent validation cohort was conducted containing 52 patients consecutively enrolled from July 2019 to April 2021. The patients were pathologically diagnosed with thoracic spinal metastases from primary lung adenocarcinoma; all underwent T1-weighted (T1W), T2-weighted (T2W), and T2-weighted fat-suppressed (T2FS) MRI scans of the thoracic spinal. Handcrafted and deep learning-based features were extracted and selected from each MRI modality, and used to build the radiomics signature. Various machine learning classifiers were developed and compared. A clinical-radiomics nomogram integrating the combined rad signature and the most important clinical factor was constructed with receiver operating characteristic (ROC), calibration, and decision curves analysis (DCA) to evaluate the prediction performance. RESULTS: The combined radiomics signature derived from the joint of three modalities can effectively classify EGFR mutation and EGFR wild-type patients, with an area under the ROC curve (AUC) of 0.886 (95% confidence interval [CI]: 0.826-0.947, SEN =0.935, SPE =0.688) in the training group and 0.803 (95% CI: 0.682-0.924, SEN = 0.700, SPE = 0.818) in the time-independent validation group. The nomogram incorporating the combined radiomics signature and smoking status achieved the best prediction performance in the training (AUC = 0.888, 95% CI: 0.849-0.958, SEN = 0.839, SPE = 0.792) and time-independent validation (AUC = 0.821, 95% CI: 0.692-0.929, SEN = 0.667, SPE = 0.909) cohorts. The DCA confirmed potential clinical usefulness of our nomogram. CONCLUSION: Our study demonstrated the potential of multi-parametric MRI-based radiomics on preoperatively predicting the EGFR mutation. The proposed nomogram model can be considered as a new biomarker to guide the selection of individual treatment strategies for patients with thoracic spinal metastases from primary lung adenocarcinoma.",2021,10.1002/mp.15137,,,,
MS-ResNet: disease-specific survival prediction using longitudinal CT images and clinical data,"PURPOSE: Medical imaging data of lung cancer in different stages contain a large amount of time information related to its evolution (emergence, development, or extinction). We try to explore the evolution process of lung images in time dimension to improve the prediction of lung cancer survival by using longitudinal CT images and clinical data jointly. METHODS: In this paper, we propose an innovative multi-branch spatiotemporal residual network (MS-ResNet) for disease-specific survival (DSS) prediction by integrating the longitudinal computed tomography (CT) images at different times and clinical data. Specifically, we first extract the deep features from the multi-period CT images by an improved residual network. Then, the feature selection algorithm is used to select the most relevant feature subset from the clinical data. Finally, we integrate the deep features and feature subsets to take full advantage of the complementarity between the two types of data to generate the final prediction results. RESULTS: The experimental results demonstrate that our MS-ResNet model is superior to other methods, achieving a promising 86.78% accuracy in the classification of short-survivor, med-survivor, and long-survivor. CONCLUSION: In computer-aided prognostic analysis of cancer, the time dimension features of the course of disease and the integration of patient clinical data and CT data can effectively improve the prediction accuracy.",2022,10.1007/s11548-022-02625-z,retrospective cohort,prognosis,CT,Lung
MSCS-DeepLN: Evaluating lung nodule malignancy using multi-scale cost-sensitive neural networks,"The accurate identification of malignant lung nodules using computed tomography (CT) screening images is vital for the early detection of lung cancer. It also offers patients the best chance of cure, because non-invasive CT imaging has the ability to capture intra-tumoral heterogeneity. Deep learning methods have obtained promising results for the malignancy identification problem; however, two substantial challenges still remain. First, small datasets cannot insufficiently train the model and tend to overfit it. Second, category imbalance in the data is a problem. In this paper, we propose a method called MSCS-DeepLN that evaluates lung nodule malignancy and simultaneously solves these two problems. Three light models are trained and combined to evaluate the malignancy of a lung nodule. Three-dimensional convolutional neural networks (CNNs) are employed as the backbone of each light model to extract the lung nodule features from CT images and preserve lung nodule spatial heterogeneity. Multi-scale input cropped from CT images enables the sub-networks to learn the multi-level contextual features and preserve diverse. To tackle the imbalance problem, our proposed method employs an AUC approximation as the penalty term. During training, the error in this penalty term is generated from each major and minor class pair, so that negatives and positives can contribute equally to updating this model. Based on these methods, we obtain state-of-the-art results on the LIDC-IDRI dataset. Furthermore, we constructed a new dataset collected from a grade-A tertiary hospital and annotated using biopsy-based cytological analysis to verify the performance of our method in clinical practice.",2020,10.1016/j.media.2020.101772,cross-sectional,diagnosis,CT,Lung
MSDS-UNet: A multi-scale deeply supervised 3D U-Net for automatic segmentation of lung tumor in CT,"Lung cancer is one of the most common and deadly malignant cancers. Accurate lung tumor segmentation from CT is therefore very important for correct diagnosis and treatment planning. The automated lung tumor segmentation is challenging due to the high variance in appearance and shape of the targeting tumors. To overcome the challenge, we present an effective 3D U-Net equipped with ResNet architecture and a two-pathway deep supervision mechanism to increase the network's capacity for learning richer representations of lung tumors from global and local perspectives. Extensive experiments on two real medical datasets: the lung CT dataset from Liaoning Cancer Hospital in China with 220 cases and the public dataset of TCIA with 422 cases. Our experiments demonstrate that our model achieves an average dice score (0.675), sensitivity (0.731) and F1-score (0.682) on the dataset from Liaoning Cancer Hospital, and an average dice score (0.691), sensitivity (0.746) and F1-score (0.724) on the TCIA dataset, respectively. The results demonstrate that the proposed 3D MSDS-UNet outperforms the state-of-the-art segmentation models for segmenting all scales of tumors, especially for small tumors. Moreover, we evaluated our proposed MSDS-UNet on another challenging volumetric medical image segmentation task: COVID-19 lung infection segmentation, which shows consistent improvement in the segmentation performance.",2021,10.1016/j.compmedimag.2021.101957,cross-sectional,informatics,CT,Lung
MTU-COVNet: A hybrid methodology for diagnosing the COVID-19 pneumonia with optimized features from multi-net,"PURPOSE: The aim of this study was to establish and evaluate a fully automatic deep learning system for the diagnosis of COVID-19 using thoracic computed tomography (CT). MATERIALS AND METHODS: In this retrospective study, a novel hybrid model (MTU-COVNet) was developed to extract visual features from volumetric thoracic CT scans for the detection of COVID-19. The collected dataset consisted of 3210 CT scans from 953 patients. Of the total 3210 scans in the final dataset, 1327 (41%) were obtained from the COVID-19 group, 929 (29%) from the CAP group, and 954 (30%) from the Normal CT group. Diagnostic performance was assessed with the area under the receiver operating characteristic (ROC) curve, sensitivity, and specificity. RESULTS: The proposed approach with the optimized features from concatenated layers reached an overall accuracy of 97.7% for the CT-MTU dataset. The rest of the total performance metrics, such as; specificity, sensitivity, precision, F1 score, and Matthew Correlation Coefficient were 98.8%, 97.6%, 97.8%, 97.7%, and 96.5%, respectively. This model showed high diagnostic performance in detecting COVID-19 pneumonia (specificity: 98.0% and sensitivity: 98.2%) and CAP (specificity: 99.1% and sensitivity: 97.1%). The areas under the ROC curves for COVID-19 and CAP were 0.997 and 0.996, respectively. CONCLUSION: A deep learning-based AI system built on the CT imaging can detect COVID-19 pneumonia with high diagnostic efficiency and distinguish it from CAP and normal CT. AI applications can have beneficial effects in the fight against COVID-19.",2022,10.1016/j.clinimag.2021.09.007,cross-sectional,diagnosis,CT,Lung
Multi-classifier-based identification of COVID-19 from chest computed tomography using generalizable and interpretable radiomics features,"PURPOSE: To investigate the efficacy of radiomics in diagnosing patients with coronavirus disease (COVID-19) and other types of viral pneumonia with clinical symptoms and CT signs similar to those of COVID-19. METHODS: Between 18 January 2020 and 20 May 2020, 110 SARS-CoV-2 positive and 108 SARS-CoV-2 negative patients were retrospectively recruited from three hospitals based on the inclusion criteria. Manual segmentation of pneumonia lesions on CT scans was performed by four radiologists. The latest version of Pyradiomics was used for feature extraction. Four classifiers (linear classifier, k-nearest neighbour, least absolute shrinkage and selection operator [LASSO], and random forest) were used to differentiate SARS-CoV-2 positive and SARS-CoV-2 negative patients. Comparison of the performance of the classifiers and radiologists was evaluated by ROC curve and Kappa score. RESULTS: We manually segmented 16,053 CT slices, comprising 32,625 pneumonia lesions, from the CT scans of all patients. Using Pyradiomics, 120 radiomic features were extracted from each image. The key radiomic features screened by different classifiers varied and lead to significant differences in classification accuracy. The LASSO achieved the best performance (sensitivity: 72.2%, specificity: 75.1%, and AUC: 0.81) on the external validation dataset and attained excellent agreement (Kappa score: 0.89) with radiologists (average sensitivity: 75.6%, specificity: 78.2%, and AUC: 0.81). All classifiers indicated that ""Original_Firstorder_RootMeanSquared"" and ""Original_Firstorder_Uniformity"" were significant features for this task. CONCLUSIONS: We identified radiomic features that were significantly associated with the classification of COVID-19 pneumonia using multiple classifiers. The quantifiable interpretation of the differences in features between the two groups extends our understanding of CT imaging characteristics of COVID-19 pneumonia.",2021,10.1016/j.ejrad.2021.109552,cross-sectional,diagnosis,CT,Lung
Multi-Dimension and Multi-Feature Hybrid Learning Network for Classifying the Sub Pathological Type of Lung Nodules through LDCT,"In order to develop appropriate treatment and rehabilitation plans with regard to different subpathological types (PILs and IAs) of lung nodules, it is important to diagnose them through low-dose spiral computed tomography (LDCT) during routine screening before surgery. Based on the characteristics of different subpathological lung nodules expressed from LDCT images, we propose a multi-dimension and multi-feature hybrid learning neural network in this paper. Our network consists of a 2D network part and a 3D network part. The feature vectors extracted from the 2D network and 3D network are further learned by XGBoost. Through this formation, the network can better integrate the feature information from the 2D and 3D networks. The main learning block of the network is a residual block combined with attention mechanism. This learning block enables the network to learn better from multiple features and pay more attention to the key feature map among all the feature maps in different channels. We conduct experiments on our dataset collected from a cooperating hospital. The results show that the accuracy, sensitivity and specificity of our network are 83%, 86%, 80%, respectively It is feasible to use this network to classify the subpathological type of lung nodule through routine screening.",2021,10.3390/s21082734,cross-sectional,diagnosis,CT,Lung
Multi-energy level fusion for nodal metastasis classification of primary lung tumor on dual energy CT using deep learning,"Lymph node metastasis also called nodal metastasis (Nmet), is a clinically primary task for physicians. The survival and recurrence of lung cancer are related to the Nmet staging from Tumor-Node-Metastasis (TNM) reports. Furthermore, preoperative Nmet prediction is still a challenge for the patient in managing the surgical plan and making treatment decisions. We proposed a multi-energy level fusion model with a principal feature enhancement (PFE) block incorporating radiologist and computer science knowledge for Nmet prediction. The proposed model is custom-designed by gemstone spectral imaging (GSI) with different energy levels on dual-energy computer tomography (CT) from a primary tumor of lung cancer. In the experiment, we take three different energy level fusion datasets: lower energy level fusion (40, 50, 60, 70 keV), higher energy level fusion (110, 120, 130, 140 keV), and average energy level fusion (40, 70, 100, 140 keV). The proposed model is trained by lower energy level fusion that is 93% accurate and the value of Kappa is 86%. When we used the lower energy level images to train the fusion model, there has been a significant difference to other energy level fusion models. Hence, we apply 5-fold cross-validation, which is used to validate the performance result of the multi-keV model with different fusion datasets of energy level images in the pathology report. The cross-validation result also demonstrates that the model with the lower energy level dataset is more robust and suitable in predicting the Nmet of the primary tumor. The lower energy level shows more information of tumor angiogenesis or heterogeneity provided the proposed fusion model with a PFE block and channel attention blocks to predict Nmet from primary tumors.",2022,10.1016/j.compbiomed.2021.105185,cross-sectional,diagnosis,CT,Lung
Multi-institutional dose-segmented dosiomic analysis for predicting radiation pneumonitis after lung stereotactic body radiation therapy,"PURPOSE: To predict radiation pneumonitis (RP) grade 2 or worse after lung stereotactic body radiation therapy (SBRT) using dose-based radiomic (dosiomic) features. METHODS: This multi-institutional study included 247 early-stage nonsmall cell lung cancer patients who underwent SBRT with a prescribed dose of 48-70 Gy at an isocenter between June 2009 and March 2016. Ten dose-volume indices (DVIs) were used, including the mean lung dose, internal target volume size, and percentage of entire lung excluding the internal target volume receiving greater than x Gy (x = 5, 10, 15, 20, 25, 30, 35, and 40). A total of 6,808 dose-segmented dosiomic features, such as shape, first order, and texture features, were extracted from the dose distribution. Patients were randomly partitioned into two groups: model training (70%) and test datasets (30%) over 100 times. Dosiomic features were converted to z-scores (standardized values) with a mean of zero and a standard deviation (SD) of one to put different variables on the same scale. The feature dimension was reduced using the following methods: interfeature correlation based on Spearman's correlation coefficients and feature importance based on a light gradient boosting machine (LightGBM) feature selection function. Three different models were developed using LightGBM as follows: (a) a model with ten DVIs (DVI model), (b) a model with the selected dosiomic features (dosiomic model), and (c) a model with ten DVIs and selected dosiomic features (hybrid model). Suitable hyperparameters were determined by searching the largest average area under the curve (AUC) value in the receiver operating characteristic curve (ROC-AUC) via stratified fivefold cross-validation. Each of the final three models with the closest the ROC-AUC value to the average ROC-AUC value was applied to the test datasets. The classification performance was evaluated by calculating the ROC-AUC, AUC in the precision-recall curve (PR-AUC), accuracy, precision, recall, and f1-score. The entire process was repeated 100 times with randomization, and 100 individual models were developed for each of the three models. Then the mean value and SD for the 100 random iterations were calculated for each performance metric. RESULTS: Thirty-seven (15.0%) patients developed RP after SBRT. The ROC-AUC and PR-AUC values in the DVI, dosiomic, and hybrid models were 0.660 ± 0.054 and 0.272 ± 0.052, 0.837 ± 0.054 and 0.510 ± 0.115, and 0.846 ± 0.049 and 0.531 ± 0.116, respectively. For each performance metric, the dosiomic and hybrid models outperformed the DVI models (P < 0.05). Texture-based dosiomic feature was confirmed as an effective indicator for predicting RP. CONCLUSIONS: Our dose-segmented dosiomic approach improved the prediction of the incidence of RP after SBRT.",2021,10.1002/mp.14769,retrospective cohort,prognosis,Dosimetry,Lung
Multi-level 3D Densenets for False-positive Reduction in Lung Nodule Detection Based on Chest Computed Tomography,"OBJECTIVE: False-positive nodule reduction is a crucial part of a computer-aided detection (CADe) system, which assists radiologists in accurate lung nodule detection. In this research, a novel scheme using multi-level 3D DenseNet framework is proposed to implement false-positive nodule reduction task. METHODS: Multi-level 3D DenseNet models were extended to differentiate lung nodules from falsepositive nodules. First, different models were fed with 3D cubes with different sizes for encoding multi-level contextual information to meet the challenges of the large variations of lung nodules. In addition, image rotation and flipping were utilized to upsample positive samples which consisted of a positive sample set. Furthermore, the 3D DenseNets were designed to keep low-level information of nodules, as densely connected structures in DenseNet can reuse features of lung nodules and then boost feature propagation. Finally, the optimal weighted linear combination of all model scores obtained the best classification result in this research. RESULTS: The proposed method was evaluated with LUNA16 dataset which contained 888 thin-slice CT scans. The performance was validated via 10-fold cross-validation. Both the Free-response Receiver Operating Characteristic (FROC) curve and the Competition Performance Metric (CPM) score show that the proposed scheme can achieve a satisfactory detection performance in the falsepositive reduction track of the LUNA16 challenge. CONCLUSION: The result shows that the proposed scheme can be significant for false-positive nodule reduction task.",2020,10.2174/1573405615666191113122840,cross-sectional,diagnosis,CT,Lung
Multi-Level Cross Residual Network for Lung Nodule Classification,"Computer-aided algorithm plays an important role in disease diagnosis through medical images. As one of the major cancers, lung cancer is commonly detected by computer tomography. To increase the survival rate of lung cancer patients, an early-stage diagnosis is necessary. In this paper, we propose a new structure, multi-level cross residual convolutional neural network (ML-xResNet), to classify the different types of lung nodule malignancies. ML-xResNet is constructed by three-level parallel ResNets with different convolution kernel sizes to extract multi-scale features of the inputs. Moreover, the residuals are connected not only with the current level but also with other levels in a crossover manner. To illustrate the performance of ML-xResNet, we apply the model to process ternary classification (benign, indeterminate, and malignant lung nodules) and binary classification (benign and malignant lung nodules) of lung nodules, respectively. Based on the experiment results, the proposed ML-xResNet achieves the best results of 85.88% accuracy for ternary classification and 92.19% accuracy for binary classification, without any additional handcrafted preprocessing algorithm.",2020,10.3390/s20102837,cross-sectional,diagnosis,CT,Lung
Multi-model Ensemble Learning Architecture Based on 3D CNN for Lung Nodule Malignancy Suspiciousness Classification,"Classification of benign and malignant in lung nodules using chest CT images is a key step in the diagnosis of early-stage lung cancer, as well as an effective way to improve the patients' survival rate. However, due to the diversity of lung nodules and the visual similarity of lung nodules to their surrounding tissues, it is difficult to construct a robust classification model with conventional deep learning-based diagnostic methods. To address this problem, we propose a multi-model ensemble learning architecture based on 3D convolutional neural network (MMEL-3DCNN). This approach incorporates three key ideas: (1) Constructed multi-model network architecture can be well adapted to the heterogeneity of lung nodules. (2) The input that concatenated of the intensity image corresponding to the nodule mask, the original image, and the enhanced image corresponding to which can help training model to extract advanced feature with more discriminative capacity. (3) Select the corresponding model to different nodule size dynamically for prediction, which can improve the generalization ability of the model effectively. In addition, ensemble learning is applied in this paper to further improve the robustness of the nodule classification model. The proposed method has been experimentally verified on the public dataset, LIDC-IDRI. The experimental results show that the proposed MMEL-3DCNN architecture can obtain satisfactory classification results.",2020,10.1007/s10278-020-00372-8,cross-sectional,diagnosis,CT,Lung
Multi-omic profiling of plasma reveals molecular alterations in children with COVID-19,"Rationale: Children usually develop less severe symptoms responding to Coronavirus Disease 2019 (COVID-19) than adults. However, little is known about the molecular alterations and pathogenesis of COVID-19 in children. Methods: We conducted plasma proteomic and metabolomic profilings of the blood samples of a cohort containing 18 COVID-19-children with mild symptoms and 12 healthy children, which were enrolled from hospital admissions and outpatients, respectively. Statistical analyses were performed to identify molecules specifically altered in COVID-19-children. We also developed a machine learning-based pipeline named inference of biomolecular combinations with minimal bias (iBM) to prioritize proteins and metabolites strongly altered in COVID-19-children, and experimentally validated the predictions. Results: By comparing to the multi-omic data in adults, we identified 44 proteins and 249 metabolites differentially altered in COVID-19-children against healthy children or COVID-19-adults. Further analyses demonstrated that both deteriorative immune response/inflammation processes and protective antioxidant or anti-inflammatory processes were markedly induced in COVID-19-children. Using iBM, we prioritized two combinations that contained 5 proteins and 5 metabolites, respectively, each exhibiting a total area under curve (AUC) value of 100% to accurately distinguish COVID-19-children from healthy children or COVID-19-adults. Further experiments validated that all the 5 proteins were up-regulated upon coronavirus infection. Interestingly, we found that the prioritized metabolites inhibited the expression of pro-inflammatory factors, and two of them, methylmalonic acid (MMA) and mannitol, also suppressed coronaviral replication, implying a protective role of these metabolites in COVID-19-children. Conclusion: The finding of a strong antagonism of deteriorative and protective effects provided new insights on the mechanism and pathogenesis of COVID-19 in children that mostly underwent mild symptoms. The identified metabolites strongly altered in COVID-19-children could serve as potential therapeutic agents of COVID-19.",2021,10.7150/thno.61832,,,,
Multi-Radiologist User Study for Artificial Intelligence-Guided Grading of COVID-19 Lung Disease Severity on Chest Radiographs,"RATIONALE AND OBJECTIVES: Radiographic findings of COVID-19 pneumonia can be used for patient risk stratification; however, radiologist reporting of disease severity is inconsistent on chest radiographs (CXRs). We aimed to see if an artificial intelligence (AI) system could help improve radiologist interrater agreement. MATERIALS AND METHODS: We performed a retrospective multi-radiologist user study to evaluate the impact of an AI system, the PXS score model, on the grading of categorical COVID-19 lung disease severity on 154 chest radiographs into four ordinal grades (normal/minimal, mild, moderate, and severe). Four radiologists (two thoracic and two emergency radiologists) independently interpreted 154 CXRs from 154 unique patients with COVID-19 hospitalized at a large academic center, before and after using the AI system (median washout time interval was 16 days). Three different thoracic radiologists assessed the same 154 CXRs using an updated version of the AI system trained on more imaging data. Radiologist interrater agreement was evaluated using Cohen and Fleiss kappa where appropriate. The lung disease severity categories were associated with clinical outcomes using a previously published outcomes dataset using Fisher's exact test and Chi-square test for trend. RESULTS: Use of the AI system improved radiologist interrater agreement (Fleiss κ = 0.40 to 0.66, before and after use of the system). The Fleiss κ for three radiologists using the updated AI system was 0.74. Severity categories were significantly associated with subsequent intubation or death within 3 days. CONCLUSION: An AI system used at the time of CXR study interpretation can improve the interrater agreement of radiologists.",2021,10.1016/j.acra.2021.01.016,cross-sectional,diagnosis,Radiograph,Lung
Multi-Reader-Multi-Split Annotation of Emphysema in Computed Tomography,"Emphysema is visible on computed tomography (CT) as low-density lesions representing the destruction of the pulmonary alveoli. To train a machine learning model on the emphysema extent in CT images, labeled image data is needed. The provision of these labels requires trained readers, who are a limited resource. The purpose of the study was to test the reading time, inter-observer reliability and validity of the multi-reader-multi-split method for acquiring CT image labels from radiologists. The approximately 500 slices of each stack of lung CT images were split into 1-cm chunks, with 17 thin axial slices per chunk. The chunks were randomly distributed to 26 readers, radiologists and radiology residents. Each chunk was given a quick score concerning emphysema type and severity in the left and right lung separately. A cohort of 102 subjects, with varying degrees of visible emphysema in the lung CT images, was selected from the SCAPIS pilot, performed in 2012 in Gothenburg, Sweden. In total, the readers created 9050 labels for 2881 chunks. Image labels were compared with regional annotations already provided at the SCAPIS pilot inclusion. The median reading time per chunk was 15 s. The inter-observer Krippendorff's alpha was 0.40 and 0.53 for emphysema type and score, respectively, and higher in the apical part than in the basal part of the lungs. The multi-split emphysema scores were generally consistent with regional annotations. In conclusion, the multi-reader-multi-split method provided reasonably valid image labels, with an estimation of the inter-observer reliability.",2020,10.1007/s10278-020-00378-2,,,,
Multi-resolution classification of exhaled aerosol images to detect obstructive lung diseases in small airways,"Exhaled aerosol patterns have been used to detect obstructive respiratory diseases in the upper airways. Signals from small airway diseases are weak and may not manifest themselves in the exhaled aerosol patterns. Therefore, it will be more challenging to detect abnormalities in small airways. The objective of this study is to develop a simulation-based classification model that can accurately classify small airway diseases. The model performance was evaluated in five obstructed models that are located in lung bifurcations G7-9. The exhaled aerosol images were quantified using local fractal dimensions at different sampling resolutions (n × n). The datasets were classified using both the random forest (RF) and support vector machine (SVM) algorithms. Results show that RF performs slightly and persistently better than SVM. The sampling resolution of 12 × 12 gave the optimal classification for both algorithms. Based on the lung models with predefined obstructive levels, the optimal classification accuracy is 87.0% for 5-class classification, and is 92.5% for 4-class classification by regrouping the mislabeled samples. The proposed model with multi-resolution fractal feature extraction and RF algorithm appears to be sensitive enough to accurately distinguish airway abnormalities in small airways beyond G7 with healthy bronchiole diameter <4 mm. This aerosol-based breath test is promising to develop into an alternative or supplemental tool to the low-dose CT scanning for lung cancer screening.",2017,10.1016/j.compbiomed.2017.05.019,cross-sectional,diagnosis,Aerosol Imaging,Lung
Multi-resolution convolutional networks for chest X-ray radiograph based lung nodule detection,"Lung cancer is the leading cause of cancer death worldwide. Early detection of lung cancer is helpful to provide the best possible clinical treatment for patients. Due to the limited number of radiologist and the huge number of chest x-ray radiographs (CXR) available for observation, a computer-aided detection scheme should be developed to assist radiologists in decision-making. While deep learning showed state-of-the-art performance in several computer vision applications, it has not been used for lung nodule detection on CXR. In this paper, a deep learning-based lung nodule detection method was proposed. We employed patch-based multi-resolution convolutional networks to extract the features and employed four different fusion methods for classification. The proposed method shows much better performance and is much more robust than those previously reported researches. For publicly available Japanese Society of Radiological Technology (JSRT) database, more than 99% of lung nodules can be detected when the false positives per image (FPs/image) was 0.2. The FAUC and R-CPM of the proposed method were 0.982 and 0.987, respectively. The proposed approach has the potential of applications in clinical practice.",2020,10.1016/j.artmed.2019.101744,cross-sectional,diagnosis,Radiograph,Lung
Multi-scale gradual integration CNN for false positive reduction in pulmonary nodule detection,"Lung cancer is a global and dangerous disease, and its early detection is crucial for reducing the risks of mortality. In this regard, it has been of great interest in developing a computer-aided system for pulmonary nodules detection as early as possible on thoracic CT scans. In general, a nodule detection system involves two steps: (i) candidate nodule detection at a high sensitivity, which captures many false positives and (ii) false positive reduction from candidates. However, due to the high variation of nodule morphological characteristics and the possibility of mistaking them for neighboring organs, candidate nodule detection remains a challenge. In this study, we propose a novel Multi-scale Gradual Integration Convolutional Neural Network (MGI-CNN), designed with three main strategies: (1) to use multi-scale inputs with different levels of contextual information, (2) to use abstract information inherent in different input scales with gradual integration, and (3) to learn multi-stream feature integration in an end-to-end manner. To verify the efficacy of the proposed network, we conducted exhaustive experiments on the LUNA16 challenge datasets by comparing the performance of the proposed method with state-of-the-art methods in the literature. On two candidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved an average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods by a large margin. Our MGI-CNN is implemented in Python using TensorFlow and the source code is available from https://github.com/ku-milab/MGICNN.",2019,10.1016/j.neunet.2019.03.003,cross-sectional,diagnosis,CT,Lung
Multi-task deep learning based CT imaging analysis for COVID-19 pneumonia: Classification and segmentation,"This paper presents an automatic classification segmentation tool for helping screening COVID-19 pneumonia using chest CT imaging. The segmented lesions can help to assess the severity of pneumonia and follow-up the patients. In this work, we propose a new multitask deep learning model to jointly identify COVID-19 patient and segment COVID-19 lesion from chest CT images. Three learning tasks: segmentation, classification and reconstruction are jointly performed with different datasets. Our motivation is on the one hand to leverage useful information contained in multiple related tasks to improve both segmentation and classification performances, and on the other hand to deal with the problems of small data because each task can have a relatively small dataset. Our architecture is composed of a common encoder for disentangled feature representation with three tasks, and two decoders and a multi-layer perceptron for reconstruction, segmentation and classification respectively. The proposed model is evaluated and compared with other image segmentation techniques using a dataset of 1369 patients including 449 patients with COVID-19, 425 normal ones, 98 with lung cancer and 397 of different kinds of pathology. The obtained results show very encouraging performance of our method with a dice coefficient higher than 0.88 for the segmentation and an area under the ROC curve higher than 97% for the classification.",2020,10.1016/j.compbiomed.2020.104037,cross-sectional,diagnosis,CT,Lung
Multi-Task Deep Model With Margin Ranking Loss for Lung Nodule Analysis,"Lung cancer is the leading cause of cancer deaths worldwide and early diagnosis of lung nodule is of great importance for therapeutic treatment and saving lives. Automated lung nodule analysis requires both accurate lung nodule benign-malignant classification and attribute score regression. However, this is quite challenging due to the considerable difficulty of lung nodule heterogeneity modeling and the limited discrimination capability on ambiguous cases. To solve these challenges, we propose a Multi-Task deep model with Margin Ranking loss (referred as MTMR-Net) for automated lung nodule analysis. Compared to existing methods which consider these two tasks separately, the relatedness between lung nodule classification and attribute score regression is explicitly explored in a cause-and-effect manner within our multi-task deep model, which can contribute to the performance gains of both tasks. The results of different tasks can be yielded simultaneously for assisting the radiologists in diagnosis interpretation. Furthermore, a Siamese network with a margin ranking loss is elaborately designed to enhance the discrimination capability on ambiguous nodule cases. To further explore the internal relationship between two tasks and validate the effectiveness of the proposed model, we use the recursive feature elimination method to iteratively rank the most malignancy-related features. We validate the efficacy of our method MTMR-Net on the public benchmark LIDC-IDRI dataset. Extensive experiments show that the diagnosis results with internal relationship explicitly explored in our model has met some similar patterns in clinical usage and also demonstrate that our approach can achieve competitive classification performance and more accurate scoring on attributes over the state-of-the-arts. Codes are publicly available at: https://github.com/CaptainWilliam/MTMR-NET.",2020,10.1109/tmi.2019.2934577,cross-sectional,diagnosis,CT,Lung
Multi-task learning for the segmentation of organs at risk with label dependence,"Automatic segmentation of organs at risk is crucial to aid diagnoses and remains a challenging task in medical image analysis domain. To perform the segmentation, we use multi-task learning (MTL) to accurately determine the contour of organs at risk in CT images. We train an encoder-decoder network for two tasks in parallel. The main task is the segmentation of organs, entailing a pixel-level classification in the CT images, and the auxiliary task is the multi-label classification of organs, entailing an image-level multi-label classification of the CT images. To boost the performance of the multi-label classification, we propose a weighted mean cross entropy loss function for the network training, where the weights are the global conditional probability between two organs. Based on MTL, we optimize the false positive filtering (FPF) algorithm to decrease the number of falsely segmented organ pixels in the CT images. Specifically, we propose a dynamic threshold selection (DTS) strategy to prevent true positive rates from decreasing when using the FPF algorithm. We validate these methods on the public ISBI 2019 segmentation of thoracic organs at risk (SegTHOR) challenge dataset and a private medical organ dataset. The experimental results show that networks using our proposed methods outperform basic encoder-decoder networks without increasing the training time complexity.",2020,10.1016/j.media.2020.101666,,,,
Multi-view radiomics and dosiomics analysis with machine learning for predicting acute-phase weight loss in lung cancer patients treated with radiotherapy,"We propose a multi-view data analysis approach using radiomics and dosiomics (R&D) texture features for predicting acute-phase weight loss (WL) in lung cancer radiotherapy. Baseline weight of 388 patients who underwent intensity modulated radiation therapy (IMRT) was measured between one month prior to and one week after the start of IMRT. Weight change between one week and two months after the commencement of IMRT was analyzed, and dichotomized at 5% WL. Each patient had a planning CT and contours of gross tumor volume (GTV) and esophagus (ESO). A total of 355 features including clinical parameter (CP), GTV and ESO (GTV&ESO) dose-volume histogram (DVH), GTV radiomics, and GTV&ESO dosiomics features were extracted. R&D features were categorized as first- (L1), second- (L2), higher-order (L3) statistics, and three combined groups, L1 + L2, L2 + L3 and L1 + L2 + L3. Multi-view texture analysis was performed to identify optimal R&D input features. In the training set (194 earlier patients), feature selection was performed using Boruta algorithm followed by collinearity removal based on variance inflation factor. Machine-learning models were developed using Laplacian kernel support vector machine (lpSVM), deep neural network (DNN) and their averaged ensemble classifiers. Prediction performance was tested on an independent test set (194 more recent patients), and compared among seven different input conditions: CP-only, DVH-only, R&D-only, DVH + CP, R&D + CP, R&D + DVH and R&D + DVH + CP. Combined GTV L1 + L2 + L3 radiomics and GTV&ESO L3 dosiomics were identified as optimal input features, which achieved the best performance with an ensemble classifier (AUC = 0.710), having statistically significantly higher predictability compared with DVH and/or CP features (p < 0.05). When this performance was compared to that with full R&D-only features which reflect traditional single-view data, there was a statistically significant difference (p < 0.05). Using optimized multi-view R&D input features is beneficial for predicting early WL in lung cancer radiotherapy, leading to improved performance compared to using conventional DVH and/or CP features.",2020,10.1088/1361-6560/ab8531,retrospective cohort,prognosis,Dosimetry,Lung
Multi-view secondary input collaborative deep learning for lung nodule 3D segmentation,"BACKGROUND: Convolutional neural networks (CNNs) have been extensively applied to two-dimensional (2D) medical image segmentation, yielding excellent performance. However, their application to three-dimensional (3D) nodule segmentation remains a challenge. METHODS: In this study, we propose a multi-view secondary input residual (MV-SIR) convolutional neural network model for 3D lung nodule segmentation using the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset of chest computed tomography (CT) images. Lung nodule cubes are prepared from the sample CT images. Further, from the axial, coronal, and sagittal perspectives, multi-view patches are generated with randomly selected voxels in the lung nodule cubes as centers. Our model consists of six submodels, which enable learning of 3D lung nodules sliced into three views of features; each submodel extracts voxel heterogeneity and shape heterogeneity features. We convert the segmentation of 3D lung nodules into voxel classification by inputting the multi-view patches into the model and determine whether the voxel points belong to the nodule. The structure of the secondary input residual submodel comprises a residual block followed by a secondary input module. We integrate the six submodels to classify whether voxel points belong to nodules, and then reconstruct the segmentation image. RESULTS: The results of tests conducted using our model and comparison with other existing CNN models indicate that the MV-SIR model achieves excellent results in the 3D segmentation of pulmonary nodules, with a Dice coefficient of 0.926 and an average surface distance of 0.072. CONCLUSION: our MV-SIR model can accurately perform 3D segmentation of lung nodules with the same segmentation accuracy as the U-net model.",2020,10.1186/s40644-020-00331-0,cross-sectional,informatics,CT,Lung
Multi-window back-projection residual networks for reconstructing COVID-19 CT super-resolution images,"BACKGROUND AND OBJECTIVE: With the increasing problem of coronavirus disease 2019 (COVID-19) in the world, improving the image resolution of COVID-19 computed tomography (CT) becomes a very important task. At present, single-image super-resolution (SISR) models based on convolutional neural networks (CNN) generally have problems such as the loss of high-frequency information and the large size of the model due to the deep network structure. METHODS: In this work, we propose an optimization model based on multi-window back-projection residual network (MWSR), which outperforms most of the state-of-the-art methods. Firstly, we use multi-window to refine the same feature map at the same time to obtain richer high/low frequency information, and fuse and filter out the features needed by the deep network. Then, we develop a back-projection network based on the dilated convolution, using up-projection and down-projection modules to extract image features. Finally, we merge several repeated and continuous residual modules with global features, merge the information flow through the network, and input them to the reconstruction module. RESULTS: The proposed method shows the superiority over the state-of-the-art methods on the benchmark dataset, and generates clear COVID-19 CT super-resolution images. CONCLUSION: Both subjective visual effects and objective evaluation indicators are improved, and the model specifications are optimized. Therefore, the MWSR method can improve the clarity of CT images of COVID-19 and effectively assist the diagnosis and quantitative assessment of COVID-19.",2021,10.1016/j.cmpb.2021.105934,cross-sectional,informatics,CT,Lung
Multicenter analysis and a rapid screening model to predict early novel coronavirus pneumonia using a random forest algorithm,"Early determination of coronavirus disease 2019 (COVID-19) pneumonia from numerous suspected cases is critical for the early isolation and treatment of patients.The purpose of the study was to develop and validate a rapid screening model to predict early COVID-19 pneumonia from suspected cases using a random forest algorithm in China.A total of 914 initially suspected COVID-19 pneumonia in multiple centers were prospectively included. The computer-assisted embedding method was used to screen the variables. The random forest algorithm was adopted to build a rapid screening model based on the training set. The screening model was evaluated by the confusion matrix and receiver operating characteristic (ROC) analysis in the validation.The rapid screening model was set up based on 4 epidemiological features, 3 clinical manifestations, decreased white blood cell count and lymphocytes, and imaging changes on chest X-ray or computed tomography. The area under the ROC curve was 0.956, and the model had a sensitivity of 83.82% and a specificity of 89.57%. The confusion matrix revealed that the prospective screening model had an accuracy of 87.0% for predicting early COVID-19 pneumonia.Here, we developed and validated a rapid screening model that could predict early COVID-19 pneumonia with high sensitivity and specificity. The use of this model to screen for COVID-19 pneumonia have epidemiological and clinical significance.",2021,10.1097/md.0000000000026279,cross-sectional,diagnosis,Radiograph,Lung
Multicenter Assessment of CT Pneumonia Analysis Prototype for Predicting Disease Severity and Patient Outcome,"To perform a multicenter assessment of the CT Pneumonia Analysis prototype for predicting disease severity and patient outcome in COVID-19 pneumonia both without and with integration of clinical information. Our IRB-approved observational study included consecutive 241 adult patients (> 18 years; 105 females; 136 males) with RT-PCR-positive COVID-19 pneumonia who underwent non-contrast chest CT at one of the two tertiary care hospitals (site A: Massachusetts General Hospital, USA; site B: Firoozgar Hospital Iran). We recorded patient age, gender, comorbid conditions, laboratory values, intensive care unit (ICU) admission, mechanical ventilation, and final outcome (recovery or death). Two thoracic radiologists reviewed all chest CTs to record type, extent of pulmonary opacities based on the percentage of lobe involved, and severity of respiratory motion artifacts. Thin-section CT images were processed with the prototype (Siemens Healthineers) to obtain quantitative features including lung volumes, volume and percentage of all-type and high-attenuation opacities (≥ -200 HU), and mean HU and standard deviation of opacities within a given lung region. These values are estimated for the total combined lung volume, and separately for each lung and each lung lobe. Multivariable analyses of variance (MANOVA) and multiple logistic regression were performed for data analyses. About 26% of chest CTs (62/241) had moderate to severe motion artifacts. There were no significant differences in the AUCs of quantitative features for predicting disease severity with and without motion artifacts (AUC 0.94-0.97) as well as for predicting patient outcome (AUC 0.7-0.77) (p > 0.5). Combination of the volume of all-attenuation opacities and the percentage of high-attenuation opacities (AUC 0.76-0.82, 95% confidence interval (CI) 0.73-0.82) had higher AUC for predicting ICU admission than the subjective severity scores (AUC 0.69-0.77, 95% CI 0.69-0.81). Despite a high frequency of motion artifacts, quantitative features of pulmonary opacities from chest CT can help differentiate patients with favorable and adverse outcomes.",2021,10.1007/s10278-021-00430-9,retrospective cohort,prognosis,CT,Lung
Multicenter cohort study demonstrates more consolidation in upper lungs on initial CT increases the risk of adverse clinical outcome in COVID-19 patients,"Rationale: Chest computed tomography (CT) has been used for the coronavirus disease 2019 (COVID-19) monitoring. However, the imaging risk factors for poor clinical outcomes remain unclear. In this study, we aimed to assess the imaging characteristics and risk factors associated with adverse composite endpoints in patients with COVID-19 pneumonia. Methods: This retrospective cohort study enrolled patients with laboratory-confirmed COVID-19 from 24 designated hospitals in Jiangsu province, China, between 10 January and 18 February 2020. Clinical and initial CT findings at admission were extracted from medical records. Patients aged < 18 years or without available clinical or CT records were excluded. The composite endpoints were admission to ICU, acute respiratory failure occurrence, or shock during hospitalization. The volume, density, and location of lesions, including ground-glass opacity (GGO) and consolidation, were quantitatively analyzed in each patient. Multivariable logistic regression models were used to identify the risk factors among age and CT parameters associated with the composite endpoints. Results: In this study, 625 laboratory-confirmed COVID-19 patients were enrolled; among them, 179 patients without an initial CT at admission and 25 patients aged < 18 years old were excluded and 421 patients were included in analysis. The median age was 48.0 years and the male proportion was 53% (224/421). During the follow-up period, 64 (15%) patients had a composite endpoint. There was an association of older age (odds ratio [OR], 1.04; 95% confidence interval [CI]: 1.01-1.06; P = 0.003), larger consolidation lesions in the upper lung (Right: OR, 1.13; 95%CI: 1.03-1.25, P =0.01; Left: OR,1.15; 95%CI: 1.01-1.32; P = 0.04) with increased odds of adverse endpoints. Conclusion: There was an association of older age and larger consolidation in upper lungs on admission with higher odds of poor outcomes in patients with COVID-19.",2020,10.7150/thno.46465,,,,
Multichannel lung sound analysis for asthma detection,"BACKGROUND AND OBJECTIVE: Lung sound signals convey valuable information of the lung status. Auscultation is an effective technique to appreciate the condition of the respiratory system using lung sound signals. The prior works on asthma detection from lung sound signals rely on the presence of wheeze. In this paper, we have classified normal and asthmatic subjects using advanced signal processing of posterior lung sound signals, even in the absence of wheeze. METHODS: We collected lung sounds of 60 subjects (30 normal and 30 asthma) using a novel 4-channel data acquisition system from four different positions over the posterior chest, as suggested by the pulmonologist. A spectral subband based feature extraction scheme is proposed that works with artificial neural network (ANN) and support vector machine (SVM) classifiers for the multichannel signal. The power spectral density (PSD) is estimated from extracted lung sound cycle using Welch's method, which then decomposed into uniform subbands. A set of statistical features is computed from each subband and applied to ANN and SVM classifiers to classify normal and asthmatic subjects. RESULTS: In the first part of this study, the performances of each individual channel and four channels together are evaluated where the combined channel performance is found superior to that of individual channels. Next, the performances of all possible combinations of the channels are investigated and the best classification accuracies of 89.2( ± 3.87)% and 93.3( ± 3.10)% are achieved for 2-channel and 3-channel combinations in ANN and SVM classifiers, respectively. CONCLUSIONS: The proposed multichannel asthma detection method where the presence of wheeze in lung sound is not a necessary requirement, outperforms commonly used lung sound classification methods in this field and provides significant relative improvement. The channel combination study gives insight into the contribution of respective lung sound collection areas and their combinations in asthma detection.",2018,10.1016/j.cmpb.2018.03.002,,,,
Multiclass Classification of Chest X-Ray Images for the Prediction of COVID-19 Using Capsule Network,"It is critical to establish a reliable method for detecting people infected with COVID-19 since the pandemic has numerous harmful consequences worldwide. If the patient is infected with COVID-19, a chest X-ray can be used to determine this. In this work, an X-ray showing a COVID-19 infection is classified by the capsule neural network model we trained to recognise. 6310 chest X-ray pictures were used to train the models, separated into three categories: normal, pneumonia, and COVID-19. This work is considered an improved deep learning model for the classification of COVID-19 disease through X-ray images. Viewpoint invariance, fewer parameters, and better generalisation are some of the advantages of CapsNet compared with the classic convolutional neural network (CNN) models. The proposed model has achieved an accuracy greater than 95% during the model's training, which is better than the other state-of-the-art algorithms. Furthermore, to aid in detecting COVID-19 in a chest X-ray, the model could provide extra information.",2022,10.1155/2022/6185013,cross-sectional,diagnosis,Radiograph,Lung
Multiclass Convolution Neural Network for Classification of COVID-19 CT Images,"In the late December of 2019, a novel coronavirus was discovered in Wuhan, China. In March 2020, WHO announced this epidemic had become a global pandemic and that the novel coronavirus may be mild to most people. However, some people may experience a severe illness that results in hospitalization or maybe death. COVID-19 classification remains challenging due to the ambiguity and similarity with other known respiratory diseases such as SARS, MERS, and other viral pneumonia. The typical symptoms of COVID-19 are fever, cough, chills, shortness of breath, loss of smell and taste, headache, sore throat, chest pains, confusion, and diarrhoea. This research paper suggests the concept of transfer learning using the deterministic algorithm in all binary classification models and evaluates the performance of various CNN architectures. The datasets of 746 CT images of COVID-19 and non-COVID-19 were divided for training, validation, and testing. Various augmentation techniques were applied to increase the number of datasets except for testing images. The images were then pretrained using CNN to obtain a binary class. ResNeXt101 and ResNet152 have the best F1 score of 0.978 and 0.938, whereas GoogleNet has an F1 score of 0.762. ResNeXt101 and ResNet152 have an accuracy of 97.81% and 93.80%. ResNeXt101, DenseNet201, and ResNet152 have 95.71%, 93.81%, and 90% sensitivity, whereas ResNeXt101, ResNet101, and ResNet152 have 100%, 99.58%, and 98.33 specificity, respectively.",2022,10.1155/2022/9167707,cross-sectional,diagnosis,CT,Lung
Multilevel Deep-Aggregated Boosted Network to Recognize COVID-19 Infection from Large-Scale Heterogeneous Radiographic Data,"In the present epidemic of the coronavirus disease 2019 (COVID-19), radiological imaging modalities, such as X-ray and computed tomography (CT), have been identified as effective diagnostic tools. However, the subjective assessment of radiographic examination is a time-consuming task and demands expert radiologists. Recent advancements in artificial intelligence have enhanced the diagnostic power of computer-aided diagnosis (CAD) tools and assisted medical specialists in making efficient diagnostic decisions. In this work, we propose an optimal multilevel deep-aggregated boosted network to recognize COVID-19 infection from heterogeneous radiographic data, including X-ray and CT images. Our method leverages multilevel deep-aggregated features and multistage training via a mutually beneficial approach to maximize the overall CAD performance. To improve the interpretation of CAD predictions, these multilevel deep features are visualized as additional outputs that can assist radiologists in validating the CAD results. A total of six publicly available datasets were fused to build a single large-scale heterogeneous radiographic collection that was used to analyze the performance of the proposed technique and other baseline methods. To preserve generality of our method, we selected different patient data for training, validation, and testing, and consequently, the data of same patient were not included in training, validation, and testing subsets. In addition, fivefold cross-validation was performed in all the experiments for a fair evaluation. Our method exhibits promising performance values of 95.38%, 95.57%, 92.53%, 98.14%, 93.16%, and 98.55% in terms of average accuracy, F-measure, specificity, sensitivity, precision, and area under the curve, respectively and outperforms various state-of-the-art methods.",2021,10.1109/jbhi.2021.3072076,cross-sectional,diagnosis,"CT, Radiograph",Lung
Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation,"Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection of PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present, the performance of existing automated methods for this challenging task is low. Segmentation tends to be done manually by different imaging experts, which is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide the segmentation. We introduce a deep learning-based framework in multimodal PET-CT segmentation with a multimodal spatial attention module (MSAM). The MSAM automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake from the PET input. The resulting spatial attention maps are subsequently employed to target a convolutional neural network (CNN) backbone for segmentation of areas with higher tumor likelihood from the CT image. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of our framework in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice similarity coefficient (DSC).",2021,10.1109/jbhi.2021.3059453,cross-sectional,informatics,PET-CT,Lung
Multimodality MRI-based radiomics approach to predict the posttreatment response of lung cancer brain metastases to gamma knife radiosurgery,"OBJECTIVES: To develop and validate a multimodality MRI-based radiomics approach to predicting the posttreatment response of lung cancer brain metastases (LCBM) to gamma knife radiosurgery (GKRS). METHODS: We retrospectively analyzed 213 lesions from 137 patients with LCBM who received GKRS between January 2017 and November 2020. The data were divided into a primary cohort (102 patients with 173 lesions) and an independent validation cohort (35 patients with 40 lesions) according to the time of treatment. Benefit result was defined using pretreatment and 3-month follow-up MRI images based on the Response Assessment in Neuro-Oncology Brain Metastases criteria. Valuable radiomics features were extracted from pretreatment multimodality MRI images using random forests. Prediction performance among the radiomics features of tumor core (RFTC) and radiomics features of peritumoral edema (RFPE) together was evaluated separately. Then, the random forest radiomics score and nomogram were developed through the primary cohort and evaluated through an independent validation cohort. Prediction performance was evaluated by ROC curve, calibration curve, and decision curve. RESULTS: Gender (p = 0.018), histological subtype (p = 0.009), epidermal growth factor receptor mutation (p = 0.034), and targeted drug treatment (p = 0.021) were significantly associated with posttreatment response. Adding RFPE to RFTC showed improved prediction performance than RFTC alone in primary cohort (AUC = 0.848 versus AUC = 0.750; p < 0.001). Finally, the radiomics nomogram had an AUC of 0.930, a C-index of 0.930 (specificity of 83.1%, sensitivity of 87.3%) in primary cohort, and an AUC of 0.852, a C-index of 0.848 (specificity of 84.2%, sensitivity of 76.2%) in validation cohort. CONCLUSIONS: Multimodality MRI-based radiomics models can predict the posttreatment response of LCBM to GKRS. KEY POINTS: • Among the selected radiomics features, texture features basically contributed the dominant force in prediction tasks (80%), especially gray-level co-occurrence matrix features (40%). • Adding RFPE to RFTC showed improved prediction performance than RFTC alone in primary cohort (AUC = 0.848 versus AUC = 0.750; p < 0.001). • The multimodality MRI-based radiomics nomogram showed high accuracy for distinguishing the posttreatment response of LCBM to GKRS (AUC = 0.930, in primary cohort; AUC = 0.852, in validation cohort).",2022,10.1007/s00330-021-08368-w,,,,
Multiparametric MRI-Based Radiomics Approaches for Preoperative Prediction of EGFR Mutation Status in Spinal Bone Metastases in Patients with Lung Adenocarcinoma,"BACKGROUND: Preoperative prediction of epidermal growth factor receptor (EGFR) mutation status in patients with spinal bone metastases (SBM) from primary lung adenocarcinoma is potentially important for treatment decisions. PURPOSE: To develop and validate multiparametric magnetic resonance imaging (MRI)-based radiomics methods for preoperative prediction of EGFR mutation based on MRI of SBM. STUDY TYPE: Retrospective. POPULATION: A total of 97 preoperative patients with lumbar SBM from lung adenocarcinoma (77 in training set and 20 in validation set). FIELD STRENGTH/SEQUENCE: T1-weighted, T2-weighted, and T2-weighted fat-suppressed fast spin echo sequences at 3.0 T. ASSESSMENT: Radiomics handcrafted and deep learning-based features were extracted and selected from each MRI sequence. The abilities of the features to predict EGFR mutation status were analyzed and compared. A radiomics nomogram was constructed integrating the selected features. STATISTICAL TESTS: The Mann-Whitney U test and χ(2) test were employed for evaluating associations between clinical characteristics and EGFR mutation status for continuous and discrete variables, respectively. Least absolute shrinkage and selection operator was used for selection of predictive features. Sensitivity (SEN), specificity (SPE), and area under the receiver operating characteristic curve (AUC) were used to evaluate the ability of radiomics models to predict the EGFR mutation. Calibration and decision curve analysis (DCA) were performed to assess and validate nomogram results. RESULTS: The radiomics signature comprised five handcrafted and one deep learning-based features and achieved good performance for predicting EGFR mutation status, with AUCs of 0.891 (95% confidence interval [CI], 0.820-0.962, SEN = 0.913, SPE = 0.710) in the training group and 0.771 (95% CI, 0.551-0.991, SEN = 0.750, SPE = 0.875) in the validation group. DCA confirmed the potential clinical usefulness of the radiomics models. DATA CONCLUSION: Multiparametric MRI-based radiomics is potentially clinical valuable for predicting EGFR mutation status in patients with SBM from lung adenocarcinoma. LEVEL OF EVIDENCE: 3 TECHNICAL EFFICACY: 2.",2021,10.1002/jmri.27579,,,,
Multiplanar analysis for pulmonary nodule classification in CT images using deep convolutional neural network and generative adversarial networks,"PURPOSE: Early detection and treatment of lung cancer holds great importance. However, pulmonary-nodule classification using CT images alone is difficult to realize. To address this concern, a method for pulmonary-nodule classification based on a deep convolutional neural network (DCNN) and generative adversarial networks (GAN) has previously been proposed by the authors. In that method, the said classification was performed exclusively using axial cross sections of pulmonary nodules. During actual medical-examination procedures, however, a comprehensive judgment can only be made via observation of various pulmonary-nodule cross sections. In the present study, a comprehensive analysis was performed by extending the application of the previously proposed DCNN- and GAN-based automatic classification method to multiple cross sections of pulmonary nodules. METHODS: Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. Firstly, multiplanar images of the pulmonary nodule are generated. Classification training was performed for three DCNNs. A certain pretraining was initially performed using GAN-generated nodule images. This was followed by fine-tuning of each pretrained DCNN using original nodule images provided as input. RESULTS: As a result of the evaluation, the specificity was 77.8% and the sensitivity was 93.9%. Additionally, the specificity was observed to have improved by 11.1% without any reduction in the sensitivity, compared to our previous report. CONCLUSION: This study reports development of a comprehensive analysis method to classify pulmonary nodules at multiple sections using GAN and DCNN. The effectiveness of the proposed discrimination method based on use of multiplanar images has been demonstrated to be improved compared to that realized in a previous study reported by the authors. In addition, the possibility of enhancing classification accuracy via application of GAN-generated images, instead of data augmentation, for pretraining even for medical datasets that contain relatively few images has also been demonstrated.",2020,10.1007/s11548-019-02092-z,cross-sectional,diagnosis,CT,Lung
Multiple Kernel Point Set Registration,"The finite Gaussian mixture model with kernel correlation is a flexible tool that has recently received attention for point set registration. While there are many algorithms for point set registration presented in the literature, an important issue arising from these studies concerns the mapping of data with nonlinear relationships and the ability to select a suitable kernel. Kernel selection is crucial for effective point set registration. We focus here on multiple kernel point set registration. We make several contributions in this paper. First, each observation is modeled using the Student's t-distribution, which is heavily tailed and more robust than the Gaussian distribution. Second, by automatically adjusting the kernel weights, the proposed method allows us to prune the ineffective kernels. This makes the choice of kernels less crucial. After parameter learning, the kernel saliencies of the irrelevant kernels go to zero. Thus, the choice of kernels is less crucial and it is easy to include other kinds of kernels. Finally, we show empirically that our model outperforms state-of-the-art methods recently proposed in the literature.",2016,10.1109/tmi.2015.2511063,,,,
Multiple Resolution Residually Connected Feature Streams for Automatic Lung Tumor Segmentation From CT Images,"Volumetric lung tumor segmentation and accurate longitudinal tracking of tumor volume changes from computed tomography images are essential for monitoring tumor response to therapy. Hence, we developed two multiple resolution residually connected network (MRRN) formulations called incremental-MRRN and dense-MRRN. Our networks simultaneously combine features across multiple image resolution and feature levels through residual connections to detect and segment the lung tumors. We evaluated our method on a total of 1210 non-small cell (NSCLC) lung tumors and nodules from three data sets consisting of 377 tumors from the open-source Cancer Imaging Archive (TCIA), 304 advanced stage NSCLC treated with anti- PD-1 checkpoint immunotherapy from internal institution MSKCC data set, and 529 lung nodules from the Lung Image Database Consortium (LIDC). The algorithm was trained using 377 tumors from the TCIA data set and validated on the MSKCC and tested on LIDC data sets. The segmentation accuracy compared to expert delineations was evaluated by computing the dice similarity coefficient, Hausdorff distances, sensitivity, and precision metrics. Our best performing incremental-MRRN method produced the highest DSC of 0.74 ± 0.13 for TCIA, 0.75±0.12 for MSKCC, and 0.68±0.23 for the LIDC data sets. There was no significant difference in the estimations of volumetric tumor changes computed using the incremental-MRRN method compared with the expert segmentation. In summary, we have developed a multi-scale CNN approach for volumetrically segmenting lung tumors which enables accurate, automated identification of and serial measurement of tumor volumes in the lung.",2019,10.1109/tmi.2018.2857800,cross-sectional,informatics,CT,Lung
Multiscale Attention Guided Network for COVID-19 Diagnosis Using Chest X-Ray Images,"Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic after millennium, forcing the world to tackle a health crisis. Automated lung infections classification using chest X-ray (CXR) images could strengthen diagnostic capability when handling COVID-19. However, classifying COVID-19 from pneumonia cases using CXR image is a difficult task because of shared spatial characteristics, high feature variation and contrast diversity between cases. Moreover, massive data collection is impractical for a newly emerged disease, which limited the performance of data thirsty deep learning models. To address these challenges, Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction vector and attention from multiscale feature maps. To improve the robustness of trained model and relieve the shortage of training data, attention guided augmentations along with a soft distance regularization are posed, which aims at generating meaningful augmentations and reduce noise. Our multiscale attention model achieves better classification performance on our pneumonia CXR image dataset. Plentiful experiments are proposed for MAG-SD which demonstrates its unique advantage in pneumonia classification over cutting-edge models. The code is available at https://github.com/JasonLeeGHub/MAG-SD.",2021,10.1109/jbhi.2021.3058293,cross-sectional,diagnosis,Radiograph,Lung
Multiscale CNN with compound fusions for false positive reduction in lung nodule detection,"Pulmonary lung nodules are often benign at the early stage but they could easily become malignant and metastasize to other locations in later stages. Morphological characteristics of these nodule instances vary largely in terms of their size, shape, and texture. There are also other co-existing lung anatomical structures such as lung walls and blood vessels surrounding these nodules resulting in complex contextual information. As a result, their early diagnosis to enable decisive intervention using Computer-Aided Diagnosis (CAD) systems face serious challenges, especially at low false positive rates. In this paper, we propose a new Convolutional Neural Network (CNN) architecture called Multiscale CNN with Compound Fusions (MCNN-CF) for this purpose which uses multiscale 3D patches as inputs and performs a fusion of intermediate features at two different depths of the network in two diverse fashions. The network is trained by a new iterative training procedure adapted to circumvent the class imbalance problem and obtained a Competitive Performance Metric (CPM) score of 0.948 when tested on the LUNA16 dataset. Experimental results illustrate the robustness of the proposed system which has increased the confidence of the prediction probabilities in the detection of the most variety of nodules.",2021,10.1016/j.artmed.2021.102017,cross-sectional,diagnosis,CT,Lung
Multiscale Mask R-CNN-Based Lung Tumor Detection Using PET Imaging,"Positron emission tomography (PET) imaging serves as one of the most competent methods for the diagnosis of various malignancies, such as lung tumor. However, with an elevation in the utilization of PET scan, radiologists are overburdened considerably. Consequently, a new approach of ""computer-aided diagnosis"" is being contemplated to curtail the heavy workloads. In this article, we propose a multiscale Mask Region-Based Convolutional Neural Network (Mask R-CNN)-based method that uses PET imaging for the detection of lung tumor. First, we produced 3 models of Mask R-CNN for lung tumor candidate detection. These 3 models were generated by fine-tuning the Mask R-CNN using certain training data that consisted of images from 3 different scales. Each of the training data set included 594 slices with lung tumor. These 3 models of Mask R-CNN models were then integrated using weighted voting strategy to diminish the false-positive outcomes. A total of 134 PET slices were employed as test set in this experiment. The precision, recall, and F score values of our proposed method were 0.90, 1, and 0.95, respectively. Experimental results exhibited strong conviction about the effectiveness of this method in detecting lung tumors, along with the capability of identifying a healthy chest pattern and reducing incorrect identification of tumors to a large extent.",2019,10.1177/1536012119863531,cross-sectional,diagnosis,PET-CT,Lung
Multispectral imaging for quantitative and compartment-specific immune infiltrates reveals distinct immune profiles that classify lung cancer patients,"Semiquantitative assessment of immune markers by immunohistochemistry (IHC) has significant limitations for describing the diversity of the immune response in cancer. Therefore, we evaluated a fluorescence-based multiplexed immunohistochemical method in combination with a multispectral imaging system to quantify immune infiltrates in situ in the environment of non-small-cell lung cancer (NSCLC). A tissue microarray including 57 NSCLC cases was stained with antibodies against CD8, CD20, CD4, FOXP3, CD45RO, and pan-cytokeratin, and immune cells were quantified in epithelial and stromal compartments. The results were compared with those of conventional IHC, and related to corresponding RNA-sequencing (RNAseq) expression values. We found a strong correlation between the visual and digital quantification of lymphocytes for CD45RO (correlation coefficient: r = 0.52), FOXP3 (r = 0.87), CD4 (r = 0.79), CD20 (r = 0.81) and CD8 (r = 0.90) cells. The correlation with RNAseq data for digital quantification (0.35-0.65) was comparable to or better than that for visual quantification (0.38-0.58). Combination of the signals of the five immune markers enabled further subpopulations of lymphocytes to be identified and localized. The specific pattern of immune cell infiltration based either on the spatial distribution (distance between regulatory CD8(+) T and cancer cells) or the relationships of lymphocyte subclasses with each other (e.g. cytotoxic/regulatory cell ratio) were associated with patient prognosis. In conclusion, the fluorescence multiplexed immunohistochemical method, based on only one tissue section, provided reliable quantification and localization of immune cells in cancer tissue. The application of this technique to clinical biopsies can provide a basic characterization of immune infiltrates to guide clinical decisions in the era of immunotherapy. Copyright © 2017 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.",2018,10.1002/path.5026,,,,
Multistage segmentation model and SVM-ensemble for precise lung nodule detection,"PURPOSE: Lung cancer detection at its initial stages increases the survival chances of patients. Automatic detection of lung nodules facilitates radiologists during the diagnosis. However, there is a challenge of false positives in automated systems which may lead to wrong findings. Precise segmentation facilitates to accurately extract nodules from lung CT images in order to improve performance of the diagnostic method. METHODS: A multistage segmentation model is presented in this study. The lung region is extracted by applying corner-seeded region growing combined with differential evolution-based optimal thresholding. In addition to this, morphological operations are applied in boundary smoothing, hole filling and juxtavascular nodule extraction. Geometric properties along with 3D edge information are applied to extract nodule candidates. Geometric texture features descriptor (GTFD) followed by support vector machine-based ensemble classification is employed to distinguish actual nodules from the candidate set. RESULTS: A publicly available dataset, namely lung image database consortium and image database resource initiative, is used to evaluate performance of the proposed method. The classification is performed over GTFD feature vector and the results show 99% accuracy, 98.6% sensitivity and 98.2% specificity with 3.4 false positives per scan (FPs/scan). CONCLUSION: A lung nodule detection method is presented to facilitate radiologists in accurately diagnosing cancer from CT images. Results indicate that the proposed method has not only reduced FPs/scan but also significantly improved sensitivity as compared to related studies.",2018,10.1007/s11548-018-1715-9,cross-sectional,informatics,CT,Lung
Natural history of pathologically confirmed pulmonary subsolid nodules with deep learning-assisted nodule segmentation,"OBJECTIVE: To explore the natural history of pulmonary subsolid nodules (SSNs) with different pathological types by deep learning-assisted nodule segmentation. METHODS: Between June 2012 and June 2019, 95 resected SSNs with preoperative long-term follow-up were enrolled in this retrospective study. SSN detection and segmentation were performed on preoperative follow-up CTs using the deep learning-based Dr. Wise system. SSNs were categorized into invasive adenocarcinoma (IAC, n = 47) and non-IAC (n = 48) groups; according to the interval change during the preoperative follow-up, SSNs were divided into growth (n = 68), nongrowth (n = 22), and new emergence (n = 5) groups. We analyzed the cumulative percentages and pattern of SSN growth and identified significant factors for IAC diagnosis and SSN growth. RESULTS: The mean preoperative follow-up was 42.1 ± 17.0 months. More SSNs showed growth or new emergence in the IAC than in the non-IAC group (89.4% vs. 64.6%, p = 0.009). Volume doubling time was non-significantly shorter for IACs than for non-IACs (1436.0 ± 1188.2 vs. 2087.5 ± 1799.7 days, p = 0.077). Median mass doubling time was significantly shorter for IACs than for non-IACs (821.7 vs. 1944.1 days, p = 0.001). Lobulated sign (p = 0.002) and SSN mass (p = 0.004) were significant factors for differentiating IACs. IACs showed significantly higher cumulative growth percentages than non-IACs in the first 70 months of follow-up. The growth pattern of SSNs may conform to the exponential model. The initial volume (p = 0.042) was a predictor for SSN growth. CONCLUSIONS: IACs appearing as SSNs showed an indolent course. The mean growth rate was larger for IACs than for non-IACs. SSNs with larger initial volume are more likely to grow. KEY POINTS: • Invasive adenocarcinomas (IACs) appearing as subsolid nodules (SSNs), with a mean volume doubling time (VDT) of 1436.0 ± 1188.2 days and median mass doubling time (MDT) of 821.7 days, showed an indolent course. • The VDT was shorter for IACs than for non-IACs (1436.0 ± 1188.2 vs. 2087.5 ± 1799.7 days), but the difference was not significant (p = 0.077). The median MDT was significantly shorter for IACs than for non-IACs (821.7 vs. 1944.1 days, p = 0.001). • SSNs with lobulated sign and larger mass (> 390.5 mg) may very likely be IACs. SSNs with larger initial volume are more likely to grow.",2021,10.1007/s00330-020-07450-z,retrospective cohort,treatment,CT,Lung
Neural mechanisms underlying breathing complexity,"Breathing is maintained and controlled by a network of automatic neurons in the brainstem that generate respiratory rhythm and receive regulatory inputs. Breathing complexity therefore arises from respiratory central pattern generators modulated by peripheral and supra-spinal inputs. Very little is known on the brainstem neural substrates underlying breathing complexity in humans. We used both experimental and theoretical approaches to decipher these mechanisms in healthy humans and patients with chronic obstructive pulmonary disease (COPD). COPD is the most frequent chronic lung disease in the general population mainly due to tobacco smoke. In patients, airflow obstruction associated with hyperinflation and respiratory muscles weakness are key factors contributing to load-capacity imbalance and hence increased respiratory drive. Unexpectedly, we found that the patients breathed with a higher level of complexity during inspiration and expiration than controls. Using functional magnetic resonance imaging (fMRI), we scanned the brain of the participants to analyze the activity of two small regions involved in respiratory rhythmogenesis, the rostral ventro-lateral (VL) medulla (pre-Bötzinger complex) and the caudal VL pons (parafacial group). fMRI revealed in controls higher activity of the VL medulla suggesting active inspiration, while in patients higher activity of the VL pons suggesting active expiration. COPD patients reactivate the parafacial to sustain ventilation. These findings may be involved in the onset of respiratory failure when the neural network becomes overwhelmed by respiratory overload We show that central neural activity correlates with airflow complexity in healthy subjects and COPD patients, at rest and during inspiratory loading. We finally used a theoretical approach of respiratory rhythmogenesis that reproduces the kernel activity of neurons involved in the automatic breathing. The model reveals how a chaotic activity in neurons can contribute to chaos in airflow and reproduces key experimental fMRI findings.",2013,10.1371/journal.pone.0075740,,,,
Neural networks for nodal staging of non-small cell lung cancer with FDG PET and CT: importance of combining uptake values and sizes of nodes and primary tumor,"PURPOSE: To evaluate the effect of adding lymph node size to three previously explored artificial neural network (ANN) input parameters (primary tumor maximum standardized uptake value or tumor uptake, tumor size, and nodal uptake at N1, N2, and N3 stations) in the structure of the ANN. The goal was to allow the resulting ANN structure to relate lymph node uptake for size to primary tumor uptake for size in the determination of the status of nodes as human readers do. MATERIALS AND METHODS: This prospective study was approved by the institutional review board, and informed consent was obtained from all participants. The authors developed a back-propagation ANN with one hidden layer and eight processing units. The data set used to train the network included node and tumor size and uptake from 133 patients with non-small cell lung cancer with surgically proved N status. Statistical analysis was performed with the paired t test. RESULTS: The ANN correctly predicted the N stage in 99.2% of cases, compared with 72.4% for the expert reader (P < .001). In categorization of N0 and N1 versus N2 and N3 disease, the ANN performed with 99.2% accuracy versus 92.2% for the expert reader (P < .001). CONCLUSION: The ANN is 99.2% accurate in predicting surgical-pathologic nodal status with use of four fluorine 18 fluorodeoxyglucose (FDG) positron emission tomography (PET)/computed tomography (CT)-derived parameters. Malignant and benign inflammatory lymph nodes have overlapping appearances at FDG PET/CT but can be differentiated by ANNs when the crucial input of node size is used.",2014,10.1148/radiol.13122427,cross-sectional,diagnosis,PET-CT,Lung
Neural-network based autocontouring algorithm for intrafractional lung-tumor tracking using Linac-MR,"PURPOSE: To develop a neural-network based autocontouring algorithm for intrafractional lung-tumor tracking using Linac-MR and evaluate its performance with phantom and in-vivo MR images. METHODS: An autocontouring algorithm was developed to determine both the shape and position of a lung tumor from each intrafractional MR image. A pulse-coupled neural network was implemented in the algorithm for contrast improvement of the tumor region. Prior to treatment, to initiate the algorithm, an expert user needs to contour the tumor and its maximum anticipated range of motion in pretreatment MR images. During treatment, however, the algorithm processes each intrafractional MR image and automatically generates a tumor contour without further user input. The algorithm is designed to produce a tumor contour that is the most similar to the expert's manual one. To evaluate the autocontouring algorithm in the author's Linac-MR environment which utilizes a 0.5 T MRI, a motion phantom and four lung cancer patients were imaged with 3 T MRI during normal breathing, and the image noise was degraded to reflect the image noise at 0.5 T. Each of the pseudo-0.5 T images was autocontoured using the author's algorithm. In each test image, the Dice similarity index (DSI) and Hausdorff distance (HD) between the expert's manual contour and the algorithm generated contour were calculated, and their centroid positions were compared (Δd centroid). RESULTS: The algorithm successfully contoured the shape of a moving tumor from dynamic MR images acquired every 275 ms. From the phantom study, mean DSI of 0.95-0.96, mean HD of 2.61-2.82 mm, and mean Δd centroid of 0.68-0.93 mm were achieved. From the in-vivo study, the author's algorithm achieved mean DSI of 0.87-0.92, mean HD of 3.12-4.35 mm, as well as Δd centroid of 1.03-1.35 mm. Autocontouring speed was less than 20 ms for each image. CONCLUSIONS: The authors have developed and evaluated a lung tumor autocontouring algorithm for intrafractional tumor tracking using Linac-MR. The autocontouring performance in the Linac-MR environment was evaluated using phantom and in-vivo MR images. From the in-vivo study, the author's algorithm achieved 87%-92% of contouring agreement and centroid tracking accuracy of 1.03-1.35 mm. These results demonstrate the feasibility of lung tumor autocontouring in the author's laboratory's Linac-MR environment.",2015,10.1118/1.4916657,cross-sectional,informatics,MRI,Lung
New machine learning method for image-based diagnosis of COVID-19,"COVID-19 is a worldwide epidemic, as announced by the World Health Organization (WHO) in March 2020. Machine learning (ML) methods can play vital roles in identifying COVID-19 patients by visually analyzing their chest x-ray images. In this paper, a new ML-method proposed to classify the chest x-ray images into two classes, COVID-19 patient or non-COVID-19 person. The features extracted from the chest x-ray images using new Fractional Multichannel Exponent Moments (FrMEMs). A parallel multi-core computational framework utilized to accelerate the computational process. Then, a modified Manta-Ray Foraging Optimization based on differential evolution used to select the most significant features. The proposed method evaluated using two COVID-19 x-ray datasets. The proposed method achieved accuracy rates of 96.09% and 98.09% for the first and second datasets, respectively.",2020,10.1371/journal.pone.0235187,cross-sectional,diagnosis,Radiograph,Lung
Next-Generation Radiogenomics Sequencing for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients Using Multimodal Imaging and Machine Learning Algorithms,"PURPOSE: Considerable progress has been made in the assessment and management of non-small cell lung cancer (NSCLC) patients based on mutation status in the epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS). At the same time, NSCLC management through KRAS and EGFR mutation profiling faces challenges. In the present work, we aimed to evaluate a comprehensive radiomics framework that enabled prediction of EGFR and KRAS mutation status in NSCLC patients based on radiomic features from low-dose computed tomography (CT), contrast-enhanced diagnostic quality CT (CTD), and positron emission tomography (PET) imaging modalities and use of machine learning algorithms. METHODS: Our study involved NSCLC patients including 150 PET, low-dose CT, and CTD images. Radiomic features from original and preprocessed (including 64 bin discretizing, Laplacian-of-Gaussian (LOG), and Wavelet) images were extracted. Conventional clinically used standard uptake value (SUV) parameters and metabolic tumor volume (MTV) were also obtained from PET images. Highly correlated features were pre-eliminated, and false discovery rate (FDR) correction was performed with the resulting q-values reported for univariate analysis. Six feature selection methods and 12 classifiers were then used for multivariate prediction of gene mutation status (provided by polymerase chain reaction (PCR)) in patients. We performed 10-fold cross-validation for model tuning to improve robustness, and our developed models were assessed on an independent validation set with 68 patients (common in all three imaging modalities). The average area under the receiver operator characteristic curve (AUC) was utilized for performance evaluation. RESULTS: The best predictive power for conventional PET parameters was achieved by SUV(peak) (AUC 0.69, p value = 0.0002) and MTV (AUC 0.55, p value = 0.0011) for EGFR and KRAS, respectively. Univariate analysis of extracted radiomics features improved AUC performance to 0.75 (q-value 0.003, Short-Run Emphasis feature of GLRLM from LOG preprocessed image of PET with sigma value 1.5) and 0.71 (q-value 0.00005, Large Dependence Low Gray-Level Emphasis feature of GLDM in LOG preprocessed image of CTD with sigma value 5) for EGFR and KRAS, respectively. Furthermore, multivariate machine learning-based AUC performances were significantly improved to 0.82 for EGFR (LOG preprocessed image of PET with sigma 3 with variance threshold (VT) feature selector and stochastic gradient descent (SGD) classifier (q-value = 4.86E-05) and 0.83 for KRAS (LOG preprocessed image of CT with sigma 3.5 with select model (SM) feature selector and SGD classifier (q-value = 2.81E-09). CONCLUSION: Our work demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients. We demonstrated that radiomic features extracted from different image-feature sets could be used for EGFR and KRAS mutation status prediction in NSCLC patients and showed improved predictive power relative to conventional image-derived metrics.",2020,10.1007/s11307-020-01487-8,cross-sectional,diagnosis,CT,Lung
No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting With Adversarial Attacks,"Detecting malignant pulmonary nodules at an early stage can allow medical interventions which may increase the survival rate of lung cancer patients. Using computer vision techniques to detect nodules can improve the sensitivity and the speed of interpreting chest CT for lung cancer screening. Many studies have used CNNs to detect nodule candidates. Though such approaches have been shown to outperform the conventional image processing based methods regarding the detection accuracy, CNNs are also known to be limited to generalize on under-represented samples in the training set and prone to imperceptible noise perturbations. Such limitations can not be easily addressed by scaling up the dataset or the models. In this work, we propose to add adversarial synthetic nodules and adversarial attack samples to the training data to improve the generalization and the robustness of the lung nodule detection systems. To generate hard examples of nodules from a differentiable nodule synthesizer, we use projected gradient descent (PGD) to search the latent code within a bounded neighbourhood that would generate nodules to decrease the detector response. To make the network more robust to unanticipated noise perturbations, we use PGD to search for noise patterns that can trigger the network to give over-confident mistakes. By evaluating on two different benchmark datasets containing consensus annotations from three radiologists, we show that the proposed techniques can improve the detection performance on real CT data. To understand the limitations of both the conventional networks and the proposed augmented networks, we also perform stress-tests on the false positive reduction networks by feeding different types of artificially produced patches. We show that the augmented networks are more robust to both under-represented nodules as well as resistant to noise perturbations.",2021,10.1109/tmi.2020.3026261,cross-sectional,diagnosis,CT,Lung
Non-invasive classification of non-small cell lung cancer: a comparison between random forest models utilising radiomic and semantic features,"OBJECTIVE: Non-invasive distinction between squamous cell carcinoma and adenocarcinoma subtypes of non-small-cell lung cancer (NSCLC) may be beneficial to patients unfit for invasive diagnostic procedures or when tissue is insufficient for diagnosis. The purpose of our study was to compare the performance of random forest algorithms utilizing CT radiomics and/or semantic features in classifying NSCLC. METHODS: Two thoracic radiologists scored 11 semantic features on CT scans of 106 patients with NSCLC. A set of 115 radiomics features was extracted from the CT scans. Random forest models were developed from semantic (RM-sem), radiomics (RM-rad), and all features combined (RM-all). External validation of models was performed using an independent test data set (n = 100) of CT scans. Model performance was measured with out-of-bag error and area under curve (AUC), and compared using receiver-operating characteristics curve analysis on the test data set. RESULTS: The median (interquartile-range) error rates of the models were: RF-sem 24.5 % (22.6 - 37.5 %), RF-rad 35.8 % (34.9 - 38.7 %), and RM-all 37.7 % (37.7 - 37.7). On training data, both RF-rad and RF-all gave perfect discrimination (AUC = 1), which was significantly higher than that achieved by RF-sem (AUC = 0.78; p < 0.0001). On test data, however, RM-sem model (AUC = 0.82) out-performed RM-rad and RM-all (AUC = 0.5 and AUC = 0.56; p < 0.0001), neither of which was significantly different from random guess ( p = 0.9 and 0.6 respectively). CONCLUSION: Non-invasive classification of NSCLC can be done accurately using random forest classification models based on well-known CT-derived descriptive features. However, radiomics-based classification models performed poorly in this scenario when tested on independent data and should be used with caution, due to their possible lack of generalizability to new data. ADVANCES IN KNOWLEDGE: Our study describes novel CT-derived random forest models based on radiologist-interpretation of CT scans (semantic features) that can assist NSCLC classification when histopathology is equivocal or when histopathological sampling is not possible. It also shows that random forest models based on semantic features may be more useful than those built from computational radiomic features.",2019,10.1259/bjr.20190159,cross-sectional,diagnosis,CT,Lung
Non-invasive decision support for NSCLC treatment using PET/CT radiomics,"Two major treatment strategies employed in non-small cell lung cancer, NSCLC, are tyrosine kinase inhibitors, TKIs, and immune checkpoint inhibitors, ICIs. The choice of strategy is based on heterogeneous biomarkers that can dynamically change during therapy. Thus, there is a compelling need to identify comprehensive biomarkers that can be used longitudinally to help guide therapy choice. Herein, we report a (18)F-FDG-PET/CT-based deep learning model, which demonstrates high accuracy in EGFR mutation status prediction across patient cohorts from different institutions. A deep learning score (EGFR-DLS) was significantly and positively associated with longer progression free survival (PFS) in patients treated with EGFR-TKIs, while EGFR-DLS is significantly and negatively associated with higher durable clinical benefit, reduced hyperprogression, and longer PFS among patients treated with ICIs. Thus, the EGFR-DLS provides a non-invasive method for precise quantification of EGFR mutation status in NSCLC patients, which is promising to identify NSCLC patients sensitive to EGFR-TKI or ICI-treatments.",2020,10.1038/s41467-020-19116-x,cross-sectional,diagnosis,PET-CT,Lung
Non-Invasive Measurement Using Deep Learning Algorithm Based on Multi-Source Features Fusion to Predict PD-L1 Expression and Survival in NSCLC,"BACKGROUND: Programmed death-ligand 1 (PD-L1) assessment of lung cancer in immunohistochemical assays was only approved diagnostic biomarker for immunotherapy. But the tumor proportion score (TPS) of PD-L1 was challenging owing to invasive sampling and intertumoral heterogeneity. There was a strong demand for the development of an artificial intelligence (AI) system to measure PD-L1 expression signature (ES) non-invasively. METHODS: We developed an AI system using deep learning (DL), radiomics and combination models based on computed tomography (CT) images of 1,135 non-small cell lung cancer (NSCLC) patients with PD-L1 status. The deep learning feature was obtained through a 3D ResNet as the feature map extractor and the specialized classifier was constructed for the prediction and evaluation tasks. Then, a Cox proportional-hazards model combined with clinical factors and PD-L1 ES was utilized to evaluate prognosis in survival cohort. RESULTS: The combination model achieved a robust high-performance with area under the receiver operating characteristic curves (AUCs) of 0.950 (95% CI, 0.938-0.960), 0.934 (95% CI, 0.906-0.964), and 0.946 (95% CI, 0.933-0.958), for predicting PD-L1ES <1%, 1-49%, and ≥50% in validation cohort, respectively. Additionally, when combination model was trained on multi-source features the performance of overall survival evaluation (C-index: 0.89) could be superior compared to these of the clinical model alone (C-index: 0.86). CONCLUSION: A non-invasive measurement using deep learning was proposed to access PD-L1 expression and survival outcomes of NSCLC. This study also indicated that deep learning model combined with clinical characteristics improved prediction capabilities, which would assist physicians in making rapid decision on clinical treatment options.",2022,10.3389/fimmu.2022.828560,retrospective cohort,prognosis,CT,Lung
Non-small cell lung cancer: quantitative phenotypic analysis of CT images as a potential marker of prognosis,"This was a retrospective study to investigate the predictive and prognostic ability of quantitative computed tomography phenotypic features in patients with non-small cell lung cancer (NSCLC). 661 patients with pathological confirmed as NSCLC were enrolled between 2007 and 2014. 592 phenotypic descriptors was automatically extracted on the pre-therapy CT images. Firstly, support vector machine (SVM) was used to evaluate the predictive value of each feature for pathology and TNM clinical stage. Secondly, Cox proportional hazards model was used to evaluate the prognostic value of these imaging signatures selected by SVM which subjected to a primary cohort of 138 patients, and an external independent validation of 61 patients. The results indicated that predictive accuracy for histopathology, N staging, and overall clinical stage was 75.16%, 79.40% and 80.33%, respectively. Besides, Cox models indicated the signatures selected by SVM: ""correlation of co-occurrence after wavelet transform"" was significantly associated with overall survival in the two datasets (hazard ratio [HR]: 1.65, 95% confidence interval [CI]: 1.41-2.75, p = 0.010; and HR: 2.74, 95%CI: 1.10-6.85, p = 0.027, respectively). Our study indicates that the phenotypic features might provide some insight in metastatic potential or aggressiveness for NSCLC, which potentially offer clinical value in directing personalized therapeutic regimen selection for NSCLC.",2016,10.1038/srep38282,cross-sectional,diagnosis,CT,Lung
Non-small-cell lung cancer classification via RNA-Seq and histology imaging probability fusion,"BACKGROUND: Adenocarcinoma and squamous cell carcinoma are the two most prevalent lung cancer types, and their distinction requires different screenings, such as the visual inspection of histology slides by an expert pathologist, the analysis of gene expression or computer tomography scans, among others. In recent years, there has been an increasing gathering of biological data for decision support systems in the diagnosis (e.g. histology imaging, next-generation sequencing technologies data, clinical information, etc.). Using all these sources to design integrative classification approaches may improve the final diagnosis of a patient, in the same way that doctors can use multiple types of screenings to reach a final decision on the diagnosis. In this work, we present a late fusion classification model using histology and RNA-Seq data for adenocarcinoma, squamous-cell carcinoma and healthy lung tissue. RESULTS: The classification model improves results over using each source of information separately, being able to reduce the diagnosis error rate up to a 64% over the isolate histology classifier and a 24% over the isolate gene expression classifier, reaching a mean F1-Score of 95.19% and a mean AUC of 0.991. CONCLUSIONS: These findings suggest that a classification model using a late fusion methodology can considerably help clinicians in the diagnosis between the aforementioned lung cancer cancer subtypes over using each source of information separately. This approach can also be applied to any cancer type or disease with heterogeneous sources of information.",2021,10.1186/s12859-021-04376-1,,,,
Novel Autosegmentation Spatial Similarity Metrics Capture the Time Required to Correct Segmentations Better Than Traditional Metrics in a Thoracic Cavity Segmentation Workflow,"Automated segmentation templates can save clinicians time compared to de novo segmentation but may still take substantial time to review and correct. It has not been thoroughly investigated which automated segmentation-corrected segmentation similarity metrics best predict clinician correction time. Bilateral thoracic cavity volumes in 329 CT scans were segmented by a UNet-inspired deep learning segmentation tool and subsequently corrected by a fourth-year medical student. Eight spatial similarity metrics were calculated between the automated and corrected segmentations and associated with correction times using Spearman's rank correlation coefficients. Nine clinical variables were also associated with metrics and correction times using Spearman's rank correlation coefficients or Mann-Whitney U tests. The added path length, false negative path length, and surface Dice similarity coefficient correlated better with correction time than traditional metrics, including the popular volumetric Dice similarity coefficient (respectively ρ = 0.69, ρ = 0.65, ρ = - 0.48 versus ρ = - 0.25; correlation p values < 0.001). Clinical variables poorly represented in the autosegmentation tool's training data were often associated with decreased accuracy but not necessarily with prolonged correction time. Metrics used to develop and evaluate autosegmentation tools should correlate with clinical time saved. To our knowledge, this is only the second investigation of which metrics correlate with time saved. Validation of our findings is indicated in other anatomic sites and clinical workflows. Novel spatial similarity metrics may be preferable to traditional metrics for developing and evaluating autosegmentation tools that are intended to save clinicians time.",2021,10.1007/s10278-021-00460-3,cross-sectional,informatics,CT,Thorax
Novel ensemble of optimized CNN and dynamic selection techniques for accurate Covid-19 screening using chest CT images,"The world is significantly affected by infectious coronavirus disease (covid-19). Timely prognosis and treatment are important to control the spread of this infection. Unreliable screening systems and limited number of clinical facilities are the major hurdles in controlling the spread of covid-19. Nowadays, many automated detection systems based on deep learning techniques using computed tomography (CT) images have been proposed to detect covid-19. However, these systems have the following drawbacks: (i) limited data problem poses a major hindrance to train the deep neural network model to provide accurate diagnosis, (ii) random choice of hyperparameters of Convolutional Neural Network (CNN) significantly affects the classification performance, since the hyperparameters have to be application dependent and, (iii) the generalization ability using CNN classification is usually not validated. To address the aforementioned issues, we propose two models: (i) based on a transfer learning approach, and (ii) using novel strategy to optimize the CNN hyperparameters using Whale optimization-based BAT algorithm + AdaBoost classifier built using dynamic ensemble selection techniques. According to our second method depending on the characteristics of test sample, the classifier is chosen, thereby reducing the risk of overfitting and simultaneously produced promising results. Our proposed methodologies are developed using 746 CT images. Our method obtained a sensitivity, specificity, accuracy, F-1 score, and precision of 0.98, 0.97, 0.98, 0.98, and 0.98, respectively with five-fold cross-validation strategy. Our developed prototype is ready to be tested with huge chest CT images database before its real-world application.",2021,10.1016/j.compbiomed.2021.104835,cross-sectional,diagnosis,CT,Lung
Novel image markers for non-small cell lung cancer classification and survival prediction,"BACKGROUND: Non-small cell lung cancer (NSCLC), the most common type of lung cancer, is one of serious diseases causing death for both men and women. Computer-aided diagnosis and survival prediction of NSCLC, is of great importance in providing assistance to diagnosis and personalize therapy planning for lung cancer patients. RESULTS: In this paper we have proposed an integrated framework for NSCLC computer-aided diagnosis and survival analysis using novel image markers. The entire biomedical imaging informatics framework consists of cell detection, segmentation, classification, discovery of image markers, and survival analysis. A robust seed detection-guided cell segmentation algorithm is proposed to accurately segment each individual cell in digital images. Based on cell segmentation results, a set of extensive cellular morphological features are extracted using efficient feature descriptors. Next, eight different classification techniques that can handle high-dimensional data have been evaluated and then compared for computer-aided diagnosis. The results show that the random forest and adaboost offer the best classification performance for NSCLC. Finally, a Cox proportional hazards model is fitted by component-wise likelihood based boosting. Significant image markers have been discovered using the bootstrap analysis and the survival prediction performance of the model is also evaluated. CONCLUSIONS: The proposed model have been applied to a lung cancer dataset that contains 122 cases with complete clinical information. The classification performance exhibits high correlations between the discovered image markers and the subtypes of NSCLC. The survival analysis demonstrates strong prediction power of the statistical model built from the discovered image markers.",2014,10.1186/1471-2105-15-310,,,,
Novel loss functions for ensemble-based medical image classification,"Medical images commonly exhibit multiple abnormalities. Predicting them requires multi-class classifiers whose training and desired reliable performance can be affected by a combination of factors, such as, dataset size, data source, distribution, and the loss function used to train deep neural networks. Currently, the cross-entropy loss remains the de-facto loss function for training deep learning classifiers. This loss function, however, asserts equal learning from all classes, leading to a bias toward the majority class. Although the choice of the loss function impacts model performance, to the best of our knowledge, we observed that no literature exists that performs a comprehensive analysis and selection of an appropriate loss function toward the classification task under study. In this work, we benchmark various state-of-the-art loss functions, critically analyze model performance, and propose improved loss functions for a multi-class classification task. We select a pediatric chest X-ray (CXR) dataset that includes images with no abnormality (normal), and those exhibiting manifestations consistent with bacterial and viral pneumonia. We construct prediction-level and model-level ensembles to improve classification performance. Our results show that compared to the individual models and the state-of-the-art literature, the weighted averaging of the predictions for top-3 and top-5 model-level ensembles delivered significantly superior classification performance (p < 0.05) in terms of MCC (0.9068, 95% confidence interval (0.8839, 0.9297)) metric. Finally, we performed localization studies to interpret model behavior and confirm that the individual models and ensembles learned task-specific features and highlighted disease-specific regions of interest. The code is available at https://github.com/sivaramakrishnan-rajaraman/multiloss_ensemble_models.",2021,10.1371/journal.pone.0261307,cross-sectional,diagnosis,Radiograph,Lung
Novel real-time tumor-contouring method using deep learning to prevent mistracking in X-ray fluoroscopy,"Robustness to obstacles is the most important factor necessary to achieve accurate tumor tracking without fiducial markers. Some high-density structures, such as bone, are enhanced on X-ray fluoroscopic images, which cause tumor mistracking. Tumor tracking should be performed by controlling ""importance recognition"": the understanding that soft-tissue is an important tracking feature and bone structure is unimportant. We propose a new real-time tumor-contouring method that uses deep learning with importance recognition control. The novelty of the proposed method is the combination of the devised random overlay method and supervised deep learning to induce the recognition of structures in tumor contouring as important or unimportant. This method can be used for tumor contouring because it uses deep learning to perform image segmentation. Our results from a simulated fluoroscopy model showed accurate tracking of a low-visibility tumor with an error of approximately 1 mm, even if enhanced bone structure acted as an obstacle. A high similarity of approximately 0.95 on the Jaccard index was observed between the segmented and ground truth tumor regions. A short processing time of 25 ms was achieved. The results of this simulated fluoroscopy model support the feasibility of robust real-time tumor contouring with fluoroscopy. Further studies using clinical fluoroscopy are highly anticipated.",2018,10.1007/s12194-017-0435-0,cross-sectional,informatics,Fluoroscopy,Thorax
"Novel, non-invasive imaging approach to identify patients with advanced non-small cell lung cancer at risk of hyperprogressive disease with immune checkpoint blockade","PURPOSE: Hyperprogression is an atypical response pattern to immune checkpoint inhibition that has been described within non-small cell lung cancer (NSCLC). The paradoxical acceleration of tumor growth after immunotherapy has been associated with significantly shortened survival, and currently, there are no clinically validated biomarkers to identify patients at risk of hyperprogression. EXPERIMENTAL DESIGN: A total of 109 patients with advanced NSCLC who underwent monotherapy with Programmed cell death protein-1 (PD1)/Programmed death-ligand-1 (PD-L1) inhibitors were included in the study. Using RECIST measurements, we divided the patients into responders (n=50) (complete/partial response or stable disease) and non-responders (n=59) (progressive disease). Tumor growth kinetics were used to further identify hyperprogressors (HPs, n=19) among non-responders. Patients were randomized into a training set (D(1)=30) and a test set (D(2)=79) with the essential caveat that HPs were evenly distributed among the two sets. A total of 198 radiomic textural patterns from within and around the target nodules and features relating to tortuosity of the nodule associated vasculature were extracted from the pretreatment CT scans. RESULTS: The random forest classifier using the top features associated with hyperprogression was able to distinguish between HP and other radiographical response patterns with an area under receiver operating curve of 0.85±0.06 in the training set (D(1)=30) and 0.96 in the validation set (D(2)=79). These features included one peritumoral texture feature from 5 to 10 mm outside the tumor and two nodule vessel-related tortuosity features. Kaplan-Meier survival curves showed a clear stratification between classifier predicted HPs versus non-HPs for overall survival (D(2): HR=2.66, 95% CI 1.27 to 5.55; p=0.009). CONCLUSIONS: Our study suggests that image-based radiomics markers extracted from baseline CTs of advanced NSCLC treated with PD-1/PD-L1 inhibitors may help identify patients at risk of hyperprogressions.",2020,10.1136/jitc-2020-001343,retrospective cohort,treatment,CT,Lung
NSCR-Based DenseNet for Lung Tumor Recognition Using Chest CT Image,"Nonnegative sparse representation has become a popular methodology in medical analysis and diagnosis in recent years. In order to resolve network degradation, higher dimensionality in feature extraction, data redundancy, and other issues faced when medical images parameters are trained using convolutional neural networks. Lung tumors in chest CT image based on nonnegative, sparse, and collaborative representation classification of DenseNet (DenseNet-NSCR) are proposed by this paper: firstly, initialization parameters of pretrained DenseNet model using transfer learning; secondly, training DenseNet using CT images to extract feature vectors for the full connectivity layer; thirdly, a nonnegative, sparse, and collaborative representation (NSCR) is used to represent the feature vector and solve the coding coefficient matrix; fourthly, the residual similarity is used for classification. The experimental results show that the DenseNet-NSCR classification is better than the other models, and the various evaluation indexes such as specificity and sensitivity are also high, and the method has better robustness and generalization ability through comparison experiment using AlexNet, GoogleNet, and DenseNet-201 models.",2020,10.1155/2020/6636321,cross-sectional,diagnosis,CT,Lung
Objective evaluation of deep uncertainty predictions for COVID-19 detection,"Deep neural networks (DNNs) have been widely applied for detecting COVID-19 in medical images. Existing studies mainly apply transfer learning and other data representation strategies to generate accurate point estimates. The generalization power of these networks is always questionable due to being developed using small datasets and failing to report their predictive confidence. Quantifying uncertainties associated with DNN predictions is a prerequisite for their trusted deployment in medical settings. Here we apply and evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray (CXR) images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced. Through comprehensive experiments, it is shown that networks pertained on CXR images outperform networks pretrained on natural image datasets such as ImageNet. Qualitatively and quantitatively evaluations also reveal that the predictive uncertainty estimates are statistically higher for erroneous predictions than correct predictions. Accordingly, uncertainty quantification methods are capable of flagging risky predictions with high uncertainty estimates. We also observe that ensemble methods more reliably capture uncertainties during the inference. DNN-based solutions for COVID-19 detection have been mainly proposed without any principled mechanism for risk mitigation. Previous studies have mainly focused on on generating single-valued predictions using pretrained DNNs. In this paper, we comprehensively apply and comparatively evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced for the first time. Using these new uncertainty performance metrics, we quantitatively demonstrate when we could trust DNN predictions for COVID-19 detection from chest X-rays. It is important to note the proposed novel uncertainty evaluation metrics are generic and could be applied for evaluation of probabilistic forecasts in all classification problems.",2022,10.1038/s41598-022-05052-x,cross-sectional,diagnosis,Radiograph,Lung
Objectively Measured Chronic Lung Injury on Chest CT,"BACKGROUND: Tobacco smoke exposure is associated with emphysema and pulmonary fibrosis, both of which are irreversible. We have developed a new objective CT analysis tool that combines densitometry with machine learning to detect high attenuation changes in visually normal appearing lung (Norm(HA)) that may precede these diseases. METHODS: We trained the classification tool by placing 34,528 training points in chest CT scans from 297 COPDGene participants. The tool was then used to classify lung tissue in 9,038 participants as normal, emphysema, fibrotic/interstitial, or Norm(HA). Associations between the quartile of Norm(HA) and plasma-based biomarkers, clinical severity, and mortality were evaluated using Jonckheere-Terpstra, pairwise Wilcoxon rank-sum tests, and multivariable linear and Cox regression. RESULTS: A higher percentage of lung occupied by Norm(HA) was associated with higher C-reactive protein and intercellular adhesion molecule 1 (P for trend for both < .001). In analyses adjusted for multiple covariates, including high and low attenuation area, compared with those in the lowest quartile of Norm(HA), those in the highest quartile had a 6.50 absolute percent lower percent predicted lower FEV(1) (P < .001), an 8.48 absolute percent lower percent predicted forced expiratory volume, a 10.78-meter shorter 6-min walk distance (P = .011), and a 56% higher risk of death (P = .003). These findings were present even in those individuals without visually defined interstitial lung abnormalities. CONCLUSIONS: A new class of Norm(HA) on CT may represent a unique tissue class associated with adverse outcomes, independent of emphysema and fibrosis.",2019,10.1016/j.chest.2019.05.020,,,,
Ocular surface assessment in times of sanitary crisis: What lessons and solutions for the present and the future?,"PURPOSE: To describe the immediate consequences of SARS-CoV-2 and the COVID-19 pandemic on the ocular surface and eye-care professionals, and to discuss the need for a mandatory switch from currently performed tele-screening to true teleconsultation for remote ocular surface assessment. MAIN FINDINGS: Ophthalmologists have been largely impacted by the COVID-19 sanitary crisis, due to both the ocular manifestations of SARS-CoV-2 and to the high contagiousness of the virus. The proximity of ophthalmologists to their patients have pushed eye-care providers to readapt their practices and develop alternatives to face-to-face consultations. However, teleconsultation has some major limitations and drawbacks, especially for ocular surface assessment that relies on high-quality graphic data for adequate diagnosis. Tele-screening, on the other hand, emphasizes on the importance of history-taking and listening to the patient in order to adequately prioritize appointments based on the presumed degree of emergency. CONCLUSION: Despite all the enthusiasm, tele-screening as currently performed with the available tools is still not capable of completely replacing a standard ophthalmic examination for the assessment of ocular surface diseases. While waiting for new emerging technologies and future implementation of imaging modalities and artificial intelligence, decision making algorithms can help eye-practitioners remotely screen their patients to assess the optimal time for follow-up appointments.",2021,10.1177/1120672120978881,,,,
On the Automated Segmentation of Epicardial and Mediastinal Cardiac Adipose Tissues Using Classification Algorithms,"The quantification of fat depots on the surroundings of the heart is an accurate procedure for evaluating health risk factors correlated with several diseases. However, this type of evaluation is not widely employed in clinical practice due to the required human workload. This work proposes a novel technique for the automatic segmentation of cardiac fat pads. The technique is based on applying classification algorithms to the segmentation of cardiac CT images. Furthermore, we extensively evaluate the performance of several algorithms on this task and discuss which provided better predictive models. Experimental results have shown that the mean accuracy for the classification of epicardial and mediastinal fats has been 98.4% with a mean true positive rate of 96.2%. On average, the Dice similarity index, regarding the segmented patients and the ground truth, was equal to 96.8%. Therfore, our technique has achieved the most accurate results for the automatic segmentation of cardiac fats, to date.",2015,,,,,
"On the performance of lung nodule detection, segmentation and classification","Computed tomography (CT) screening is an effective way for early detection of lung cancer in order to improve the survival rate of such a deadly disease. For more than two decades, image processing techniques such as nodule detection, segmentation, and classification have been extensively studied to assist physicians in identifying nodules from hundreds of CT slices to measure shapes and HU distributions of nodules automatically and to distinguish their malignancy. Thanks to new parallel computation, multi-layer convolution, nonlinear pooling operation, and the big data learning strategy, recent development of deep-learning algorithms has shown great progress in lung nodule screening and computer-assisted diagnosis (CADx) applications due to their high sensitivity and low false positive rates. This paper presents a survey of state-of-the-art deep-learning-based lung nodule screening and analysis techniques focusing on their performance and clinical applications, aiming to help better understand the current performance, the limitation, and the future trends of lung nodule analysis.",2021,10.1016/j.compmedimag.2021.101886,,,,
On the robustness of deep learning-based lung-nodule classification for CT images with respect to image noise,"Robustness is an important aspect when evaluating a method of medical image analysis. In this study, we investigated the robustness of a deep learning (DL)-based lung-nodule classification model for CT images with respect to noise perturbations. A deep neural network (DNN) was established to classify 3D CT images of lung nodules into malignant or benign groups. The established DNN was able to predict malignancy rate of lung nodules based on CT images, achieving the area under the curve of 0.91 for the testing dataset in a tenfold cross validation as compared to radiologists' prediction. We then evaluated its robustness against noise perturbations. We added to the input CT images noise signals generated randomly or via an optimization scheme using a realistic noise model based on a noise power spectrum for a given mAs level, and monitored the DNN's output. The results showed that the CT noise was able to affect the prediction results of the established DNN model. With random noise perturbations at 100 mAs, DNN's predictions for 11.2% of training data and 17.4% of testing data were successfully altered by at least once. The percentage increased to 23.4% and 34.3%, respectively, for optimization-based perturbations. We further evaluated robustness of models with different architectures, parameters, number of output labels, etc, and robustness concern was found in these models to different degrees. To improve model robustness, we empirically proposed an adaptive training scheme. It fine-tuned the DNN model by including perturbations in the training dataset that successfully altered the DNN's perturbations. The adaptive scheme was repeatedly performed to gradually improve DNN's robustness. The numbers of perturbations at 100 mAs affecting DNN's predictions were reduced to 10.8% for training and 21.1% for testing by the adaptive training scheme after two iterations. Our study illustrated that robustness may potentially be a concern for an exemplary DL-based lung-nodule classification model for CT images, indicating the needs for evaluating and ensuring model robustness when developing similar models. The proposed adaptive training scheme may be able to improve model robustness.",2020,10.1088/1361-6560/abc812,cross-sectional,diagnosis,CT,Lung
On the Use of Deep Learning for Imaging-Based COVID-19 Detection Using Chest X-rays,"The global COVID-19 pandemic that started in 2019 and created major disruptions around the world demonstrated the imperative need for quick, inexpensive, accessible and reliable diagnostic methods that would allow the detection of infected individuals with minimal resources. Radiography, and more specifically, chest radiography, is a relatively inexpensive medical imaging modality that can potentially offer a solution for the diagnosis of COVID-19 cases. In this work, we examined eleven deep convolutional neural network architectures for the task of classifying chest X-ray images as belonging to healthy individuals, individuals with COVID-19 or individuals with viral pneumonia. All the examined networks are established architectures that have been proven to be efficient in image classification tasks, and we evaluated three different adjustments to modify the architectures for the task at hand by expanding them with additional layers. The proposed approaches were evaluated for all the examined architectures on a dataset with real chest X-ray images, reaching the highest classification accuracy of 98.04% and the highest F1-score of 98.22% for the best-performing setting.",2021,10.3390/s21175702,cross-sectional,diagnosis,Radiograph,Lung
One deep learning local-global model based on CT imaging to differentiate between nodular cryptococcosis and lung cancer which are hard to be diagnosed,"OBJECTIVES: We aim to evaluate a deep learning (DL) model and radiomic model for preoperative differentiation of nodular cryptococcosis from solitary lung cancer in patients with malignant features on CT images. MATERIALS AND METHODS: We retrospectively recruited 319 patients with solitary pulmonary nodules and suspicious signs of malignancy from three hospitals. All lung nodules were resected, and one by one radiologic-pathologic correlation was performed. A three-dimensional DL model was used for tumor segmentation and extraction of three-dimensional radiomic features. We used the Max-Relevance and Min-Redundancy algorithm and the eXtreme Gradient Boosting algorithm to select the nodular radiomics features. We proposed a DL local-global model, a DL local model and radiomic model to preoperatively differentiate nodular cryptococcosis from solitary lung cancer. The DL local-global model includes information of both nodules and the whole lung, while the DL local model only includes information of solitary lung nodules. Five-fold cross-validation was used to select and validate these models. The prediction performance of the model was evaluated using receiver operating characteristic curve (ROC) and calibration curve. A new loss function was applied in our deep learning framework to optimize the area under the ROC curve (AUC) directly. RESULTS: 295 patients were enrolled and they were non-symptomatic, with negative tumor markers and fungus markers in blood tests. These patients have not been diagnosed by the combination of CT imaging, laboratory results and clinical data. The lung volume was slightly larger in patients with lung cancers than that in patients with cryptococcosis (3552.8 ± 1184.6 ml vs 3491.9 ± 1017.8 ml). The DL local-global model achieved the best performance in differentiating between nodular cryptococcosis and lung cancer (area under the curve [AUC] = 0.88), which was higher than that of the DL local model (AUC = 0.84) and radiomic (AUC = 0.79) model. CONCLUSION: The DL local-global model is a non-invasive diagnostic tool to differentiate between nodular cryptococcosis and lung cancer nodules which are hard to be diagnosed by the combination of CT imaging, laboratory results and clinical data, and overtreatment may be avoided.",2021,10.1016/j.compmedimag.2021.102009,case control,diagnosis,CT,Lung
One-stage pulmonary nodule detection using 3-D DCNN with feature fusion and attention mechanism in CT image,"BACKGROUND AND OBJECTIVE: Lung cancer is the most common cause of cancer-related death in the world. Low-dose computed tomography (LDCT) is a widely used modality in lung cancer detection. The nodule is an abnormal tissue and may evolve into lung cancer. Hence, it is crucial to detect nodules in the early detection stage. However, reviewing the LDCT scans to observe suspicious nodules is a time-consuming task. Recently, designing a computer-aided detection (CADe) system with convolutional neural network (CNN) architecture has been proven that it is helpful for radiologists. Hence, in this study, a 3-D YOLO-based CADe system, 3-D OSAF-YOLOv3, is proposed for nodule detection in LDCT images. METHODS: The proposed CADe system consists of data preprocessing, nodule detection, and non-maximum suppression algorithm (NMS). At first, the data preprocessing including the background elimination, the spacing normalization, and the volume of interest (VOI) extraction, are conducted to remove the non-lung region, normalize the image spacing, and divide LDCT image into numerous VOIs. Then, the VOIs are fed into the 3-D OSAF-YOLOv3 model, to detect the suspicious nodules. The proposed model is constructed by integrating the 3-D YOLOv3 with the one-shot aggregation module (OSA), the receptive field block (RFB), and the feature fusion scheme (FFS). Finally, the NMS algorithm is performed to eliminate the duplicated detection generated by the model. RESULTS: In this study, the LUNA-16 dataset composed 1186 nodules from 888 LDCT scans and the competition performance metric (CPM) are used to evaluate our CADe system. In the experiment results, the proposed system can achieve a sensitivities rate of 0.962 with the false positive rate of 8 and complete a CPM value of 0.905. Moreover, according to the ablation study results, the employment of OSA module, RFB, and FFS could improve the detection performance actually. Furthermore, compared to other start-of-the-art (SOTA) models, our detection system could also achieve the higher performance. CONCLUSIONS: In this study, a YOLO-based CADe system for nodule detection in CT image system integrating additional modules and scheme is proposed for nodule detection in LDCT. The result indicates that the proposed the modification can significantly improve detection performance.",2022,10.1016/j.cmpb.2022.106786,cross-sectional,diagnosis,CT,Lung
Open resource of clinical data from patients with pneumonia for the prediction of COVID-19 outcomes via deep learning,"Data from patients with coronavirus disease 2019 (COVID-19) are essential for guiding clinical decision making, for furthering the understanding of this viral disease, and for diagnostic modelling. Here, we describe an open resource containing data from 1,521 patients with pneumonia (including COVID-19 pneumonia) consisting of chest computed tomography (CT) images, 130 clinical features (from a range of biochemical and cellular analyses of blood and urine samples) and laboratory-confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) clinical status. We show the utility of the database for prediction of COVID-19 morbidity and mortality outcomes using a deep learning algorithm trained with data from 1,170 patients and 19,685 manually labelled CT slices. In an independent validation cohort of 351 patients, the algorithm discriminated between negative, mild and severe cases with areas under the receiver operating characteristic curve of 0.944, 0.860 and 0.884, respectively. The open database may have further uses in the diagnosis and management of patients with COVID-19.",2020,10.1038/s41551-020-00633-5,retrospective cohort,prognosis,CT,Lung
Optical coherence tomography for identification of malignant pulmonary nodules based on random forest machine learning algorithm,"OBJECTIVE: To explore the feasibility of using random forest (RF) machine learning algorithm in assessing normal and malignant peripheral pulmonary nodules based on in vivo endobronchial optical coherence tomography (EB-OCT). METHODS: A total of 31 patients with pulmonary nodules were admitted to Department of Respiratory Medicine, Zhongda Hospital, Southeast University, and underwent chest CT, EB-OCT and biopsy. Attenuation coefficient and up to 56 different image features were extracted from A-line and B-scan of 1703 EB-OCT images. Attenuation coefficient and 29 image features with significant p-values were used to analyze the differences between normal and malignant samples. A RF classifier was trained using 70% images as training set, while 30% images were included in the testing set. The accuracy of the automated classification was validated by clinically proven pathological results. RESULTS: Attenuation coefficient and 29 image features were found to present different properties with significant p-values between normal and malignant EB-OCT images. The RF algorithm successfully classified the malignant pulmonary nodules with sensitivity, specificity, and accuracy of 90.41%, 77.87% and 83.51% respectively. CONCLUSION: It is clinically practical to distinguish the nature of pulmonary nodules by integrating EB-OCT imaging with automated machine learning algorithm. Diagnosis of malignant pulmonary nodules by analyzing quantitative features from EB-OCT images could be a potentially powerful way for early detection of lung cancer.",2021,10.1371/journal.pone.0260600,cross-sectional,diagnosis,OCT,Lung
Optimal Deep-Learning-Enabled Intelligent Decision Support System for SARS-CoV-2 Classification,"Intelligent decision support systems (IDSS) for complex healthcare applications aim to examine a large quantity of complex healthcare data to assist doctors, researchers, pathologists, and other healthcare professionals. A decision support system (DSS) is an intelligent system that provides improved assistance in various stages of health-related disease diagnosis. At the same time, the SARS-CoV-2 infection that causes COVID-19 disease has spread globally from the beginning of 2020. Several research works reported that the imaging pattern based on computed tomography (CT) can be utilized to detect SARS-CoV-2. Earlier identification and detection of the diseases is essential to offer adequate treatment and avoid the severity of the disease. With this motivation, this study develops an efficient deep-learning-based fusion model with swarm intelligence (EDLFM-SI) for SARS-CoV-2 identification. The proposed EDLFM-SI technique aims to detect and classify the SARS-CoV-2 infection or not. Also, the EDLFM-SI technique comprises various processes, namely, data augmentation, preprocessing, feature extraction, and classification. Moreover, a fusion of capsule network (CapsNet) and MobileNet based feature extractors are employed. Besides, a water strider algorithm (WSA) is applied to fine-tune the hyperparameters involved in the DL models. Finally, a cascaded neural network (CNN) classifier is applied for detecting the existence of SARS-CoV-2. In order to showcase the improved performance of the EDLFM-SI technique, a wide range of simulations take place on the COVID-19 CT data set and the SARS-CoV-2 CT scan data set. The simulation outcomes highlighted the supremacy of the EDLFM-SI technique over the recent approaches.",2022,10.1155/2022/4130674,cross-sectional,diagnosis,CT,Lung
Optimal Diagnosis of COVID-19 Based on Convolutional Neural Network and Red Fox Optimization Algorithm,"SARS-CoV-2 is a specific type of Coronavirus that was firstly reported in China in December 2019 and is the causative agent of coronavirus disease 2019 (COVID-19). In March 2020, this disease spread to different parts of the world causing a global pandemic. Although this disease is still increasing exponentially day by day, early diagnosis of this disease is very important to reduce the death rate and to reduce the prevalence of this pandemic. Since there are sometimes human errors by physicians in the diagnosis of this disease, using computer-aided diagnostic systems can be helpful to get more accurate results. In this paper, chest X-ray images have been examined using a new pipeline machine vision-based system to provide more accurate results. In the proposed method, after preprocessing the input X-ray images, the region of interest has been segmented. Then, a combined gray-level cooccurrence matrix (GLCM) and Discrete Wavelet Transform (DWT) features have been extracted from the processed images. Finally, an improved version of Convolutional Neural Network (CNN) based on the Red Fox Optimization algorithm is employed for the classification of the images based on the features. The proposed method is validated by performing to three datasets and its results are compared with some state-of-the-art methods. The final results show that the suggested method has proper efficiency toward the others for the diagnosis of COVID-19.",2021,10.1155/2021/4454507,cross-sectional,diagnosis,Radiograph,Lung
Optimal matrix size of chest radiographs for computer-aided detection on lung nodule or mass with deep learning,"OBJECTIVES: To investigate the optimal input matrix size for deep learning-based computer-aided detection (CAD) of nodules and masses on chest radiographs. METHODS: We retrospectively collected 2088 abnormal (nodule/mass) and 352 normal chest radiographs from two institutions. Three thoracic radiologists drew 2758 abnormalities regions. A total of 1736 abnormal chest radiographs were used for training and tuning convolutional neural networks (CNNs). The remaining 352 abnormal and 352 normal chest radiographs were used as a test set. Two CNNs (Mask R-CNN and RetinaNet) were selected to validate the effects of the squared different matrix size of chest radiograph (256, 448, 896, 1344, and 1792). For comparison, figure of merit (FOM) of jackknife free-response receiver operating curve and sensitivity were obtained. RESULTS: In Mask R-CNN, matrix size 896 and 1344 achieved significantly higher FOM (0.869 and 0.856, respectively) for detecting abnormalities than 256, 448, and 1792 (0.667-0.820) (p < 0.05). In RetinaNet, matrix size 896 was significantly higher FOM (0.906) than others (0.329-0.832) (p < 0.05). For sensitivity of abnormalities, there was a tendency to increase sensitivity when lesion size increases. For small nodules (< 10 mm), the sensitivities were 0.418 and 0.409, whereas the sensitivities were 0.937 and 0.956 for masses. Matrix size 896 and 1344 in Mask R-CNN and matrix size 896 in RetinaNet showed significantly higher sensitivity than others (p < 0.05). CONCLUSIONS: Matrix size 896 had the highest performance for various sizes of abnormalities using different CNNs. The optimal matrix size of chest radiograph could improve CAD performance without additional training data. KEY POINTS: • Input matrix size significantly affected the performance of a deep learning-based CAD for detection of nodules or masses on chest radiographs. • The matrix size 896 showed the best performance in two different CNN detection models. • The optimal matrix size of chest radiographs could enhance CAD performance without additional training data.",2020,10.1007/s00330-020-06892-9,cross-sectional,diagnosis,Radiograph,Lung
Optimised genetic algorithm-extreme learning machine approach for automatic COVID-19 detection,"The coronavirus disease (COVID-19), is an ongoing global pandemic caused by severe acute respiratory syndrome. Chest Computed Tomography (CT) is an effective method for detecting lung illnesses, including COVID-19. However, the CT scan is expensive and time-consuming. Therefore, this work focus on detecting COVID-19 using chest X-ray images because it is widely available, faster, and cheaper than CT scan. Many machine learning approaches such as Deep Learning, Neural Network, and Support Vector Machine; have used X-ray for detecting the COVID-19. Although the performance of those approaches is acceptable in terms of accuracy, however, they require high computational time and more memory space. Therefore, this work employs an Optimised Genetic Algorithm-Extreme Learning Machine (OGA-ELM) with three selection criteria (i.e., random, K-tournament, and roulette wheel) to detect COVID-19 using X-ray images. The most crucial strength factors of the Extreme Learning Machine (ELM) are: (i) high capability of the ELM in avoiding overfitting; (ii) its usability on binary and multi-type classifiers; and (iii) ELM could work as a kernel-based support vector machine with a structure of a neural network. These advantages make the ELM efficient in achieving an excellent learning performance. ELMs have successfully been applied in many domains, including medical domains such as breast cancer detection, pathological brain detection, and ductal carcinoma in situ detection, but not yet tested on detecting COVID-19. Hence, this work aims to identify the effectiveness of employing OGA-ELM in detecting COVID-19 using chest X-ray images. In order to reduce the dimensionality of a histogram oriented gradient features, we use principal component analysis. The performance of OGA-ELM is evaluated on a benchmark dataset containing 188 chest X-ray images with two classes: a healthy and a COVID-19 infected. The experimental result shows that the OGA-ELM achieves 100.00% accuracy with fast computation time. This demonstrates that OGA-ELM is an efficient method for COVID-19 detecting using chest X-ray images.",2020,10.1371/journal.pone.0242899,cross-sectional,diagnosis,CT,Lung
Optimized chest X-ray image semantic segmentation networks for COVID-19 early detection,"BACKGROUND: Although detection of COVID-19 from chest X-ray radiography (CXR) images is faster than PCR sputum testing, the accuracy of detecting COVID-19 from CXR images is lacking in the existing deep learning models. OBJECTIVE: This study aims to classify COVID-19 and normal patients from CXR images using semantic segmentation networks for detecting and labeling COVID-19 infected lung lobes in CXR images. METHODS: For semantically segmenting infected lung lobes in CXR images for COVID-19 early detection, three structurally different deep learning (DL) networks such as SegNet, U-Net and hybrid CNN with SegNet plus U-Net, are proposed and investigated. Further, the optimized CXR image semantic segmentation networks such as GWO SegNet, GWO U-Net, and GWO hybrid CNN are developed with the grey wolf optimization (GWO) algorithm. The proposed DL networks are trained, tested, and validated without and with optimization on the openly available dataset that contains 2,572 COVID-19 CXR images including 2,174 training images and 398 testing images. The DL networks and their GWO optimized networks are also compared with other state-of-the-art models used to detect COVID-19 CXR images. RESULTS: All optimized CXR image semantic segmentation networks for COVID-19 image detection developed in this study achieved detection accuracy higher than 92%. The result shows the superiority of optimized SegNet in segmenting COVID-19 infected lung lobes and classifying with an accuracy of 98.08% compared to optimized U-Net and hybrid CNN. CONCLUSION: The optimized DL networks has potential to be utilised to more objectively and accurately identify COVID-19 disease using semantic segmentation of COVID-19 CXR images of the lungs.",2022,10.3233/xst-211113,cross-sectional,informatics,Radiograph,Lung
Optimizing the radiomics-machine-learning model based on non-contrast enhanced CT for the simplified risk categorization of thymic epithelial tumors: A large cohort retrospective study,"PURPOSE: This study aimed to establish and compare the radiomics machine learning (ML) models based on non-contrast enhanced computed tomography (NECT) and clinical features for predicting the simplified risk categorization of thymic epithelial tumors (TETs). EXPERIMENTAL DESIGN: A total of 509 patients with pathologically confirmed TETs from January 2009 to May 2018 were retrospectively enrolled, consisting of 238 low-risk thymoma (LRT), 232 high-risk thymoma (HRT), and 39 thymic carcinoma (TC), and were divided into training (n = 433) and testing cohorts (n = 76) according to the admission time. Volumes of interest (VOIs) covering the whole tumor were manually segmented on preoperative NECT images. A total of 1218 radiomic features were extracted from the VOIs, and 4 clinical variables were collected from the hospital database. Fourteen ML models, along with varied feature selection strategies, were used to establish triple-classification models using the radiomic features (radiomic models), while clinical-radiomic models were built after combining with the clinical variables. The diagnostic accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC) of radiologist assessment, the radiomic and clinical-radiomic models were evaluated on the testing cohort. RESULTS: The Support Vector Machine (SVM) clinical-radiomic model demonstrated the highest AUC of 0.841 (95% CI 0.820 to 0.861) on the cross-validation result and reached an AUC of 0.844 (95% CI 0.793 to 0.894) in the testing cohort. For the one-vs-rest question of LRT vs HRT + TC, the sensitivity, specificity, and accuracy reached 80.00%, 63.41%, and 71.05%, respectively. For HRT vs LRT + TC, they reached 60.53%, 78.95%, and 69.74%. For TC vs LRT + HRT they reached 33.33%, 98.63%, and 96.05%, respectively. Compared with the radiomic models, superior diagnostic efficacy was demonstrated for most clinical-radiomics models, and the AUC of the Bernoulli Naive Bayes model was significantly improved. Radiologist2's assessment achieved a higher AUC of 0.813 (95% CI: 0.756-0.8761) than other radiologists, which was slightly lower than the SVM clinical-radiomic model. Combined with other evaluation indicators, SVM, as the best ML model, demonstrated the potential of predicting the simplified risk categorization of TETs with superior predictive performance to that of radiologists' assessment. CONCLUSION: Most of the ML models are promising in predicting the simplified TETs risk categorization with superior efficacy to that of radiologists' assessment, especially the SVM models, demonstrated the integration of ML with NECT may be valuable in aiding the diagnosis and treatment planning.",2022,10.1016/j.lungcan.2022.03.007,cross-sectional,diagnosis,CT,Thymus
Overall Survival Prognostic Modelling of Non-small Cell Lung Cancer Patients Using Positron Emission Tomography/Computed Tomography Harmonised Radiomics Features: The Quest for the Optimal Machine Learning Algorithm,"AIMS: Despite the promising results achieved by radiomics prognostic models for various clinical applications, multiple challenges still need to be addressed. The two main limitations of radiomics prognostic models include information limitation owing to single imaging modalities and the selection of optimum machine learning and feature selection methods for the considered modality and clinical outcome. In this work, we applied several feature selection and machine learning methods to single-modality positron emission tomography (PET) and computed tomography (CT) and multimodality PET/CT fusion to identify the best combinations for different radiomics modalities towards overall survival prediction in non-small cell lung cancer patients. MATERIALS AND METHODS: A PET/CT dataset from The Cancer Imaging Archive, including subjects from two independent institutions (87 and 95 patients), was used in this study. Each cohort was used once as training and once as a test, followed by averaging of the results. ComBat harmonisation was used to address the centre effect. In our proposed radiomics framework, apart from single-modality PET and CT models, multimodality radiomics models were developed using multilevel (feature and image levels) fusion. Two different methods were considered for the feature-level strategy, including concatenating PET and CT features into a single feature set and alternatively averaging them. For image-level fusion, we used three different fusion methods, namely wavelet fusion, guided filtering-based fusion and latent low-rank representation fusion. In the proposed prognostic modelling framework, combinations of four feature selection and seven machine learning methods were applied to all radiomics modalities (two single and five multimodalities), machine learning hyper-parameters were optimised and finally the models were evaluated in the test cohort with 1000 repetitions via bootstrapping. Feature selection and machine learning methods were selected as popular techniques in the literature, supported by open source software in the public domain and their ability to cope with continuous time-to-event survival data. Multifactor ANOVA was used to carry out variability analysis and the proportion of total variance explained by radiomics modality, feature selection and machine learning methods was calculated by a bias-corrected effect size estimate known as ω(2). RESULTS: Optimum feature selection and machine learning methods differed owing to the applied radiomics modality. However, minimum depth (MD) as feature selection and Lasso and Elastic-Net regularized generalized linear model (glmnet) as machine learning method had the highest average results. Results from the ANOVA test indicated that the variability that each factor (radiomics modality, feature selection and machine learning methods) introduces to the performance of models is case specific, i.e. variances differ regarding different radiomics modalities and fusion strategies. Overall, the greatest proportion of variance was explained by machine learning, except for models in feature-level fusion strategy. CONCLUSION: The identification of optimal feature selection and machine learning methods is a crucial step in developing sound and accurate radiomics risk models. Furthermore, optimum methods are case specific, differing due to the radiomics modality and fusion strategy used.",2022,10.1016/j.clon.2021.11.014,cross-sectional,prognosis,PET-CT,Lung
PadChest: A large chest x-ray image dataset with multi-label annotated reports,"We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray databases suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/.",2020,10.1016/j.media.2020.101797,,,,
Pandemic analysis of infection and death correlated with genomic open reading frame 10 mutation in severe acute respiratory syndrome coronavirus 2 victims,"BACKGROUND: Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) continues the pandemic spread of the coronavirus disease 2019 (COVID-19), over 60 million people confirmed infected and at least 1.8 million dead. One of the most known features of this RNA virus is its easiness to be mutated. In late 2020, almost no region of this SARS-CoV-2 genome can be found completely conserved within the original Wuhan coronavirus. Any information of the SARS-CoV-2 variants emerged through as time being will be evaluated for diagnosis, treatment, and prevention of COVID-19. METHODS: We extracted more than two million data of SARS-CoV-2 infected patients from the open COVID-19 dashboard. The sequences of the 38-amino acid putative open reading frame 10 (Orf10) protein within infected patients were gathered output through from National Center for Biotechnology Information and the mutation rates in each position were analyzed and presented in each month of 2020. The mutation rates of A8 and V30 within Orf10 are displayed in selected counties: United States, India, German, and Japan. RESULTS: The numbers of COVID-19 patients are correlated to the death numbers, but not with the death rates (stable and <3%). The amino acid positions locating at A8(F/G/L), I13, and V30(L) within the Orf10 sequence stay the highest mutation rate; N5, N25, and N36 rank at the lowest one. A8F expressed highly dominant in Japan (over 80%) and German (around 40%) coming to the end of 2020, but no significant finding in other countries. CONCLUSION: The results demonstrate via mutation analysis of Orf10 can be further combined with advanced tools such as molecular simulation, artificial intelligence, and biosensors that can practically revealed for protein interactions and thus to imply the authentic Orf10 function of SARS-CoV-2 in the future.",2021,10.1097/jcma.0000000000000542,,,,
Parameter tuning in machine learning based on radiomics biomarkers of lung cancer,"BACKGROUND: Lung cancer is one of the most common cancers, and early diagnosis and intervention can improve cancer cure rate. OBJECTIVE: To improve predictive performance of radiomics features for lung cancer by tuning the machine learning model parameters. METHODS: Using a dataset involving 263 cases (125 benign and 138 malignant) acquired from our hospital, each classifier model is trained and tested using 237 and 26 cases, respectively. We initially extract 867 radiomics features of CT images for model development and then test 10 feature selections and 7 models to determine the best method. We further tune the parameter of the final model to reach the best performance. The adjusted final model is then validated using 224 cases acquired from Lung Image Database Consortium (LIDC) dataset (64 benign and 160 malignant) with the same set of selected radiomics features. RESULTS: During model development, the feature selection via concave minimization method show the best performance of area under ROC curve (AUC = 0.765), followed by l0-norm regularization (AUC = 0.741) and Fisher discrimination criterion (AUC = 0.734). Support vector machine (SVM) and random forest (RF) are the top two machine learning algorithms showing the best performance (AUC = 0.765 and 0.734, respectively), using by the default parameter. After parameter tuning, SVM with linear kernel achieves the best performance (AUC = 0.837), whereas the best tuned RF with the number of trees is 510 and yields a slightly lower performance (AUC = 0.775) in 26 test samples data. During model validation, the SVM and RF models yield AUC = 0.78 and 0.77, respectively. CONCLUSION: Appropriate quantitative radiomics features and accurate parameters can improve the model's performance to predict lung cancer.",2022,10.3233/xst-211096,cross-sectional,diagnosis,CT,Lung
Patient-specific deep learning model to enhance 4D-CBCT image for radiomics analysis,"Objective.4D-CBCT provides phase-resolved images valuable for radiomics analysis for outcome prediction throughout treatment courses. However, 4D-CBCT suffers from streak artifacts caused by under-sampling, which severely degrades the accuracy of radiomic features. Previously we developed group-patient-trained deep learning methods to enhance the 4D-CBCT quality for radiomics analysis, which was not optimized for individual patients. In this study, a patient-specific model was developed to further improve the accuracy of 4D-CBCT based radiomics analysis for individual patients.Approach.This patient-specific model was trained with intra-patient data. Specifically, patient planning 4D-CT was augmented through image translation, rotation, and deformation to generate 305 CT volumes from 10 volumes to simulate possible patient positions during the onboard image acquisition. 72 projections were simulated from 4D-CT for each phase and were used to reconstruct 4D-CBCT using FDK back-projection algorithm. The patient-specific model was trained using these 305 paired sets of patient-specific 4D-CT and 4D-CBCT data to enhance the 4D-CBCT image to match with 4D-CT images as ground truth. For model testing, 4D-CBCT were simulated from a separate set of 4D-CT scan images acquired from the same patient and were then enhanced by this patient-specific model. Radiomics features were then extracted from the testing 4D-CT, 4D-CBCT, and enhanced 4D-CBCT image sets for comparison. The patient-specific model was tested using 4 lung-SBRT patients' data and compared with the performance of the group-based model. The impact of model dimensionality, region of interest (ROI) selection, and loss function on the model accuracy was also investigated.Main results.Compared with a group-based model, the patient-specific training model further improved the accuracy of radiomic features, especially for features with large errors in the group-based model. For example, the 3D whole-body and ROI loss-based patient-specific model reduces the errors of the first-order median feature by 83.67%, the wavelet LLL feature maximum by 91.98%, and the wavelet HLL skewness feature by 15.0% on average for the four patients tested. In addition, the patient-specific models with different dimensionality (2D versus 3D) or loss functions (L1 versus L1 + VGG + GAN) achieved comparable results for improving the radiomics accuracy. Using whole-body or whole-body+ROI L1 loss for the model achieved better results than using the ROI L1 loss alone as the loss function.Significance.This study demonstrated that the patient-specific model is more effective than the group-based model on improving the accuracy of the 4D-CBCT radiomic features analysis, which could potentially improve the precision for outcome prediction in radiotherapy.",2022,10.1088/1361-6560/ac5f6e,cross-sectional,informatics,CBCT,Lung
Performance and clinical impact of machine learning based lung nodule detection using vessel suppression in melanoma patients,"PURPOSE: To evaluate performance and the clinical impact of a novel machine learning based vessel-suppressing computer-aided detection (CAD) software in chest computed tomography (CT) of patients with malignant melanoma. MATERIALS AND METHODS: We retrospectively included consecutive malignant melanoma patients with a chest CT between 01/2015 and 01/2016. Machine learning based CAD software was used to reconstruct additional vessel-suppressed axial images. Three radiologists independently reviewed a maximum of 15 lung nodules per patient. Vessel-suppressed reconstructions were reviewed independently and results were compared. Follow-up CT examinations and clinical follow-up were used to assess the outcome. Impact of additional nodules on clinical management was assessed. RESULTS: In 46 patients, vessel-suppressed axial images led to the detection of additional nodules in 25/46 (54.3%) patients. CT or clinical follow up was available in 25/25 (100%) patients with additionally detected nodules. 2/25 (8%) of these patients developed new pulmonary metastases. None of the additionally detected nodules were found to be metastases. None of the lung nodules detected by the radiologists was missed by the CAD software. The mean diameter of the 92 additional nodules was 1.5 ± 0.8 mm. The additional nodules did not affect therapeutic management. However, in 14/46 (30.4%) of patients the additional nodules might have had an impact on the radiological follow-up recommendations. CONCLUSION: Machine learning based vessel suppression led to the detection of significantly more lung nodules in melanoma patients. Radiological follow-up recommendations were altered in 30% of the patients. However, all lung nodules turned out to be non-malignant on follow-up.",2018,10.1016/j.clinimag.2018.09.001,,,,
Performance and educational training of radiographers in lung nodule or mass detection: Retrospective comparison with different deep learning algorithms,"The aim of this investigation was to compare the diagnostic performance of radiographers and deep learning algorithms in pulmonary nodule/mass detection on chest radiograph.A test set of 100 chest radiographs containing 53 cases with no pathology (normal) and 47 abnormal cases (pulmonary nodules/masses) independently interpreted by 6 trained radiographers and deep learning algorithems in a random order. The diagnostic performances of both deep learning algorithms and trained radiographers for pulmonary nodules/masses detection were compared.QUIBIM Chest X-ray Classifier, a deep learning through mass algorithm that performs superiorly to practicing radiographers in the detection of pulmonary nodules/masses (AUCMass: 0.916 vs AUCTrained radiographer: 0.778, P < .001). In addition, heat-map algorithm could automatically detect and localize pulmonary nodules/masses in chest radiographs with high specificity.In conclusion, the deep-learning based computer-aided diagnosis system through 4 algorithms could potentially assist trained radiographers by increasing the confidence and access to chest radiograph interpretation in the age of digital age with the growing demand of medical imaging usage and radiologist burnout.",2021,10.1097/md.0000000000026270,cross-sectional,diagnosis,Radiograph,Lung
Performance and reading time of lung nodule identification on multidetector CT with or without an artificial intelligence-powered computer-aided detection system,"AIM: To compare the performance and reading time of different readers using automatic artificial intelligence (AI)-powered computer-aided detection (CAD) to detect lung nodules in different reading modes. MATERIALS AND METHODS: One hundred and fifty multidetector computed tomography (CT) datasets containing 340 nodules ≤10 mm in diameter were collected retrospectively. A CAD with vessel-suppressed function was used to interpret the images. Three junior and three senior readers were assigned to read (1) CT images without CAD, (2) second-read using CAD in which CAD was applied only after initial unassisted assessment, and (3) a concurrent read with CAD in which CAD was applied at the start of assessment. Diagnostic performances and reading times were compared using analysis of variance. RESULTS: For all readers, the mean sensitivity improved from 64% (95% confidence interval [CI]: 62%, 66%) for the without-CAD mode to 82% (95% CI: 80%, 84%) for the second-reading mode and to 80% (95% CI: 79%, 82%) for the concurrent-reading mode (p<0.001). There was no significant difference between the two modes in terms of the mean sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) for both junior and senior readers and all readers (p>0.05). The reading time of all readers was significantly shorter for the concurrent-reading mode (124 ± 25 seconds) compared to without CAD (156 ± 34 seconds; p<0.001) and the second-reading mode (197 ± 46 seconds; p<0.001). CONCLUSION: In CAD for lung nodules at CT, the second-reading mode and concurrent-reading mode may improve detection performance for all readers in both screening and clinical routine practice. Concurrent use of CAD is more efficient for both junior and senior readers.",2021,10.1016/j.crad.2021.04.006,cross-sectional,diagnosis,CT,Lung
Performance change with the number of training data: A case study on the binary classification of COVID-19 chest X-ray by using convolutional neural networks,"One of the features of artificial intelligence/machine learning-based medical devices resides in their ability to learn from real-world data. However, obtaining a large number of training data in the early phase is difficult, and the device performance may change after their first introduction into the market. To introduce the safety and effectiveness of these devices into the market in a timely manner, an appropriate post-market performance change plan must be established at the timing of the premarket approval. In this work, we evaluate the performance change with the variation of the number of training data. Two publicly available datasets are used: one consisting of 4000 images for COVID-19 and another comprising 4000 images for Normal. The dataset was split into 7000 images for training and validation, also 1000 images for test. Furthermore, the training and validation data were selected as different 16 datasets. Two different convolutional neural networks, namely AlexNet and ResNet34, with and without a fine-tuning method were used to classify two image types. The area under the curve, sensitivity, and specificity were evaluated for each dataset. Our result shows that all performances were rapidly improved as the number of training data was increased and reached an equilibrium state. AlexNet outperformed ResNet34 when the number of images was small. The difference tended to decrease as the number of training data increased, and the fine-tuning method improved all performances. In conclusion, the appropriate model and method should be selected considering the intended performance and available number of data.",2022,10.1016/j.compbiomed.2022.105251,cross-sectional,diagnosis,Radiograph,Lung
"Performance evaluation of a deep learning image reconstruction (DLIR) algorithm in ""double low"" chest CTA in children: a feasibility study","BACKGROUND: Chest CT angiography (CTA) is a convenient clinical examination for children with an increasing need to reduce both radiation and contrast medium doses. Iterative Reconstruction algorithms are often used to reduce image noise but encounter limitations under low radiation dose and conventional 100 kVp tube voltage may not provide adequate enhancement under low contrast dose. PURPOSE: To evaluate the performance of a deep learning image reconstruction (DLIR) algorithm in conjunction with lower tube voltage in chest CTA in children under reduced radiation and contrast medium (CM) dose. MATERIALS AND METHODS: 46 Children (age 5.9 ± 4.2 years) in the study group underwent chest CTA with 70 kVp and CM dose of 0.8-1.2 ml/kg. Images were reconstructed at 0.625 mm using a high setting DLIR (DLIR-H). The control group consisted of 46 age-matching children scanned with 100 kVp, CM dose of 1.3-1.8 ml/kg and images reconstructed with 50% and 100% adaptive statistical iterative reconstruction-V. Two radiologists evaluated images subjectively for overall image noise, vessel contrast and vessel margin clarity separately on a 5-point scale (5, excellent and 1, not acceptable). CT value and image noise of aorta and erector spinae muscle were measured. RESULTS: Compared to the control group, the study group reduced the dose-length-product by 11.2% (p = 0.01) and CM dose by 24% (p < 0.001), improved the enhancement in aorta (416.5 ± 113.1HU vs. 342.0 ± 57.6HU, p < 0.001) and reduced noise (15.1 ± 3.5HU vs. 18.6 ± 4.4HU, p < 0.001). The DLIR-H images provided acceptable scores on all 3 aspects of the qualitative evaluation. CONCLUSION: ""Double low"" chest CTA in children using 70 kVp and DLIR provides high image quality with reduced noise and improved vessel enhancement for diagnosis while further reduces radiation and CM dose.",2021,10.1007/s11547-021-01384-2,cross-sectional,diagnosis,CT Angiography,Lung
Performance Evaluation of a Deep Learning System for Differential Diagnosis of Lung Cancer With Conventional CT and FDG PET/CT Using Transfer Learning and Metadata,"PURPOSE: We aimed to evaluate the performance of a deep learning system for differential diagnosis of lung cancer with conventional CT and FDG PET/CT using transfer learning (TL) and metadata. METHODS: A total of 359 patients with a lung mass or nodule who underwent noncontrast chest CT and FDG PET/CT prior to treatment were enrolled retrospectively. All pulmonary lesions were classified by pathology (257 malignant, 102 benign). Deep learning classification models based on ResNet-18 were developed using the pretrained weights obtained from ImageNet data set. We propose a deep TL model for differential diagnosis of lung cancer using CT imaging data and metadata with SUVmax and lesion size derived from PET/CT. The area under the receiver operating characteristic curve (AUC) of the deep learning model was measured as a performance metric and verified by 5-fold cross-validation. RESULTS: The performance metrics of the conventional CT model were generally better than those of the CT of PET/CT model. Introducing metadata with SUVmax and lesion size derived from PET/CT into baseline CT models improved the diagnostic performance of the CT of PET/CT model (AUC = 0.837 vs 0.762) and the conventional CT model (AUC = 0.877 vs 0.817). CONCLUSIONS: Deep TL models with CT imaging data provide good diagnostic performance for lung cancer, and the conventional CT model showed overall better performance than the CT of PET/CT model. Metadata information derived from PET/CT can improve the performance of deep learning systems.",2021,10.1097/rlu.0000000000003661,cross-sectional,diagnosis,PET-CT,Lung
Performance improvement of mediastinal lymph node severity detection using GAN and Inception network,"BACKGROUND AND OBJECTIVE: In lung cancer, the determination of mediastinal lymph node (MLN) status as benign or malignant influence treatment planning and survival rate. Invasive pathological tests for the classification of MLNs into benign and malignant have various shortcomings like painfulness, the risk associated with anesthesia, and depends to a large extent on skillset and preferences of the surgeon performing the test. Hence, computer-aided system for MLNs severity detection has been explored widely by the researchers. Very recently, in our earlier concluded work on non-invasive method for MLNs differential diagnosis in computed tomography (CT) images, combination of different data augmentation approaches and state-of-art fully convolutional network (FCN) were implemented to enhance the performance of malignancy detection. However, the performance of FCN network were highly depended on the selection of appropriate data augmentation approach and control of their hyperparameters. Moreover, a standard practice to get hierarchical features in convolutional neural network (CNN) models requires deeper stacking of layers. This leads to an increase in number of trainable parameters which prone to overfitting of the network. METHODS: In view of the above mention limitations, in this paper, authors have proposed an approach that includes: 1) Generative Adversarial Network (GAN) for data augmentation, and 2) Inception network for malignancy detection. Unlike conventional data augmentation strategy, GAN based augmentation approach generates data that correlates to original data distribution. In the case of Inception based model, it uses multiple size kernels with factorized convolution for hierarchical feature extraction. This helps to a significant reduction in trainable parameters and the problem of overfitting. RESULTS: In this paper, experiments with different GAN approaches, as well as with different Inception architectures, are conducted to evaluate and justify the selection of appropriate GAN and Inception architecture, respectively for MLNs severity detection. The proposed approach achieves superior results with an average accuracy, sensitivity, specificity, and area under curve of 94.95%, 93.65%, 96.67%, and 95%, respectively. CONCLUSION: The obtained results validate the usefulness of GANs for data augmentation in the differential diagnosis of benign and malignant MLNs. The proposed Inception network based classifier for malignancy detection shows promising results compared to all investigated methods presented in various literature.",2020,10.1016/j.cmpb.2020.105478,cross-sectional,diagnosis,CT,Lung
Performance of a computer aided diagnosis system for SARS-CoV-2 pneumonia based on ultrasound images,"PURPOSE: In this study we aimed to leverage deep learning to develop a computer aided diagnosis (CAD) system toward helping radiologists in the diagnosis of SARS-CoV-2 virus syndrome on Lung ultrasonography (LUS). METHOD: A CAD system is developed based on a transfer learning of a residual network (ResNet) to extract features on LUS and help radiologists to distinguish SARS-CoV-2 virus syndrome from healthy and non-SARS-CoV-2 pneumonia. A publicly available LUS dataset for SARS-CoV-2 virus syndrome consisting of 3909 images has been employed. Six radiologists with different experiences participated in the experiment. A comprehensive LUS data set was constructed and employed to train and verify the proposed method. Several metrics such as accuracy, recall, precision, and F1-score, are used to evaluate the performance of the proposed CAD approach. The performances of the radiologists with and without the help of CAD are also evaluated quantitively. The p-values of the t-test shows that with the help of the CAD system, both junior and senior radiologists significantly improve their diagnosis performance on both balanced and unbalanced datasets. RESULTS: Experimental results indicate the proposed CAD approach and the machine features from it can significantly improve the radiologists' performance in the SARS-CoV-2 virus syndrome diagnosis. With the help of the proposed CAD system, the junior and senior radiologists achieved F1-score values of 91.33% and 95.79% on balanced dataset and 94.20% and 96.43% on unbalanced dataset. The proposed approach is verified on an independent test dataset and reports promising performance. CONCLUSIONS: The proposed CAD system reports promising performance in facilitating radiologists' diagnosis SARS-CoV-2 virus syndrome and might assist the development of a fast, accessible screening method for pulmonary diseases.",2022,10.1016/j.ejrad.2021.110066,cross-sectional,diagnosis,Ultrasound,Lung
Performance of a Deep Learning Algorithm Compared with Radiologic Interpretation for Lung Cancer Detection on Chest Radiographs in a Health Screening Population,"Background The performance of a deep learning algorithm for lung cancer detection on chest radiographs in a health screening population is unknown. Purpose To validate a commercially available deep learning algorithm for lung cancer detection on chest radiographs in a health screening population. Materials and Methods Out-of-sample testing of a deep learning algorithm was retrospectively performed using chest radiographs from individuals undergoing a comprehensive medical check-up between July 2008 and December 2008 (validation test). To evaluate the algorithm performance for visible lung cancer detection, the area under the receiver operating characteristic curve (AUC) and diagnostic measures, including sensitivity and false-positive rate (FPR), were calculated. The algorithm performance was compared with that of radiologists using the McNemar test and the Moskowitz method. Additionally, the deep learning algorithm was applied to a screening cohort undergoing chest radiography between January 2008 and December 2012, and its performances were calculated. Results In a validation test comprising 10 285 radiographs from 10 202 individuals (mean age, 54 years ± 11 [standard deviation]; 5857 men) with 10 radiographs of visible lung cancers, the algorithm's AUC was 0.99 (95% confidence interval: 0.97, 1), and it showed comparable sensitivity (90% [nine of 10 radiographs]) to that of the radiologists (60% [six of 10 radiographs]; P = .25) with a higher FPR (3.1% [319 of 10 275 radiographs] vs 0.3% [26 of 10 275 radiographs]; P < .001). In the screening cohort of 100 525 chest radiographs from 50 070 individuals (mean age, 53 years ± 11; 28 090 men) with 47 radiographs of visible lung cancers, the algorithm's AUC was 0.97 (95% confidence interval: 0.95, 0.99), and its sensitivity and FPR were 83% (39 of 47 radiographs) and 3% (2999 of 100 478 radiographs), respectively. Conclusion A deep learning algorithm detected lung cancers on chest radiographs with a performance comparable to that of radiologists, which will be helpful for radiologists in healthy populations with a low prevalence of lung cancer. © RSNA, 2020 Online supplemental material is available for this article. See also the editorial by Armato in this issue.",2020,10.1148/radiol.2020201240,cross-sectional,diagnosis,Radiograph,Lung
Performance of a deep learning-based lung nodule detection system as an alternative reader in a Chinese lung cancer screening program,"OBJECTIVE: To evaluate the performance of a deep learning-based computer-aided detection (DL-CAD) system in a Chinese low-dose CT (LDCT) lung cancer screening program. MATERIALS AND METHODS: One-hundred-and-eighty individuals with a lung nodule on their baseline LDCT lung cancer screening scan were randomly mixed with screenees without nodules in a 1:1 ratio (total: 360 individuals). All scans were assessed by double reading and subsequently processed by an academic DL-CAD system. The findings of double reading and the DL-CAD system were then evaluated by two senior radiologists to derive the reference standard. The detection performance was evaluated by the Free Response Operating Characteristic curve, sensitivity and false-positive (FP) rate. The senior radiologists categorized nodules according to nodule diameter, type (solid, part-solid, non-solid) and Lung-RADS. RESULTS: The reference standard consisted of 262 nodules ≥ 4 mm in 196 individuals; 359 findings were considered false positives. The DL-CAD system achieved a sensitivity of 90.1% with 1.0 FP/scan for detection of lung nodules regardless of size or type, whereas double reading had a sensitivity of 76.0% with 0.04 FP/scan (P = 0.001). The sensitivity for detection of nodules ≥ 4 - ≤ 6 mm was significantly higher with DL-CAD than with double reading (86.3% vs. 58.9% respectively; P = 0.001). Sixty-three nodules were only identified by the DL-CAD system, and 27 nodules only found by double reading. The DL-CAD system reached similar performance compared to double reading in Lung-RADS 3 (94.3% vs. 90.0%, P = 0.549) and Lung-RADS 4 nodules (100.0% vs. 97.0%, P = 1.000), but showed a higher sensitivity in Lung-RADS 2 (86.2% vs. 65.4%, P < 0.001). CONCLUSIONS: The DL-CAD system can accurately detect pulmonary nodules on LDCT, with an acceptable false-positive rate of 1 nodule per scan and has higher detection performance than double reading. This DL-CAD system may assist radiologists in nodule detection in LDCT lung cancer screening.",2022,10.1016/j.ejrad.2021.110068,cross-sectional,diagnosis,CT,Lung
Performance of Deep Learning Model in Detecting Operable Lung Cancer With Chest Radiographs,"PURPOSE: The aim of this study was to evaluate the diagnostic performance of a trained deep convolutional neural network (DCNN) model for detecting operable lung cancer with chest radiographs (CXRs). MATERIALS AND METHODS: The institutional review board approved this study. A deep learning model (DLM) based on DCNN was trained with 17,211 CXRs (5700 CT-confirmed lung nodules in 3500 CXRs and 13,711 normal CXRs), finally augmented to 600,000 images. For validation, a trained DLM was tested with 1483 CXRs with surgically resected lung cancer, marked and scored by 2 radiologists. Furthermore, diagnostic performances of DLM and 6 human observers were compared with 500 cases (200 visible T1 lung cancer on CXR and 300 normal CXRs) and analyzed using free-response receiver-operating characteristics curve (FROC) analysis. RESULTS: The overall detection rate of DLM for resected lung cancers (27.2±14.6 mm) was a sensitivity of 76.8% (1139/1483) with a false positive per image (FPPI) of 0.3 and area under the FROC curve (AUC) of 0.732. In the comparison with human readers, DLM demonstrated a sensitivity of 86.5% at 0.1 FPPI and a sensitivity of 92% at 0.3 FPPI with AUC of 0.899 at an FPPI range of 0.03 to 0.44 for detecting visible T1 lung cancers, which were superior to the average of 6 human readers [mean sensitivity; 78% (range, 71.6% to 82.6%) at an FPPI of 0.1% and 85% (range, 80.2% to 89.2%) at an FPPI of 0.3, AUC of 0.819 (range, 0.754 to 0.862) at an FPPI of 0.03 to 0.44). CONCLUSIONS: A DLM has high diagnostic performance in detecting operable lung cancer with CXR, demonstrating a potential of playing a pivotal role for lung cancer screening.",2019,10.1097/rti.0000000000000388,cross-sectional,treatment,Radiograph,Lung
Perinodular and Intranodular Radiomic Features on Lung CT Images Distinguish Adenocarcinomas from Granulomas,"Purpose To evaluate ability of radiomic (computer-extracted imaging) features to distinguish non-small cell lung cancer adenocarcinomas from granulomas at noncontrast CT. Materials and Methods For this retrospective study, screening or standard diagnostic noncontrast CT images were collected for 290 patients (mean age, 68 years; range, 18-92 years; 125 men [mean age, 67 years; range, 18-90 years] and 165 women [mean age, 68 years; range, 33-92 years]) from two institutions between 2007 and 2013. Histopathologic analysis was available for one nodule per patient. Corresponding nodule of interest was identified on axial CT images by a radiologist with manual annotation. Nodule shape, wavelet (Gabor), and texture-based (Haralick and Laws energy) features were extracted from intra- and perinodular regions. Features were pruned to train machine learning classifiers with 145 patients. In a test set of 145 patients, classifier results were compared against a convolutional neural network (CNN) and diagnostic readings of two radiologists. Results Support vector machine classifier with intranodular radiomic features achieved an area under the receiver operating characteristic curve (AUC) of 0.75 on the test set. Combining radiomics of intranodular with perinodular regions improved the AUC to 0.80. On the same test set, CNN resulted in an AUC of 0.76. Radiologist readers achieved AUCs of 0.61 and 0.60, respectively. Conclusion Radiomic features from intranodular and perinodular regions of nodules can distinguish non-small cell lung cancer adenocarcinomas from benign granulomas at noncontrast CT. © RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Nishino in this issue.",2019,10.1148/radiol.2018180910,cross-sectional,diagnosis,CT,Lung
Peripheral granular lymphocytopenia and dysmorphic leukocytosis as simple prognostic markers in COVID-19,"INTRODUCTION: Developing prognostic markers can be useful for clinical decision-making. Peripheral blood (PB) examination is simple and basic that can be performed in any facility. We aimed to investigate whether PB examination can predict prognosis in coronavirus disease (COVID-19). METHODS: Complete blood count (CBC) and PB cell morphology were examined in 38 healthy controls (HCs) and 40 patients with COVID-19. Patients with COVID-19, including 26 mild and 14 severe cases, were hospitalized in Juntendo University Hospital (Tokyo, Japan) between April 1 and August 6, 2020. PB examinations were performed using Sysmex XN-3000 automated hematology analyzer and Sysmex DI-60 employing the convolutional neural network-based automatic image-recognition system. RESULTS: Compared with mild cases, severe cases showed a significantly higher incidence of anemia, lymphopenia, and leukocytosis (P < .001). Granular lymphocyte counts were normal or higher in mild cases and persistently decreased in fatal cases. Temporary increase in granular lymphocytes was associated with survival of patients with severe infection. Red cell distribution width was significantly higher in severe cases than in mild cases (P < .001). Neutrophil dysplasia was consistently observed in COVID-19 cases, but not in HCs. Levels of giant neutrophils and toxic granulation/Döhle bodies were increased in severe cases. CONCLUSION: Basic PB examination can be useful to predict the prognosis of COVID-19, by detecting SARS-CoV-2 infection-induced multi-lineage changes in blood cell counts and morphological anomalies. These changes were dynamically correlated with disease severity and may be associated with disruption of hematopoiesis and the immunological system due to bone marrow stress in severe infection.",2021,10.1111/ijlh.13696,,,,
PET image denoising using unsupervised deep learning,"PURPOSE: Image quality of positron emission tomography (PET) is limited by various physical degradation factors. Our study aims to perform PET image denoising by utilizing prior information from the same patient. The proposed method is based on unsupervised deep learning, where no training pairs are needed. METHODS: In this method, the prior high-quality image from the patient was employed as the network input and the noisy PET image itself was treated as the training label. Constrained by the network structure and the prior image input, the network was trained to learn the intrinsic structure information from the noisy image and output a restored PET image. To validate the performance of the proposed method, a computer simulation study based on the BrainWeb phantom was first performed. A (68)Ga-PRGD2 PET/CT dataset containing 10 patients and a (18)F-FDG PET/MR dataset containing 30 patients were later on used for clinical data evaluation. The Gaussian, non-local mean (NLM) using CT/MR image as priors, BM4D, and Deep Decoder methods were included as reference methods. The contrast-to-noise ratio (CNR) improvements were used to rank different methods based on Wilcoxon signed-rank test. RESULTS: For the simulation study, contrast recovery coefficient (CRC) vs. standard deviation (STD) curves showed that the proposed method achieved the best performance regarding the bias-variance tradeoff. For the clinical PET/CT dataset, the proposed method achieved the highest CNR improvement ratio (53.35% ± 21.78%), compared with the Gaussian (12.64% ± 6.15%, P = 0.002), NLM guided by CT (24.35% ± 16.30%, P = 0.002), BM4D (38.31% ± 20.26%, P = 0.002), and Deep Decoder (41.67% ± 22.28%, P = 0.002) methods. For the clinical PET/MR dataset, the CNR improvement ratio of the proposed method achieved 46.80% ± 25.23%, higher than the Gaussian (18.16% ± 10.02%, P < 0.0001), NLM guided by MR (25.36% ± 19.48%, P < 0.0001), BM4D (37.02% ± 21.38%, P < 0.0001), and Deep Decoder (30.03% ± 20.64%, P < 0.0001) methods. Restored images for all the datasets demonstrate that the proposed method can effectively smooth out the noise while recovering image details. CONCLUSION: The proposed unsupervised deep learning framework provides excellent image restoration effects, outperforming the Gaussian, NLM methods, BM4D, and Deep Decoder methods.",2019,10.1007/s00259-019-04468-4,,,,
Phased searching with NEAT in a time-scaled framework: experiments on a computer-aided detection system for lung nodules,"OBJECTIVE: In the field of computer-aided detection (CAD) systems for lung nodules in computed tomography (CT) scans, many image features are presented and many artificial neural network (ANN) classifiers with various structural topologies are analyzed; frequently, the classifier topologies are selected by trial-and-error experiments. To avoid these trial and error approaches, we present a novel classifier that evolves ANNs using genetic algorithms, called ""Phased Searching with NEAT in a Time or Generation-Scaled Framework"", integrating feature selection with the classification task. METHODS AND MATERIALS: We analyzed our method's performance on 360 CT scans from the public Lung Image Database Consortium database. We compare our method's performance with other more-established classifiers, namely regular NEAT, Feature-Deselective NEAT (FD-NEAT), fixed-topology ANNs, and support vector machines (SVMs) using ten-fold cross-validation experiments of all 360 scans. RESULTS: The results show that the proposed ""Phased Searching"" method performs better and faster than regular NEAT, better than FD-NEAT, and achieves sensitivities at 3 and 4 false positives (FP) per scan that are comparable with the fixed-topology ANN and SVM classifiers, but with fewer input features. It achieves a detection sensitivity of 83.0±9.7% with an average of 4FP/scan, for nodules with a diameter greater than or equal to 3mm. It also evolves networks with shorter evolution times and with lower complexities than regular NEAT (p=0.026 and p<0.001, respectively). Analysis on the average and best network complexities evolved by regular NEAT and by our approach shows that our approach searches for good solutions in lower dimensional search spaces, and evolves networks without superfluous structure. CONCLUSIONS: We have presented a novel approach that combines feature selection with the evolution of ANN topology and weights. Compared with the original threshold-based Phased Searching method of Green, our method requires fewer parameters and converges to the optimal network complexity required for the classification task at hand. The results of the ten-fold cross-validation experiments also show that our proposed CAD system for lung nodule detection performs well with respect to other methods in the literature.",2013,10.1016/j.artmed.2013.07.002,cross-sectional,diagnosis,CT,Lung
Plasma Proteomics Identify Biomarkers and Pathogenesis of COVID-19,"The coronavirus disease 2019 (COVID-19) pandemic is a global public health crisis. However, little is known about the pathogenesis and biomarkers of COVID-19. Here, we profiled host responses to COVID-19 by performing plasma proteomics of a cohort of COVID-19 patients, including non-survivors and survivors recovered from mild or severe symptoms, and uncovered numerous COVID-19-associated alterations of plasma proteins. We developed a machine-learning-based pipeline to identify 11 proteins as biomarkers and a set of biomarker combinations, which were validated by an independent cohort and accurately distinguished and predicted COVID-19 outcomes. Some of the biomarkers were further validated by enzyme-linked immunosorbent assay (ELISA) using a larger cohort. These markedly altered proteins, including the biomarkers, mediate pathophysiological pathways, such as immune or inflammatory responses, platelet degranulation and coagulation, and metabolism, that likely contribute to the pathogenesis. Our findings provide valuable knowledge about COVID-19 biomarkers and shed light on the pathogenesis and potential therapeutic targets of COVID-19.",2020,10.1016/j.immuni.2020.10.008,,,,
PM₂.₅ Monitoring: Use Information Abundance Measurement and Wide and Deep Learning,"This article devises a photograph-based monitoring model to estimate the real-time PM(2.5) concentrations, overcoming currently popular electrochemical sensor-based PM(2.5) monitoring methods' shortcomings such as low-density spatial distribution and time delay. Combining the proposed monitoring model, the photographs taken by various camera devices (e.g., surveillance camera, automobile data recorder, and mobile phone) can widely monitor PM(2.5) concentration in megacities. This is beneficial to offering helpful decision-making information for atmospheric forecast and control, thus reducing the epidemic of COVID-19. To specify, the proposed model fuses Information Abundance measurement and Wide and Deep learning, dubbed as IAWD, for PM(2.5) monitoring. First, our model extracts two categories of features in a newly proposed DS transform space to measure the information abundance (IA) of a given photograph since the growth of PM(2.5) concentration decreases its IA. Second, to simultaneously possess the advantages of memorization and generalization, a new wide and deep neural network is devised to learn a nonlinear mapping between the above-mentioned extracted features and the groundtruth PM(2.5) concentration. Experiments on two recently established datasets totally including more than 100 000 photographs demonstrate the effectiveness of our extracted features and the superiority of our proposed IAWD model as compared to state-of-the-art relevant computing techniques.",2021,10.1109/tnnls.2021.3105394,,,,
Pneumonia Detection in Chest X-Ray Dose-Equivalent CT: Impact of Dose Reduction on Detectability by Artificial Intelligence,"RATIONALE AND OBJECTIVES: There has been a significant increase of immunocompromised patients in recent years due to new treatment modalities for previously fatal diseases. This comes at the cost of an elevated risk for infectious diseases, most notably pathogens affecting the respiratory tract. Because early diagnosis and treatment of pneumonia can help reducing morbidity and mortality, we assessed the performance of a deep neural network in the detection of pulmonary infection in chest X-ray dose-equivalent computed tomography (CT). MATERIALS AND METHODS: The 100 patients included in this retrospective study were referred to our department for suspicion of pulmonary infection and/or follow-up of known pulmonary nodules. Every patient was scanned with a standard dose (1.43 ± 0.54 mSv) and a 20 times dose-reduced (0.07 ± 0.03 mSv) CT protocol. We trained a deep neural network to perform binary classification (pulmonary consolidation present or not) and assessed diagnostic performance on both standard dose and reduced dose CT images. RESULTS: The areas under the curve of the deep learning algorithm for the standard dose CT was 0.923 (confidence interval [CI] 95%: 0.905-0.941) and significantly higher than the areas under the curve (0.881, CI 95%: 0.859-0.903) of the reduced dose CT (p = 0.001). Sensitivity and specificity of the standard dose CT was 82.9% and 93.8%, and of the reduced dose CT 71.0% and 93.3%. CONCLUSION: Pneumonia detection with X-ray dose-equivalent CT using artificial intelligence is feasible and may contribute to a more robust and reproducible diagnostic performance. Dose reduction lowered the performance of the deep neural network, which calls for optimization and adaption of CT protocols when using AI algorithms at reduced doses.",2021,10.1016/j.acra.2020.05.031,cross-sectional,diagnosis,Radiograph,Lung
Pneumonia detection in chest X-ray images using an ensemble of deep learning models,"Pneumonia is a respiratory infection caused by bacteria or viruses; it affects many individuals, especially in developing and underdeveloped nations, where high levels of pollution, unhygienic living conditions, and overcrowding are relatively common, together with inadequate medical infrastructure. Pneumonia causes pleural effusion, a condition in which fluids fill the lung, causing respiratory difficulty. Early diagnosis of pneumonia is crucial to ensure curative treatment and increase survival rates. Chest X-ray imaging is the most frequently used method for diagnosing pneumonia. However, the examination of chest X-rays is a challenging task and is prone to subjective variability. In this study, we developed a computer-aided diagnosis system for automatic pneumonia detection using chest X-ray images. We employed deep transfer learning to handle the scarcity of available data and designed an ensemble of three convolutional neural network models: GoogLeNet, ResNet-18, and DenseNet-121. A weighted average ensemble technique was adopted, wherein the weights assigned to the base learners were determined using a novel approach. The scores of four standard evaluation metrics, precision, recall, f1-score, and the area under the curve, are fused to form the weight vector, which in studies in the literature was frequently set experimentally, a method that is prone to error. The proposed approach was evaluated on two publicly available pneumonia X-ray datasets, provided by Kermany et al. and the Radiological Society of North America (RSNA), respectively, using a five-fold cross-validation scheme. The proposed method achieved accuracy rates of 98.81% and 86.85% and sensitivity rates of 98.80% and 87.02% on the Kermany and RSNA datasets, respectively. The results were superior to those of state-of-the-art methods and our method performed better than the widely used ensemble techniques. Statistical analyses on the datasets using McNemar's and ANOVA tests showed the robustness of the approach. The codes for the proposed work are available at https://github.com/Rohit-Kundu/Ensemble-Pneumonia-Detection.",2021,10.1371/journal.pone.0256630,cross-sectional,diagnosis,Radiograph,Lung
Potential diagnosis of COVID-19 from chest X-ray and CT findings using semi-supervised learning,"COVID-19 is an infectious disease, which has adversely affected public health and the economy across the world. On account of the highly infectious nature of the disease, rapid automated diagnosis of COVID-19 is urgently needed. A few recent findings suggest that chest X-rays and CT scans can be used by machine learning for the diagnosis of COVID-19. Herein, we employed semi-supervised learning (SSL) approaches to detect COVID-19 cases accurately by analyzing digital chest X-rays and CT scans. On a relatively small COVID-19 radiography dataset, which contains only 219 COVID-19 positive images, 1341 normal and 1345 viral pneumonia images, our algorithm, COVIDCon, which takes advantage of data augmentation, consistency regularization, and multicontrastive learning, attains 97.07% average class prediction accuracy, with 1000 labeled images, which is 7.65% better than the next best SSL method, virtual adversarial training. COVIDCon performs even better on a larger COVID-19 CT Scan dataset that contains 82,767 images. It achieved an excellent accuracy of 99.13%, at 20,000 labels, which is 6.45% better than the next best pseudo-labeling approach. COVIDCon outperforms other state-of-the-art algorithms at every label that we have investigated. These results demonstrate COVIDCon as the benchmark SSL algorithm for potential diagnosis of COVID-19 from chest X-rays and CT-Scans. Furthermore, COVIDCon performs exceptionally well in identifying COVID-19 positive cases from a completely unseen repository with a confirmed COVID-19 case history. COVIDCon, may provide a fast, accurate, and reliable method for screening COVID-19 patients.",2022,10.1007/s13246-021-01075-2,cross-sectional,diagnosis,Radiograph,Lung
Potential feature exploration and model development based on 18F-FDG PET/CT images for differentiating benign and malignant lung lesions,"PURPOSE: The study is to explore potential features and develop classification models for distinguishing benign and malignant lung lesions based on CT-radiomics features and PET metabolic parameters extracted from PET/CT images. MATERIALS AND METHODS: A retrospective study was conducted in baseline 18 F-flurodeoxyglucose positron emission tomography/ computed tomography (18 F-FDG PET/CT) images of 135 patients. The dataset was utilized for feature extraction of CT-radiomics features and PET metabolic parameters based on volume of interest, then went through feature selection and model development with strategy of five-fold cross-validation. Specifically, model development used support vector machine, PET metabolic parameters selection used Akaike's information criterion, and CT-radiomics were reduced by the least absolute shrinkage and selection operator method then forward selection approach. The diagnostic performances of CT-radiomics, PET metabolic parameters and combination of both were illustrated by receiver operating characteristic (ROC) curves, and compared by Delong test. Five groups of selected PET metabolic parameters and CT-radiomics were counted, and potential features were found and analyzed with Mann-Whitney U test. RESULTS: The CT-radiomics, PET metabolic parameters, and combination of both among five subsets showed mean area under the curve (AUC) of 0.820 ± 0.053, 0.874 ± 0.081, and 0.887 ± 0.046, respectively. No significant differences in ROC among models were observed through pairwise comparison in each fold (P-value from 0.09 to 0.81, Delong test). The potential features were found to be SurfaceVolumeRatio and SUVpeak (P < 0.001 of both, U test). CONCLUSION: The classification models developed by CT-radiomics features and PET metabolic parameters based on PET/CT images have substantial diagnostic capacity on lung lesions.",2019,10.1016/j.ejrad.2019.108735,cross-sectional,diagnosis,PET-CT,Lung
Potential for dose reduction in CT emphysema densitometry with post-scan noise reduction: a phantom study,"OBJECTIVE: The aim of this phantom study was to investigate the effect of scan parameters and noise suppression techniques on the minimum radiation dose for acceptable image quality for CT emphysema densitometry. METHODS: The COPDGene phantom was scanned on a third generation dual-source CT system with 16 scan setups (CTDI(vol) 0.035-10.680 mGy). Images were reconstructed at 1.0/0.7 mm slice thickness/increment, with three kernels (one soft, two hard), filtered backprojection and three grades of third-generation iterative reconstruction (IR). Additionally, deep learning-based noise suppression software was applied. Main outcomes: overlap in area of the normalized histograms of CT density for the emphysema insert and lung material, and the radiation dose required for a maximum of 4.3% overlap (defined as acceptable image quality). RESULTS: In total, 384 scan reconstructions were analyzed. Decreasing radiation dose resulted in an exponential increase of the overlap in normalized histograms of CT density. The overlap was 11-91% for the lowest dose setting (CTDI(vol) 0.035mGy). The soft kernel reconstruction showed less histogram overlap than hard filter kernels. IR and noise suppression also reduced overlap. Using intermediate grade IR plus noise suppression software allowed for 85% radiation dose reduction while maintaining acceptable image quality. CONCLUSION: CT density histogram overlap can quantify the degree of discernibility of emphysema and healthy lung tissue. Noise suppression software, IR, and soft reconstruction kernels substantially decrease the dose required for acceptable image quality. ADVANCES IN KNOWLEDGE: Noise suppression software, IR, and soft reconstruction kernels allow radiation dose reduction by 85% while still allowing differentiation between emphysema and normal lung tissue.",2020,10.1259/bjr.20181019,cross-sectional,informatics,Dual Enervy CT,Lung
Pre- and Post-publication Verification for Reproducible Data Mining in Macromolecular Crystallography,"Like an article narrative is deemed by an editor and referees to be worthy of being a version of record on acceptance as a publication, so must the underpinning data also be scrutinized before passing it as a version of record. Indeed without the underpinning data, a study and its conclusions cannot be reproduced at any stage of evaluation, pre- or post-publication. Likewise, an independent study without its own underpinning data also cannot be reproduced let alone be considered a replicate of the first study. The PDB is a modern marvel of achievement providing an organized open access to depositor and user of the data held there opening numerous applications. Methods for modeling protein structures and for determination of structures are still improving their precision, and artifacts of the method exist. So their accuracy is realized if they are reproduced by other methods. It is on such foundations that reproducible data mining is based. Data rates are expanding considerably be they at synchrotrons, the X-ray free electron lasers (XFELs), electron cryomicroscopes (cryoEM), or at the neutron facilities. The work of a person as a referee or user with a narrative and its underpinning data may well be complemented in future by artificial intelligence with machine learning, the former for specific refereeing and the latter for the more general validation, both ideally before publication. Examples are described involving rhenium theranostics, the anti-cancer platins and the SARS-CoV-2 main protease.",2022,10.1007/978-1-0716-2095-3_10,,,,
Pre-processing methods in chest X-ray image classification,"BACKGROUND: The SARS-CoV-2 pandemic began in early 2020, paralyzing human life all over the world and threatening our security. Thus, the need for an effective, novel approach to diagnosing, preventing, and treating COVID-19 infections became paramount. METHODS: This article proposes a machine learning-based method for the classification of chest X-ray images. We also examined some of the pre-processing methods such as thresholding, blurring, and histogram equalization. RESULTS: We found the F1-score results rose to 97%, 96%, and 99% for the three analyzed classes: healthy, COVID-19, and pneumonia, respectively. CONCLUSION: Our research provides proof that machine learning can be used to support medics in chest X-ray classification and improving pre-processing leads to improvements in accuracy, precision, recall, and F1-scores.",2022,10.1371/journal.pone.0265949,cross-sectional,diagnosis,Radiograph,Lung
Pre-treatment (18)F-FDG PET-based radiomics predict survival in resected non-small cell lung cancer,"AIM: To assess the prognostic value of 2-[(18)F]-fluoro-2-deoxy-d-glucose (FDG) positron-emission tomography (PET)-based radiomics using a machine learning approach in patients with non-small cell lung cancer (NSCLC). MATERIALS AND METHODS: Ninety-three patients with stage I-III NSCLC who underwent combined PET/computed tomography (CT) followed by curative resection. A total of 35 unique quantitative radiomic features was extracted from the PET images, which included imaging phenotypes such as pixel intensity, shape, and texture. Radiomic features were ranked based on score according to their correlation with disease recurrence status within a 3-year follow-up. The recurrence risk classification performances of machine learning algorithms (random forest, neural network, naive Bayes, logistic regression, and support vector machine) using the 20 best-ranked features were compared using the areas under the receiver operating characteristic curve (AUC) and validated by the random sampling method. RESULTS: Contrast and busyness texture features from neighbourhood grey-level difference matrix were found to be the two best predictors of disease recurrence. The random forest model obtained the best performance (AUC: 0.956, accuracy: 0.901, F1 score: 0.872, precision: 0.905, recall: 0.842), followed by the neural network model (AUC: 0.871, accuracy: 0.780, F1 score: 0.708, precision: 0.755, recall: 0.666). CONCLUSION: A PET-based radiomic model was developed and validated for risk classification in NSCLC. The machine learning approach with random forest classifier exhibited good performance in predicting the recurrence risk. Radiomic features may help clinicians to improve the risk stratification for clinical practice.",2019,10.1016/j.crad.2019.02.008,retrospective cohort,diagnosis,PET-CT,Lung
Precise Segmentation of COVID-19 Infected Lung from CT Images Based on Adaptive First-Order Appearance Model with Morphological/Anatomical Constraints,"A new segmentation technique is introduced for delineating the lung region in 3D computed tomography (CT) images. To accurately model the distribution of Hounsfield scale values within both chest and lung regions, a new probabilistic model is developed that depends on a linear combination of Gaussian (LCG). Moreover, we modified the conventional expectation-maximization (EM) algorithm to be run in a sequential way to estimate both the dominant Gaussian components (one for the lung region and one for the chest region) and the subdominant Gaussian components, which are used to refine the final estimated joint density. To estimate the marginal density from the mixed density, a modified k-means clustering approach is employed to classify the Gaussian subdominant components to determine which components belong properly to a lung and which components belong to a chest. The initial segmentation, based on the LCG-model, is then refined by the imposition of 3D morphological constraints based on a 3D Markov-Gibbs random field (MGRF) with analytically estimated potentials. The proposed approach was tested on CT data from 32 coronavirus disease 2019 (COVID-19) patients. Segmentation quality was quantitatively evaluated using four metrics: Dice similarity coefficient (DSC), overlap coefficient, 95th-percentile bidirectional Hausdorff distance (BHD), and absolute lung volume difference (ALVD), and it achieved 95.67±1.83%, 91.76±3.29%, 4.86±5.01, and 2.93±2.39, respectively. The reported results showed the capability of the proposed approach to accurately segment healthy lung tissues in addition to pathological lung tissues caused by COVID-19, outperforming four current, state-of-the-art deep learning-based lung segmentation approaches.",2021,10.3390/s21165482,cross-sectional,informatics,CT,Lung
Precision Medicine: Using Artificial Intelligence to Improve Diagnostics and Healthcare,"The continued generation of large amounts of data within healthcare-from imaging to electronic medical health records to genomics and multi-omics -necessitates tools and methods to parse and interpret these data to improve healthcare outcomes. Artificial intelligence, and in particular deep learning, has enabled researchers to gain new insights from large scale and multimodal data. At the 2022 Pacific Symposium on Biocomputing (PSB) session entitled ""Precision Medicine: Using Artificial Intelligence to Improve Diagnostics and Healthcare"", we showcase the latest research, influenced and inspired by the idea of using technology to build a more fair, tailored, and cost-effective healthcare system after the COVID-19 pandemic.",2022,,,,,
Predicted airway obstruction distribution based on dynamical lung ventilation data: A coupled modeling-machine learning methodology,"In asthma and chronic obstructive pulmonary disease, some airways of the tracheobronchial tree can be constricted, from moderate narrowing up to closure. Those pathological patterns of obstructions affect the lung ventilation distribution. While some imaging techniques enable visualization and quantification of constrictions in proximal generations, no noninvasive technique exists to provide the airway morphology and obstruction distribution in distal areas. In this work, we propose a method that exploits lung ventilation measures to access positions of airway obstructions (restrictions and closures) in the tree. This identification approach combines a lung ventilation model, in which a 0D tree is strongly coupled to a 3D parenchyma description, along with a machine learning approach. On the basis of synthetic data generated with typical temporal and spatial resolutions as well as reconstruction errors, we obtain very encouraging results of the obstruction distribution, with a detection rate higher than 85%.",2018,10.1002/cnm.3108,,,,
Predicting adenocarcinoma recurrence using computational texture models of nodule components in lung CT,"PURPOSE: To investigate the importance of presurgical computed tomography (CT) intensity and texture information from ground-glass opacities (GGO) and solid nodule components for the prediction of adenocarcinoma recurrence. METHODS: For this study, 101 patients with surgically resected stage I adenocarcinoma were selected. During the follow-up period, 17 patients had disease recurrence with six associated cancer-related deaths. GGO and solid tumor components were delineated on presurgical CT scans by a radiologist. Computational texture models of GGO and solid regions were built using linear combinations of steerable Riesz wavelets learned with linear support vector machines (SVMs). Unlike other traditional texture attributes, the proposed texture models are designed to encode local image scales and directions that are specific to GGO and solid tissue. The responses of the locally steered models were used as texture attributes and compared to the responses of unaligned Riesz wavelets. The texture attributes were combined with CT intensities to predict tumor recurrence and patient hazard according to disease-free survival (DFS) time. Two families of predictive models were compared: LASSO and SVMs, and their survival counterparts: Cox-LASSO and survival SVMs. RESULTS: The best-performing predictive model of patient hazard was associated with a concordance index (C-index) of 0.81 ± 0.02 and was based on the combination of the steered models and CT intensities with survival SVMs. The same feature group and the LASSO model yielded the highest area under the receiver operating characteristic curve (AUC) of 0.8 ± 0.01 for predicting tumor recurrence, although no statistically significant difference was found when compared to using intensity features solely. For all models, the performance was found to be significantly higher when image attributes were based on the solid components solely versus using the entire tumors (p < 3.08 × 10(-5)). CONCLUSIONS: This study constitutes a novel perspective on how to interpret imaging information from CT examinations by suggesting that most of the information related to adenocarcinoma aggressiveness is related to the intensity and morphological properties of solid components of the tumor. The prediction of adenocarcinoma relapse was found to have low specificity but very high sensitivity. Our results could be useful in clinical practice to identify patients for which no recurrence is expected with a very high confidence using a presurgical CT scan only. It also provided an accurate estimation of the risk of recurrence after a given duration t from surgical resection (i.e., C-index = 0.81 ± 0.02).",2015,10.1118/1.4916088,retrospective cohort,prognosis,CT,Lung
"Predicting benign, preinvasive, and invasive lung nodules on computed tomography scans using machine learning","OBJECTIVE: The study objective was to investigate if machine learning algorithms can predict whether a lung nodule is benign, adenocarcinoma, or its preinvasive subtype from computed tomography images alone. METHODS: A dataset of chest computed tomography scans containing lung nodules was collected with their pathologic diagnosis from several sources. The dataset was split randomly into training (70%), internal validation (15%), and independent test sets (15%) at the patient level. Two machine learning algorithms were developed, trained, and validated. The first algorithm used the support vector machine model, and the second used deep learning technology: a convolutional neural network. Receiver operating characteristic analysis was used to evaluate the performance of the classification on the test dataset. RESULTS: The support vector machine/convolutional neural network-based models classified nodules into 6 categories resulting in an area under the curve of 0.59/0.65 when differentiating atypical adenomatous hyperplasia versus adenocarcinoma in situ, 0.87/0.86 with minimally invasive adenocarcinoma versus invasive adenocarcinoma, 0.76/0.72 atypical adenomatous hyperplasia + adenocarcinoma in situ versus minimally invasive adenocarcinoma, 0.89/0.87 atypical adenomatous hyperplasia + adenocarcinoma in situ versus minimally invasive adenocarcinoma + invasive adenocarcinoma, and 0.93/0.92 atypical adenomatous hyperplasia + adenocarcinoma in situ + minimally invasive adenocarcinoma versus invasive adenocarcinoma. Classifying benign versus atypical adenomatous hyperplasia + adenocarcinoma in situ + minimally invasive adenocarcinoma versus invasive adenocarcinoma resulted in a micro-average area under the curve of 0.93/0.94 for the support vector machine/convolutional neural network models, respectively. The convolutional neural network-based methods had higher sensitivities than the support vector machine-based methods but lower specificities and accuracies. CONCLUSIONS: The machine learning algorithms demonstrated reasonable performance in differentiating benign versus preinvasive versus invasive adenocarcinoma from computed tomography images alone. However, the prediction accuracy varies across its subtypes. This holds the potential for improved diagnostic capabilities with less-invasive means.",2022,10.1016/j.jtcvs.2021.02.010,cross-sectional,diagnosis,CT,Lung
Predicting clinical outcomes in COVID-19 using radiomics on chest radiographs,"OBJECTIVES: For optimal utilization of healthcare resources, there is a critical need for early identification of COVID-19 patients at risk of poor prognosis as defined by the need for intensive unit care and mechanical ventilation. We tested the feasibility of chest X-ray (CXR)-based radiomics metrics to develop machine-learning algorithms for predicting patients with poor outcomes. METHODS: In this Institutional Review Board (IRB) approved, Health Insurance Portability and Accountability Act (HIPAA) compliant, retrospective study, we evaluated CXRs performed around the time of admission from 167 COVID-19 patients. Of the 167 patients, 68 (40.72%) required intensive care during their stay, 45 (26.95%) required intubation, and 25 (14.97%) died. Lung opacities were manually segmented using ITK-SNAP (open-source software). CaPTk (open-source software) was used to perform 2D radiomics analysis. RESULTS: Of all the algorithms considered, the AdaBoost classifier performed the best with AUC = 0.72 to predict the need for intubation, AUC = 0.71 to predict death, and AUC = 0.61 to predict the need for admission to the intensive care unit (ICU). AdaBoost had similar performance with ElasticNet in predicting the need for admission to ICU. Analysis of the key radiomic metrics that drive model prediction and performance showed the importance of first-order texture metrics compared to other radiomics panel metrics. Using a Venn-diagram analysis, two first-order texture metrics and one second-order texture metric that consistently played an important role in driving model performance in all three outcome predictions were identified. CONCLUSIONS: Considering the quantitative nature and reliability of radiomic metrics, they can be used prospectively as prognostic markers to individualize treatment plans for COVID-19 patients and also assist with healthcare resource management. ADVANCES IN KNOWLEDGE: We report on the performance of CXR-based imaging metrics extracted from RT-PCR positive COVID-19 patients at admission to develop machine-learning algorithms for predicting the need for ICU, the need for intubation, and mortality, respectively.",2021,10.1259/bjr.20210221,retrospective cohort,prognosis,Radiograph,Lung
Predicting EGFR and PD-L1 Status in NSCLC Patients Using Multitask AI System Based on CT Images,"BACKGROUND: Epidermal growth factor receptor (EGFR) genotyping and programmed death ligand-1 (PD-L1) expressions are of paramount importance for treatment guidelines such as the use of tyrosine kinase inhibitors (TKIs) and immune checkpoint inhibitors (ICIs) in lung cancer. Conventional identification of EGFR or PD-L1 status requires surgical or biopsied tumor specimens, which are obtained through invasive procedures associated with risk of morbidities and may be unavailable to access tissue samples. Here, we developed an artificial intelligence (AI) system that can predict EGFR and PD-L1 status in using non-invasive computed tomography (CT) images. METHODS: A multitask AI system including deep learning (DL) module, radiomics (RA) module, and joint (JO) module combining the DL, RA, and clinical features was developed, trained, and optimized with CT images to predict the EGFR and PD-L1 status. We used feature selectors and feature fusion methods to find the best model among combinations of module types. The models were evaluated using the areas under the receiver operating characteristic curves (AUCs). RESULTS: Our multitask AI system yielded promising performance for gene expression status, subtype classification, and joint prediction. The AUCs of DL module achieved 0.842 (95% CI, 0.825-0.855) in the EGFR mutated status and 0.805 (95% CI, 0.779-0.829) in the mutated-EGFR subtypes discrimination (19Del, L858R, other mutations). DL module also demonstrated the AUCs of 0.799 (95% CI, 0.762-0.854) in the PD-L1 expression status and 0.837 (95% CI, 0.775-0.911) in the positive-PD-L1 subtypes (PD-L1 tumor proportion score, 1%-49% and ≥50%). Furthermore, the JO module of our AI system performed well in the EGFR and PD-L1 joint cohort, with an AUC of 0.928 (95% CI, 0.909-0.946) for distinguishing EGFR mutated status and 0.905 (95% CI, 0.886-0.930) for discriminating PD-L1 expression status. CONCLUSION: Our AI system has demonstrated the encouraging results for identifying gene status and further assessing the genotypes. Both clinical indicators and radiomics features showed a complementary role in prediction and provided accurate estimates to predict EGFR and PD-L1 status. Furthermore, this non-invasive, high-throughput, and interpretable AI system can be used as an assistive tool in conjunction with or in lieu of ancillary tests and extensive diagnostic workups to facilitate early intervention.",2022,10.3389/fimmu.2022.813072,cross-sectional,diagnosis,CT,Lung
Predicting EGFR mutation status by a deep learning approach in patients with non-small cell lung cancer brain metastases,"PURPOSE: Non-small cell lung cancer (NSCLC) tends to metastasize to the brain. Between 10 and 60% of NSCLCs harbor an activating mutation in the epidermal growth-factor receptor (EGFR), which may be targeted with selective EGFR inhibitors. However, due to a high discordance rate between the molecular profile of the primary tumor and the brain metastases (BMs), identifying an individual patient's EGFR status of the BMs necessitates tissue diagnosis via an invasive surgical procedure. We employed a deep learning (DL) method with the aim of noninvasive detection of the EGFR mutation status in NSCLC BM. METHODS: We retrospectively collected clinical, radiological, and pathological-molecular data of all the NSCLC patients who had been diagnosed with BMs and underwent resection of their BM during 2009-2019. The study population was then divided into two groups based upon EGFR mutational status. We further employed a DL technique to classify the two groups according to their preoperative magnetic resonance imaging features. Augmentation techniques, transfer learning approach, and post-processing of the predicted results were applied to overcome the relatively small cohort. Finally, we established the accuracy of our model in predicting EGFR mutation status of BM of NSCLC. RESULTS: Fifty-nine patients were included in the study, 16 patients harbored EGFR mutations. Our model predicted mutational status with mean accuracy of 89.8%, sensitivity of 68.7%, specificity of 97.7%, and a receiver operating characteristic curve value of 0.91 across the 5 validation datasets. CONCLUSION: DL-based noninvasive molecular characterization is feasible, has high accuracy and should be further validated in large prospective cohorts.",2022,10.1007/s11060-022-03946-4,,,,
Predicting EGFR mutation status in lung adenocarcinoma on computed tomography image using deep learning,"Epidermal growth factor receptor (EGFR) genotyping is critical for treatment guidelines such as the use of tyrosine kinase inhibitors in lung adenocarcinoma. Conventional identification of EGFR genotype requires biopsy and sequence testing which is invasive and may suffer from the difficulty of accessing tissue samples. Here, we propose a deep learning model to predict EGFR mutation status in lung adenocarcinoma using non-invasive computed tomography (CT).We retrospectively collected data from 844 lung adenocarcinoma patients with pre-operative CT images, EGFR mutation and clinical information from two hospitals. An end-to-end deep learning model was proposed to predict the EGFR mutation status by CT scanning.By training in 14 926 CT images, the deep learning model achieved encouraging predictive performance in both the primary cohort (n=603; AUC 0.85, 95% CI 0.83-0.88) and the independent validation cohort (n=241; AUC 0.81, 95% CI 0.79-0.83), which showed significant improvement over previous studies using hand-crafted CT features or clinical characteristics (p<0.001). The deep learning score demonstrated significant differences in EGFR-mutant and EGFR-wild type tumours (p<0.001).Since CT is routinely used in lung cancer diagnosis, the deep learning model provides a non-invasive and easy-to-use method for EGFR mutation status prediction.",2019,10.1183/13993003.00986-2018,cross-sectional,diagnosis,CT,Lung
Predicting lung nodule malignancies by combining deep convolutional neural network and handcrafted features,"To predict lung nodule malignancy with a high sensitivity and specificity for low dose CT (LDCT) lung cancer screening, we propose a fusion algorithm that combines handcrafted features (HF) into the features learned at the output layer of a 3D deep convolutional neural network (CNN). First, we extracted twenty-nine HF, including nine intensity features, eight geometric features, and twelve texture features based on grey-level co-occurrence matrix (GLCM). We then trained 3D CNNs modified from three 2D CNN architectures (AlexNet, VGG-16 Net and Multi-crop Net) to extract the CNN features learned at the output layer. For each 3D CNN, the CNN features combined with the 29 HF were used as the input for the support vector machine (SVM) coupled with the sequential forward feature selection (SFS) method to select the optimal feature subset and construct the classifiers. The fusion algorithm takes full advantage of the HF and the highest level CNN features learned at the output layer. It can overcome the disadvantage of the HF that may not fully reflect the unique characteristics of a particular lesion by combining the intrinsic CNN features. Meanwhile, it also alleviates the requirement of a large scale annotated dataset for the CNNs based on the complementary of HF. The patient cohort includes 431 malignant nodules and 795 benign nodules extracted from the LIDC/IDRI database. For each investigated CNN architecture, the proposed fusion algorithm achieved the highest AUC, accuracy, sensitivity, and specificity scores among all competitive classification models.",2019,10.1088/1361-6560/ab326a,cross-sectional,diagnosis,CT,Lung
Predicting Malignant Nodules from Screening CT Scans,"OBJECTIVES: The aim of this study was to determine whether quantitative analyses (""radiomics"") of low-dose computed tomography lung cancer screening images at baseline can predict subsequent emergence of cancer. METHODS: Public data from the National Lung Screening Trial (ACRIN 6684) were assembled into two cohorts of 104 and 92 patients with screen-detected lung cancer and then matched with cohorts of 208 and 196 screening subjects with benign pulmonary nodules. Image features were extracted from each nodule and used to predict the subsequent emergence of cancer. RESULTS: The best models used 23 stable features in a random forests classifier and could predict nodules that would become cancerous 1 and 2 years hence with accuracies of 80% (area under the curve 0.83) and 79% (area under the curve 0.75), respectively. Radiomics outperformed the Lung Imaging Reporting and Data System and volume-only approaches. The performance of the McWilliams risk assessment model was commensurate. CONCLUSIONS: The radiomics of lung cancer screening computed tomography scans at baseline can be used to assess risk for development of cancer.",2016,10.1016/j.jtho.2016.07.002,cross-sectional,diagnosis,CT,Lung
Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features,"Lung cancer is the most prevalent cancer worldwide, and histopathological assessment is indispensable for its diagnosis. However, human evaluation of pathology slides cannot accurately predict patients' prognoses. In this study, we obtain 2,186 haematoxylin and eosin stained histopathology whole-slide images of lung adenocarcinoma and squamous cell carcinoma patients from The Cancer Genome Atlas (TCGA), and 294 additional images from Stanford Tissue Microarray (TMA) Database. We extract 9,879 quantitative image features and use regularized machine-learning methods to select the top features and to distinguish shorter-term survivors from longer-term survivors with stage I adenocarcinoma (P<0.003) or squamous cell carcinoma (P=0.023) in the TCGA data set. We validate the survival prediction framework with the TMA cohort (P<0.036 for both tumour types). Our results suggest that automatically derived image features can predict the prognosis of lung cancer patients and thereby contribute to precision oncology. Our methods are extensible to histopathology images of other organs.",2016,10.1038/ncomms12474,,,,
Predicting polysomnographic severity thresholds in children using machine learning,"BACKGROUND: Approximately 500,000 children undergo tonsillectomy and adenoidectomy (T&A) annually for treatment of obstructive sleep disordered breathing (oSDB). Although polysomnography is beneficial for preoperative risk stratification in these children, its expanded use is limited by the associated costs and resources needed. Therefore, we used machine learning and data from potentially wearable sensors to identify children needing postoperative overnight monitoring based on the polysomnographic severity of oSDB. METHODS: Children aged 2-17 years undergoing polysomnography were included. Six machine learning models were created using (i) clinical parameters and (ii) nocturnal actigraphy and oxygen desaturation index. The prediction performance for polysomnography-derived severity of oSDB measured by apnea hypopnea index (AHI) >2 and >10 were evaluated. RESULTS: One hundred and ninety children were included. One hundred and eight were male (57%), mean age was 6.7 years [95% confidence interval; 6.1, 7.2], and mean AHI was 10.6 [7.8, 13.4]. Predictive performance utilizing clinical parameters was poor for both AHI > 2 (accuracy range: 48-56% for all models) and AHI > 10 (50-61%). Combining oximetry and actigraphy improved the accuracy to 87-89% for AHI > 2 and 95-96% for AHI > 10. CONCLUSIONS: Machine learning with oximetry and actigraphy identifies most children needing overnight monitoring as determined by polysomnographic severity of oSDB, supporting a potential resource-conscious screening pathway for children undergoing T&A. IMPACT: We provide proof of principle for the utility of machine learning, oximetry, and actigraphy to screen for severe obstructive sleep apnea syndrome (OSAS) in children. Clinical parameters perform poorly in predicting the severity of OSAS, which is confirmed in the current study. The predictive accuracy for severe OSAS was improved by a smaller subset of quantifiable physiologic parameters, such as oximetry. The results of this study support a lower cost, patient-friendly screening pathway to identify children in need of in-hospital observation after surgery.",2020,10.1038/s41390-020-0944-0,,,,
Predicting response to cancer immunotherapy using noninvasive radiomic biomarkers,"INTRODUCTION: Immunotherapy is regarded as one of the major breakthroughs in cancer treatment. Despite its success, only a subset of patients responds-urging the quest for predictive biomarkers. We hypothesize that artificial intelligence (AI) algorithms can automatically quantify radiographic characteristics that are related to and may therefore act as noninvasive radiomic biomarkers for immunotherapy response. PATIENTS AND METHODS: In this study, we analyzed 1055 primary and metastatic lesions from 203 patients with advanced melanoma and non-small-cell lung cancer (NSCLC) undergoing anti-PD1 therapy. We carried out an AI-based characterization of each lesion on the pretreatment contrast-enhanced CT imaging data to develop and validate a noninvasive machine learning biomarker capable of distinguishing between immunotherapy responding and nonresponding. To define the biological basis of the radiographic biomarker, we carried out gene set enrichment analysis in an independent dataset of 262 NSCLC patients. RESULTS: The biomarker reached significant performance on NSCLC lesions (up to 0.83 AUC, P < 0.001) and borderline significant for melanoma lymph nodes (0.64 AUC, P = 0.05). Combining these lesion-wide predictions on a patient level, immunotherapy response could be predicted with an AUC of up to 0.76 for both cancer types (P < 0.001), resulting in a 1-year survival difference of 24% (P = 0.02). We found highly significant associations with pathways involved in mitosis, indicating a relationship between increased proliferative potential and preferential response to immunotherapy. CONCLUSIONS: These results indicate that radiographic characteristics of lesions on standard-of-care imaging may function as noninvasive biomarkers for response to immunotherapy, and may show utility for improved patient stratification in both neoadjuvant and palliative settings.",2019,10.1093/annonc/mdz108,retrospective cohort,treatment,CT,Lung
Predicting response to immunotherapy in advanced non-small-cell lung cancer using tumor mutational burden radiomic biomarker,"BACKGROUND: Tumor mutational burden (TMB) is a significant predictor of immune checkpoint inhibitors (ICIs) efficacy. This study investigated the correlation between deep learning radiomic biomarker and TMB, including its predictive value for ICIs treatment response in patients with advanced non-small-cell lung cancer (NSCLC). METHODS: CT images from 327 patients with TMB data (TMB median=6.067 mutations per megabase (range: 0 to 42.151)) were retrospectively collected and randomly divided into a training (n=236), validation (n=26), and test cohort (n=65). We used 3D-densenet to estimate the target tumor area, which used 1020 deep learning features to distinguish High-TMB from Low-TMB patients and establish the TMB radiomic biomarker (TMBRB). The TMBRB was developed in the training cohort combined with validation cohort and evaluated in the test cohort. The predictive value of TMBRB was assessed in a cohort of 123 NSCLC patients who had received ICIs (survival median=462 days (range: 16 to 1128)). RESULTS: TMBRB discriminated between High-TMB and Low-TMB patients in the training cohort (area under the curve (AUC): 0.85, 95% CI: 0.84 to 0.87))and test cohort (AUC: 0.81, 95% CI: 0.77 to 0.85). In this study, the predictive value of TMBRB was better than that of a histological subtype (AUC of training cohort: 0.75, 95% CI: 0.72 to 0.77; AUC of test cohort: 0.71, 95% CI: 0.66 to 0.76) or Radiomic model (AUC of training cohort: 0.75, 95% CI: 0.72 to 0.77; AUC of test cohort: 0.74, 95% CI: 0.69 to 0.79). When predicting immunotherapy efficacy, TMBRB divided patients into a high- and low-risk group with distinctly different overall survival (OS; HR: 0.54, 95% CI: 0.31 to 0.95; p=0.030) and progression-free survival (PFS; HR: 1.78, 95% CI: 1.07 to 2.95; p=0.023). Moreover, TMBRB had a better predictive ability when combined with the Eastern Cooperative Oncology Group performance status (OS: p=0.007; PFS: p=0.003). Visual analysis revealed that tumor microenvironment was important for predicting TMB. CONCLUSION: By combining deep learning technology and CT images, we developed an individual non-invasive biomarker that could distinguish High-TMB from Low-TMB, which might inform decisions on the use of ICIs in patients with advanced NSCLC.",2020,10.1136/jitc-2020-000550,retrospective cohort,treatment,CT,Lung
"Predicting tumor hypoxia in non-small cell lung cancer by combining CT, FDG PET and dynamic contrast-enhanced CT","BACKGROUND: Most solid tumors contain inadequately oxygenated (i.e., hypoxic) regions, which tend to be more aggressive and treatment resistant. Hypoxia PET allows visualization of hypoxia and may enable treatment adaptation. However, hypoxia PET imaging is expensive, time-consuming and not widely available. We aimed to predict hypoxia levels in non-small cell lung cancer (NSCLC) using more easily available imaging modalities: FDG-PET/CT and dynamic contrast-enhanced CT (DCE-CT). MATERIAL AND METHODS: For 34 NSCLC patients, included in two clinical trials, hypoxia HX4-PET/CT, planning FDG-PET/CT and DCE-CT scans were acquired before radiotherapy. Scans were non-rigidly registered to the planning CT. Tumor blood flow (BF) and blood volume (BV) were calculated by kinetic analysis of DCE-CT images. Within the gross tumor volume, independent clusters, i.e., supervoxels, were created based on FDG-PET/CT. For each supervoxel, tumor-to-background ratios (TBR) were calculated (median SUV/aorta SUV(mean)) for HX4-PET/CT and supervoxel features (median, SD, entropy) for the other modalities. Two random forest models (cross-validated: 10 folds, five repeats) were trained to predict the hypoxia TBR; one based on CT, FDG, BF and BV, and one with only CT and FDG features. Patients were split in a training (trial NCT01024829) and independent test set (trial NCT01210378). For each patient, predicted, and observed hypoxic volumes (HV) (TBR > 1.2) were compared. RESULTS: Fifteen patients (3291 supervoxels) were used for training and 19 patients (1502 supervoxels) for testing. The model with all features (RMSE training: 0.19 ± 0.01, test: 0.27) outperformed the model with only CT and FDG-PET features (RMSE training: 0.20 ± 0.01, test: 0.29). All tumors of the test set were correctly classified as normoxic or hypoxic (HV > 1 cm(3)) by the best performing model. CONCLUSIONS: We created a data-driven methodology to predict hypoxia levels and hypoxia spatial patterns using CT, FDG-PET and DCE-CT features in NSCLC. The model correctly classifies all tumors, and could therefore, aid tumor hypoxia classification and patient stratification.",2017,10.1080/0284186x.2017.1349332,cross-sectional,diagnosis,CT,Lung
"Predicting Unnecessary Nodule Biopsies from a Small, Unbalanced, and Pathologically Proven Dataset by Transfer Learning","This study explores an automatic diagnosis method to predict unnecessary nodule biopsy from a small, unbalanced, and pathologically proven database. The automatic diagnosis method is based on a convolutional neural network (CNN) model. Because of the small and unbalanced samples, the presented method aims to improve the transfer learning capability via the VGG16 architecture and optimize the related transfer learning parameters. For comparison purpose, a traditional machine learning method is implemented, which extracts the texture features and classifies the features by support vector machine (SVM). The database includes 68 biopsied nodules, 16 are pathologically proven benign and the remaining 52 are malignant. To consider the volumetric data by the CNN model, each image slice from each nodule volume is selected randomly until all image slices of each nodule are utilized. The leave-one-out and 10-folder cross validations are applied to train and test the randomly selected 68 image slices (one image slice from one nodule) in each experiment, respectively. The averages over all the experimental outcomes are the final results. The experiments revealed that the features from both the medical and the natural images share the similarity of focusing on simpler and less-abstract objects, leading to the conclusion that not the more the transfer convolutional layers, the better the classification results. Transfer learning from other larger datasets can supply additional information to small and unbalanced datasets to improve the classification performance. The presented method has shown the potential to adapt CNN architecture to improve the prediction of unnecessary nodule biopsy from small, unbalanced, and pathologically proven volumetric dataset.",2020,10.1007/s10278-019-00306-z,cross-sectional,diagnosis,CT,Lung
"Prediction of COVID Criticality Score with Laboratory, Clinical and CT Images using Hybrid Regression Models","BACKGROUND AND OBJECTIVE: Rapid and precise diagnosis of COVID-19 is very critical in hotspot regions. The main aim of this proposed work is to investigate the baseline, laboratory and CT features of COVID-19 affected patients of two groups (Early and Critical stages). The detection model for COVID-19 is built depending upon the manifestations that define the severity of the disease. METHODS: The CT scan images are fed into the various deep learning, machine learning and hybrid learning models to mine the necessary features and predict CT Score. The predicted CT score along with other clinical, laboratory and CT scan image features are then passed to train the various Regression models for predicting the COVID Criticality (CC) Score. These baseline, laboratory and CT features of COVID-19 are reduced using Statistical analysis and Univariate logistic regression analysis. RESULTS: When analysing the prediction of CT scores using images alone, AlexNet+Lasso yields better outcome with regression score of 0.9643 and RMSE of 0.0023 when compared with Decision tree (RMSE of 0.0034; Regression score of 0.9578) and GRU (RMSE of 0.1253; regression score of 0.9323). When analysing the prediction of CC scores using CT scores and other baseline, laboratory and CT features, VGG-16+Linear Regression yields better results with regression score of 0.9911 and RMSE of 0.0002 when compared with Linear SVR (RMSE of 0.0006; Regression score of 0.9911) and LSTM (RMSE of 0.0005; Regression score of 0.9877). The correlation analysis is performed to identify the significance of utilizing other features in prediction of CC Score. The correlation coefficient of CT scores with actual value is 0.93 and 0.92 for Early stage group and Critical stage group respectively. The correlation coefficient of CC scores with actual value is 0.96 for Early stage group and 0.95 for Critical stage group.The classification of COVID-19 patients are carried out with the help of predicted CC Scores. CONCLUSIONS: This proposed work is carried out in the motive of helping radiologists in faster categorization of COVID patients as Early or Severe staged using CC Scores. The automated prediction of COVID Criticality Score using our diagnostic model can help radiologists and physicians save time for carrying out further treatment and procedures.",2021,10.1016/j.cmpb.2021.106336,cross-sectional,diagnosis,CT,Lung
Prediction of disease progression in patients with COVID-19 by artificial intelligence assisted lesion quantification,"To investigate the value of artificial intelligence (AI) assisted quantification on initial chest CT for prediction of disease progression and clinical outcome in patients with coronavirus disease 2019 (COVID-19). Patients with confirmed COVID-19 infection and initially of non-severe type were retrospectively included. The initial CT scan on admission was used for imaging analysis. The presence of ground glass opacity (GGO), consolidation and other findings were visually evaluated. CT severity score was calculated according to the extent of lesion involvement. In addition, AI based quantification of GGO and consolidation volume were also performed. 123 patients (mean age: 64.43 ± 14.02; 62 males) were included. GGO + consolidation was more frequently revealed in progress-to-severe group whereas pure GGO was more likely to be found in non-severe group. Compared to non-severe group, patients in progress-to-severe group had larger GGO volume (167.33 ± 167.88 cm(3) versus 101.12 ± 127 cm(3), p = 0.013) as well as consolidation volume (40.85 ± 60.4 cm(3) versus 6.63 ± 14.91 cm(3), p < 0.001). Among imaging parameters, consolidation volume had the largest area under curve (AUC) in discriminating non-severe from progress-to-severe group (AUC = 0.796, p < 0.001) and patients with or without critical events (AUC = 0.754, p < 0.001). According to multivariate regression, consolidation volume and age were two strongest predictors for disease progression (hazard ratio: 1.053 and 1.071, p: 0.006 and 0.008) whereas age and diabetes were predictors for unfavorable outcome. Consolidation volume quantified on initial chest CT was the strongest predictor for disease severity progression and larger consolidation volume was associated with unfavorable clinical outcome.",2020,10.1038/s41598-020-79097-1,retrospective cohort,prognosis,CT,Lung
Prediction of lung cancer incidence on the low-dose computed tomography arm of the National Lung Screening Trial: A dynamic Bayesian network,"INTRODUCTION: Identifying high-risk lung cancer individuals at an early disease stage is the most effective way of improving survival. The landmark National Lung Screening Trial (NLST) demonstrated the utility of low-dose computed tomography (LDCT) imaging to reduce mortality (relative to X-ray screening). As a result of the NLST and other studies, imaging-based lung cancer screening programs are now being implemented. However, LDCT interpretation results in a high number of false positives. A set of dynamic Bayesian networks (DBN) were designed and evaluated to provide insight into how longitudinal data can be used to help inform lung cancer screening decisions. METHODS: The LDCT arm of the NLST dataset was used to build and explore five DBNs for high-risk individuals. Three of these DBNs were built using a backward construction process, and two using structure learning methods. All models employ demographics, smoking status, cancer history, family lung cancer history, exposure risk factors, comorbidities related to lung cancer, and LDCT screening outcome information. Given the uncertainty arising from lung cancer screening, a cancer state-space model based on lung cancer staging was utilized to characterize the cancer status of an individual over time. The models were evaluated on balanced training and test sets of cancer and non-cancer cases to deal with data imbalance and overfitting. RESULTS: Results were comparable to expert decisions. The average area under the curve (AUC) of the receiver operating characteristic (ROC) for the three intervention points of the NLST trial was higher than 0.75 for all models. Evaluation of the models on the complete LDCT arm of the NLST dataset (N=25,486) demonstrated satisfactory generalization. Consensus of predictions over similar cases is reported in concordance statistics between the models' and the physicians' predictions. The models' predictive ability with respect to missing data was also evaluated with the sample of cases that missed the second screening exam of the trial (N=417). The DBNs outperformed comparison models such as logistic regression and naïve Bayes. CONCLUSION: The lung cancer screening DBNs demonstrated high discrimination and predictive power with the majority of cancer and non-cancer cases.",2016,10.1016/j.artmed.2016.07.001,RCT,diagnosis,CT,Lung
Prediction of lung cancer risk at follow-up screening with low-dose CT: a training and validation study of a deep learning method,"BACKGROUND: Current lung cancer screening guidelines use mean diameter, volume or density of the largest lung nodule in the prior computed tomography (CT) or appearance of new nodule to determine the timing of the next CT. We aimed at developing a more accurate screening protocol by estimating the 3-year lung cancer risk after two screening CTs using deep machine learning (ML) of radiologist CT reading and other universally available clinical information. METHODS: A deep machine learning (ML) algorithm was developed from 25,097 participants who had received at least two CT screenings up to two years apart in the National Lung Screening Trial. Double-blinded validation was performed using 2,294 participants from the Pan-Canadian Early Detection of Lung Cancer Study (PanCan). Performance of ML score to inform lung cancer incidence was compared with Lung-RADS and volume doubling time using time-dependent ROC analysis. Exploratory analysis was performed to identify individuals with aggressive cancers and higher mortality rates. FINDINGS: In the PanCan validation cohort, ML showed excellent discrimination with a 1-, 2- and 3-year time-dependent AUC values for cancer diagnosis of 0·968±0·013, 0·946±0·013 and 0·899±0·017. Although high ML score cohort included only 10% of the PanCan sample, it identified 94%, 85%, and 71% of incident and interval lung cancers diagnosed within 1, 2, and 3 years, respectively, after the second screening CT. Furthermore, individuals with high ML score had significantly higher mortality rates (HR=16·07, p<0·001) compared to those with lower risk. INTERPRETATION: ML tool that recognizes patterns in both temporal and spatial changes as well as synergy among changes in nodule and non-nodule features may be used to accurately guide clinical management after the next scheduled repeat screening CT.",2019,10.1016/s2589-7500(19)30159-1,RCT,diagnosis,CT,Lung
Prediction of lung emphysema in COPD by spirometry and clinical symptoms: results from COSYCONET,"BACKGROUND: Lung emphysema is an important phenotype of chronic obstructive pulmonary disease (COPD), and CT scanning is strongly recommended to establish the diagnosis. This study aimed to identify criteria by which physicians with limited technical resources can improve the diagnosis of emphysema. METHODS: We studied 436 COPD patients with prospective CT scans from the COSYCONET cohort. All items of the COPD Assessment Test (CAT) and the St George's Respiratory Questionnaire (SGRQ), the modified Medical Research Council (mMRC) scale, as well as data from spirometry and CO diffusing capacity, were used to construct binary decision trees. The importance of parameters was checked by the Random Forest and AdaBoost machine learning algorithms. RESULTS: When relying on questionnaires only, items CAT 1 & 7 and SGRQ 8 & 12 sub-item 3 were most important for the emphysema- versus airway-dominated phenotype, and among the spirometric measures FEV(1)/FVC. The combination of CAT item 1 (≤ 2) with mMRC (> 1) and FEV(1)/FVC, could raise the odds for emphysema by factor 7.7. About 50% of patients showed combinations of values that did not markedly alter the likelihood for the phenotypes, and these could be easily identified in the trees. Inclusion of CO diffusing capacity revealed the transfer coefficient as dominant measure. The results of machine learning were consistent with those of the single trees. CONCLUSIONS: Selected items (cough, sleep, breathlessness, chest condition, slow walking) from comprehensive COPD questionnaires in combination with FEV(1)/FVC could raise or lower the likelihood for lung emphysema in patients with COPD. The simple, parsimonious approach proposed by us might help if diagnostic resources regarding respiratory diseases are limited. Trial registration ClinicalTrials.gov, Identifier: NCT01245933, registered 18 November 2010, https://clinicaltrials.gov/ct2/show/record/NCT01245933 .",2021,10.1186/s12931-021-01837-2,prospective cohort,prognosis,CT,Lung
Fetal thymus volume estimation by virtual organ computer-aided analysis in normal pregnancies,"OBJECTIVES: The thymus has a pyramidal shape, which is best shown in coronal planes. The aim of this study was to evaluate the potential of virtual organ computer-aided analysis to estimate fetal thymus volume in normal pregnancies. METHODS: Three-dimensional volume data sets from the axial upper mediastinal section were acquired from 37 normal pregnancies between 12 and 35 weeks' gestation. Thymus volume was calculated by virtual organ computer-aided analysis by 2 separate examiners. In 12 cases, volumes were also acquired with 4-dimensional sonography and spatiotemporal image correlation software to assess the variability in thymus size between the systolic and diastolic periods of fetal heart motion. Linear regression analysis was used to assess the relationship between the fetal thymus volume and gestational age. Paired Student t tests were used to evaluate both the level of agreement for interobserver and intraobserver variability and the difference between diastolic and systolic thymus volumes. RESULTS: Identification of the borders of the thymus and calculation of its volume were successful in 28 patients (77.7%). Statistically significant linear growth of the thymus during pregnancy, from 12 to 35 weeks, was found. The growth coefficient for each gestational age was 0.43 (95% confidence interval, 0.355 to 0.504; P < .001). The difference in thymus size between systole and diastole was minor (0.0798 cm(3); 95% confidence interval, -0.044 to 0.203 cm(3)). Interobserver and intraobserver variability was not statistically significant. CONCLUSIONS: Although the thymus has a complex shape, it was possible to determine its borders and to calculate its volume by virtual organ computer-aided analysis in 77.7% of cases. Linear growth during pregnancy was found, and the minor changes during systole and diastole could be explained by condensation of the soft tissue of the thymus secondary to cardiac activity.",2015,10.7863/ultra.34.5.847,,,,
Machine learning in the prediction of cardiac epicardial and mediastinal fat volumes,"We propose a methodology to predict the cardiac epicardial and mediastinal fat volumes in computed tomography images using regression algorithms. The obtained results indicate that it is feasible to predict these fats with a high degree of correlation, thus alleviating the requirement for manual or automatic segmentation of both fat volumes. Instead, segmenting just one of them suffices, while the volume of the other may be predicted fairly precisely. The correlation coefficient obtained by the Rotation Forest algorithm using MLP Regressor for predicting the mediastinal fat based on the epicardial fat was 0.9876, with a relative absolute error of 14.4% and a root relative squared error of 15.7%. The best correlation coefficient obtained in the prediction of the epicardial fat based on the mediastinal was 0.9683 with a relative absolute error of 19.6% and a relative squared error of 24.9%. Moreover, we analysed the feasibility of using linear regressors, which provide an intuitive interpretation of the underlying approximations. In this case, the obtained correlation coefficient was 0.9534 for predicting the mediastinal fat based on the epicardial, with a relative absolute error of 31.6% and a root relative squared error of 30.1%. On the prediction of the epicardial fat based on the mediastinal fat, the correlation coefficient was 0.8531, with a relative absolute error of 50.43% and a root relative squared error of 52.06%. In summary, it is possible to speed up general medical analyses and some segmentation and quantification methods that are currently employed in the state-of-the-art by using this prediction approach, which consequently reduces costs and therefore enables preventive treatments that may lead to a reduction of health problems.",2017,10.1016/j.compbiomed.2017.02.010,,,,
Anatomical knowledge based level set segmentation of cardiac ventricles from MRI,"This paper represents a novel level set framework for segmentation of cardiac left ventricle (LV) and right ventricle (RV) from magnetic resonance images based on anatomical structures of the heart. We first propose a level set approach to recover the endocardium and epicardium of LV by using a bi-layer level set (BILLS) formulation, in which the endocardium and epicardium are represented by the 0-level set and k-level set of a level set function. Furthermore, the recovery of LV endocardium and epicardium is achieved by a level set evolution process, called convexity preserving bi-layer level set (CP-BILLS). During the CP-BILLS evolution, the 0-level set and k-level set simultaneously evolve and move toward the true endocardium and epicardium under the guidance of image information and the impact of the convexity preserving mechanism as well. To eliminate the manual selection of the k-level, we develop an algorithm for automatic selection of an optimal k-level. As a result, the obtained endocardial and epicardial contours are convex and consistent with the anatomy of cardiac ventricles. For segmentation of the whole ventricle, we extend this method to the segmentation of RV and myocardium of both left and right ventricles by using a convex shape decomposition (CSD) structure of cardiac ventricles based on anatomical knowledge. Experimental results demonstrate promising performance of our method. Compared with some traditional methods, our method exhibits superior performance in terms of segmentation accuracy and algorithm stability. Our method is comparable with the state-of-the-art deep learning-based method in terms of segmentation accuracy and algorithm stability, but our method has no need for training and the manual segmentation of the training data.",2022,10.1016/j.mri.2021.10.005,cross-sectional,informatics,MRI,Heart
Automatic left ventricle segmentation in short-axis MRI using deep convolutional neural networks and central-line guided level set approach,"In the clinical diagnosis of cardiovascular diseases, left ventricle (LV) segmentation in cardiac magnetic resonance images (MRI) is an indispensable procedure for doctors. To reduce the time needed for diagnosis, we develop an automatic LV segmentation method by integrating the convolutional neural network (CNN) with the level set approach. Firstly, a CNN based myocardial central-line detection algorithm was proposed to replace the manual initialization process for traditional level set approaches. Secondly, we present a novel central-line guided level set approach (CGLS) for delineating the myocardium region. In particular, we incorporate the myocardial central-line into the level set energy formulation as a constraint term. It plays two important roles in the iterative process: restricting the zero-level contour to stay around the myocardial central-line and preserving the anatomical geometry of myocardium segmentation result. In experiments, our method yields results as below: (1) 1.74 mm and 2.06 mm in terms of epicardium and endocardium perpendicular distance on MICCAI 2009 dataset, (2) 0.955 and 0.853 in terms of LV and myocardium Dice metric at the end-diastole on ACDC MICCAI 2017 dataset. The experimental data demonstrate that our method outperforms some state-of-the-art methods and achieves a good agreement with the manual segmentation results.",2020,10.1016/j.compbiomed.2020.103877,cross-sectional,informatics,MRI,Heart
Prediction of Lung Infection during Palliative Chemotherapy of Lung Cancer Based on Artificial Neural Network,"Lung infection seriously affects the effect of chemotherapy in patients with lung cancer and increases pain. The study is aimed at establishing the prediction model of infection in patients with lung cancer during chemotherapy by an artificial neural network (ANN). Based on the data of historical cases in our hospital, the variables were screened, and the prediction model was established. A logistic regression (LR) model was used to screen the data. The indexes with statistical significance were selected, and the LR model and back propagation neural network model were established. A total of 80 cases of advanced lung cancer patients with palliative chemotherapy were predicted, and the prediction performance of different model was evaluated by the receiver operating characteristic curve (ROC). It was found that age≧60 years, length of stay≧14 d, surgery history, combined chemotherapy, myelosuppression, diabetes, and hormone application were risk factors of infection in lung cancer patients during chemotherapy. The area under the ROC curve of the LR model for prediction lung infection was 0.729 ± 0.084, which was less than that of the ANN model (0.897 ± 0.045). The results concluded that the neural network model is better than the LR model in predicting lung infection of lung cancer patients during chemotherapy.",2022,10.1155/2022/4312117,,,,
Prediction of mediastinal lymph node metastasis based on (18)F-FDG PET/CT imaging using support vector machine in non-small cell lung cancer,"OBJECTIVE: The purpose of this study was to develop a classification method based on support vector machine (SVM) to improve the diagnostic performance of (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) to detect the lymph node (LN) metastasis in non-small cell lung cancer (NSCLC). METHOD: Two hundred nineteen lymph nodes (37 metastatic) from 71 patients were evaluated in this study. SVM models were developed with 7 LN features. The area under the curve (AUC) and accuracy of 9 models were compared to select the best model. The best SVM model was simplified on the basis of the feature weights and value distribution to further suit the clinical application. RESULTS: The maximum, minimum, and mean accuracy of the best model was 91.89% (68/74, 95% CI 83.11~96.54%), 66.22% (49/74, 95% CI 54.85~75.98%), and 80.09% (59,266/74,000, 95% CI 70.27~89.19%), respectively, with an AUC of 0.94, 0.66, and 0.81, respectively. The best SVM model was finally simplified into a score rule: LNs with scores more than 3.0 were considered as malignant ones, whereas LNs with scores less than 1.5 tended to be benign ones. For the LNs with scores within a range of 1.5-3.0, metastasis was suspected. CONCLUSION: An SVM model based on (18)F-FDG PET/CT images was able to predict the metastatic LNs for patients with NSCLC. The ratio of the maximum of standard uptake value of LNs to aortic arch played a major role in the model. After simplification, the model could be transferred into a scoring method which may partly help clinicians determine the clinical staging of patients with NSCLC relatively easier. KEY POINTS: • The SVM model based on (18)F-FDG PET/CT features may help clinicians to make a decision for metastatic mediastinal lymph nodes in patients with NSCLC. • The SUR(blood) plays a major role in the SVM model. • The score rule based on the SVM model simplified the complexity of the model and may partly help clinicians determine the clinical staging of patients with NSCLC relatively easier.",2021,10.1007/s00330-020-07466-5,cross-sectional,diagnosis,PET-CT,Mediastinal lymph node
Prediction of Non-small Cell Lung Cancer Histology by a Deep Ensemble of Convolutional and Bidirectional Recurrent Neural Network,"Histology subtype prediction is a major task for grading non-small cell lung cancer (NSCLC) tumors. Invasive methods such as biopsy often lack in tumor sample, and as a result radiologists or oncologists find it difficult to detect proper histology of NSCLC tumors. The non-invasive methods such as machine learning may play a useful role to predict NSCLC histology by using medical image biomarkers. Few attempts have so far been made to predict NSCLC histology by considering all the major subtypes. The present study aimed to develop a more accurate deep learning model by clubbing convolutional and bidirectional recurrent neural networks. The NSCLC Radiogenomics dataset having 211 subjects was used in the study. Ten best models found during experimentation were averaged to form an ensemble. The model ensemble was executed with 10-fold repeated stratified cross-validation, and the results got were tested with metrics like accuracy, recall, precision, F1-score, Cohen's kappa, and ROC-AUC score. The accuracy of the ensemble model showed considerable improvement over the best model found with the single model. The proposed model may help significantly in the automated prognosis of NSCLC and other types of cancers.",2020,10.1007/s10278-020-00337-x,,,,
Prediction of Obstructive Lung Disease from Chest Radiographs via Deep Learning Trained on Pulmonary Function Data,"BACKGROUND: Chronic obstructive pulmonary disease (COPD), the third leading cause of death worldwide, is often underdiagnosed. PURPOSE: To develop machine learning methods to predict COPD using chest radiographs and a convolutional neural network (CNN) trained with near-concurrent pulmonary function test (PFT) data. Comparison is made to natural language processing (NLP) of the associated radiologist text reports. MATERIALS AND METHODS: This IRB-approved single-institution retrospective study uses 6749 two-view chest radiograph exams (2012-2017, 4436 unique subjects, 54% female, 46% male), same-day associated radiologist text reports, and PFT exams acquired within 180 days. The Image Model (Resnet18 pre-trained with ImageNet CNN) is trained using frontal and lateral radiographs and PFTs with 10% of the subjects for validation and 19% for testing. The NLP Model is trained using radiologist text reports and PFTs. The primary metric of model comparison is the area under the receiver operating characteristic curve (AUC). RESULTS: The Image Model achieves an AUC of 0.814 for prediction of obstructive lung disease (FEV1/FVC <0.7) from chest radiographs and performs better than the NLP Model (AUC 0.704, p<0.001) from radiologist text reports where FEV1 = forced expiratory volume in 1 second and FVC = forced vital capacity. The Image Model performs better for prediction of severe or very severe COPD (FEV1 <0.5) with an AUC of 0.837 versus the NLP model AUC of 0.770 (p<0.001). CONCLUSION: A CNN Image Model trained on physiologic lung function data (PFTs) can be applied to chest radiographs for quantitative prediction of obstructive lung disease with good accuracy.",2020,10.2147/copd.S279850,cross-sectional,diagnosis,CXR,Lungs
Prediction of pathologic femoral fractures in patients with lung cancer using machine learning algorithms: Comparison of computed tomography-based radiological features with clinical features versus without clinical features,"PURPOSE: The purpose of this article is to compare the predictive power of two models trained with computed tomography (CT)-based radiological features and both CT-based radiological and clinical features for pathologic femoral fractures in patients with lung cancer using machine learning algorithms. METHODS: Between January 2010 and December 2014, 315 lung cancer patients with metastasis to the femur were included. Among them, 84 patients who underwent CT scan and were followed up for more than 3 months were enrolled. We examined clinical and radiological risk factors affecting pathologic fracture through logistic regression. Predictive analysis was performed using five different supervised learning algorithms. The power of predictive model trained with CT-based radiological features was compared to those trained with both CT-based radiological and clinical features. RESULTS: In multivariate logistic regression, female sex (odds ratio = 0.25, p = 0.0126), osteolysis (odds ratio = 7.62, p = 0.0239), and absence of radiation therapy (odds ratio = 10.25, p = 0.0258) significantly increased the risk of pathologic fracture in proximal femur. The predictive model trained with both CT-based radiological and clinical features showed the highest area under the receiver operating characteristic curve (0.80 ± 0.14, p < 0.0001) through gradient boosting algorithm. CONCLUSION: We believe that machine learning algorithms may be useful in the prediction of pathologic femoral fracture, which are multifactorial problem.",2017,10.1177/2309499017716243,,,,
Prediction of pathologic stage in non-small cell lung cancer using machine learning algorithm based on CT image feature analysis,"PURPOSE: To explore imaging biomarkers that can be used for diagnosis and prediction of pathologic stage in non-small cell lung cancer (NSCLC) using multiple machine learning algorithms based on CT image feature analysis. METHODS: Patients with stage IA to IV NSCLC were included, and the whole dataset was divided into training and testing sets and an external validation set. To tackle imbalanced datasets in NSCLC, we generated a new dataset and achieved equilibrium of class distribution by using SMOTE algorithm. The datasets were randomly split up into a training/testing set. We calculated the importance value of CT image features by means of mean decrease gini impurity generated by random forest algorithm and selected optimal features according to feature importance (mean decrease gini impurity > 0.005). The performance of prediction model in training and testing sets were evaluated from the perspectives of classification accuracy, average precision (AP) score and precision-recall curve. The predictive accuracy of the model was externally validated using lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) samples from TCGA database. RESULTS: The prediction model that incorporated nine image features exhibited a high classification accuracy, precision and recall scores in the training and testing sets. In the external validation, the predictive accuracy of the model in LUAD outperformed that in LUSC. CONCLUSIONS: The pathologic stage of patients with NSCLC can be accurately predicted based on CT image features, especially for LUAD. Our findings extend the application of machine learning algorithms in CT image feature prediction for pathologic staging and identify potential imaging biomarkers that can be used for diagnosis of pathologic stage in NSCLC patients.",2019,10.1186/s12885-019-5646-9,cross-sectional,diagnosis,CT,Lungs
Prediction of Patient Management in COVID-19 Using Deep Learning-Based Fully Automated Extraction of Cardiothoracic CT Metrics and Laboratory Findings,"OBJECTIVE: To extract pulmonary and cardiovascular metrics from chest CTs of patients with coronavirus disease 2019 (COVID-19) using a fully automated deep learning-based approach and assess their potential to predict patient management. MATERIALS AND METHODS: All initial chest CTs of patients who tested positive for severe acute respiratory syndrome coronavirus 2 at our emergency department between March 25 and April 25, 2020, were identified (n = 120). Three patient management groups were defined: group 1 (outpatient), group 2 (general ward), and group 3 (intensive care unit [ICU]). Multiple pulmonary and cardiovascular metrics were extracted from the chest CT images using deep learning. Additionally, six laboratory findings indicating inflammation and cellular damage were considered. Differences in CT metrics, laboratory findings, and demographics between the patient management groups were assessed. The potential of these parameters to predict patients' needs for intensive care (yes/no) was analyzed using logistic regression and receiver operating characteristic curves. Internal and external validity were assessed using 109 independent chest CT scans. RESULTS: While demographic parameters alone (sex and age) were not sufficient to predict ICU management status, both CT metrics alone (including both pulmonary and cardiovascular metrics; area under the curve [AUC] = 0.88; 95% confidence interval [CI] = 0.79-0.97) and laboratory findings alone (C-reactive protein, lactate dehydrogenase, white blood cell count, and albumin; AUC = 0.86; 95% CI = 0.77-0.94) were good classifiers. Excellent performance was achieved by a combination of demographic parameters, CT metrics, and laboratory findings (AUC = 0.91; 95% CI = 0.85-0.98). Application of a model that combined both pulmonary CT metrics and demographic parameters on a dataset from another hospital indicated its external validity (AUC = 0.77; 95% CI = 0.66-0.88). CONCLUSION: Chest CT of patients with COVID-19 contains valuable information that can be accessed using automated image analysis. These metrics are useful for the prediction of patient management.",2021,10.3348/kjr.2020.0994,retrospective cohort,prognosis,CT,Lungs
Prediction of Pulmonary Fibrosis Based on X-Rays by Deep Neural Network,"As a fatal lung disease, pulmonary fibrosis can cause irreversible damage to the lung, affect normal lung function, and eventually lead to death. At present, the pathogenesis of this kind of disease is not completely clear, and there is no radical cure. The main purpose of the treatment of this disease is to slow down the deterioration of pulmonary fibrosis. For this kind of disease, if it can be found early, it can be treated as soon as possible and the life of patients will be prolonged. Clinically, the diagnosis of pulmonary fibrosis depends on the relevant imaging examination, lung biopsy, lung function examination, and so on. Imaging data such as X-rays is a common examination means in clinical medicine and also plays an important role in the prediction of pulmonary fibrosis. Through X-ray, radiologists can clearly see the relevant lung lesions so as to make the relevant diagnosis. Based on the common medical image data, this paper designs related models to complete the prediction of pulmonary fibrosis. The model designed in this paper is mainly divided into two parts: first, this paper uses a neural network to complete the segmentation of lung organs; second, the neural network of image classification is designed to complete the process from lung image to disease prediction. In the design of these two parts, this paper improves on the basis of previous research methods. Through the design of a neural network with higher performance, more optimized results are achieved on the key indicators which can be applied to the real scene of pulmonary fibrosis prediction.",2022,10.1155/2022/3845008,cross-sectional,diagnosis,CXR,Lungs
Prediction of radiation pneumonitis after definitive radiotherapy for locally advanced non-small cell lung cancer using multi-region radiomics analysis,"To predict grade ≥ 2 radiation pneumonitis (RP) in patients with locally advanced non-small cell lung cancer (NSCLC) using multi-region radiomics analysis. Data from 77 patients with NSCLC who underwent definitive radiotherapy between 2008 and 2018 were analyzed. Radiomic feature extraction from the whole lung (whole-lung radiomics analysis) and imaging- and dosimetric-based segmentation (multi-region radiomics analysis) were performed. Patients with RP grade ≥ 2 or < 2 were classified. Predictors were selected with least absolute shrinkage and selection operator logistic regression and the model was built with neural network classifiers. A total of 49,383 radiomics features per patient image were extracted from the radiotherapy planning computed tomography. We identified 4 features and 13 radiomics features in the whole-lung and multi-region radiomics analysis for classification, respectively. The accuracy and area under the curve (AUC) without the synthetic minority over-sampling technique (SMOTE) were 60.8%, and 0.62 for whole-lung and 80.1%, and 0.84 for multi-region radiomics analysis. These were improved 1.7% for whole-lung and 2.1% for multi-region radiomics analysis with the SMOTE. The developed multi-region radiomics analysis can help predict grade ≥ 2 RP. The radiomics features in the median- and high-dose regions, and the local intensity roughness and variation were important factors in predicting grade ≥ 2 RP.",2021,10.1038/s41598-021-95643-x,retrospective cohort,diagnosis,CT,Lungs
Prediction of radiation pneumonitis with machine learning using 4D-CT based dose-function features,"In this article, we highlight the fundamental importance of the simultaneous use of dose-volume histogram (DVH) and dose-function histogram (DFH) features based on functional images calculated from 4-dimensional computed tomography (4D-CT) and deformable image registration (DIR) in developing a multivariate radiation pneumonitis (RP) prediction model. The patient characteristics, DVH features and DFH features were calculated from functional images by Hounsfield unit (HU) and Jacobian metrics, for an RP grade ≥ 2 multivariate prediction models were computed from 85 non-small cell lung cancer patients. The prediction model is developed using machine learning via a kernel-based support vector machine (SVM) machine. In the patient cohort, 21 of the 85 patients (24.7%) presented with RP grade ≥ 2. The median area under curve (AUC) was 0.58 for the generated 50 prediction models with patient clinical features and DVH features. When HU metric and Jacobian metric DFH features were added, the AUC improved to 0.73 and 0.68, respectively. We conclude that predictive RP models that incorporate DFH features were successfully developed via kernel-based SVM. These results demonstrate that effectiveness of the simultaneous use of DVH features and DFH features calculated from 4D-CT and DIR on functional image-guided radiotherapy.",2022,10.1093/jrr/rrab097,retrospective cohort,diagnosis,CT,Lungs
Prediction of the motion of chest internal points using a recurrent neural network trained with real-time recurrent learning for latency compensation in lung cancer radiotherapy,"During the radiotherapy treatment of patients with lung cancer, the radiation delivered to healthy tissue around the tumor needs to be minimized, which is difficult because of respiratory motion and the latency of linear accelerator (LINAC) systems. In the proposed study, we first use the Lucas-Kanade pyramidal optical flow algorithm to perform deformable image registration (DIR) of chest computed tomography (CT) scan images of four patients with lung cancer. We then track three internal points close to the lung tumor based on the previously computed deformation field and predict their position with a recurrent neural network (RNN) trained using real-time recurrent learning (RTRL) and gradient clipping. The breathing data is quite regular, sampled at approximately 2.5 Hz, and includes artificially added drift in the spine direction. The amplitude of the motion of the tracked points ranged from 12.0 mm to 22.7 mm. Finally, we propose a simple method for recovering and predicting three-dimensional (3D) tumor images from the tracked points and the initial tumor image, based on a linear correspondence model and the Nadaraya-Watson non-linear regression. The root-mean-square (RMS) error, maximum error and jitter corresponding to the RNN prediction on the test set were smaller than the same performance measures obtained with linear prediction and least mean squares (LMS). In particular, the maximum prediction error associated with the RNN, equal to 1.51 mm, is respectively 16.1% and 5.0% lower than the error given by a linear predictor and LMS. The average prediction time per time step with RTRL is equal to 119 ms, which is less than the 400 ms marker position sampling time. The tumor position in the predicted images appears visually correct, which is confirmed by the high mean cross-correlation between the original and predicted images, equal to 0.955. The standard deviation of the Gaussian kernel and the number of layers in the optical flow algorithm were the parameters having the most significant impact on registration performance. Their optimization led respectively to a 31.3% and 36.2% decrease in the registration error. Using only a single layer proved to be detrimental to the registration quality because tissue motion in the lower part of the lung has a high amplitude relative to the resolution of the CT scan images. The random initialization of the hidden units and the number of these hidden units were found to be the most important factors affecting the performance of the RNN. Increasing the number of hidden units from 15 to 250 led to a 56.3% decrease in the prediction error on the cross-validation data. Similarly, optimizing the standard deviation of the initial Gaussian distribution of the synaptic weights σ(init)(RNN) led to a 28.4% decrease in the prediction error on the cross-validation data, with the error minimized for σ(init)(RNN)=0.02 with the four patients.",2021,10.1016/j.compmedimag.2021.101941,cross-sectional,treatment,CT,Lungs
Prediction of visceral pleural invasion in lung cancer on CT: deep learning model achieves a radiologist-level performance with adaptive sensitivity and specificity to clinical needs,"OBJECTIVES: To develop and validate a preoperative CT-based deep learning model for the prediction of visceral pleural invasion (VPI) in early-stage lung cancer. METHODS: In this retrospective study, dataset 1 (for training, tuning, and internal validation) included 676 patients with clinical stage IA lung adenocarcinomas resected between 2009 and 2015. Dataset 2 (for temporal validation) included 141 patients with clinical stage I adenocarcinomas resected between 2017 and 2018. A CT-based deep learning model was developed for the prediction of VPI and validated in terms of discrimination and calibration. An observer performance study and a multivariable regression analysis were performed. RESULTS: The area under the receiver operating characteristic curve (AUC) of the model was 0.75 (95% CI, 0.67-0.84), which was comparable to those of board-certified radiologists (AUC, 0.73-0.79; all p > 0.05). The model had a higher standardized partial AUC for a specificity range of 90 to 100% than the radiologists (all p < 0.05). The high sensitivity cutoff (0.245) yielded a sensitivity of 93.8% and a specificity of 31.2%, and the high specificity cutoff (0.448) resulted in a sensitivity of 47.9% and a specificity of 86.0%. Two of the three radiologists provided highly sensitive (93.8% and 97.9%) but not specific (48.4% and 40.9%) diagnoses. The model showed good calibration (p > 0.05), and its output was an independent predictor for VPI (adjusted odds ratio, 1.07; 95% CI, 1.03-1.11; p < 0.001). CONCLUSIONS: The deep learning model demonstrated a radiologist-level performance. The model could achieve either highly sensitive or highly specific diagnoses depending on clinical needs. KEY POINTS: • The preoperative CT-based deep learning model demonstrated an expert-level diagnostic performance for the presence of visceral pleural invasion in early-stage lung cancer. • Radiologists had a tendency toward highly sensitive, but not specific diagnoses for the visceral pleural invasion.",2021,10.1007/s00330-020-07431-2,cross-sectional,diagnosis,CT,Lungs
Predictive Approaches for Acute Dialysis Requirement and Death in COVID-19,"BACKGROUND AND OBJECTIVES: AKI treated with dialysis initiation is a common complication of coronavirus disease 2019 (COVID-19) among hospitalized patients. However, dialysis supplies and personnel are often limited. DESIGN, SETTING, PARTICIPANTS, & MEASUREMENTS: Using data from adult patients hospitalized with COVID-19 from five hospitals from the Mount Sinai Health System who were admitted between March 10 and December 26, 2020, we developed and validated several models (logistic regression, Least Absolute Shrinkage and Selection Operator (LASSO), random forest, and eXtreme GradientBoosting [XGBoost; with and without imputation]) for predicting treatment with dialysis or death at various time horizons (1, 3, 5, and 7 days) after hospital admission. Patients admitted to the Mount Sinai Hospital were used for internal validation, whereas the other hospitals formed part of the external validation cohort. Features included demographics, comorbidities, and laboratory and vital signs within 12 hours of hospital admission. RESULTS: A total of 6093 patients (2442 in training and 3651 in external validation) were included in the final cohort. Of the different modeling approaches used, XGBoost without imputation had the highest area under the receiver operating characteristic (AUROC) curve on internal validation (range of 0.93-0.98) and area under the precision-recall curve (AUPRC; range of 0.78-0.82) for all time points. XGBoost without imputation also had the highest test parameters on external validation (AUROC range of 0.85-0.87, and AUPRC range of 0.27-0.54) across all time windows. XGBoost without imputation outperformed all models with higher precision and recall (mean difference in AUROC of 0.04; mean difference in AUPRC of 0.15). Features of creatinine, BUN, and red cell distribution width were major drivers of the model's prediction. CONCLUSIONS: An XGBoost model without imputation for prediction of a composite outcome of either death or dialysis in patients positive for COVID-19 had the best performance, as compared with standard and other machine learning models. PODCAST: This article contains a podcast at https://www.asn-online.org/media/podcast/CJASN/2021_07_09_CJN17311120.mp3.",2021,10.2215/cjn.17311120,,,,
Predictive diagnosis of chronic obstructive pulmonary disease using serum metabolic biomarkers and least-squares support vector machine,"OBJECTIVE: Development of biofluid-based biomarkers is attractive for the diagnosis of chronic obstructive pulmonary disease (COPD) but still lacking. Thus, here we aimed to identify serum metabolic biomarkers for the diagnosis of COPD. METHODS: In this study, we investigated serum metabolic features between COPD patients (n = 54) and normal individuals (n = 74) using a (1) H NMR-based metabolomics approach and developed an integrated method of least-squares support vector machine (LS-SVM) and serum metabolic biomarkers to assist COPD diagnosis. RESULTS: We observed a hypometabolic state in serum of COPD patients, as indicated by decreases in N-acetyl-glycoprotein (NAG), lipoprotein (LOP, mainly LDL/VLDL), polyunsaturated fatty acid (pUFA), glucose, alanine, leucine, histidine, valine, and lactate. Using an integrated method of multivariable and univariate analyses, NAG and LOP were identified as two important metabolites for distinguishing between COPD patients and controls. Subsequently, we developed a LS-SVM classifier using these two markers and found that LS-SVM classifiers with linear and polynomial kernels performed better than the classifier with RBF kernel. Linear and polynomial LS-SVM classifiers can achieve the total accuracy rates of 80.77% and 84.62% and the AUC values of 0.87 and 0.90 for COPD diagnosis, respectively. CONCLUSIONS: This study suggests that artificial intelligence integrated with serum metabolic biomarkers has a great potential for auxiliary diagnosis of COPD.",2021,10.1002/jcla.23641,,,,
Predictive models for patients with lung carcinomas to identify EGFR mutation status via an artificial neural network based on multiple clinical information,"PURPOSE: Epidermal growth factor receptor (EGFR) mutation testing has several limitations. Therefore, we built predictive models to determine the EGFR mutation status of patients and guide therapeutic decision-making. METHODS: We collected data from 320 patients with lung carcinoma, including sex, age, smoking history, serum tumour marker levels, maximum standardized uptake value, pathological results, computed tomography images, and EGFR mutation status. Artificial neural network (ANN) models based on multiple clinical characteristics were proposed to predict EGFR mutation status. RESULTS: A training set (n = 200) was used to develop predictive models of the EGFR mutation status (Model 1: area under the receiver operating characteristic curve [AUROC] = 0.910, 95% CI 0.861-0.945; Model 2: AUROC = 0.859, 95% CI 0.803-0.904; Model 3: AUROC = 0.711, 95% CI 0.643-0.773). A testing set (n = 50) and temporal validation data set (n = 70) were used to evaluate the generalisation performance of the established models (testing set: Model 1, AUROC = 0.845, 95% CI 0.715-0.932; Model 2, AUROC = 0.882, 95% CI 0.759-0.956; Model 3, AUROC = 0.817, 95% CI 0.682-0.912; temporal validation dataset: Model 1, AUROC = 0.909, 95% CI 0.816-0.964; Model 2, AUROC = 0.855, 95% CI 0.751-0.928; Model 3, AUROC = 0.831, 95% CI 0.723-0.910). The predictive abilities of the three ANN models were superior to that of a previous logistic regression model (P < 0.001, 0.027, and 0.050, respectively). CONCLUSIONS: ANN models provide a non-invasive and readily available method for EGFR mutation status prediction.",2020,10.1007/s00432-019-03103-x,cross-sectional,diagnosisy,CT,Lungs
"Predictive usefulness of RT-PCR testing in different patterns of Covid-19 symptomatology: analysis of a French cohort of 12,810 outpatients","Reverse transcriptase polymerase chain reaction (RT-PCR) is a key tool to diagnose Covid-19. Yet it may not be the most efficient test in all patients. In this paper, we develop a clinical strategy for prescribing RT-PCR to patients based on data from COVIDOM, a French cohort of 54,000 patients with clinically suspected Covid-19, including 12,810 patients tested by RT-PCR. We use a machine-learning algorithm (decision tree) in order to predict RT-PCR results based on the clinical presentation. We show that symptoms alone are sufficient to predict RT-PCR outcome with a mean average precision of 86%. We identify combinations of symptoms that are predictive of RT-PCR positivity (90% for anosmia/ageusia) or negativity (only 30% of RT-PCR+ for a subgroup with cardiopulmonary symptoms): in both cases, RT-PCR provides little added diagnostic value. We propose a prescribing strategy based on clinical presentation that can improve the global efficiency of RT-PCR testing.",2021,10.1038/s41598-021-99991-6,cross-sectional,diagnosis,CT,Lungs
Predictive value of a novel Asian lung cancer screening nomogram based on artificial intelligence and epidemiological characteristics,"BACKGROUND: To develop and validate a risk prediction nomogram based on a deep learning convolutional neural networks (CNN) model and epidemiological characteristics for lung cancer screening in patients with small pulmonary nodules (SPN). METHODS: This study included three data sets. First, a CNN model was developed and tested on data set 1. Then, a hybrid prediction model was developed on data set 2 by multivariable binary logistic regression analysis. We combined the CNN model score and the selected epidemiological risk factors, and a risk prediction nomogram was presented. An independent multicenter cohort was used for model external validation. The performance of the nomogram was assessed with respect to its calibration and discrimination. RESULTS: The final hybrid model included the CNN model score and the screened risk factors included age, gender, smoking status and family history of cancer. The nomogram showed good discrimination and calibration with an area under the curve (AUC) of 91.6% (95% CI: 89.4%-93.5%), compare with the CNN model, the improvement was significance. The performance of the nomogram still showed good discrimination and good calibration in the multicenter validation cohort, with an AUC of 88.3% (95% CI: 83.1%-92.3%). CONCLUSIONS: Our study showed that epidemiological characteristics should be considered in lung cancer screening, which can significantly improve the efficiency of the artificial intelligence (AI) model alone. We combined the CNN model score with Asian lung cancer epidemiological characteristics to develop a new nomogram to facilitate and accurately perform individualized lung cancer screening, especially for Asians.",2021,10.1111/1759-7714.14140,cross-sectional,diagnosis,CT,Lungs
Predictors at Admission of Mechanical Ventilation and Death in an Observational Cohort of Adults Hospitalized With Coronavirus Disease 2019,"BACKGROUND: Coronavirus disease (COVID-19) can cause severe illness and death. Predictors of poor outcome collected on hospital admission may inform clinical and public health decisions. METHODS: We conducted a retrospective observational cohort investigation of 297 adults admitted to 8 academic and community hospitals in Georgia, United States, during March 2020. Using standardized medical record abstraction, we collected data on predictors including admission demographics, underlying medical conditions, outpatient antihypertensive medications, recorded symptoms, vital signs, radiographic findings, and laboratory values. We used random forest models to calculate adjusted odds ratios (aORs) and 95% confidence intervals (CIs) for predictors of invasive mechanical ventilation (IMV) and death. RESULTS: Compared with age <45 years, ages 65-74 years and ≥75 years were predictors of IMV (aORs, 3.12 [95% CI, 1.47-6.60] and 2.79 [95% CI, 1.23-6.33], respectively) and the strongest predictors for death (aORs, 12.92 [95% CI, 3.26-51.25] and 18.06 [95% CI, 4.43-73.63], respectively). Comorbidities associated with death (aORs, 2.4-3.8; P < .05) included end-stage renal disease, coronary artery disease, and neurologic disorders, but not pulmonary disease, immunocompromise, or hypertension. Prehospital use vs nonuse of angiotensin receptor blockers (aOR, 2.02 [95% CI, 1.03-3.96]) and dihydropyridine calcium channel blockers (aOR, 1.91 [95% CI, 1.03-3.55]) were associated with death. CONCLUSIONS: After adjustment for patient and clinical characteristics, older age was the strongest predictor of death, exceeding comorbidities, abnormal vital signs, and laboratory test abnormalities. That coronary artery disease, but not chronic lung disease, was associated with death among hospitalized patients warrants further investigation, as do associations between certain antihypertensive medications and death.",2021,10.1093/cid/ciaa1459,retrospective cohort,prognosis,CXR,Lungs
Predictors of acute deep venous thrombosis in patients hospitalized for COVID-19,"Deep venous thrombosis (DVT) is associated with high mortality in coronavirus disease 2019 (COVID-19) but there remains uncertainty about the benefit of anti-coagulation prophylaxis and how to decide when ultrasound screening is indicated. We aimed to determine parameters predicting which COVID-19 patients are at risk of DVT and to assess the benefit of prophylactic anti-coagulation.Adult hospitalized patients with positive severe acute respiratory syndrome coronavirus-2 reverse transcription-polymerase chain reaction (RT-PCR) undergoing venous duplex ultrasound for DVT assessment (n = 451) were retrospectively reviewed. Clinical and laboratory data within 72 hours of ultrasound were collected. Using split sampling and a 10-fold cross-validation, a random forest model was developed to find the most important variables for predicting DVT. Different d-dimer cutoffs were examined for classification of DVT. We also compared the rate of DVT between the patients going and not going under thromboprophylaxis.DVT was found in 65 (14%) of 451 reverse transcription-polymerase chain reaction positive patients. The random forest model, trained and cross-validated on 2/3 of the original sample (n = 301), had area under the receiver operating characteristic curve = 0.91 (95% confidence interval [CI]: 0.85-0.97) for prediction of DVT in the test set (n = 150), with sensitivity = 93% (95%CI: 68%-99%) and specificity = 82% (95%CI: 75%-88%). The following variables had the highest importance: d-dimer, thromboprophylaxis, systolic blood pressure, admission to ultrasound interval, and platelets. Thromboprophylaxis reduced DVT risk 4-fold from 26% to 6% (P < .001), while anti-coagulation therapy led to hemorrhagic complications in 14 (22%) of 65 patients with DVT including 2 fatal intra-cranial hemorrhages. D-dimer was the most important predictor with area under curve = 0.79 (95%CI: 0.73-0.86) by itself, and a 5000 ng/mL threshold at 7 days postCOVID-19 symptom onset had 75% (95%CI: 53%-90%) sensitivity and 81% (95%CI: 72%-88%) specificity. In comparison with d-dimer alone, the random forest model showed 68% versus 32% specificity at 95% sensitivity, and 44% versus 23% sensitivity at 95% specificity.D-dimer >5000 ng/mL predicts DVT with high accuracy suggesting regular monitoring with d-dimer in the early stages of COVID-19 may be useful. A random forest model improved the prediction of DVT. Thromboprophylaxis reduced DVT in COVID-19 patients and should be considered in all patients. Full anti-coagulation therapy has a risk of life-threatening hemorrhage.",2021,10.1097/md.0000000000027216,,,,
Preliminary study of generalized semiautomatic segmentation for 3D voxel labeling of lesions based on deep learning,"PURPOSE: The three-dimensional (3D) voxel labeling of lesions requires significant radiologists' effort in the development of computer-aided detection software. To reduce the time required for the 3D voxel labeling, we aimed to develop a generalized semiautomatic segmentation method based on deep learning via a data augmentation-based domain generalization framework. In this study, we investigated whether a generalized semiautomatic segmentation model trained using two types of lesion can segment previously unseen types of lesion. METHODS: We targeted lung nodules in chest CT images, liver lesions in hepatobiliary-phase images of Gd-EOB-DTPA-enhanced MR imaging, and brain metastases in contrast-enhanced MR images. For each lesion, the 32 × 32 × 32 isotropic volume of interest (VOI) around the center of gravity of the lesion was extracted. The VOI was input into a 3D U-Net model to define the label of the lesion. For each type of target lesion, we compared five types of data augmentation and two types of input data. RESULTS: For all considered target lesions, the highest dice coefficients among the training patterns were obtained when using a combination of the existing data augmentation-based domain generalization framework and random monochrome inversion and when using the resized VOI as the input image. The dice coefficients were 0.639 ± 0.124 for the lung nodules, 0.660 ± 0.137 for the liver lesions, and 0.727 ± 0.115 for the brain metastases. CONCLUSIONS: Our generalized semiautomatic segmentation model could label unseen three types of lesion with different contrasts from the surroundings. In addition, the resized VOI as the input image enables the adaptation to the various sizes of lesions even when the size distribution differed between the training set and the test set.",2021,10.1007/s11548-021-02504-z,cross-sectional,informatics,CT,Lungs
Preoperative CT-based Deep Learning Model for Predicting Disease-Free Survival in Patients with Lung Adenocarcinomas,"Background Deep learning models have the potential for lung cancer prognostication, but model output as an independent prognostic factor must be validated with clinical risk factors. Purpose To develop and validate a preoperative CT-based deep learning model for predicting disease-free survival in patients with lung adenocarcinoma. Materials and Methods In this retrospective study, a deep learning model was trained to extract prognostic information from preoperative CT examinations. Data set 1 for training, tuning, and internal validation consisted of patients with T1-4N0M0 adenocarcinoma resected between 2009 and 2015. Data set 2 for external validation included patients with clinical T1-2aN0M0 (stage I) adenocarcinomas resected in 2014. Discrimination was assessed by using Harrell C index and benchmarked against the clinical T category. The Greenwood-Nam-D'Agostino test was used for model calibration. The multivariable-adjusted hazard ratios (HRs) were analyzed with clinical prognostic factors by using the Cox regression. Results Evaluated were 800 patients (median age, 64 years; interquartile range, 56-70 years; 450 women) in data set 1 and 108 patients (median age, 63 years; interquartile range, 57-71 years; 60 women) in data set 2. The C indexes were 0.74-0.80 in the internal validation and 0.71-0.78 in the external validation, both comparable with the clinical T category (0.78 in the internal validation and 0.74 in the external validation; all P > .05). The model exhibited good calibration in all data sets (P > .05). Multivariable Cox regression revealed that model outputs were independent prognostic factors (hazard ratio [HR] of the categorical output, 2.5 [95% confidence interval {CI}: 1.03, 5.9; P = .04] in the internal validation and 3.6 [95% CI: 1.6, 8.5; P = .003] in the external validation). Other than the deep learning model, only smoking status (HR, 3.4; 95% CI: 1.4, 8.5; P = .007) contributed further to prediction of disease-free survival for patients after resection of clinical stage I adenocarcinomas. Conclusion A deep learning model for chest CT predicted disease-free survival for patients undergoing an operation for clinical stage I lung adenocarcinoma. © RSNA, 2020 Online supplemental material is available for this article. See also the editorial by Shaffer in this issue.",2020,10.1148/radiol.2020192764,retrospective cohort,prognosis,CT,Lungs
Preoperative CT-based radiomics combined with intraoperative frozen section is predictive of invasive adenocarcinoma in pulmonary nodules: a multicenter study,"OBJECTIVES: Develop a CT-based radiomics model and combine it with frozen section (FS) and clinical data to distinguish invasive adenocarcinomas (IA) from preinvasive lesions/minimally invasive adenocarcinomas (PM). METHODS: This multicenter study cohort of 623 lung adenocarcinomas was split into training (n = 331), testing (n = 143), and external validation dataset (n = 149). Random forest models were built using selected radiomics features, results from FS, lesion volume, clinical and semantic features, and combinations thereof. The area under the receiver operator characteristic curves (AUC) was used to evaluate model performances. The diagnosis accuracy, calibration, and decision curves of models were tested. RESULTS: The radiomics-based model shows good predictive performance and diagnostic accuracy for distinguishing IA from PM, with AUCs of 0.89, 0.89, and 0.88, in the training, testing, and validation datasets, respectively, and with corresponding accuracies of 0.82, 0.79, and 0.85. Adding lesion volume and FS significantly increases the performance of the model with AUCs of 0.96, 0.97, and 0.96, and with accuracies of 0.91, 0.94, and 0.93 in the three datasets. There is no significant difference in AUC between the FS model enriched with radiomics and volume against an FS model enriched with volume alone, while the former has higher accuracy. The model combining all available information shows minor non-significant improvements in AUC and accuracy compared with an FS model enriched with radiomics and volume. CONCLUSIONS: Radiomics signatures are potential biomarkers for the risk of IA, especially in combination with FS, and could help guide surgical strategy for pulmonary nodules patients. KEY POINTS: • A CT-based radiomics model may be a valuable tool for preoperative prediction of invasive adenocarcinoma for patients with pulmonary nodules. • Radiomics combined with frozen sections could help in guiding surgery strategy for patients with pulmonary nodules.",2020,10.1007/s00330-019-06597-8,cross-sectional,diagnosis,CT,Lungs
Preoperative diagnosis of malignant pulmonary nodules in lung cancer screening with a radiomics nomogram,"BACKGROUND: Lung cancer is the most commonly diagnosed cancer worldwide. Its survival rate can be significantly improved by early screening. Biomarkers based on radiomics features have been found to provide important physiological information on tumors and considered as having the potential to be used in the early screening of lung cancer. In this study, we aim to establish a radiomics model and develop a tool to improve the discrimination between benign and malignant pulmonary nodules. METHODS: A retrospective study was conducted on 875 patients with benign or malignant pulmonary nodules who underwent computed tomography (CT) examinations between June 2013 and June 2018. We assigned 612 patients to a training cohort and 263 patients to a validation cohort. Radiomics features were extracted from the CT images of each patient. Least absolute shrinkage and selection operator (LASSO) was used for radiomics feature selection and radiomics score calculation. Multivariate logistic regression analysis was used to develop a classification model and radiomics nomogram. Radiomics score and clinical variables were used to distinguish benign and malignant pulmonary nodules in logistic model. The performance of the radiomics nomogram was evaluated by the area under the curve (AUC), calibration curve and Hosmer-Lemeshow test in both the training and validation cohorts. RESULTS: A radiomics score was built and consisted of 20 features selected by LASSO from 1288 radiomics features in the training cohort. The multivariate logistic model and radiomics nomogram were constructed using the radiomics score and patients' age. Good discrimination of benign and malignant pulmonary nodules was obtained from the training cohort (AUC, 0.836; 95% confidence interval [CI]: 0.793-0.879) and validation cohort (AUC, 0.809; 95% CI: 0.745-0.872). The Hosmer-Lemeshow test also showed good performance for the logistic regression model in the training cohort (P = 0.765) and validation cohort (P = 0.064). Good alignment with the calibration curve indicated the good performance of the nomogram. CONCLUSIONS: The established radiomics nomogram is a noninvasive preoperative prediction tool for malignant pulmonary nodule diagnosis. Validation revealed that this nomogram exhibited excellent discrimination and calibration capacities, suggesting its clinical utility in the early screening of lung cancer.",2020,10.1002/cac2.12002,cross-sectional,diagnosis,CT,Lungs
Presence of mEGFR ctDNA predicts a poor clinical outcome in lung adenocarcinoma,"BACKGROUND: Circulating tumor DNA (ctDNA) is a biomarker for the selection of target agents in various malignancies. In this study, we examined the effect of ctDNA presence on the response to EGFR-tyrosine kinase inhibitor (TKI) and on the prognosis in lung adenocarcinoma. METHODS: ctDNA of EGFR-TKI sensitizing mutations (mEGFR), L858R substitution and Exon 19 deletion (E19d) mutation, was evaluated using droplet digital PCR (ddPCR) in 81 patients with lung adenocarcinoma which harbored mEGFR in the corresponding tumor tissues. RESULTS: The study recruited lung cancer patients at various stages, and the sensitivity, specificity, and area under the curve (AUC) of mEGFR ctDNA detection by ddPCR were 40.0%, 88.5%, and 0.68, respectively. It showed higher sensitivity (75.0% vs. 10.0%) and AUC (0.83 vs. 0.49) in the advanced stages of lung adenocarcinoma compared with the early stages and the number of metastases and the fractional abundance of mEGFR ctDNA showed a strong correlation (σ = 0.516; P < 0.001, Spearman correlation test). There was a significantly shorter progression-free survival and duration of disease control by EGFR-TKIs in the ctDNA-positive group than the negative group (14.0 vs. 41.0 months, P = 0.02 and 12.0 vs. 23.0 months, P = 0.02, log-rank test, respectively). There was a trend for overall survival time to be shorter in patients with mEGFR ctDNA than for patients without mEGFR ctDNA (35.6 vs. 67.1 months, P = 0.06, log-rank test). CONCLUSIONS: These data showed that mEGFR ctDNA detection using ddPCR is useful in the advanced stages and its presence predicted distant metastasis and poor clinical outcome in lung adenocarcinoma.",2019,10.1111/1759-7714.13219,,,,
Prior-Attention Residual Learning for More Discriminative COVID-19 Screening in CT Images,"We propose a conceptually simple framework for fast COVID-19 screening in 3D chest CT images. The framework can efficiently predict whether or not a CT scan contains pneumonia while simultaneously identifying pneumonia types between COVID-19 and Interstitial Lung Disease (ILD) caused by other viruses. In the proposed method, two 3D-ResNets are coupled together into a single model for the two above-mentioned tasks via a novel prior-attention strategy. We extend residual learning with the proposed prior-attention mechanism and design a new so-called prior-attention residual learning (PARL) block. The model can be easily built by stacking the PARL blocks and trained end-to-end using multi-task losses. More specifically, one 3D-ResNet branch is trained as a binary classifier using lung images with and without pneumonia so that it can highlight the lesion areas within the lungs. Simultaneously, inside the PARL blocks, prior-attention maps are generated from this branch and used to guide another branch to learn more discriminative representations for the pneumonia-type classification. Experimental results demonstrate that the proposed framework can significantly improve the performance of COVID-19 screening. Compared to other methods, it achieves a state-of-the-art result. Moreover, the proposed method can be easily extended to other similar clinical applications such as computer-aided detection and diagnosis of pulmonary nodules in CT images, glaucoma lesions in Retina fundus images, etc.",2020,10.1109/tmi.2020.2994908,cross-sectional,diagnosis,CT,Lungs
"Profiling metabolites and lipoproteins in COMETA, an Italian cohort of COVID-19 patients","Metabolomics and lipidomics have been used in several studies to define the biochemical alterations induced by COVID-19 in comparison with healthy controls. Those studies highlighted the presence of a strong signature, attributable to both metabolites and lipoproteins/lipids. Here, 1H NMR spectra were acquired on EDTA-plasma from three groups of subjects: i) hospitalized COVID-19 positive patients (≤21 days from the first positive nasopharyngeal swab); ii) hospitalized COVID-19 positive patients (>21 days from the first positive nasopharyngeal swab); iii) subjects after 2-6 months from SARS-CoV-2 eradication. A Random Forest model built using the EDTA-plasma spectra of COVID-19 patients ≤21 days and Post COVID-19 subjects, provided a high discrimination accuracy (93.6%), indicating both the presence of a strong fingerprint of the acute infection and the substantial metabolic healing of Post COVID-19 subjects. The differences originate from significant alterations in the concentrations of 16 metabolites and 74 lipoprotein components. The model was then used to predict the spectra of COVID-19>21 days subjects. In this group, the metabolite levels are closer to those of the Post COVID-19 subjects than to those of the COVID-19≤21 days; the opposite occurs for the lipoproteins. Within the acute phase patients, characteristic trends in metabolite levels are observed as a function of the disease severity. The metabolites found altered in COVID-19≤21 days patients with respect to Post COVID-19 individuals overlap with acute infection biomarkers identified previously in comparison with healthy subjects. Along the trajectory towards healing, the metabolome reverts back to the ""healthy"" state faster than the lipoproteome.",2022,10.1371/journal.ppat.1010443,,,,
Prognostic and Predictive Values of Metabolic Parameters of (18)F-FDG PET/CT in Patients With Non-Small Cell Lung Cancer Treated With Chemotherapy,"OBJECTIVES: Increasing interests have been focused on using artificial intelligence (AI) to extend prognostic value of medical imaging. Feature extraction is a critical step for successful application of AI. The aim of this study was to explore several metabolic parameters measured by (18)F-fluorodeoxyglucose positron emission tomography/computed tomography (PET/CT) as potential AI features in predicting the effectiveness of chemotherapy in patients with non-small cell lung cancer (NSCLC). METHODS: A set of metabolic parameters of PET/CT and clinical characteristics were detected from 137 patients with NSCLC treated with at least 1 cycle of chemotherapy. Survival receiver-operating characteristic (ROC) analysis was used to define the more significant parameters chosen for the following survival analysis. Patient survival was analyzed by Kaplan-Meier method, log-rank test, and Cox regression. RESULTS: Survival ROC showed that maximum standardized uptake value (SUVmax), metabolic tumor volume 50% (MTV50), and total lesion glycolysis 50% (TLG50) had larger area under the curve, and the optimal cutoff values were 11.72, 4.04, and 34.55, respectively. Univariate and multivariate analyses synergistically showed that late PET/CT stage and MTV50 >4.04 were independent factors of poor survival in patients with NSCLC who received chemotherapy. CONCLUSIONS: Several potential prognostic biomarkers of PET/CT imaging have been extracted for predicting survival and selecting patients with NSCLC who are more likely to benefit from chemotherapy. The identification may accelerate the development of AI methods to improve treatment outcome for NSCLC.",2019,10.1177/1536012119846025,retrospective cohort,prognosis,PET-CT,Lungs
Prognostic Implications of CT Feature Analysis in Patients with COVID-19: a Nationwide Cohort Study,"BACKGROUND: Few studies have classified chest computed tomography (CT) findings of coronavirus disease 2019 (COVID-19) and analyzed their correlations with prognosis. The present study aimed to evaluate retrospectively the clinical and chest CT findings of COVID-19 and to analyze CT findings and determine their relationships with clinical severity. METHODS: Chest CT and clinical features of 271 COVID-19 patients were assessed. The presence of CT findings and distribution of parenchymal abnormalities were evaluated, and CT patterns were classified as bronchopneumonia, organizing pneumonia (OP), or diffuse alveolar damage (DAD). Total extents were assessed using a visual scoring system and artificial intelligence software. Patients were allocated to two groups based on clinical outcomes, that is, to a severe group (requiring O₂ therapy or mechanical ventilation, n = 55) or a mild group (not requiring O₂ therapy or mechanical ventilation, n = 216). Clinical and CT features of these two groups were compared and univariate and multivariate logistic regression analyses were performed to identify independent prognostic factors. RESULTS: Age, lymphocyte count, levels of C-reactive protein, and procalcitonin were significantly different in the two groups. Forty-five of the 271 patients had normal chest CT findings. The most common CT findings among the remaining 226 patients were ground-glass opacity (98%), followed by consolidation (53%). CT findings were classified as OP (93%), DAD (4%), or bronchopneumonia (3%) and all nine patients with DAD pattern were included in the severe group. Uivariate and multivariate analyses showed an elevated procalcitonin (odds ratio [OR], 2.521; 95% confidence interval [CI], 1.001-6.303, P = 0.048), and higher visual CT scores (OR, 1.137; 95% CI, 1.042-1.236; P = 0.003) or higher total extent by AI measurement (OR, 1.048; 95% CI, 1.020-1.076; P < 0.001) were significantly associated with a severe clinical course. CONCLUSION: CT findings of COVID-19 pneumonia can be classified into OP, DAD, or bronchopneumonia patterns and all patients with DAD pattern were included in severe group. Elevated inflammatory markers and higher CT scores were found to be significant predictors of poor prognosis in patients with COVID-19 pneumonia.",2021,10.3346/jkms.2021.36.e51,retrospective cohort,prognosis,CT,Lungs
Prognostic value of anthropometric measures extracted from whole-body CT using deep learning in patients with non-small-cell lung cancer,"INTRODUCTION: The aim of the study was to extract anthropometric measures from CT by deep learning and to evaluate their prognostic value in patients with non-small-cell lung cancer (NSCLC). METHODS: A convolutional neural network was trained to perform automatic segmentation of subcutaneous adipose tissue (SAT), visceral adipose tissue (VAT), and muscular body mass (MBM) from low-dose CT images in 189 patients with NSCLC who underwent pretherapy PET/CT. After a fivefold cross-validation in a subset of 35 patients, anthropometric measures extracted by deep learning were normalized to the body surface area (BSA) to control the various patient morphologies. VAT/SAT ratio and clinical parameters were included in a Cox proportional-hazards model for progression-free survival (PFS) and overall survival (OS). RESULTS: Inference time for a whole volume was about 3 s. Mean Dice similarity coefficients in the validation set were 0.95, 0.93, and 0.91 for SAT, VAT, and MBM, respectively. For PFS prediction, T-stage, N-stage, chemotherapy, radiation therapy, and VAT/SAT ratio were associated with disease progression on univariate analysis. On multivariate analysis, only N-stage (HR = 1.7 [1.2-2.4]; p = 0.006), radiation therapy (HR = 2.4 [1.0-5.4]; p = 0.04), and VAT/SAT ratio (HR = 10.0 [2.7-37.9]; p < 0.001) remained significant prognosticators. For OS, male gender, smoking status, N-stage, a lower SAT/BSA ratio, and a higher VAT/SAT ratio were associated with mortality on univariate analysis. On multivariate analysis, male gender (HR = 2.8 [1.2-6.7]; p = 0.02), N-stage (HR = 2.1 [1.5-2.9]; p < 0.001), and the VAT/SAT ratio (HR = 7.9 [1.7-37.1]; p < 0.001) remained significant prognosticators. CONCLUSION: The BSA-normalized VAT/SAT ratio is an independent predictor of both PFS and OS in NSCLC patients. KEY POINTS: • Deep learning will make CT-derived anthropometric measures clinically usable as they are currently too time-consuming to calculate in routine practice. • Whole-body CT-derived anthropometrics in non-small-cell lung cancer are associated with progression-free survival and overall survival. • A priori medical knowledge can be implemented in the neural network loss function calculation.",2020,10.1007/s00330-019-06630-w,retrospective cohort,prognosis,CT,Lungs
Prognostication of patients with COVID-19 using artificial intelligence based on chest x-rays and clinical data: a retrospective study,"BACKGROUND: Chest x-ray is a relatively accessible, inexpensive, fast imaging modality that might be valuable in the prognostication of patients with COVID-19. We aimed to develop and evaluate an artificial intelligence system using chest x-rays and clinical data to predict disease severity and progression in patients with COVID-19. METHODS: We did a retrospective study in multiple hospitals in the University of Pennsylvania Health System in Philadelphia, PA, USA, and Brown University affiliated hospitals in Providence, RI, USA. Patients who presented to a hospital in the University of Pennsylvania Health System via the emergency department, with a diagnosis of COVID-19 confirmed by RT-PCR and with an available chest x-ray from their initial presentation or admission, were retrospectively identified and randomly divided into training, validation, and test sets (7:1:2). Using the chest x-rays as input to an EfficientNet deep neural network and clinical data, models were trained to predict the binary outcome of disease severity (ie, critical or non-critical). The deep-learning features extracted from the model and clinical data were used to build time-to-event models to predict the risk of disease progression. The models were externally tested on patients who presented to an independent multicentre institution, Brown University affiliated hospitals, and compared with severity scores provided by radiologists. FINDINGS: 1834 patients who presented via the University of Pennsylvania Health System between March 9 and July 20, 2020, were identified and assigned to the model training (n=1285), validation (n=183), or testing (n=366) sets. 475 patients who presented via the Brown University affiliated hospitals between March 1 and July 18, 2020, were identified for external testing of the models. When chest x-rays were added to clinical data for severity prediction, area under the receiver operating characteristic curve (ROC-AUC) increased from 0·821 (95% CI 0·796-0·828) to 0·846 (0·815-0·852; p<0·0001) on internal testing and 0·731 (0·712-0·738) to 0·792 (0·780-0 ·803; p<0·0001) on external testing. When deep-learning features were added to clinical data for progression prediction, the concordance index (C-index) increased from 0·769 (0·755-0·786) to 0·805 (0·800-0·820; p<0·0001) on internal testing and 0·707 (0·695-0·729) to 0·752 (0·739-0·764; p<0·0001) on external testing. The image and clinical data combined model had significantly better prognostic performance than combined severity scores and clinical data on internal testing (C-index 0·805 vs 0·781; p=0·0002) and external testing (C-index 0·752 vs 0·715; p<0·0001). INTERPRETATION: In patients with COVID-19, artificial intelligence based on chest x-rays had better prognostic performance than clinical data or radiologist-derived severity scores. Using artificial intelligence, chest x-rays can augment clinical data in predicting the risk of progression to critical illness in patients with COVID-19. FUNDING: Brown University, Amazon Web Services Diagnostic Development Initiative, Radiological Society of North America, National Cancer Institute and National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health.",2021,10.1016/s2589-7500(21)00039-x,prospective cohort,prognosis,CXR,Lungs
Progressive back-projection network for COVID-CT super-resolution,"BACKGROUND AND OBJECTIVE: Recently, the COVID-19 epidemic has become more and more serious around the world, how to improve the image resolution of COVID-CT is a very important task. The network based on progressive upsampling for COVID-CT super-resolution increases the reconstruction error. This paper proposes a progressive back-projection network (PBPN) for COVID-CT super-resolution to solve this problem. METHODS: In this paper, we propose a progressive back-projection network (PBPN) for COVID-CT super-resolution. PBPN is divided into two stages, and each stage consists of back-projection, deep feature extraction and upscaling. We design an up-projection and down-projection residual module to minimize the reconstruction error and construct a residual attention module to extract deep features. In each stage, firstly, PBPN performs back-projection to extract shallow features by two up-projection and down-projection residual modules; then, PBPN extracts deep features from the shallow features by two residual attention modules; finally, PBPN upsamples the deep features through sub-pixel convolution. RESULTS: The proposed method achieves the improvements of about 0.14~0.47 dB/0.0012~0.0060 for × 2 scale factor, 0.02~0.08 dB/0.0024~0.0059 for × 3 scale factor, and 0.08~0.41 dB/ 0.0040~0.0147 for × 4 scale factor than state-of-the-art methods (Bicubic, SRCNN, FSRCNN, VDSR, LapSRN, DRCN and DSRN) in terms of PSNR/SSIM on benchmark datasets. CONCLUSIONS: The proposed mehtod obtains better performance for COVID-CT super-resolution and reconstructs high-quality high-resolution COVID-CT images that contain more details and edges.",2021,10.1016/j.cmpb.2021.106193,cross-sectional,informatics,CT,Lungs
Proof of concept for real-time detection of SARS CoV-2 infection with an electronic nose,"Rapid diagnosis is key to curtailing the Covid-19 pandemic. One path to such rapid diagnosis may rely on identifying volatile organic compounds (VOCs) emitted by the infected body, or in other words, identifying the smell of the infection. Consistent with this rationale, dogs can use their nose to identify Covid-19 patients. Given the scale of the pandemic, however, animal deployment is a challenging solution. In contrast, electronic noses (eNoses) are machines aimed at mimicking animal olfaction, and these can be deployed at scale. To test the hypothesis that SARS CoV-2 infection is associated with a body-odor detectable by an eNose, we placed a generic eNose in-line at a drive-through testing station. We applied a deep learning classifier to the eNose measurements, and achieved real-time detection of SARS CoV-2 infection at a level significantly better than chance, for both symptomatic and non-symptomatic participants. This proof of concept with a generic eNose implies that an optimized eNose may allow effective real-time diagnosis, which would provide for extensive relief in the Covid-19 pandemic.",2021,10.1371/journal.pone.0252121,,,,
Proposal of Local Automatic Weighing Attribute in CBIR,"Lung cancer is the most common malignant lesion and the principal cause of cancer-related death worldwide. This problem encourages researchers to build computer-aided solutions to help diagnose lung cancer. Content-based image retrieval (CBIR) systems are very promising in this context due to a large number of image generated everyday. However, semantic gaps have limited CBIR applicability. This work proposes a new approach to automatically adjust CBIR attribute weights to reflect users' semantic interpretation on retrieval process, minimizing the semantic gap problem and improving retrieval accuracy.",2015,,,,,
Proposing a deep learning-based method for improving the diagnostic certainty of pulmonary nodules in CT scan of chest,"OBJECTIVE: To compare the performance of a deep learning (DL)-based method for diagnosing pulmonary nodules compared with radiologists' diagnostic approach in computed tomography (CT) of the chest. MATERIALS AND METHODS: A total of 150 pathologically confirmed pulmonary nodules (60% malignant) assessed and reported by radiologists were included. CT images were processed by the proposed DL-based method to generate the probability of malignancy (0-100%), and the nodules were divided into the groups of benign (0-39.9%), indeterminate (40.0-59.9%), and malignant (60.0-100%). Taking the pathological results as the gold standard, we compared the diagnostic performance of the proposed DL-based method with the radiologists' diagnostic approach using the McNemar-Bowker test. RESULTS: There was a statistically significant difference between the diagnosis results of the proposed DL-based method and the radiologists' diagnostic approach (p < 0.001). Moreover, there was no statistically significant difference in the composition of the diagnosis results between the proposed DL-based method and the radiologists' diagnostic approach (all p > 0.05). The difference in diagnostic accuracy between the proposed DL-based method (70%) and radiologists' diagnostic performance (64%) was not statistically significant (p = 0.243). CONCLUSIONS: The proposed DL-based method achieved an accuracy comparable with the radiologists' diagnostic approach in clinical practice. Furthermore, its advantage in improving diagnostic certainty may raise the radiologists' confidence in diagnosing pulmonary nodules and may help clinical management. Therefore, the proposed DL-based method showed great potential in a certain clinical application. KEY POINTS: • Deep learning-based method for diagnosing the pulmonary nodules in computed tomography provides a higher diagnostic certainty.",2021,10.1007/s00330-021-07919-5,cross-sectional,diagnosis,CT,Lungs
Prospective Case-Control Study of Cardiovascular Abnormalities 6 Months Following Mild COVID-19 in Healthcare Workers,"OBJECTIVES: The purpose of this study was to detect cardiovascular changes after mild severe acute respiratory syndrome-coronavirus-2 infection. BACKGROUND: Concern exists that mild coronavirus disease 2019 may cause myocardial and vascular disease. METHODS: Participants were recruited from COVIDsortium, a 3-hospital prospective study of 731 health care workers who underwent first-wave weekly symptom, polymerase chain reaction, and serology assessment over 4 months, with seroconversion in 21.5% (n = 157). At 6 months post-infection, 74 seropositive and 75 age-, sex-, and ethnicity-matched seronegative control subjects were recruited for cardiovascular phenotyping (comprehensive phantom-calibrated cardiovascular magnetic resonance and blood biomarkers). Analysis was blinded, using objective artificial intelligence analytics where available. RESULTS: A total of 149 subjects (mean age 37 years, range 18 to 63 years, 58% women) were recruited. Seropositive infections had been mild with case definition, noncase definition, and asymptomatic disease in 45 (61%), 18 (24%), and 11 (15%), respectively, with 1 person hospitalized (for 2 days). Between seropositive and seronegative groups, there were no differences in cardiac structure (left ventricular volumes, mass, atrial area), function (ejection fraction, global longitudinal shortening, aortic distensibility), tissue characterization (T(1), T(2), extracellular volume fraction mapping, late gadolinium enhancement) or biomarkers (troponin, N-terminal pro-B-type natriuretic peptide). With abnormal defined by the 75 seronegatives (2 SDs from mean, e.g., ejection fraction <54%, septal T(1) >1,072 ms, septal T(2) >52.4 ms), individuals had abnormalities including reduced ejection fraction (n = 2, minimum 50%), T(1) elevation (n = 6), T(2) elevation (n = 9), late gadolinium enhancement (n = 13, median 1%, max 5% of myocardium), biomarker elevation (borderline troponin elevation in 4; all N-terminal pro-B-type natriuretic peptide normal). These were distributed equally between seropositive and seronegative individuals. CONCLUSIONS: Cardiovascular abnormalities are no more common in seropositive versus seronegative otherwise healthy, workforce representative individuals 6 months post-mild severe acute respiratory syndrome-coronavirus-2 infection.",2021,10.1016/j.jcmg.2021.04.011,case control,diagnosis,Cardiac MRI,Heart
Protein-Ligand Docking Simulations with AutoDock4 Focused on the Main Protease of SARS-CoV-2,"BACKGROUND: The main protease of SARS-CoV-2 (M(pro)) is one of the targets identified in SARS-CoV-2, the causative agent of COVID-19. The application of X-ray diffraction crystallography made available the three-dimensional structure of this protein target in complex with ligands, which paved the way for docking studies. OBJECTIVE: Our goal here is to review recent efforts in the application of docking simulations to identify inhibitors of the M(pro) using the program AutoDock4. METHODS: We searched PubMed to identify studies that applied AutoDock4 for docking against this protein target. We used the structures available for M(pro) to analyze intermolecular interactions and reviewed the methods used to search for inhibitors. RESULTS: The application of docking against the structures available for the M(pro) found ligands with an estimated inhibition in the nanomolar range. Such computational approaches focused on the crystal structures revealed potential inhibitors of M(pro) that might exhibit pharmacological activity against SARS-CoV-2. Nevertheless, most of these studies lack the proper validation of the docking protocol. Also, they all ignored the potential use of machine learning to predict affinity. CONCLUSION: The combination of structural data with computational approaches opened the possibility to accelerate the search for drugs to treat COVID-19. Several studies used AutoDock4 to search for inhibitors of M(pro). Most of them did not employ a validated docking protocol, which lends support to critics of their computational methodology. Furthermore, one of these studies reported the binding of chloroquine and hydroxychloroquine to M(pro). This study ignores the scientific evidence against the use of these antimalarial drugs to treat COVID-19.",2021,10.2174/0929867328666210329094111,,,,
Pulmonary Artery-Vein Classification in CT Images Using Deep Learning,"Recent studies show that pulmonary vascular diseases may specifically affect arteries or veins through different physiologic mechanisms. To detect changes in the two vascular trees, physicians manually analyze the chest computed tomography (CT) image of the patients in search of abnormalities. This process is time consuming, difficult to standardize, and thus not feasible for large clinical studies or useful in real-world clinical decision making. Therefore, automatic separation of arteries and veins in CT images is becoming of great interest, as it may help physicians to accurately diagnose pathological conditions. In this paper, we present a novel, fully automatic approach to classify vessels from chest CT images into arteries and veins. The algorithm follows three main steps: first, a scale-space particles segmentation to isolate vessels; then a 3-D convolutional neural network (CNN) to obtain a first classification of vessels; finally, graph-cuts' optimization to refine the results. To justify the usage of the proposed CNN architecture, we compared different 2-D and 3-D CNNs that may use local information from bronchus- and vessel-enhanced images provided to the network with different strategies. We also compared the proposed CNN approach with a random forests (RFs) classifier. The methodology was trained and evaluated on the superior and inferior lobes of the right lung of 18 clinical cases with noncontrast chest CT scans, in comparison with manual classification. The proposed algorithm achieves an overall accuracy of 94%, which is higher than the accuracy obtained using other CNN architectures and RF. Our method was also validated with contrast-enhanced CT scans of patients with chronic thromboembolic pulmonary hypertension to demonstrate that our model generalizes well to contrast-enhanced modalities. The proposed method outperforms state-of-the-art methods, paving the way for future use of 3-D CNN for artery/vein classification in CT images.",2018,10.1109/tmi.2018.2833385,cross-sectional,diagnosis,CT,Lungs
Pulmonary COVID-19: Learning Spatiotemporal Features Combining CNN and LSTM Networks for Lung Ultrasound Video Classification,"Deep Learning is a very active and important area for building Computer-Aided Diagnosis (CAD) applications. This work aims to present a hybrid model to classify lung ultrasound (LUS) videos captured by convex transducers to diagnose COVID-19. A Convolutional Neural Network (CNN) performed the extraction of spatial features, and the temporal dependence was learned using a Long Short-Term Memory (LSTM). Different types of convolutional architectures were used for feature extraction. The hybrid model (CNN-LSTM) hyperparameters were optimized using the Optuna framework. The best hybrid model was composed of an Xception pre-trained on ImageNet and an LSTM containing 512 units, configured with a dropout rate of 0.4, two fully connected layers containing 1024 neurons each, and a sequence of 20 frames in the input layer (20×2018). The model presented an average accuracy of 93% and sensitivity of 97% for COVID-19, outperforming models based purely on spatial approaches. Furthermore, feature extraction using transfer learning with models pre-trained on ImageNet provided comparable results to models pre-trained on LUS images. The results corroborate with other studies showing that this model for LUS classification can be an important tool in the fight against COVID-19 and other lung diseases.",2021,10.3390/s21165486,cross-sectional,diagnosis,Ultrasound,Lungs
"Pulmonary Embolism in Acute Asthma Exacerbation: Clinical Characteristics, Prediction Model and Hospital Outcomes","PURPOSE: Little is known about the characteristics and impact of acute pulmonary embolism (PE) during episodes of asthma exacerbation. We aimed to characterize patients diagnosed with acute PE in the setting of asthma exacerbation, develop a prediction model to help identify future patients and assess the impact of acute PE on hospital outcomes. METHODS: We included 758 patients who were treated for asthma exacerbation and underwent a computed tomographic pulmonary angiography (CTA) during the same encounter at a university-based hospital between June 2011 and October 2018. We compared clinical characteristics of patients with and without acute PE and developed a machine learning prediction model to classify the PE status based on the clinical variables. We used multivariable regression analysis to evaluate the impact of acute PE on hospital outcomes. RESULTS: Twenty percent of the asthma exacerbation patients who underwent CTA had an acute PE. Factors associated with acute PE included previous history of PE, high CHA(2)DS(2)-VASc score, hyperlipidemia, history of deep vein thrombosis, malignancy, chronic systemic corticosteroids use, high body mass index and atrial fibrillation. Using these factors, we developed a random forest machine learning prediction model which had an 88% accuracy in classifying the acute PE status of the patients (area under the receiver operating characteristic curve = 0.899; 95% confidence interval: 0.885-0.913). Acute PE in asthma exacerbation was associated with longer hospital stay and intensive care unit stay. CONCLUSION: It is important to consider acute PE, a potentially life-threatening event, in the setting of asthma exacerbation especially when other risk factors are present.",2020,10.1007/s00408-020-00363-0,,,,
Pulmonary emphysema quantification at low dose chest CT using Deep Learning image reconstruction,"PURPOSE: Quantitative analysis of emphysema volume is affected by the radiation dose and the CT reconstruction technique. We aim to evaluate the influence of a commercially available deep learning image reconstruction algorithm (DLIR) on the quantification of pulmonary emphysema in low-dose chest CT. METHODS: We performed a retrospective study of low dose chest CT scans in 54 patients with chronic obstructive pulmonary disease (COPD). Raw data were reconstructed using FBP, iterative reconstruction (ASIR-V 70%) and deep learning based algorithms at high, medium and low-strength (DLIR -H, -M, -L). Filtered FBP images served as reference. Pulmonary emphysema volume (proportion of voxels below -950 UH) was measured on each reconstruction dataset and visually assessed by a chest radiologist. Quantitative image quality was assessed by placing 3 regions of interest in the trachea, in air and in a paraspinal muscle. Signal to noise ratio was also measured. RESULTS: The mean CDTIvol was 2.38 ± 0.68 mGy. Significant differences in emphysema volumes between the filtered FBP reference and ASIR-V, DLIR-H, DLIR-M or DLIR-L were observed, (p < 10(-3)) for all. A strong correlation between filtered FBP volumes and DLIR-H was reported (r = 0.999, p < 10(-4)), a 10% overestimation with DLIR-H being observed. Noise was significantly reduced in DLIR-H volumes compared to the other reconstruction methods. Signal to noise ratio was improved when using DLIR-H (p < 10(-6)). CONCLUSION: There are significant differences regarding emphysema volumes between FBP, iterative reconstruction or deep learning-based DLIR algorithm. DLIR-H shows the closest correlation to filtered FBP while increasing SNR.",2022,10.1016/j.ejrad.2022.110338,cross-sectional,informatics,CT,Lungs
Pulmonary lesion subtypes recognition of COVID-19 from radiomics data with three-dimensional texture characterization in computed tomography images,"BACKGROUND: The COVID-19 disease is putting unprecedented pressure on the global healthcare system. The CT (computed tomography) examination as a auxiliary confirmed diagnostic method can help clinicians quickly detect lesions locations of COVID-19 once screening by PCR test. Furthermore, the lesion subtypes classification plays a critical role in the consequent treatment decision. Identifying the subtypes of lesions accurately can help doctors discover changes in lesions in time and better assess the severity of COVID-19. METHOD: The most four typical lesion subtypes of COVID-19 are discussed in this paper, which are GGO (ground-glass opacity), cord, solid and subsolid. A computer-aided diagnosis approach of lesion subtype is proposed in this paper. The radiomics data of lesions are segmented from COVID-19 patients CT images with diagnosis and lesions annotations by radiologists. Then the three-dimensional texture descriptors are applied on the volume data of lesions as well as shape and first-order features. The massive feature data are selected by HAFS (hybrid adaptive feature selection) algorithm and a classification model is trained at the same time. The classifier is used to predict lesion subtypes as side decision information for radiologists. RESULTS: There are 3734 lesions extracted from the dataset with 319 patients collection and then 189 radiomics features are obtained finally. The random forest classifier is trained with data augmentation that the number of different subtypes of lesions is imbalanced in initial dataset. The experimental results show that the accuracy of the four subtypes of lesions is (93.06%, 96.84%, 99.58%, and 94.30%), the recall is (95.52%, 91.58%, 95.80% and 80.75%) and the f-score is (93.84%, 92.37%, 95.47%, and 84.42%). CONCLUSION: The three-dimensional radiomics features used in this paper can better express the high-level information of COVID-19 lesions in CT slices. HAFS method aggregates the results of multiple feature selection algorithms intersects with traditional methods to filter out redundant features more accurately. After selection, the subtype of COVID-19 lesion can be judged by inputting the features into the RF (random forest) model, which can help clinicians more accurately identify the subtypes of COVID-19 lesions and provide help for further research.",2021,10.1186/s12938-021-00961-w,cross-sectional,diagnosis,CT,Lungs
Pulmonary Nodule Classification with Deep Convolutional Neural Networks on Computed Tomography Images,"Computer aided detection (CAD) systems can assist radiologists by offering a second opinion on early diagnosis of lung cancer. Classification and feature representation play critical roles in false-positive reduction (FPR) in lung nodule CAD. We design a deep convolutional neural networks method for nodule classification, which has an advantage of autolearning representation and strong generalization ability. A specified network structure for nodule images is proposed to solve the recognition of three types of nodules, that is, solid, semisolid, and ground glass opacity (GGO). Deep convolutional neural networks are trained by 62,492 regions-of-interest (ROIs) samples including 40,772 nodules and 21,720 nonnodules from the Lung Image Database Consortium (LIDC) database. Experimental results demonstrate the effectiveness of the proposed method in terms of sensitivity and overall accuracy and that it consistently outperforms the competing methods.",2016,10.1155/2016/6215085,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodule classification with deep residual networks,"PURPOSE : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules. METHODS: We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification. RESULTS: Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy. CONCLUSIONS: The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains.",2017,10.1007/s11548-017-1605-6,cross-sectional,diagnosis,CT,Lungs
Pulmonary Nodule Clinical Trial Data Collection and Intelligent Differential Diagnosis for Medical Internet of Things,"In this paper, the medical Internet of things (IoT) is used to pool data from clinical trials of pulmonary nodules, and on this basis, intelligent differential diagnosis techniques are investigated. A filtered orthogonal frequency division multiplexing model based on polarisation coding is proposed, where the input data are fed to a modulator after polarisation cascade coding, and the system performance is analysed under a medical Internet of things modulated additive Gaussian white noise channel. The above polarisation-coded filtered orthogonal frequency division multiplexing system components are applied to electroencephalogram (EEG) signal transmission, to which a threshold compression module and a vector reconstruction module are added to address the system power burden associated with the acquisition and transmission of large amounts of real-time EEG data in the medical IoT. In the threshold compression module, the inherent characteristics of EEG signals are analysed, and the generated EEG data are decomposed into multiple symbolic streams and compressed by applying different thresholds to improve the compression ratio while ensuring the quality of service of the application. A deep neural network-based approach is proposed for the detection and diagnosis of lung nodules. Automatic identification and measurement of simulated lung nodules and the corresponding volumes of nodules in images under different conditions are applied. The sensitivity of each AIADS in identifying lung nodules under different convolution kernel conditions, false positives (FP), false negatives (FN), relative volume errors (RVE), the miss detection rate (MDR) for different types of lung nodules, and the performance of each system in predicting the four types of nodules are calculated. In this paper, an interpretable multibranch feature convolutional neural network model is proposed for the diagnosis of benign and malignant lung nodules. It is demonstrated that the proposed model not only yields interpretable lung nodule classification results but also achieves better lung nodule classification performance with an accuracy rate of 97.8%.",2022,10.1155/2022/2058284,,,,
Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks,"We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.",2016,10.1109/tmi.2016.2536809,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodule detection in CT scans with equivariant CNNs,"Convolutional Neural Networks (CNNs) require a large amount of annotated data to learn from, which is often difficult to obtain for medical imaging problems. In this work we show that the sample complexity of CNNs can be significantly improved by using 3D roto-translation group convolutions instead of standard translational convolutions. 3D CNNs with group convolutions (3D G-CNNs) were applied to the problem of false positive reduction for pulmonary nodule detection in CT scans, and proved to be substantially more effective in terms of accuracy, sensitivity to malignant nodules, and speed of convergence compared to a strong and comparable baseline architecture with regular convolutions, extensive data augmentation and a similar number of parameters. For every dataset size tested, the G-CNN achieved a FROC score close to the CNN trained on ten times more data.",2019,10.1016/j.media.2019.03.010,cross-sectional,diagnosis,CT,Lungs
Pulmonary Nodule Detection Model Based on SVM and CT Image Feature-Level Fusion with Rough Sets,"In order to improve the detection accuracy of pulmonary nodules in CT image, considering two problems of pulmonary nodules detection model, including unreasonable feature structure and nontightness of feature representation, a pulmonary nodules detection algorithm is proposed based on SVM and CT image feature-level fusion with rough sets. Firstly, CT images of pulmonary nodule are analyzed, and 42-dimensional feature components are extracted, including six new 3-dimensional features proposed by this paper and others 2-dimensional and 3-dimensional features. Secondly, these features are reduced for five times with rough set based on feature-level fusion. Thirdly, a grid optimization model is used to optimize the kernel function of support vector machine (SVM), which is used as a classifier to identify pulmonary nodules. Finally, lung CT images of 70 patients with pulmonary nodules are collected as the original samples, which are used to verify the effectiveness and stability of the proposed model by four groups' comparative experiments. The experimental results show that the effectiveness and stability of the proposed model based on rough set feature-level fusion are improved in some degrees.",2016,10.1155/2016/8052436,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodule detection on chest radiographs using balanced convolutional neural network and classic candidate detection,"Computer-aided detection (CADe) systems play a crucial role in pulmonary nodule detection via chest radiographs (CXRs). A two-stage CADe scheme usually includes nodule candidate detection and false positive reduction. A pure deep learning model, such as faster region convolutional neural network (faster R-CNN), has been successfully applied for nodule candidate detection via computed tomography (CT). The model is yet to achieve a satisfactory performance in CXR, because the size of the CXR is relatively large and the nodule in CXR has been obscured by structures such as ribs. In contrast, the CNN has proved effective for false positive reduction compared to the shallow method. In this paper, we developed a CADe scheme using the balanced CNN with classic candidate detection. First, the scheme applied a multi-segment active shape model to accurately segment pulmonary parenchyma. The grayscale morphological enhancement technique was then used to improve the conspicuity of the nodule structure. Based on the nodule enhancement image, 200 nodule candidates were selected and a region of interest (ROI) was cropped for each. Nodules in CXR exhibit a large variation in density, and rib crossing and vessel tissue usually present similar features to the nodule. Compared to the original ROI image, the nodule enhancement ROI image has potential discriminative features from false positive reduction. In this study, the nodule enhancement ROI image, corresponding segmentation result, and original ROI image were encoded into a red-green-blue (RGB) color image instead of the duplicated original ROI image as input of the CNN (GoogLeNet) for false positive reduction. With the Japanese Society of Radiological Technology database, the CADe scheme achieved high performance of the published literatures (a sensitivity of 91.4 % and 97.1 %, with 2.0 false positives per image (FPs/image) and 5.0 FPs/image, respectively) for nodule cases.",2020,10.1016/j.artmed.2020.101881,cross-sectional,diagnosis,CXR,Lungs
Pulmonary nodule detection using hybrid two-stage 3D CNNs,"PURPOSE: Early detection of pulmonary nodules is an effective way to improve patients' chances of survival. In this work, we propose a novel and efficient way to build a computer-aided detection (CAD) system for pulmonary nodules based on computed tomography (CT) scans. METHODS: The system can be roughly divided into two steps: nodule candidate detection and false positive reduction. Considering the three-dimensional (3D) nature of nodules, the CAD system adopts 3D convolutional neural networks (CNNs) in both stages. Specifically, in the first stage, a segmentation-based 3D CNN with a hybrid loss is designed to segment nodules. According to the probability maps produced by the segmentation network, a threshold method and connected component analysis are applied to generate nodule candidates. In the second stage, we employ three classification-based 3D CNNs with different types of inputs to reduce false positives. In addition to simple raw data input, we also introduce hybrid inputs to make better use of the output of the previous segmentation network. In experiments, we use data augmentation and batch normalization to avoid overfitting. RESULTS: We evaluate the system on 888 CT scans from the publicly available LIDC-IDRI dataset, and our method achieves the best performance by comparing with the state-of-the-art methods, which has a high detection sensitivity of 97.5% with an average of only one false positive per scan. An additional evaluation on 115 CT scans from local hospitals is also performed. CONCLUSIONS: Experimental results demonstrate that our method is highly suited for the detection of pulmonary nodules.",2020,10.1002/mp.14161,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodule segmentation with CT sample synthesis using adversarial networks,"PURPOSE: Segmentation of pulmonary nodules is critical for the analysis of nodules and lung cancer diagnosis. We present a novel framework of segmentation for various types of nodules using convolutional neural networks (CNNs). METHODS: The proposed framework is composed of two major parts. The first part is to increase the variety of samples and build a more balanced dataset. A conditional generative adversarial network (cGAN) is employed to produce synthetic CT images. Semantic labels are generated to impart spatial contextual knowledge to the network. Nine attribute scoring labels are combined as well to preserve nodule features. To refine the realism of synthesized samples, reconstruction error loss is introduced into cGAN. The second part is to train a nodule segmentation network on the extended dataset. We build a three-dimensional (3D) CNN model that exploits heterogeneous maps including edge maps and local binary pattern maps. The incorporation of these maps informs the model of texture patterns and boundary information of nodules, which assists high-level feature learning for segmentation. Residual unit, which learns to reduce residual error, is adopted to accelerate training and improve accuracy. RESULTS: Validation on LIDC-IDRI dataset demonstrates that the generated samples are realistic. The mean squared error and average cosine similarity between real and synthesized samples are 1.55 × 10-2 and 0.9534, respectively. The Dice coefficient, positive predicted value, sensitivity, and accuracy are, respectively, 0.8483, 0.8895, 0.8511, and 0.9904 for the segmentation results. CONCLUSIONS: The proposed 3D CNN segmentation framework, based on the use of synthesized samples and multiple maps with residual learning, achieves more accurate nodule segmentation compared to existing state-of-the-art methods. The proposed CT image synthesis method can not only output samples close to real images but also allow for stochastic variation in image diversity.",2019,10.1002/mp.13349,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodules detection assistant platform: An effective computer aided system for early pulmonary nodules detection in physical examination,"BACKGROUND AND OBJECTIVE: Early detection of the pulmonary nodule from physical examination low-dose computer tomography (LDCT) images is an effective measure to reduce the mortality rate of lung cancer. Although there are many computer aided diagnosis (CAD) methods used for detecting pulmonary nodules, there are few CAD systems for small pulmonary nodule detection with a large amount of physical examination LDCT images. METHODS: In this work, we designed a CAD system called Pulmonary Nodules Detection Assistant Platform for early pulmonary nodules detection and classification based on the physical examination LDCT images. Based on the preprocessed physical examination CT images, the three-dimensional (3D) CNN-based model is presented to detect candidate pulmonary nodules and output detection results with quantitative parameters, the 3D ResNet is used to classify the detected nodules into intrapulmonary nodules and pleural nodules to reduce the physician workloads, and the Fully Connected Neural Network (FCNN) is used to classify ground-glass opacity (GGO) nodules and non-GGO nodules to help doctor pay more attention to those suspected early lung cancer nodules. RESULTS: Experiments are performed on our 1000 samples of physical examinations (LNPE1000) with an average diameter of 5.3 mm and LUNA16 dataset with an average diameter of 8.31 mm, which show that the designed CAD system is automatic and efficient for detecting smaller and larger nodules from different datasets, especially for the detection of smaller nodules with diameter between 3 mm and 6 mm in physical examinations. The accuracy of pulmonary nodule detection reaches 0.879 with an average of 1 false positive per CT in LNPE1000 dataset, which is comparable to the experienced physicians. The classification accuracy reaches 0.911 between intrapulmonary and pleural nodules, and 0.950 between GGO and non-GGO nodules, respectively. CONCLUSION: Experimental results show that the proposed pulmonary nodule detection model is robust for different datasets, which can successfully detect smaller and larger nodules in CT images obtained by physical examination. The interactive platform of the designed CAD system has been on trial in a hospital by combining with manual reading, which helps doctors analyze clinical data dynamically and improves the nodule detection efficiency in physical examination applications.",2022,10.1016/j.cmpb.2022.106680,cross-sectional,diagnosis,CT,Lungs
Pulmonary nodules detection based on multi-scale attention networks,"Pulmonary nodules are the main manifestation of early lung cancer. Therefore, accurate detection of nodules in CT images is vital for lung cancer diagnosis. A 3D automatic detection system of pulmonary nodules based on multi-scale attention networks is proposed in this paper to use multi-scale features of nodules and avoid network over-fitting problems. The system consists of two parts, nodule candidate detection (determining the locations of candidate nodules), false positive reduction (minimizing the number of false positive nodules). Specifically, with Res2Net structure, using pre-activation operation and convolutional quadruplet attention module, the 3D multi-scale attention block is designed. It makes full use of multi-scale information of pulmonary nodules by extracting multi-scale features at a granular level and alleviates over-fitting by pre-activation. The U-Net-like encoder-decoder structure is combined with multi-scale attention blocks as the backbone network of Faster R-CNN for detection of candidate nodules. Then a 3D deep convolutional neural network based on multi-scale attention blocks is designed for false positive reduction. The extensive experiments on LUNA16 and TianChi competition datasets demonstrate that the proposed approach can effectively improve the detection sensitivity and control the number of false positive nodules, which has clinical application value.",2022,10.1038/s41598-022-05372-y,cross-sectional,diagnosis,CT,Lungs
Pulmonary ventilation imaging in asthma and cystic fibrosis using oxygen-enhanced 3D radial ultrashort echo time MRI,"BACKGROUND: A previous study demonstrated the feasibility of using 3D radial ultrashort echo time (UTE) oxygen-enhanced MRI (UTE OE-MRI) for functional imaging of healthy human lungs. The repeatability of quantitative measures from UTE OE-MRI needs to be established prior to its application in clinical research. PURPOSE: To evaluate repeatability of obstructive patterns in asthma and cystic fibrosis (CF) with UTE OE-MRI with isotropic spatial resolution and full chest coverage. STUDY TYPE: Volunteer and patient repeatability. POPULATION: Eighteen human subjects (five asthma, six CF, and seven normal subjects). FIELD STRENGTH/SEQUENCE: Respiratory-gated free-breathing 3D radial UTE (80 μs) sequence at 1.5T. ASSESSMENT: Two 3D radial UTE volumes were acquired sequentially under normoxic and hyperoxic conditions. A subset of subjects underwent repeat acquisitions on either the same day or ≤15 days apart. Asthma and CF subjects also underwent spirometry. A workflow including deformable registration and retrospective lung density correction was used to compute 3D isotropic percent signal enhancement (PSE) maps. Median PSE (MPSE) and ventilation defect percent (VDP) of the lung were measured from the PSE map. STATISTICAL TESTS: The relations between MPSE, VDP, and spirometric measures were assessed using Spearman correlations. The test-retest repeatability was evaluated using Bland-Altman analysis and intraclass correlation coefficients (ICC). RESULTS: Ventilation measures in normal subjects (MPSE = 8.0%, VDP = 3.3%) were significantly different from those in asthma (MPSE = 6.0%, P = 0.042; VDP = 21.7%, P = 0.018) and CF group (MPSE = 4.5%, P = 0.0006; VDP = 27.2%, P = 0.002). MPSE correlated significantly with forced expiratory lung volume in 1 second percent predicted (ρ = 0.72, P = 0.017). The ICC of the test-retest VDP and MPSE were both ≥0.90. In all subject groups, an anterior/posterior gradient was observed with higher MPSE and lower VDP in the posterior compared to anterior regions (P ≤ 0.0021 for all comparisons). DATA CONCLUSION: 3D radial UTE OE-MRI supports quantitative differentiation of diseased vs. healthy lungs using either whole lung VDP or MPSE with excellent test-retest repeatability. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2018;47:1287-1297.",2018,10.1002/jmri.25877,cross-sectional,diagnosis,MRI,Lungs
QIBA guidance: Computed tomography imaging for COVID-19 quantitative imaging applications,"As the COVID-19 pandemic impacts global populations, computed tomography (CT) lung imaging is being used in many countries to help manage patient care as well as to rapidly identify potentially useful quantitative COVID-19 CT imaging biomarkers. Quantitative COVID-19 CT imaging applications, typically based on computer vision modeling and artificial intelligence algorithms, include the potential for better methods to assess COVID-19 extent and severity, assist with differential diagnosis of COVID-19 versus other respiratory conditions, and predict disease trajectory. To help accelerate the development of robust quantitative imaging algorithms and tools, it is critical that CT imaging is obtained following best practices of the quantitative lung CT imaging community. Toward this end, the Radiological Society of North America's (RSNA) Quantitative Imaging Biomarkers Alliance (QIBA) CT Lung Density Profile Committee and CT Small Lung Nodule Profile Committee developed a set of best practices to guide clinical sites using quantitative imaging solutions and to accelerate the international development of quantitative CT algorithms for COVID-19. This guidance document provides quantitative CT lung imaging recommendations for COVID-19 CT imaging, including recommended CT image acquisition settings for contemporary CT scanners. Additional best practice guidance is provided on scientific publication reporting of quantitative CT imaging methods and the importance of contributing COVID-19 CT imaging datasets to open science research databases.",2021,10.1016/j.clinimag.2021.02.017,,,,
Quantification of pulmonary involvement in COVID-19 pneumonia by means of a cascade of two U-nets: training and assessment on multiple datasets using different annotation criteria,"PURPOSE: This study aims at exploiting artificial intelligence (AI) for the identification, segmentation and quantification of COVID-19 pulmonary lesions. The limited data availability and the annotation quality are relevant factors in training AI-methods. We investigated the effects of using multiple datasets, heterogeneously populated and annotated according to different criteria. METHODS: We developed an automated analysis pipeline, the LungQuant system, based on a cascade of two U-nets. The first one (U-net[Formula: see text]) is devoted to the identification of the lung parenchyma; the second one (U-net[Formula: see text]) acts on a bounding box enclosing the segmented lungs to identify the areas affected by COVID-19 lesions. Different public datasets were used to train the U-nets and to evaluate their segmentation performances, which have been quantified in terms of the Dice Similarity Coefficients. The accuracy in predicting the CT-Severity Score (CT-SS) of the LungQuant system has been also evaluated. RESULTS: Both the volumetric DSC (vDSC) and the accuracy showed a dependency on the annotation quality of the released data samples. On an independent dataset (COVID-19-CT-Seg), both the vDSC and the surface DSC (sDSC) were measured between the masks predicted by LungQuant system and the reference ones. The vDSC (sDSC) values of 0.95±0.01 and 0.66±0.13 (0.95±0.02 and 0.76±0.18, with 5 mm tolerance) were obtained for the segmentation of lungs and COVID-19 lesions, respectively. The system achieved an accuracy of 90% in CT-SS identification on this benchmark dataset. CONCLUSION: We analysed the impact of using data samples with different annotation criteria in training an AI-based quantification system for pulmonary involvement in COVID-19 pneumonia. In terms of vDSC measures, the U-net segmentation strongly depends on the quality of the lesion annotations. Nevertheless, the CT-SS can be accurately predicted on independent test sets, demonstrating the satisfactory generalization ability of the LungQuant.",2022,10.1007/s11548-021-02501-2,cross-sectional,diagnosis,CT,Lungs
Quantifying the incremental value of deep learning: Application to lung nodule detection,"We present a case study for implementing a machine learning algorithm with an incremental value framework in the domain of lung cancer research. Machine learning methods have often been shown to be competitive with prediction models in some domains; however, implementation of these methods is in early development. Often these methods are only directly compared to existing methods; here we present a framework for assessing the value of a machine learning model by assessing the incremental value. We developed a machine learning model to identify and classify lung nodules and assessed the incremental value added to existing risk prediction models. Multiple external datasets were used for validation. We found that our image model, trained on a dataset from The Cancer Imaging Archive (TCIA), improves upon existing models that are restricted to patient characteristics, but it was inconclusive about whether it improves on models that consider nodule features. Another interesting finding is the variable performance on different datasets, suggesting population generalization with machine learning models may be more challenging than is often considered.",2020,10.1371/journal.pone.0231468,cross-sectional,diagnosis,CT,Lungs
Quantitative Analysis and Automated Lung Ultrasound Scoring for Evaluating COVID-19 Pneumonia With Neural Networks,"As being radiation-free, portable, and capable of repetitive use, ultrasonography is playing an important role in diagnosing and evaluating the COVID-19 Pneumonia (PN) in this epidemic. By virtue of lung ultrasound scores (LUSS), lung ultrasound (LUS) was used to estimate the excessive lung fluid that is an important clinical manifestation of COVID-19 PN, with high sensitivity and specificity. However, as a qualitative method, LUSS suffered from large interobserver variations and requirement for experienced clinicians. Considering this limitation, we developed a quantitative and automatic lung ultrasound scoring system for evaluating the COVID-19 PN. A total of 1527 ultrasound images prospectively collected from 31 COVID-19 PN patients with different clinical conditions were evaluated and scored with LUSS by experienced clinicians. All images were processed via a series of computer-aided analysis, including curve-to-linear conversion, pleural line detection, region-of-interest (ROI) selection, and feature extraction. A collection of 28 features extracted from the ROI was specifically defined for mimicking the LUSS. Multilayer fully connected neural networks, support vector machines, and decision trees were developed for scoring LUS images using the fivefold cross validation. The model with 128×256 two fully connected layers gave the best accuracy of 87%. It is concluded that the proposed method could assess the ultrasound images by assigning LUSS automatically with high accuracy, potentially applicable to the clinics.",2021,10.1109/tuffc.2021.3070696,cross-sectional,diagnosis,CT,Lungs
Quantitative analysis of metastatic breast cancer in mice using deep learning on cryo-image data,"Cryo-imaging sections and images a whole mouse and provides ~ 120-GBytes of microscopic 3D color anatomy and fluorescence images, making fully manual analysis of metastases an onerous task. A convolutional neural network (CNN)-based metastases segmentation algorithm included three steps: candidate segmentation, candidate classification, and semi-automatic correction of the classification result. The candidate segmentation generated > 5000 candidates in each of the breast cancer-bearing mice. Random forest classifier with multi-scale CNN features and hand-crafted intensity and morphology features achieved 0.8645 ± 0.0858, 0.9738 ± 0.0074, and 0.9709 ± 0.0182 sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC), with fourfold cross validation. Classification results guided manual correction by an expert with our in-house MATLAB software. Finally, 225, 148, 165, and 344 metastases were identified in the four cancer mice. With CNN-based segmentation, the human intervention time was reduced from > 12 to ~ 2 h. We demonstrated that 4T1 breast cancer metastases spread to the lung, liver, bone, and brain. Assessing the size and distribution of metastases proves the usefulness and robustness of cryo-imaging and our software for evaluating new cancer imaging and therapeutics technologies. Application of the method with only minor modification to a pancreatic metastatic cancer model demonstrated generalizability to other tumor models.",2021,10.1038/s41598-021-96838-y,,,,
Quantitative assessment of lung involvement on chest CT at admission: Impact on hypoxia and outcome in COVID-19 patients,"BACKGROUND: The aim of this study was to quantify COVID-19 pneumonia features using CT performed at time of admission to emergency department in order to predict patients' hypoxia during the hospitalization and outcome. METHODS: Consecutive chest CT performed in the emergency department between March 1st and April 7th 2020 for COVID-19 pneumonia were analyzed. The three features of pneumonia (GGO, semi-consolidation and consolidation) and the percentage of well-aerated lung were quantified using a HU threshold based software. ROC curves identified the optimal cut-off values of CT parameters to predict hypoxia worsening and hospital discharge. Multiple Cox proportional hazards regression was used to analyze the capability of CT quantitative features, demographic and clinical variables to predict the time to hospital discharge. RESULTS: Seventy-seven patients (median age 56-years-old, 51 men) with COVID-19 pneumonia at CT were enrolled. The quantitative features of COVID-19 pneumonia were not associated to age, sex and time-from-symptoms onset, whereas higher number of comorbidities was correlated to lower well-aerated parenchyma ratio (rho = -0.234, p = 0.04) and increased semi-consolidation ratio (rho = -0.303, p = 0.008). Well-aerated lung (≤57%), semi-consolidation (≥17%) and consolidation (≥9%) predicted worst hypoxemia during hospitalization, with moderate areas under curves (AUC 0.76, 0.75, 0.77, respectively). Multiple Cox regression identified younger age (p < 0.01), female sex (p < 0.001), longer time-from-symptoms onset (p = 0.049), semi-consolidation ≤17% (p < 0.01) and consolidation ≤13% (p = 0.03) as independent predictors of shorter time to hospital discharge. CONCLUSION: Quantification of pneumonia features on admitting chest CT predicted hypoxia worsening during hospitalization and time to hospital discharge in COVID-19 patients.",2021,10.1016/j.clinimag.2021.04.033,retrospective cohort,diagnosis,CT,Lungs
Quantitative CT analysis of pulmonary nodules for lung adenocarcinoma risk classification based on an exponential weighted grey scale angular density distribution feature,"BACKGROUND AND OBJECTIVES: To improve lung nodule classification efficiency, we propose a lung nodule CT image characterization method. We propose a multi-directional feature extraction method to effectively represent nodules of different risk levels. The proposed feature combined with pattern recognition model to classify lung adenocarcinomas risk to four categories: Atypical Adenomatous Hyperplasia (AAH), Adenocarcinoma In Situ (AIS), Minimally Invasive Adenocarcinoma (MIA), and Invasive Adenocarcinoma (IA). METHODS: First, we constructed the reference map using an integral image and labelled this map using a K-means approach. The density distribution map of the lung nodule image was generated after scanning all pixels in the nodule image. An exponential function was designed to weight the angular histogram for each component of the distribution map, and the features of the image were described. Then, quantitative measurement was performed using a Random Forest classifier. The evaluation data were obtained from the LIDC-IDRI database and the CT database which provided by Shanghai Zhongshan hospital (ZSDB). In the LIDC-IDRI, the nodules are categorized into three configurations with five ranks of malignancy (""1"" to ""5""). In the ZSDB, the nodule categories are AAH, AIS, MIA, and IA. RESULTS: The average of Student's t-test p-values were less than 0.02. The AUCs for the LIDC-IDRI database were 0.9568, 0.9320, and 0.8288 for Configurations 1, 2, and 3, respectively. The AUCs for the ZSDB were 0.9771, 0.9917, 0.9590, and 0.9971 for AAH, AIS, MIA and IA, respectively. CONCLUSION: The experimental results demonstrate that the proposed method outperforms the state-of-the-art and is robust for different lung CT image datasets.",2018,10.1016/j.cmpb.2018.04.001,cross-sectional,diagnosis,CT,Lungs
Quantitative CT for detecting COVID‑19 pneumonia in suspected cases,"BACKGROUND: Corona Virus Disease 2019 (COVID-19) is currently a worldwide pandemic and has a huge impact on public health and socio-economic development. The purpose of this study is to explore the diagnostic value of the quantitative computed tomography (CT) method by using different threshold segmentation techniques to distinguish between patients with or without COVID-19 pneumonia. METHODS: A total of 47 patients with suspected COVID-19 were retrospectively analyzed, including nine patients with positive real-time fluorescence reverse transcription polymerase chain reaction (RT-PCR) test (confirmed case group) and 38 patients with negative RT-PCR test (excluded case group). An improved 3D convolutional neural network (VB-Net) was used to automatically extract lung lesions. Eight different threshold segmentation methods were used to define the ground glass opacity (GGO) and consolidation. The receiver operating characteristic (ROC) curves were used to compare the performance of various parameters with different thresholds for diagnosing COVID-19 pneumonia. RESULTS: The volume of GGO (VOGGO) and GGO percentage in the whole lung (GGOPITWL) were the most effective values for diagnosing COVID-19 at a threshold of - 300 HU, with areas under the curve (AUCs) of 0.769 and 0.769, sensitivity of 66.67 and 66.67%, specificity of 94.74 and 86.84%. Compared with VOGGO or GGOPITWL at a threshold of - 300 Hounsfield units (HU), the consolidation percentage in the whole lung (CPITWL) with thresholds at - 400 HU, - 350 HU, and - 250 HU were statistically different. There were statistical differences in the infection volume and percentage of the whole lung, right lung, and lobes between the two groups. VOGGO, GGOPITWL, and volume of consolidation (VOC) were also statistically different at the threshold of - 300 HU. CONCLUSIONS: Quantitative CT provides an image quantification method for the auxiliary diagnosis of COVID-19 and is expected to assist in confirming patients with COVID-19 pneumonia in suspected cases.",2021,10.1186/s12879-021-06556-z,cross-sectional,diagnosis,CT,Lungs
Quantitative evaluation of COVID-19 pneumonia severity by CT pneumonia analysis algorithm using deep learning technology and blood test results,"PURPOSE: To evaluate whether early chest computed tomography (CT) lesions quantified by an artificial intelligence (AI)-based commercial software and blood test values at the initial presentation can differentiate the severity of COVID-19 pneumonia. MATERIALS AND METHODS: This retrospective study included 100 SARS-CoV-2-positive patients with mild (n = 23), moderate (n = 37) or severe (n = 40) pneumonia classified according to the Japanese guidelines. Univariate Kruskal-Wallis and multivariate ordinal logistic analyses were used to examine whether CT parameters (opacity score, volume of opacity, % opacity, volume of high opacity, % high opacity and mean HU total on CT) as well as blood test parameters [procalcitonin, estimated glomerular filtration rate (eGFR), C-reactive protein, % lymphocyte, ferritin, aspartate aminotransferase, lactate dehydrogenase, alanine aminotransferase, creatine kinase, hemoglobin A1c, prothrombin time, activated partial prothrombin time (APTT), white blood cell count and creatinine] differed by disease severity. RESULTS: All CT parameters and all blood test parameters except procalcitonin and APPT were significantly different among mild, moderate and severe groups. By multivariate analysis, mean HU total and eGFR were two independent factors associated with severity (p < 0.0001). Cutoff values for mean HU total and eGFR were, respectively, - 801 HU and 77 ml/min/1.73 m(2) between mild and moderate pneumonia and - 704 HU and 53 ml/min/1.73 m(2) between moderate and severe pneumonia. CONCLUSION: The mean HU total of the whole lung, determined by the AI algorithm, and eGFR reflect the severity of COVID-19 pneumonia.",2021,10.1007/s11604-021-01134-4,cross-sectional,diagnosis,CT,Lungs
Quantitative lung lesion features and temporal changes on chest CT in patients with common and severe SARS-CoV-2 pneumonia,"The purpose of this study was to describe the temporal evolution of quantitative lung lesion features on chest computed tomography (CT) in patients with common and severe types of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pneumonia. Records of patients diagnosed with SARS-CoV-2 pneumonia were reviewed retrospectively from 24 January 2020 to 15 March 2020. Patients were classified into common and severe groups according to the diagnostic criteria of severe pneumonia. The quantitative CT features of lung lesions were automatically calculated using artificial intelligence algorithms, and the percentages of ground-glass opacity volume (PGV), consolidation volume (PCV) and total lesion volume (PTV) were determined in both lungs. PGV, PCV and PTV were analyzed based on the time from the onset of initial symptoms in the common and severe groups. In the common group, PTV increased slowly and peaked at approximately 12 days from the onset of the initial symptoms. In the severe group, PTV peaked at approximately 17 days. The severe pneumonia group exhibited increased PGV, PCV and PTV compared with the common group. These features started to appear in Stage 2 (4-7 days from onset of initial symptoms) and were observed in all subsequent stages (p<0.05). In severe SARS-CoV-2 pneumonia patients, PGV, PCV and PTV began to significantly increase in Stage 2 and decrease in Stage 5 (22-30 days). Compared with common SARS-CoV-2 pneumonia patients, the patients in the severe group exhibited increased PGV, PCV and PTV as well as a later peak time of lesion and recovery time.",2020,10.1371/journal.pone.0236858,cross-sectional,diagnosis,CT,Lungs
Quantitative vessel tortuosity: A potential CT imaging biomarker for distinguishing lung granulomas from adenocarcinomas,"Adenocarcinomas and active granulomas can both have a spiculated appearance on computed tomography (CT) and both are often fluorodeoxyglucose (FDG) avid on positron emission tomography (PET) scan, making them difficult to distinguish. Consequently, patients with benign granulomas are often subjected to invasive surgical biopsies or resections. In this study, quantitative vessel tortuosity (QVT), a novel CT imaging biomarker to distinguish between benign granulomas and adenocarcinomas on routine non-contrast lung CT scans is introduced. Our study comprised of CT scans of 290 patients from two different institutions, one cohort for training (N = 145) and the other (N = 145) for independent validation. In conjunction with a machine learning classifier, the top informative and stable QVT features yielded an area under receiver operating characteristic curve (ROC AUC) of 0.85 in the independent validation set. On the same cohort, the corresponding AUCs for two human experts including a radiologist and a pulmonologist were found to be 0.61 and 0.60, respectively. QVT features also outperformed well known shape and textural radiomic features which had a maximum AUC of 0.73 (p-value = 0.002), as well as features learned using a convolutional neural network AUC = 0.76 (p-value = 0.028). Our results suggest that QVT features could potentially serve as a non-invasive imaging biomarker to distinguish granulomas from adenocarcinomas on non-contrast CT scans.",2018,10.1038/s41598-018-33473-0,cross-sectional,diagnosis,CT,Lungs
Quantum algorithm for quicker clinical prognostic analysis: an application and experimental study using CT scan images of COVID-19 patients,"BACKGROUND: In medical diagnosis and clinical practice, diagnosing a disease early is crucial for accurate treatment, lessening the stress on the healthcare system. In medical imaging research, image processing techniques tend to be vital in analyzing and resolving diseases with a high degree of accuracy. This paper establishes a new image classification and segmentation method through simulation techniques, conducted over images of COVID-19 patients in India, introducing the use of Quantum Machine Learning (QML) in medical practice. METHODS: This study establishes a prototype model for classifying COVID-19, comparing it with non-COVID pneumonia signals in Computed tomography (CT) images. The simulation work evaluates the usage of quantum machine learning algorithms, while assessing the efficacy for deep learning models for image classification problems, and thereby establishes performance quality that is required for improved prediction rate when dealing with complex clinical image data exhibiting high biases. RESULTS: The study considers a novel algorithmic implementation leveraging quantum neural network (QNN). The proposed model outperformed the conventional deep learning models for specific classification task. The performance was evident because of the efficiency of quantum simulation and faster convergence property solving for an optimization problem for network training particularly for large-scale biased image classification task. The model run-time observed on quantum optimized hardware was 52 min, while on K80 GPU hardware it was 1 h 30 min for similar sample size. The simulation shows that QNN outperforms DNN, CNN, 2D CNN by more than 2.92% in gain in accuracy measure with an average recall of around 97.7%. CONCLUSION: The results suggest that quantum neural networks outperform in COVID-19 traits' classification task, comparing to deep learning w.r.t model efficacy and training time. However, a further study needs to be conducted to evaluate implementation scenarios by integrating the model within medical devices.",2021,10.1186/s12911-021-01588-6,retrospective cohort,diagnosis,CT,Lungs
Radiation Versus Immune Checkpoint Inhibitor Associated Pneumonitis: Distinct Radiologic Morphologies,"BACKGROUND: Patients with non-small cell lung cancer may develop pneumonitis after thoracic radiotherapy (RT) and immune checkpoint inhibitors (ICIs). We hypothesized that distinct morphologic features are associated with different pneumonitis etiologies. MATERIALS AND METHODS: We systematically compared computed tomography (CT) features of RT- versus ICI-pneumonitis. Clinical and imaging features were tested for association with pneumonitis severity. Lastly, we constructed an exploratory radiomics-based machine learning (ML) model to discern pneumonitis etiology. RESULTS: Between 2009 and 2019, 82 patients developed pneumonitis: 29 after thoracic RT, 23 after ICI, and 30 after RT + ICI. Fifty patients had grade 2 pneumonitis, 22 grade 3, and 7 grade 4. ICI-pneumonitis was more likely bilateral (65% vs. 28%; p = .01) and involved more lobes (66% vs. 45% involving at least three lobes) and was less likely to have sharp border (17% vs. 59%; p = .004) compared with RT-pneumonitis. Pneumonitis morphology after RT + ICI was heterogeneous, with 47% bilateral, 37% involving at least three lobes, and 40% sharp borders. Among all patients, risk factors for severe pneumonitis included poor performance status, smoking history, worse lung function, and bilateral and multifocal involvement on CT. An ML model based on seven radiomic features alone could distinguish ICI- from RT-pneumonitis with an area under the receiver-operating curve of 0.76 and identified the predominant etiology after RT + ICI concordant with multidisciplinary consensus. CONCLUSION: RT- and ICI-pneumonitis exhibit distinct spatial features on CT. Bilateral and multifocal lung involvement is associated with severe pneumonitis. Integrating these morphologic features in the clinical management of patients who develop pneumonitis after RT and ICIs may improve treatment decision-making. IMPLICATIONS FOR PRACTICE: Patients with non-small cell lung cancer often receive thoracic radiation and immune checkpoint inhibitors (ICIs), both of which can cause pneumonitis. This study identified similarities and differences in pneumonitis morphology on computed tomography (CT) scans among pneumonitis due to radiotherapy (RT) alone, ICI alone, and the combination of both. Patients who have bilateral CT changes involving at least three lobes are more likely to have ICI-pneumonitis, whereas those with unilateral CT changes with sharp borders are more likely to have radiation pneumonitis. After RT and/or ICI, severe pneumonitis is associated with bilateral and multifocal CT changes. These results can help guide clinicians in triaging patients who develop pneumonitis after radiation and during ICI treatment.",2021,10.1002/onco.13900,cross-sectional,diagnosis,CT,Lungs
Radiogenomic Models Using Machine Learning Techniques to Predict EGFR Mutations in Non-Small Cell Lung Cancer,"BACKGROUND: The purpose of this study was to build radiogenomics models from texture signatures derived from computed tomography (CT) and (18)F-FDG PET-CT (FDG PET-CT) images of non-small cell lung cancer (NSCLC) with and without epidermal growth factor receptor (EGFR) mutations. METHODS: Fifty patients diagnosed with NSCLC between 2011 and 2015 and with known EGFR mutation status were retrospectively identified. Texture features extracted from pretreatment CT and FDG PET-CT images by manual contouring of the primary tumor were used to develop multivariate logistic regression (LR) models to predict EGFR mutations in exon 19 and exon 20. RESULTS: An LR model evaluating FDG PET-texture features was able to differentiate EGFR mutant from wild type with an area under the curve (AUC), sensitivity, specificity, and accuracy of 0.87, 0.76, 0.66, and 0.71, respectively. The model derived from CT texture features had an AUC, sensitivity, specificity, and accuracy of 0.83, 0.84, 0.73, and 0.78, respectively. FDG PET-texture features that could discriminate between mutations in EGFR exon 19 and 21 demonstrated AUC, sensitivity, specificity, and accuracy of 0.86, 0.84, 0.73, and 0.78, respectively. Based on CT texture features, the AUC, sensitivity, specificity, and accuracy were 0.75, 0.81, 0.69, and 0.75, respectively. CONCLUSION: Non-small cell lung cancer texture analysis using FGD-PET and CT images can identify tumors with mutations in EGFR. Imaging signatures could be valuable for pretreatment assessment and prognosis in precision therapy.",2021,10.1177/0846537119899526,cross-sectional,diagnosis,CT,Lungs
Radiological Image Traits Predictive of Cancer Status in Pulmonary Nodules,"Purpose: We propose a systematic methodology to quantify incidentally identified pulmonary nodules based on observed radiological traits (semantics) quantified on a point scale and a machine-learning method using these data to predict cancer status.Experimental Design: We investigated 172 patients who had low-dose CT images, with 102 and 70 patients grouped into training and validation cohorts, respectively. On the images, 24 radiological traits were systematically scored and a linear classifier was built to relate the traits to malignant status. The model was formed both with and without size descriptors to remove bias due to nodule size. The multivariate pairs formed on the training set were tested on an independent validation data set to evaluate their performance.Results: The best 4-feature set that included a size measurement (set 1), was short axis, contour, concavity, and texture, which had an area under the receiver operator characteristic curve (AUROC) of 0.88 (accuracy = 81%, sensitivity = 76.2%, specificity = 91.7%). If size measures were excluded, the four best features (set 2) were location, fissure attachment, lobulation, and spiculation, which had an AUROC of 0.83 (accuracy = 73.2%, sensitivity = 73.8%, specificity = 81.7%) in predicting malignancy in primary nodules. The validation test AUROC was 0.8 (accuracy = 74.3%, sensitivity = 66.7%, specificity = 75.6%) and 0.74 (accuracy = 71.4%, sensitivity = 61.9%, specificity = 75.5%) for sets 1 and 2, respectively.Conclusions: Radiological image traits are useful in predicting malignancy in lung nodules. These semantic traits can be used in combination with size-based measures to enhance prediction accuracy and reduce false-positives. Clin Cancer Res; 23(6); 1442-9. ©2016 AACR.",2017,10.1158/1078-0432.Ccr-15-3102,cross-sectional,diagnosis,CT,Lungs
Radiologist-supervised Transfer Learning: Improving Radiographic Localization of Pneumonia and Prognostication of Patients With COVID-19,"PURPOSE: To assess the potential of a transfer learning strategy leveraging radiologist supervision to enhance convolutional neural network-based (CNN) localization of pneumonia on radiographs and to further assess the prognostic value of CNN severity quantification on patients evaluated for COVID-19 pneumonia, for whom severity on the presenting radiograph is a known predictor of mortality and intubation. MATERIALS AND METHODS: We obtained an initial CNN previously trained to localize pneumonia along with 25,684 radiographs used for its training. We additionally curated 1466 radiographs from patients who had a computed tomography (CT) performed on the same day. Regional likelihoods of pneumonia were then annotated by cardiothoracic radiologists, referencing these CTs. Combining data, a preexisting CNN was fine-tuned using transfer learning. Whole-image and regional performance of the updated CNN was assessed using receiver-operating characteristic area under the curve and Dice. Finally, the value of CNN measurements was assessed with survival analysis on 203 patients with COVID-19 and compared against modified radiographic assessment of lung edema (mRALE) score. RESULTS: Pneumonia detection area under the curve improved on both internal (0.756 to 0.841) and external (0.864 to 0.876) validation data. Dice overlap also improved, particularly in the lung bases (R: 0.121 to 0.433, L: 0.111 to 0.486). There was strong correlation between radiologist mRALE score and CNN fractional area of involvement (ρ=0.85). Survival analysis showed similar, strong prognostic ability of the CNN and mRALE for mortality, likelihood of intubation, and duration of hospitalization among patients with COVID-19. CONCLUSIONS: Radiologist-supervised transfer learning can enhance the ability of CNNs to localize and quantify the severity of disease. Closed-loop systems incorporating radiologists may be beneficial for continued improvement of artificial intelligence algorithms.",2022,10.1097/rti.0000000000000618,retrospective cohort,diagnosis,CXR - CT,Lungs
Radiologists versus Deep Convolutional Neural Networks: A Comparative Study for Diagnosing COVID-19,"The reverse transcriptase polymerase chain reaction (RT-PCR) is still the routinely used test for the diagnosis of SARS-CoV-2 (COVID-19). However, according to several reports, RT-PCR showed a low sensitivity and multiple tests may be required to rule out false negative results. Recently, chest computed tomography (CT) has been an efficient tool to diagnose COVID-19 as it is directly affecting the lungs. In this paper, we investigate the application of pre-trained models in diagnosing patients who are positive for COVID-19 and differentiating it from normal patients, who tested negative for coronavirus. The study aims to compare the generalization capabilities of deep learning models with two thoracic radiologists in diagnosing COVID-19 chest CT images. A dataset of 3000 images was obtained from the Near East Hospital, Cyprus, and used to train and to test the three employed pre-trained models. In a test set of 250 images used to evaluate the deep neural networks and the radiologists, it was found that deep networks (ResNet-18, ResNet-50, and DenseNet-201) can outperform the radiologists in terms of higher accuracy (97.8%), sensitivity (98.1%), specificity (97.3%), precision (98.4%), and F1-score (198.25%), in classifying COVID-19 images.",2021,10.1155/2021/5527271,cross-sectional,diagnosis,CXR - CT,Lungs
Radiomic Analysis of CT Predicts Tumor Response in Human Lung Cancer with Radiotherapy,"PURPOSE: Radiomics features can be positioned to monitor changes throughout treatment. In this study, we evaluated machine learning for predicting tumor response by analyzing CT images of lung cancer patients treated with radiotherapy. EXPERIMENTAL DESIGN: For this retrospective study, screening or standard diagnostic CT images were collected for 100 patients (mean age, 67 years; range, 55-82 years; 64 men [mean age, 68 years; range, 55-82 years] and 36 women [mean age, 65 years; range, 60-72 years]) from two institutions between 2013 and 2017. Radiomics analysis was available for each patient. Features were pruned to train machine learning classifiers with 50 patients, then trained in the test dataset. RESULT: A support vector machine classifier with 2 radiomic features (flatness and coefficient of variation) achieved an area under the receiver operating characteristic curve (AUC) of 0.91 on the test set. CONCLUSION: The 2 radiomic features, flatness, and coefficient of variation, from the volume of interest of lung tumor, can be the biomarkers for predicting tumor response at CT.",2020,10.1007/s10278-020-00385-3,cross-sectional,diagnosis,CT,Lungs
Radiomic analysis of planning computed tomograms for predicting radiation-induced lung injury and outcome in lung cancer patients treated with robotic stereotactic body radiation therapy,"OBJECTIVES: To predict radiation-induced lung injury and outcome in non-small cell lung cancer (NSCLC) patients treated with robotic stereotactic body radiation therapy (SBRT) from radiomic features of the primary tumor. METHODS: In all, 110 patients with primary stage I/IIa NSCLC were analyzed for local control (LC), disease-free survival (DFS), overall survival (OS) and development of local lung injury up to fibrosis (LF). First-order (histogram), second-order (GLCM, Gray Level Co-occurrence Matrix) and shape-related radiomic features were determined from the unprocessed or filtered planning CT images of the gross tumor volume (GTV), subjected to LASSO (Least Absolute Shrinkage and Selection Operator) regularization and used to construct continuous and dichotomous risk scores for each endpoint. RESULTS: Continuous scores comprising 1-5 histogram or GLCM features had a significant (p = 0.0001-0.032) impact on all endpoints that was preserved in a multifactorial Cox regression analysis comprising additional clinical and dosimetric factors. At 36 months, LC did not differ between the dichotomous risk groups (93% vs. 85%, HR 0.892, 95%CI 0.222-3.590), while DFS (45% vs. 17%, p < 0.05, HR 0.457, 95%CI 0.240-0.868) and OS (80% vs. 37%, p < 0.001, HR 0.190, 95%CI 0.065-0.556) were significantly lower in the high-risk groups. Also, the frequency of LF differed significantly between the two risk groups (63% vs. 20% at 24 months, p < 0.001, HR 0.158, 95%CI 0.054-0.458). CONCLUSION: Radiomic analysis of the gross tumor volume may help to predict DFS and OS and the development of local lung fibrosis in early stage NSCLC patients treated with stereotactic radiotherapy.",2019,10.1007/s00066-019-01452-7,cross-sectional,diagnosis,CT,Lungs
Radiomic Detection of EGFR Mutations in NSCLC,"Radiomics is defined as the use of automated or semi-automated post-processing and analysis of multiple features derived from imaging exams. Extracted features might generate models able to predict the molecular profile of solid tumors. The aim of this study was to develop a predictive algorithm to define the mutational status of EGFR in treatment-naïve patients with advanced non-small cell lung cancer (NSCLC). CT scans from 109 treatment-naïve patients with NSCLC (21 EGFR-mutant and 88 EGFR-wild type) underwent radiomics analysis to develop a machine learning model able to recognize EGFR-mutant from EGFR-WT patients via CT scans. A ""test-retest"" approach was used to identify stable radiomics features. The accuracy of the model was tested on an external validation set from another institution and on a dataset from the Cancer Imaging Archive (TCIA). The machine learning model that considered both radiomic and clinical features (gender and smoking status) reached a diagnostic accuracy of 88.1% in our dataset with an AUC at the ROC curve of 0.85, whereas the accuracy values in the datasets from TCIA and the external institution were 76.6% and 83.3%, respectively. Furthermore, 17 distinct radiomics features detected at baseline CT scan were associated with subsequent development of T790M during treatment with an EGFR inhibitor. In conclusion, our machine learning model was able to identify EGFR-mutant patients in multiple validation sets with globally good accuracy, especially after data optimization. More comprehensive training sets might result in further improvement of radiomics-based algorithms. SIGNIFICANCE: These findings demonstrate that data normalization and ""test-retest"" methods might improve the performance of machine learning models on radiomics images and increase their reliability when used on external validation datasets.",2021,10.1158/0008-5472.Can-20-0999,cross-sectional,diagnosis,CT,Lungs
Radiomic prediction of mutation status based on MR imaging of lung cancer brain metastases,"Lung cancer metastases comprise most of all brain metastases in adults and most brain metastases are diagnosed by magnetic resonance (MR) scans. The purpose of this study was to conduct an MR imaging-based radiomic analysis of brain metastatic lesions from patients with primary lung cancer to classify mutational status of the metastatic disease. We retrospectively identified lung cancer patients with brain metastases treated at our institution between 2009 and 2017 who underwent genotype testing of their primary lung cancer. Brain MR Images were used for segmentation of enhancing tumors and peritumoral edema, and for radiomic feature extraction. The most relevant radiomic features were identified and used with clinical data to train random forest classifiers to classify the mutation status. Of 110 patients in the study cohort (mean age 57.51 ± 12.32 years; M: F = 37:73), 75 had an EGFR mutation, 21 had an ALK translocation, and 15 had a KRAS mutation. One patient had both ALK translocation and EGFR mutation. Majority of radiomic features most relevant for mutation classification were textural. Model building using both radiomic features and clinical data yielded more accurate classifications than using either alone. For classification of EGFR, ALK, and KRAS mutation status, the model built with both radiomic features and clinical data resulted in area-under-the-curve (AUC) values based on cross-validation of 0.912, 0.915, and 0.985, respectively. Our study demonstrated that MR imaging-based radiomic analysis of brain metastases in patients with primary lung cancer may be used to classify mutation status. This approach may be useful for devising treatment strategies and informing prognosis.",2020,10.1016/j.mri.2020.03.002,,,,
Radiomic signature as a diagnostic factor for histologic subtype classification of non-small cell lung cancer,"OBJECTIVES: To distinguish squamous cell carcinoma (SCC) from lung adenocarcinoma (ADC) based on a radiomic signature METHODS: This study involved 129 patients with non-small cell lung cancer (NSCLC) (81 in the training cohort and 48 in the independent validation cohort). Approximately 485 features were extracted from a manually outlined tumor region. The LASSO logistic regression model selected the key features of a radiomic signature. Receiver operating characteristic curve and area under the curve (AUC) were used to evaluate the performance of the radiomic signature in the training and validation cohorts. RESULTS: Five features were selected to construct the radiomic signature for histologic subtype classification. The performance of the radiomic signature to distinguish between lung ADC and SCC in both training and validation cohorts was good, with an AUC of 0.905 (95% confidence interval [CI]: 0.838 to 0.971), sensitivity of 0.830, and specificity of 0.929. In the validation cohort, the radiomic signature showed an AUC of 0.893 (95% CI: 0.789 to 0.996), sensitivity of 0.828, and specificity of 0.900. CONCLUSIONS: A unique radiomic signature was constructed for use as a diagnostic factor for discriminating lung ADC from SCC. Patients with NSCLC will benefit from the proposed radiomic signature. KEY POINTS: • Machine learning can be used for auxiliary distinguish in lung cancer. • Radiomic signature can discriminate lung ADC from SCC. • Radiomics can help to achieve precision medical treatment.",2018,10.1007/s00330-017-5221-1,cross-sectional,diagnosis,CT,Lungs
Radiomic-Based Pathological Response Prediction from Primary Tumors and Lymph Nodes in NSCLC,"INTRODUCTION: Noninvasive biomarkers that capture the total tumor burden could provide important complementary information for precision medicine to aid clinical decision making. We investigated the value of radiomic data extracted from pretreatment computed tomography images of the primary tumor and lymph nodes in predicting pathological response after neoadjuvant chemoradiation before surgery. METHODS: A total of 85 patients with resectable locally advanced (stage II-III) NSCLC (median age 60.3 years, 65% female) treated from 2003 to 2013 were included in this institutional review board-approved study. Radiomics analysis was performed on 85 primary tumors and 178 lymph nodes to discriminate between pathological complete response (pCR) and gross residual disease (GRD). Twenty nonredundant and stable features (10 from each site) were evaluated by using the area under the curve (AUC) (all p values were corrected for multiple hypothesis testing). Classification performance of each feature set was evaluated by random forest and nested cross validation. RESULTS: Three radiomic features (describing primary tumor sphericity and lymph node homogeneity) were significantly predictive of pCR with similar performances (all AUC = 0.67, p < 0.05). Two features (quantifying lymph node homogeneity) were predictive of GRD (AUC range 0.72-0.75, p < 0.05) and performed significantly better than the primary features (AUC = 0.62). Multivariate analysis showed that for pCR, the radiomic features set alone had the best-performing classification (median AUC = 0.68). Furthermore, for GRD classification, the combination of radiomic and clinical data significantly outperformed all other feature sets (median AUC = 0.73). CONCLUSION: Lymph node phenotypic information was significantly predictive for pathological response and showed higher classification performance than radiomic features obtained from the primary tumor.",2017,10.1016/j.jtho.2016.11.2226,cross-sectional,diagnosis,CT,Lungs
Radiomics analysis of 3D dose distributions to predict toxicity of radiotherapy for lung cancer,"PURPOSE: (Chemo)-radiotherapy (RT) is the gold standard treatment for patients with locally advanced lung cancer non accessible for surgery. However, current toxicity prediction models rely on clinical and dose volume histograms (DVHs) and remain unsufficient. The goal of this work is to investigate the added predictive value of the radiomics approach applied to dose maps regarding acute and late toxicities in both the lungs and esophagus. METHODS: Acute and late toxicities scored using the CTCAE v4.0 were retrospectively collected on patients treated with RT in our institution. Radiomic features were extracted from 3D dose maps considering Gy values as grey-levels in images. DVH and usual clinical factors were also considered. Three toxicity prediction models (clinical only, clinical + DVH and combined, i.e., including clinical + DVH + radiomics) were incrementally trained using a neural network on 70% of the patients for prediction of grade ≥2 acute and late pulmonary toxicities (APT/LPT) and grade ≥2 acute esophageal toxicities (AET). After bootstrapping (n = 1000), optimal cut-off values were determined based on the Youden Index. The trained models were then evaluated in the remaining 30% of patients using balanced accuracy (BAcc). RESULTS: 167 patients were treated from 2015 to 2018: 78% non small-cell lung cancers, 14% small-cell lung cancers and 8% other histology with a median age at treatment of 66 years. Respectively, 22.2%, 16.8% and 30.0% experienced APT, LPT and AET. In the training set (n = 117), the corresponding BAcc for clinical only/clinical + DVH/combined were 0.68/0.79/0.92, 0.66/0.77/0.87 and 0.68/0.73/0.84. In the testing evaluation (n = 50), these trained models obtained a corresponding BAcc of 0.69/0.69/0.92, 0.76/0.80/0.89 and 0.58/0.73/0.72. CONCLUSION: In patients with a lung cancer treated with RT, radiomic features extracted from 3D dose maps seem to surpass usual models based on clinical factors and DVHs for the prediction of APT and LPT.",2021,10.1016/j.radonc.2020.10.040,,,,
Radiomics analysis of pulmonary nodules in low-dose CT for early detection of lung cancer,"PURPOSE: To develop a radiomics prediction model to improve pulmonary nodule (PN) classification in low-dose CT. To compare the model with the American College of Radiology (ACR) Lung CT Screening Reporting and Data System (Lung-RADS) for early detection of lung cancer. METHODS: We examined a set of 72 PNs (31 benign and 41 malignant) from the Lung Image Database Consortium image collection (LIDC-IDRI). One hundred three CT radiomic features were extracted from each PN. Before the model building process, distinctive features were identified using a hierarchical clustering method. We then constructed a prediction model by using a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). A tenfold cross-validation (CV) was repeated ten times (10 × 10-fold CV) to evaluate the accuracy of the SVM-LASSO model. Finally, the best model from the 10 × 10-fold CV was further evaluated using 20 × 5- and 50 × 2-fold CVs. RESULTS: The best SVM-LASSO model consisted of only two features: the bounding box anterior-posterior dimension (BB_AP) and the standard deviation of inverse difference moment (SD_IDM). The BB_AP measured the extension of a PN in the anterior-posterior direction and was highly correlated (r = 0.94) with the PN size. The SD_IDM was a texture feature that measured the directional variation of the local homogeneity feature IDM. Univariate analysis showed that both features were statistically significant and discriminative (P = 0.00013 and 0.000038, respectively). PNs with larger BB_AP or smaller SD_IDM were more likely malignant. The 10 × 10-fold CV of the best SVM model using the two features achieved an accuracy of 84.6% and 0.89 AUC. By comparison, Lung-RADS achieved an accuracy of 72.2% and 0.77 AUC using four features (size, type, calcification, and spiculation). The prediction improvement of SVM-LASSO comparing to Lung-RADS was statistically significant (McNemar's test P = 0.026). Lung-RADS misclassified 19 cases because it was mainly based on PN size, whereas the SVM-LASSO model correctly classified 10 of these cases by combining a size (BB_AP) feature and a texture (SD_IDM) feature. The performance of the SVM-LASSO model was stable when leaving more patients out with five- and twofold CVs (accuracy 84.1% and 81.6%, respectively). CONCLUSION: We developed an SVM-LASSO model to predict malignancy of PNs with two CT radiomic features. We demonstrated that the model achieved an accuracy of 84.6%, which was 12.4% higher than Lung-RADS.",2018,10.1002/mp.12820,cross-sectional,diagnosis,CT,Lungs
Radiomics analysis using stability selection supervised component analysis for right-censored survival data,"Radiomics is a newly emerging field that involves the extraction of massive quantitative features from biomedical images by using data-characterization algorithms. Distinctive imaging features identified from biomedical images can be used for prognosis and therapeutic response prediction, and they can provide a noninvasive approach for personalized therapy. So far, many of the published radiomics studies utilize existing out of the box algorithms to identify the prognostic markers from biomedical images that are not specific to radiomics data. To better utilize biomedical images, we propose a novel machine learning approach, stability selection supervised principal component analysis (SSSuperPCA) that identifies stable features from radiomics big data coupled with dimension reduction for right-censored survival outcomes. The proposed approach allows us to identify a set of stable features that are highly associated with the survival outcomes in a simple yet meaningful manner, while controlling the per-family error rate. We evaluate the performance of SSSuperPCA using simulations and real data sets for non-small cell lung cancer and head and neck cancer, and compare it with other machine learning algorithms. The results demonstrate that our method has a competitive edge over other existing methods in identifying the prognostic markers from biomedical imaging data for the prediction of right-censored survival outcomes.",2020,10.1016/j.compbiomed.2020.103959,cross-sectional,diagnosis,CT,Lungs
Radiomics and gene expression profile to characterise the disease and predict outcome in patients with lung cancer,"OBJECTIVE: The objectives of our study were to assess the association of radiomic and genomic data with histology and patient outcome in non-small cell lung cancer (NSCLC). METHODS: In this retrospective single-centre observational study, we selected 151 surgically treated patients with adenocarcinoma or squamous cell carcinoma who performed baseline [18F] FDG PET/CT. A subgroup of patients with cancer tissue samples at the Institutional Biobank (n = 74/151) was included in the genomic analysis. Features were extracted from both PET and CT images using an in-house tool. The genomic analysis included detection of genetic variants, fusion transcripts, and gene expression. Generalised linear model (GLM) and machine learning (ML) algorithms were used to predict histology and tumour recurrence. RESULTS: Standardised uptake value (SUV) and kurtosis (among the PET and CT radiomic features, respectively), and the expression of TP63, EPHA10, FBN2, and IL1RAP were associated with the histotype. No correlation was found between radiomic features/genomic data and relapse using GLM. The ML approach identified several radiomic/genomic rules to predict the histotype successfully. The ML approach showed a modest ability of PET radiomic features to predict relapse, while it identified a robust gene expression signature able to predict patient relapse correctly. The best-performing ML radiogenomic rule predicting the outcome resulted in an area under the curve (AUC) of 0.87. CONCLUSIONS: Radiogenomic data may provide clinically relevant information in NSCLC patients regarding the histotype, aggressiveness, and progression. Gene expression analysis showed potential new biomarkers and targets valuable for patient management and treatment. The application of ML allows to increase the efficacy of radiogenomic analysis and provides novel insights into cancer biology.",2021,10.1007/s00259-021-05371-7,retrospective cohort,diagnosis,PET - CT,Lungs
Radiomics Approach to Prediction of Occult Mediastinal Lymph Node Metastasis of Lung Adenocarcinoma,"OBJECTIVE: The purpose of this study was to evaluate the prognostic impact of radiomic features from CT scans in predicting occult mediastinal lymph node (LN) metastasis of lung adenocarcinoma. MATERIALS AND METHODS: A total of 492 patients with lung adenocarcinoma who underwent preoperative unenhanced chest CT were enrolled in the study. A total of 300 radiomics features quantifying tumor intensity, texture, and wavelet were extracted from the segmented entire-tumor volume of interest of the primary tumor. A radiomics signature was generated by use of the relief-based feature method and the support vector machine classification method. A ROC regression curve was drawn for the predictive performance of radiomics features. Multivariate logistic regression models based on clinicopathologic and radiomics features were compared for discriminating mediastinal LN metastasis. RESULTS: Clinical variables (sex, tumor diameter, tumor location) and predominant subtype were risk factors for pathologic mediastinal LN metastasis. The accuracy of radiomics signature for predicting mediastinal LN metastasis was 91.1% in ROC analysis (AUC, 0.972; sensitivity, 94.8%; specificity, 92%). Radiomics signature (Akaike information criterion [AIC] value, 80.9%) showed model fit superior to that of the clinicohistopathologic model (AIC value, 61.1%) for predicting mediastinal LN metastasis. CONCLUSION: The radiomics signature of a primary tumor based on CT scans can be used for quantitative and noninvasive prediction of occult mediastinal LN metastasis of lung adenocarcinoma.",2018,10.2214/ajr.17.19074,cross-sectional,diagnosis,CT,Lungs
Radiomics combined with clinical characteristics predicted the progression-free survival time in first-line targeted therapy for advanced non-small cell lung cancer with EGFR mutation,"OBJECTIVE: This study was to explore the most appropriate radiomics modeling method to predict the progression-free survival of EGFR-TKI treatment in advanced non-small cell lung cancer with EGFR mutations. Different machine learning methods may vary considerably and the selection of a proper model is essential for accurate treatment outcome prediction. Our study were established 176 discrimination models constructed with 22 feature selection methods and 8 classifiers. The predictive performance of each model were evaluated using the AUC, ACC, sensitivity and specificity, where the optimal model was identified. RESULTS: There were totally 107 radiomics features and 7 clinical features obtained from each patient. After feature selection, the top-ten most relevant features were fed to train 176 models. Significant performance variations were observed in the established models, with the best performance achieved by the logistic regression model using gini-index feature selection (AUC = 0.797, ACC = 0.722, sensitivity = 0.758, specificity = 0.693). The median R-score was 0.518 (IQR, 0.023-0.987), and the patients were divided into high-risk and low-risk groups based on this cut-off value. The KM survival curves of the two groups demonstrated evident stratification results (p = 0.000).",2022,10.1186/s13104-022-06019-x,cross-sectional,diagnosis,CT,Lungs
"Radiomics complements clinical, radiological, and technical features to assess local control of colorectal cancer lung metastases treated with radiofrequency ablation","OBJECTIVES: Radiofrequency ablation (RFA) of lung metastases of colorectal origin can improve patient survival and quality of life. Our aim was to identify pre- and per-RFA features predicting local control of lung metastases following RFA. METHODS: This case-control single-center retrospective study included 119 lung metastases treated with RFA in 48 patients (median age: 60 years). Clinical, technical, and radiological data before and on early CT scan (at 48 h) were retrieved. After CT scan preprocessing, 64 radiomics features were extracted from pre-RFA and early control CT scans. Log-rank tests were used to detect categorical variables correlating with post-RFA local tumor progression-free survival (LTPFS). Radiomics prognostic scores (RPS) were developed on reproducible radiomics features using Monte-Carlo cross-validated LASSO Cox regressions. RESULTS: Twenty-six of 119 (21.8%) nodules demonstrated local progression (median delay: 11.2 months). In univariate analysis, four non-radiomics variables correlated with post-RFA-LTPFS: nodule size (> 15 mm, p < 0.001), chosen electrode (with difference between covered array and nodule diameter < 20 mm or non-expandable electrode, p = 0.03), per-RFA intra-alveolar hemorrhage (IAH, p = 0.002), and nodule location into the ablation zone (not seen or in contact with borders, p = 0.005). The highest prognostic performance was reached with the multivariate model including a RPS built on 4 radiomics features from pre-RFA and early revaluation CT scans (cross-validated concordance index= 0.74) in which this RPS remained an independent predictor (cross-validated HR = 3.49, 95% confidence interval = [1.76 - 6.96]). CONCLUSIONS: Technical, radiological, and radiomics features of the lung metastases before RFA and of the ablation zone at 48 h can help discriminate nodules at risk of local progression that could benefit from complementary local procedure. KEY POINTS: • The highest prognostic performance to predict post-RFA LTPFS was reached with a parsimonious model including a radiomics score built with 4 radiomics features. • Nodule size, difference between electrode diameter, use of non-expandable electrode, per-RFA hemorrhage, and a tumor not seen or in contact with the ablation zone borders at 48-h CT were correlated with post-RFA LTPFS.",2021,10.1007/s00330-021-07998-4,case control,diagnosis,CT,Lungs
Radiomics for Classification of Lung Cancer Histological Subtypes Based on Nonenhanced Computed Tomography,"OBJECTIVES: To evaluate the performance of using radiomics method to classify lung cancer histological subtypes based on nonenhanced computed tomography images. MATERIALS AND METHODS: 278 patients with pathologically confirmed lung cancer were collected, including 181 nonsmall cell lung cancer (NSCLC) and 97 small cell lung cancers (SCLC) patients. Among the NSCLC patients, 88 patients were adenocarcinomas (AD) and 93 patients were squamous cell carcinomas (SCC). In total, 1695 quantitative radiomic features (QRF) were calculated from the primary lung cancer tumor in each patient. To build radiomic classification model based on the extracted QRFs, several machine-learning algorithms were applied sequentially. First, unsupervised hierarchical clustering was used to exclude highly correlated QRFs; second, the minimum Redundancy Maximum Relevance feature selection algorithm was employed to select informative and nonredundant QRFs; finally, the Incremental Forward Search and Support Vector Machine classification algorithms were used to combine the selected QRFs and build the model. In our work, to study the phenotypic differences among lung cancer histological subtypes, four classification models were built. They were models of SCLC vs NSCLC, SCLC vs AD, SCLC vs SCC, and AD vs SCC. The performance of the classification models was evaluated by the area under the receiver operating characteristic curve (AUC) estimated by three-fold cross-validation. RESULTS: The AUC (95% confidence interval) for the model of SCLC vs NSCLC was 0.741(0.678, 0.795). For the models of SCLC vs AD and SCLC vs SCC, the AUCs were 0.822(0.755, 0.875) and 0.665(0.583, 0.738), respectively. The AUC for the model of AD vs SCC was 0.655(0.570, 0.731). Several QRFs (""Law_15,"" ""LoG_Uniformity,"" ""GLCM_Contrast,"" and ""Compactness Factor"") that characterize tumor heterogeneity and shape were selected as the significant features to build the models. CONCLUSION: Our results show that phenotypic differences exist among different lung cancer histological subtypes on nonenhanced computed tomography image.",2019,10.1016/j.acra.2018.10.013,cross-sectional,diagnosis,CT,Lungs
Radiomics for lung adenocarcinoma manifesting as pure ground-glass nodules: invasive prediction,"OBJECTIVES: To investigate the value of radiomics based on CT imaging in predicting invasive adenocarcinoma manifesting as pure ground-glass nodules (pGGNs). METHODS: This study enrolled 395 pGGNs with histopathology-confirmed benign nodules or adenocarcinoma. A total of 396 radiomic features were extracted from each labeled nodule. A Rad-score was constructed with the least absolute shrinkage and selection operator (LASSO) in the training set. Multivariate logistic regression analysis was conducted to establish the radiographic model and the combined radiographic-radiomics model. The predictive performance was validated by receiver operating characteristic (ROC) curve. Based on the multivariate logistic regression analysis, an individual prediction nomogram was developed and the clinical utility was assessed. RESULTS: Five radiomic features and four radiographic features were selected for predicting the invasive lesions. The combined radiographic-radiomics model (AUC 0.77; 95% CI, 0.69-0.86) performed better than the radiographic model (AUC 0.71; 95% CI, 0.62-0.81) and Rad-score (AUC 0.72; 95% CI, 0.63-0.81) in the validation set. The clinical utility of the individualized prediction nomogram developed using the Rad-score, margin, spiculation, and size was confirmed in the validation set. The decision curve analysis (DCA) indicated that using a model with Rad-score to predict the invasive lesion would be more beneficial than that without Rad-score and the clinical model. CONCLUSIONS: The proposed radiomics-based nomogram that incorporated the Rad-score, margin, spiculation, and size may be utilized as a noninvasive biomarker for the assessment of invasive prediction in patients with pGGNs. KEY POINTS: • CT-based radiomics analysis helps invasive prediction manifested as pGGNs. • The combined radiographic-radiomics model may be utilized as a noninvasive biomarker for predicting invasive lesion for pGGNs. • Radiomics-based individual nomogram may serve as a vital decision support tool to identify invasive pGGNs, obviating further workup and blind follow-up.",2020,10.1007/s00330-020-06776-y,cross-sectional,diagnosis,CT,Nodules
Radiomics is feasible for prediction of spread through air spaces in patients with nonsmall cell lung cancer,"Tumor spread through air spaces (STAS) in non-small-cell lung cancer (NSCLC) is known to influence a poor patient outcome, even in patients presenting with early-stage disease. However, the pre-operative diagnosis of STAS remains challenging. With the progress of radiomics-based analyses several attempts have been made to predict STAS based on radiological findings. In the present study, patients with NSCLC which is located peripherally and tumors ≤ 2 cm in size on computed tomography (CT) that were potential candidates for sublobar resection were enrolled in this study. The radiologic features of the targeted tumors on thin-section CT were extracted using the PyRadiomics v3.0 software package, and a predictive model for STAS was built using the t-test and XGBoost. Thirty-five out of 226 patients had a STAS histology. The predictive model of STAS indicated an area under the receiver-operator characteristic curve (AUC) of 0.77. There was no significant difference in the overall survival (OS) for lobectomy between the predicted-STAS (+) and (-) groups (p = 0.19), but an unfavorable OS for sublobar resection was indicated in the predicted-STAS (+) group (p < 0.01). These results suggest that radiomics with machine-learning helped to develop a favorable model of STAS (+) NSCLC, which might be useful for the proper selection of candidates who should undergo sublobar resection.",2021,10.1038/s41598-021-93002-4,cross-sectional,prognosis,CT,Lungs
Radiomics nomogram for preoperative differentiation of lung tuberculoma from adenocarcinoma in solitary pulmonary solid nodule,"PURPOSE: To investigate the preoperative differential diagnostic performance of a radiomics nomogram in tuberculous granuloma (TBG) and lung adenocarcinoma (LAC) appearing as solitary pulmonary solid nodules (SPSNs). METHOD: We retrospectively recruited 426 patients with SPSNs from two centers and assigned them to training (n = 123), internal validation (n = 121), and external validation cohorts (n = 182). A model of deep learning (DL) was built for tumor segmentation from routine computed tomography (CT) images and extraction of 3D radiomics features. We used the least absolute shrinkage and selection operator (LASSO) logistic regression to build a radiomics signature. A clinical model was developed with clinical factors, including age, gender, and CT-based subjective findings (eg, lesion size, lesion location, lesion margin, lobulated sharp, and spiculation sign). We constructed individualized radiomics nomograms incorporating the radiomics signature and clinical factors to validate the diagnostic ability. RESULTS: Three factors - radiomics signature, age, and spiculation sign - were found to be independent predictors and were used to build the radiomics nomogram, which showed better diagnostic accuracy than any single model (all net reclassification improvement p < 0.05). The area under curve yielded was 0.9660 (95% confidence interval [CI], 0.9390-0.9931), 0.9342 (95% CI, 0.8944-0.9739), and 0.9064 (95% CI, 0.8639-0.9490) for the training, internal validation, and external validation cohorts, respectively. Decision curve analysis (DCA) and stratification analysis showed the nomogram has potential for generalizability. CONCLUSION: The radiomics nomogram we developed can preoperatively distinguish between LAC and TBG in patient with a SPSN.",2020,10.1016/j.ejrad.2020.109022,cross-sectional,diagnosis,CT,Lungs
Radiomics of (18)F-FDG PET/CT images predicts clinical benefit of advanced NSCLC patients to checkpoint blockade immunotherapy,"INTRODUCTION: Immunotherapy has improved outcomes for patients with non-small cell lung cancer (NSCLC), yet durable clinical benefit (DCB) is experienced in only a fraction of patients. Here, we test the hypothesis that radiomics features from baseline pretreatment (18)F-FDG PET/CT scans can predict clinical outcomes of NSCLC patients treated with checkpoint blockade immunotherapy. METHODS: This study included 194 patients with histologically confirmed stage IIIB-IV NSCLC with pretreatment PET/CT images. Radiomics features were extracted from PET, CT, and PET+CT fusion images based on minimum Kullback-Leibler divergence (KLD) criteria. The radiomics features from 99 retrospective patients were used to train a multiparametric radiomics signature (mpRS) to predict DCB using an improved least absolute shrinkage and selection operator (LASSO) method, which was subsequently validated in both retrospective (N = 47) and prospective test cohorts (N = 48). Using these cohorts, the mpRS was also used to predict progression-free survival (PFS) and overall survival (OS) by training nomogram models using multivariable Cox regression analyses with additional clinical characteristics incorporated. RESULTS: The mpRS could predict patients who will receive DCB, with areas under receiver operating characteristic curves (AUCs) of 0.86 (95%CI 0.79-0.94), 0.83 (95%CI 0.71-0.94), and 0.81 (95%CI 0.68-0.92) in the training, retrospective test, and prospective test cohorts, respectively. In the same three cohorts, respectively, nomogram models achieved C-indices of 0.74 (95%CI 0.68-0.80), 0.74 (95%CI 0.66-0.82), and 0.77 (95%CI 0.69-0.84) to predict PFS and C-indices of 0.83 (95%CI 0.77-0.88), 0.83 (95%CI 0.71-0.94), and 0.80 (95%CI 0.69-0.91) to predict OS. CONCLUSION: PET/CT-based signature can be used prior to initiation of immunotherapy to identify NSCLC patients most likely to benefit from immunotherapy. As such, these data may be leveraged to improve more precise and individualized decision support in the treatment of patients with advanced NSCLC.",2020,10.1007/s00259-019-04625-9,prospective cohort,diagnosis,PET - CT,Lungs
Radiomics outperforms semantic features for prediction of response to stereotactic radiosurgery in brain metastases,"BACKGROUND: Brain metastases show different patterns of contrast enhancement, potentially reflecting hypoxic and necrotic tumor regions with reduced radiosensitivity. An objective evaluation of these patterns might allow a prediction of response to radiotherapy. We therefore investigated the potential of MRI radiomics in comparison with the visual assessment of semantic features to predict early response to stereotactic radiosurgery in patients with brain metastases. PATIENTS AND METHODS: In this retrospective study, 150 patients with 308 brain metastases from solid tumors (NSCLC in 53% of patients) treated by stereotactic radiosurgery (single dose of 17-20 Gy) were evaluated. The response of each metastasis (partial or complete remission vs. stabilization or progression) was assessed within 180 days after radiosurgery. Patterns of contrast enhancement in the pre-treatment T1-weighted MR images were either visually classified (homogenous, heterogeneous, necrotic ring-like) or subjected to a radiomics analysis. Random forest models were optimized by cross-validation and evaluated in a hold-out test data set (30% of metastases). RESULTS: In total, 221/308 metastases (72%) responded to radiosurgery. The optimal radiomics model comprised 10 features and outperformed the model solely based on semantic features in the test data set (AUC, 0.71 vs. 0.56; accuracy, 69% vs. 54%). The diagnostic performance could be further improved by combining semantic and radiomics features resulting in an AUC of 0.74 and an accuracy of 75% in the test data set. CONCLUSION: The developed radiomics model allowed prediction of early response to radiosurgery in patients with brain metastases and outperformed the visual assessment of patterns of contrast enhancement.",2022,10.1016/j.radonc.2021.11.010,,,,
Radiomics-based features for pattern recognition of lung cancer histopathology and metastases,"BACKGROUND AND OBJECTIVES: lung cancer is the leading cause of cancer-related deaths in the world, and its poor prognosis varies markedly according to tumor staging. Computed tomography (CT) is the imaging modality of choice for lung cancer evaluation, being used for diagnosis and clinical staging. Besides tumor stage, other features, like histopathological subtype, can also add prognostic information. In this work, radiomics-based CT features were used to predict lung cancer histopathology and metastases using machine learning models. METHODS: local image datasets of confirmed primary malignant pulmonary tumors were retrospectively evaluated for testing and validation. CT images acquired with same protocol were semiautomatically segmented. Tumors were characterized by clinical features and computer attributes of intensity, histogram, texture, shape, and volume. Three machine learning classifiers used up to 100 selected features to perform the analysis. RESULTS: radiomics-based features yielded areas under the receiver operating characteristic curve of 0.89, 0.97, and 0.92 at testing and 0.75, 0.71, and 0.81 at validation for lymph nodal metastasis, distant metastasis, and histopathology pattern recognition, respectively. CONCLUSIONS: the radiomics characterization approach presented great potential to be used in a computational model to aid lung cancer histopathological subtype diagnosis as a ""virtual biopsy"" and metastatic prediction for therapy decision support without the necessity of a whole-body imaging scanning.",2018,10.1016/j.cmpb.2018.02.015,cross-sectional,diagnosis,CT,Lungs
"Radiomics-based machine learning differentiates ""ground-glass"" opacities due to COVID-19 from acute non-COVID-19 lung disease","Ground-glass opacities (GGOs) are a non-specific high-resolution computed tomography (HRCT) finding tipically observed in early Coronavirus disesase 19 (COVID-19) pneumonia. However, GGOs are also seen in other acute lung diseases, thus making challenging the differential diagnosis. To this aim, we investigated the performance of a radiomics-based machine learning method to discriminate GGOs due to COVID-19 from those due to other acute lung diseases. Two sets of patients were included: a first set of 28 patients (COVID) diagnosed with COVID-19 infection confirmed by real-time polymerase chain reaction (RT-PCR) between March and April 2020 having (a) baseline HRCT at hospital admission and (b) predominant GGOs pattern on HRCT; a second set of 30 patients (nCOVID) showing (a) predominant GGOs pattern on HRCT performed between August 2019 and April 2020 and (b) availability of final diagnosis. Two readers independently segmented GGOs on HRCTs using a semi-automated approach, and radiomics features were extracted using a standard open source software (PyRadiomics). Partial least square (PLS) regression was used as the multivariate machine-learning algorithm. A leave-one-out nested cross-validation was implemented. PLS β-weights of radiomics features, including the 5% features with the largest β-weights in magnitude (top 5%), were obtained. The diagnostic performance of the radiomics model was assessed through receiver operating characteristic (ROC) analysis. The Youden's test assessed sensitivity and specificity of the classification. A null hypothesis probability threshold of 5% was chosen (p < 0.05). The predictive model delivered an AUC of 0.868 (Youden's index = 0.68, sensitivity = 93%, specificity 75%, p = 4.2 × 10(-7)). Of the seven features included in the top 5% features, five were texture-related. A radiomics-based machine learning signature showed the potential to accurately differentiate GGOs due to COVID-19 pneumonia from those due to other acute lung diseases. Most of the discriminant radiomics features were texture-related. This approach may assist clinician to adopt the appropriate management early, while improving the triage of patients.",2021,10.1038/s41598-021-96755-0,cross-sectional,diagnosis,CT,Lungs
Radiomics-based machine-learning method for prediction of distant metastasis from soft-tissue sarcomas,"AIM: To construct and validate a radiomics-based machine-learning method for preoperative prediction of distant metastasis (DM) from soft-tissue sarcoma. MATERIALS AND METHODS: Seventy-seven soft-tissue sarcomas were divided into a training set (n=54) and a validation set (n=23). The performance of three feature selection methods (ReliefF, least absolute shrinkage and selection operator [LASSO], and regularised discriminative feature selection for unsupervised learning [UDFS]) and four classifiers, random forest (RF), logistic regression (LOG), K nearest neighbour (KNN), and support vector machines (SVMs), were compared for predicting the likelihood of DM. To counter the imbalance in the frequencies of DM, each machine-learning method was trained first without subsampling, then with the synthetic minority oversampling technique (SMOTE). The performance of the radiomics model was assessed using area under the receiver-operating characteristic curve (AUC) and accuracy (ACC) values. RESULTS: The performance of the LASSO and SVM algorithm combination used with SMOTE was superior to that of the algorithm combination alone. The combination of SMOTE with feature screening by LASSO and SVM classifiers had an AUC of 0.9020 and ACC of 91.30% in the validation dataset. CONCLUSION: A machine-learning model based on radiomics was favourable for predicting the likelihood of DM from soft-tissue sarcoma. This will help decide treatment strategies.",2021,10.1016/j.crad.2020.08.038,,,,
Radiomics-based prediction for tumour spread through air spaces in stage I lung adenocarcinoma using machine learning,"OBJECTIVES: As evidence has proven that sublobar resection is oncologically contraindicated by tumour spread through air spaces (STAS), its preoperative recognition is vital in customizing surgical strategies. We aimed to assess the value of radiomics in predicting STAS in stage I lung adenocarcinoma. METHODS: We retrospectively reviewed the patients with stage I lung adenocarcinoma, who accepted curative resection in our institution between January 2011 and December 2013. Using 'PyRadiomics' package, 88 radiomics features were extracted from computed tomography (CT) images and a prediction model was consequently constructed using Naïve Bayes machine-learning approach. The accuracy of the model was assessed through receiver operating curve analysis, and the performance of the model was validated both internally and externally. RESULTS: A total of 233 patients were included as the training cohort with 69 (29.6%) patients being STAS (+). Patients with STAS had worse recurrence-free survival and overall survival (P < 0.001). After feature extraction, 5 most contributing radiomics features were selected out to develop a Naïve Bayes model. In the internal validation, the model exhibited good performance with an area under the curve value of 0.63 (0.55-0.71). External validation was conducted on a test cohort with 112 patients and produced an area under the curve value of 0.69. CONCLUSIONS: CT-based radiomics is valuable in preoperatively predicting STAS in stage I lung adenocarcinoma, which may aid surgeons in determining the optimal surgical approach.",2020,10.1093/ejcts/ezaa011,cross-sectional,diagnosis,CT,Lungs
Radiomics-based Prognosis Analysis for Non-Small Cell Lung Cancer,"Radiomics characterizes tumor phenotypes by extracting large numbers of quantitative features from radiological images. Radiomic features have been shown to provide prognostic value in predicting clinical outcomes in several studies. However, several challenges including feature redundancy, unbalanced data, and small sample sizes have led to relatively low predictive accuracy. In this study, we explore different strategies for overcoming these challenges and improving predictive performance of radiomics-based prognosis for non-small cell lung cancer (NSCLC). CT images of 112 patients (mean age 75 years) with NSCLC who underwent stereotactic body radiotherapy were used to predict recurrence, death, and recurrence-free survival using a comprehensive radiomics analysis. Different feature selection and predictive modeling techniques were used to determine the optimal configuration of prognosis analysis. To address feature redundancy, comprehensive analysis indicated that Random Forest models and Principal Component Analysis were optimum predictive modeling and feature selection methods, respectively, for achieving high prognosis performance. To address unbalanced data, Synthetic Minority Over-sampling technique was found to significantly increase predictive accuracy. A full analysis of variance showed that data endpoints, feature selection techniques, and classifiers were significant factors in affecting predictive accuracy, suggesting that these factors must be investigated when building radiomics-based predictive models for cancer prognosis.",2017,10.1038/srep46349,retrospective cohort,prognosis,CT,Lungs
Radiomics-guided deep neural networks stratify lung adenocarcinoma prognosis from CT scans,"Deep learning (DL) is a breakthrough technology for medical imaging with high sample size requirements and interpretability issues. Using a pretrained DL model through a radiomics-guided approach, we propose a methodology for stratifying the prognosis of lung adenocarcinomas based on pretreatment CT. Our approach allows us to apply DL with smaller sample size requirements and enhanced interpretability. Baseline radiomics and DL models for the prognosis of lung adenocarcinomas were developed and tested using local (n = 617) cohort. The DL models were further tested in an external validation (n = 70) cohort. The local cohort was divided into training and test cohorts. A radiomics risk score (RRS) was developed using Cox-LASSO. Three pretrained DL networks derived from natural images were used to extract the DL features. The features were further guided using radiomics by retaining those DL features whose correlations with the radiomics features were high and Bonferroni-corrected p-values were low. The retained DL features were subject to a Cox-LASSO when constructing DL risk scores (DRS). The risk groups stratified by the RRS and DRS showed a significant difference in training, testing, and validation cohorts. The DL features were interpreted using existing radiomics features, and the texture features explained the DL features well.",2021,10.1038/s42003-021-02814-7,retrospective cohort,prognosis,CT,Lungs
RadTranslate: An Artificial Intelligence-Powered Intervention for Urgent Imaging to Enhance Care Equity for Patients With Limited English Proficiency During the COVID-19 Pandemic,"PURPOSE: Disproportionally high rates of coronavirus disease 2019 (COVID-19) have been noted among communities with limited English proficiency, resulting in an unmet need for improved multilingual care and interpreter services. To enhance multilingual care, the authors created a freely available web application, RadTranslate, that provides multilingual radiology examination instructions. The purpose of this study was to evaluate the implementation of this intervention in radiology. METHODS: The device-agnostic web application leverages artificial intelligence text-to-speech technology to provide standardized, human-like spoken examination instructions in the patient's preferred language. Standardized phrases were collected from a consensus group consisting of technologists, radiologists, and ancillary staff members. RadTranslate was piloted in Spanish for chest radiography performed at a COVID-19 triage outpatient center that served a predominantly Spanish-speaking Latino community. Implementation included a tablet displaying the application in the chest radiography room. Imaging appointment duration was measured and compared between pre- and postimplementation groups. RESULTS: In the 63-day test period after launch, there were 1,267 application uses, with technologists voluntarily switching exclusively to RadTranslate for Spanish-speaking patients. The most used phrases were a general explanation of the examination (30% of total), followed by instructions to disrobe and remove any jewelry (12%). There was no significant difference in imaging appointment duration (11 ± 7 and 12 ± 3 min for standard of care versus RadTranslate, respectively), but variability was significantly lower when RadTranslate was used (P = .003). CONCLUSIONS: Artificial intelligence-aided multilingual audio instructions were successfully integrated into imaging workflows, reducing strain on medical interpreters and variance in throughput and resulting in more reliable average examination length.",2021,10.1016/j.jacr.2021.01.013,,,,
Rapid 4D-MRI reconstruction using a deep radial convolutional neural network: Dracula,"BACKGROUND AND PURPOSE: 4D and midposition MRI could inform plan adaptation in lung and abdominal MR-guided radiotherapy. We present deep learning-based solutions to overcome long 4D-MRI reconstruction times while maintaining high image quality and short scan times. METHODS: Two 3D U-net deep convolutional neural networks were trained to accelerate the 4D joint MoCo-HDTV reconstruction. For the first network, gridded and joint MoCo-HDTV-reconstructed 4D-MRI were used as input and target data, respectively, whereas the second network was trained to directly calculate the midposition image. For both networks, input and target data had dimensions of 256 × 256 voxels (2D) and 16 respiratory phases. Deep learning-based MRI were verified against joint MoCo-HDTV-reconstructed MRI using the structural similarity index (SSIM) and the naturalness image quality evaluator (NIQE). Moreover, two experienced observers contoured the gross tumour volume and scored the images in a blinded study. RESULTS: For 12 subjects, previously unseen by the networks, high-quality 4D and midposition MRI (1.25 × 1.25 × 3.3 mm(3)) were each reconstructed from gridded images in only 28 seconds per subject. Excellent agreement was found between deep-learning-based and joint MoCo-HDTV-reconstructed MRI (average SSIM ≥ 0.96, NIQE scores 7.94 and 5.66). Deep-learning-based 4D-MRI were clinically acceptable for target and organ-at-risk delineation. Tumour positions agreed within 0.7 mm on midposition images. CONCLUSION: Our results suggest that the joint MoCo-HDTV and midposition algorithms can each be approximated by a deep convolutional neural network. This rapid reconstruction of 4D and midposition MRI facilitates online treatment adaptation in thoracic or abdominal MR-guided radiotherapy.",2021,10.1016/j.radonc.2021.03.034,cross-sectional,informatics,MRI,Lungs
Rapid Retrieval of Lung Nodule CT Images Based on Hashing and Pruning Methods,"The similarity-based retrieval of lung nodule computed tomography (CT) images is an important task in the computer-aided diagnosis of lung lesions. It can provide similar clinical cases for physicians and help them make reliable clinical diagnostic decisions. However, when handling large-scale lung images with a general-purpose computer, traditional image retrieval methods may not be efficient. In this paper, a new retrieval framework based on a hashing method for lung nodule CT images is proposed. This method can translate high-dimensional image features into a compact hash code, so the retrieval time and required memory space can be reduced greatly. Moreover, a pruning algorithm is presented to further improve the retrieval speed, and a pruning-based decision rule is presented to improve the retrieval precision. Finally, the proposed retrieval method is validated on 2,450 lung nodule CT images selected from the public Lung Image Database Consortium (LIDC) database. The experimental results show that the proposed pruning algorithm effectively reduces the retrieval time of lung nodule CT images and improves the retrieval precision. In addition, the retrieval framework is evaluated by differentiating benign and malignant nodules, and the classification accuracy can reach 86.62%, outperforming other commonly used classification methods.",2016,10.1155/2016/3162649,cross-sectional,informatics,CT,Lungs
Rapidly deploying a COVID-19 decision support system in one of the largest Brazilian hospitals,"The COVID-19 pandemic generated research interest in automated models to perform classification and segmentation from medical imaging of COVID-19 patients, However, applications in real-world scenarios are still needed. We describe the development and deployment of COVID-19 decision support and segmentation system. A partnership with a Brazilian radiologist consortium, gave us access to 1000s of labeled computed tomography (CT) and X-ray images from São Paulo Hospitals. The system used EfficientNet and EfficientDet networks, state-of-the-art convolutional neural networks for natural images classification and segmentation, in a real-time scalable scenario in communication with a Picture Archiving and Communication System (PACS). Additionally, the system could reject non-related images, using header analysis and classifiers. We achieved CT and X-ray classification accuracies of 0.94 and 0.98, respectively, and Dice coefficient for lung and covid findings segmentations of 0.98 and 0.73, respectively. The median response time was 7 s for X-ray and 4 min for CT.",2021,10.1177/14604582211033017,cross-sectional,diagnosis,CXR - CT,Lungs
RCoNet: Deformable Mutual Information Maximization and High-Order Uncertainty-Aware Learning for Robust COVID-19 Detection,"The novel 2019 Coronavirus (COVID-19) infection has spread worldwide and is currently a major healthcare challenge around the world. Chest computed tomography (CT) and X-ray images have been well recognized to be two effective techniques for clinical COVID-19 disease diagnoses. Due to faster imaging time and considerably lower cost than CT, detecting COVID-19 in chest X-ray (CXR) images is preferred for efficient diagnosis, assessment, and treatment. However, considering the similarity between COVID-19 and pneumonia, CXR samples with deep features distributed near category boundaries are easily misclassified by the hyperplanes learned from limited training data. Moreover, most existing approaches for COVID-19 detection focus on the accuracy of prediction and overlook uncertainty estimation, which is particularly important when dealing with noisy datasets. To alleviate these concerns, we propose a novel deep network named RCoNet (k)(s) for robust COVID-19 detection which employs Deformable Mutual Information Maximization (DeIM), Mixed High-order Moment Feature (MHMF), and Multiexpert Uncertainty-aware Learning (MUL). With DeIM, the mutual information (MI) between input data and the corresponding latent representations can be well estimated and maximized to capture compact and disentangled representational characteristics. Meanwhile, MHMF can fully explore the benefits of using high-order statistics and extract discriminative features of complex distributions in medical imaging. Finally, MUL creates multiple parallel dropout networks for each CXR image to evaluate uncertainty and thus prevent performance degradation caused by the noise in the data. The experimental results show that RCoNet (k)(s) achieves the state-of-the-art performance on an open-source COVIDx dataset of 15 134 original CXR images across several metrics. Crucially, our method is shown to be more effective than existing methods with the presence of noise in the data.",2021,10.1109/tnnls.2021.3086570,cross-sectional,diagnosis,CXR ,Lungs
Re-Identification and growth detection of pulmonary nodules without image registration using 3D siamese neural networks,"Lung cancer follow-up is a complex, error prone, and time consuming task for clinical radiologists. Several lung CT scan images taken at different time points of a given patient need to be individually inspected, looking for possible cancerogenous nodules. Radiologists mainly focus their attention in nodule size, density, and growth to assess the existence of malignancy. In this study, we present a novel method based on a 3D siamese neural network, for the re-identification of nodules in a pair of CT scans of the same patient without the need for image registration. The network was integrated into a two-stage automatic pipeline to detect, match, and predict nodule growth given pairs of CT scans. Results on an independent test set reported a nodule detection sensitivity of 94.7%, an accuracy for temporal nodule matching of 88.8%, and a sensitivity of 92.0% with a precision of 88.4% for nodule growth detection.",2021,10.1016/j.media.2020.101823,cross-sectional,diagnosis,CT,Lungs
Real-time markerless tumour tracking with patient-specific deep learning using a personalised data generation strategy: proof of concept by phantom study,"OBJECTIVE: For real-time markerless tumour tracking in stereotactic lung radiotherapy, we propose a different approach which uses patient-specific deep learning (DL) using a personalised data generation strategy, avoiding the need for collection of a large patient data set. We validated our strategy with digital phantom simulation and epoxy phantom studies. METHODS: We developed lung tumour tracking for radiotherapy using a convolutional neural network trained for each phantom's lesion by using multiple digitally reconstructed radiographs (DRRs) generated from each phantom's treatment planning four-dimensional CT. We trained tumour-bone differentiation using large numbers of training DRRs generated with various projection geometries to simulate tumour motion. We solved the problem of using DRRs for training and X-ray images for tracking using the training DRRs with random contrast transformation and random noise addition. RESULTS: We defined adequate tracking accuracy as the percentage frames satisfying <1 mm tracking error of the isocentre. In the simulation study, we achieved 100% tracking accuracy in 3 cm spherical and 1.5×2.25×3 cm ovoid masses. In the phantom study, we achieved 100 and 94.7% tracking accuracy in 3 cm and 2 cm spherical masses, respectively. This required 32.5 ms/frame (30.8 fps) real-time processing. CONCLUSIONS: We proved the potential feasibility of a real-time markerless tumour tracking framework for stereotactic lung radiotherapy based on patient-specific DL with personalised data generation with digital phantom and epoxy phantom studies. ADVANCES IN KNOWLEDGE: Using DL with personalised data generation is an efficient strategy for real-time lung tumour tracking.",2020,10.1259/bjr.20190420,cross-sectional,diagnosis,DRR,Lungs
Real-time tumor tracking using fluoroscopic imaging with deep neural network analysis,"PURPOSE: To improve respiratory gating accuracy and treatment throughput, we developed a fluoroscopic markerless tumor tracking algorithm based on a deep neural network (DNN). METHODS: In the learning stage, target positions were projected onto digitally reconstructed radiography (DRR) images from four-dimensional computed tomography (4DCT). DRR images were cropped into subimages of the target or surrounding regions to build a network that takes input of the image pattern of subimages and produces a target probability map (TPM) for estimating the target position. Using multiple subimages, a DNN was trained to generate a TPM based on the target position projected onto the DRRs. In the tracking stage, the network takes in the subimages cropped from fluoroscopic images at the same position of the subimages on the DRRs and produces TPMs, which are used to estimate target positions. We integrated the lateral correction to modify an estimated target position by using a linear regression model. We tracked five lung and five liver cases, and calculated tracking accuracy (Euclidian distance in 3D space) by subtracting the estimated position from the reference. RESULTS: Tracking accuracy averaged over all patients was 1.64 ± 0.73 mm. Accuracy for liver cases (1.37 ± 0.81 mm) was better than that for lung cases (1.90 ± 0.65 mm). Computation time was <40 ms for a pair of fluoroscopic images. CONCLUSIONS: Our markerless tracking algorithm successfully estimated tumor positions. We believe our results will provide useful information to advance tumor tracking technology.",2019,10.1016/j.ejmp.2019.02.006,cross-sectional,diagnosis,DRR,Lungs
Recognition of Abnormal Chest Compression Depth Using One-Dimensional Convolutional Neural Networks,"When the displacement of an object is evaluated using sensor data, its movement back to the starting point can be used to correct the measurement error of the sensor. In medicine, the movements of chest compressions also involve a reciprocating movement back to the starting point. The traditional method of evaluating the effects of chest compression depth (CCD) is to use an acceleration sensor or gyroscope to obtain chest compression movement data; from these data, the displacement value can be calculated and the CCD effect evaluated. However, this evaluation procedure suffers from sensor errors and environmental interference, limiting its applicability. Our objective is to reduce the auxiliary computing devices employed for CCD effectiveness evaluation and improve the accuracy of the evaluation results. To this end, we propose a one-dimensional convolutional neural network (1D-CNN) classification method. First, we use the chest compression evaluation criterion to classify the pre-collected sensor signal data, from which the proposed 1D-CNN model learns classification features. After training, the model is used to classify and evaluate sensor signal data instead of distance measurements; this effectively avoids the influence of pressure occlusion and electromagnetic waves. We collect and label 937 valid CCD results from an emergency care simulator. In addition, the proposed 1D-CNN structure is experimentally evaluated and compared against other CNN models and support vector machines. The results show that after sufficient training, the proposed 1D-CNN model can recognize the CCD results with an accuracy rate of more than 95%. The execution time suggests that the model balances accuracy and hardware requirements and can be embedded in portable devices.",2021,10.3390/s21030846,,,,
Recognition of COVID-19 from CT Scans Using Two-Stage Deep-Learning-Based Approach: CNR-IEMN,"Since the appearance of the COVID-19 pandemic (at the end of 2019, Wuhan, China), the recognition of COVID-19 with medical imaging has become an active research topic for the machine learning and computer vision community. This paper is based on the results obtained from the 2021 COVID-19 SPGC challenge, which aims to classify volumetric CT scans into normal, COVID-19, or community-acquired pneumonia (Cap) classes. To this end, we proposed a deep-learning-based approach (CNR-IEMN) that consists of two main stages. In the first stage, we trained four deep learning architectures with a multi-tasks strategy for slice-level classification. In the second stage, we used the previously trained models with an XG-boost classifier to classify the whole CT scan into normal, COVID-19, or Cap classes. Our approach achieved a good result on the validation set, with an overall accuracy of 87.75% and 96.36%, 52.63%, and 95.83% sensitivities for COVID-19, Cap, and normal, respectively. On the other hand, our approach achieved fifth place on the three test datasets of SPGC in the COVID-19 challenge, where our approach achieved the best result for COVID-19 sensitivity. In addition, our approach achieved second place on two of the three testing sets.",2021,10.3390/s21175878,cross-sectional,diagnosis,CT,Lungs
Recognition of Lung Adenocarcinoma-specific Gene Pairs Based on Genetic Algorithm and Establishment of a Deep Learning Prediction Model,"AIM AND OBJECTIVE: Lung cancer is a disease with a dismal prognosis and is the major cause of cancer deaths in many countries. Nonetheless, rapid technological developments in genome science guarantees more effective prevention and treatment strategies. MATERIALS AND METHODS: In this study, genes were pair-matched and screened for lung adenocarcinomaspecific gene relationships. False positives due to fluctuations in single gene expression were avoided and the stability and accuracy of the results was improved. RESULTS: Finally, a deep learning model was constructed with machine learning algorithm to realize the clinical diagnosis of lung adenocarcinoma in patients. CONCLUSION: Comparing with the traditional methods which takes ingle gene as a feature, the relative difference between gene pairs is a higher order feature, leverage high-order features to build the model can avoid instability caused by a single gene mutation, making the prediction results more reliable.",2019,10.2174/1386207322666190530102245,,,,
Recognition of Peripheral Lung Cancer and Focal Pneumonia on Chest Computed Tomography Images Based on Convolutional Neural Network,"Introduction: Chest computed tomography (CT) is important for the early screening of lung diseases and clinical diagnosis, particularly during the COVID-19 pandemic. We propose a method for classifying peripheral lung cancer and focal pneumonia on chest CT images and undertake 5 window settings to study the effect on the artificial intelligence processing results. Methods: A retrospective collection of CT images from 357 patients with peripheral lung cancer having solitary solid nodule or focal pneumonia with a solitary consolidation was applied. We segmented and aligned the lung parenchyma based on some morphological methods and cropped this region of the lung parenchyma with the minimum 3D bounding box. Using these 3D cropped volumes of all cases, we designed a 3D neural network to classify them into 2 categories. We also compared the classification results of the 3 physicians with different experience levels on the same dataset. Results: We conducted experiments using 5 window settings. After cropping and alignment based on an automatic preprocessing procedure, our neural network achieved an average classification accuracy of 91.596% under a 5-fold cross-validation in the full window, in which the area under the curve (AUC) was 0.946. The classification accuracy and AUC value were 90.48% and 0.957 for the junior physician, 94.96% and 0.989 for the intermediate physician, and 96.92% and 0.980 for the senior physician, respectively. After removing the error prediction, the accuracy improved significantly, reaching 98.79% in the self-defined window2. Conclusion: Using the proposed neural network, in separating peripheral lung cancer and focal pneumonia in chest CT data, we achieved an accuracy competitive to that of a junior physician. Through a data ablation study, the proposed 3D CNN can achieve a slightly higher accuracy compared with senior physicians in the same subset. The self-defined window2 was the best for data training and evaluation.",2022,10.1177/15330338221085375,cross-sectional,diagnosis,CT,Lungs
Recurrent attention network for false positive reduction in the detection of pulmonary nodules in thoracic CT scans,"PURPOSE: Multiview two-dimensional (2D) convolutional neural networks (CNNs) and three-dimensional (3D) CNNs have been successfully used for analyzing volumetric data in many state-of-the-art medical imaging applications. We propose an alternative modular framework that analyzes volumetric data with an approach that is analogous to radiologists' interpretation, and apply the framework to reduce false positives that are generated in computer-aided detection (CADe) systems for pulmonary nodules in thoracic computed tomography (CT) scans. METHODS: In our approach, a deep network consisting of 2D CNNs first processes slices individually. The features extracted in this stage are then passed to a recurrent neural network (RNN), thereby modeling consecutive slices as a sequence of temporal data and capturing the contextual information across all three dimensions in the volume of interest. Outputs of the RNN layer are weighed before the final fully connected layer, enabling the network to scale the importance of different slices within a volume of interest in an end-to-end training framework. RESULTS: We validated the proposed architecture on the false positive reduction track of the lung nodule analysis (LUNA) challenge for pulmonary nodule detection in chest CT scans, and obtained competitive results compared to 3D CNNs. Our results show that the proposed approach can encode the 3D information in volumetric data effectively by achieving a sensitivity >0.8 with just 1/8 false positives per scan. CONCLUSIONS: Our experimental results demonstrate the effectiveness of temporal analysis of volumetric images for the application of false positive reduction in chest CT scans and show that state-of-the-art 2D architectures from the literature can be directly applied to analyzing volumetric medical data. As newer and better 2D architectures are being developed at a much faster rate compared to 3D architectures, our approach makes it easy to obtain state-of-the-art performance on volumetric data using new 2D architectures.",2020,10.1002/mp.14076,cross-sectional,diagnosis,CT,Lungs
Recurrent feature fusion learning for multi-modality pet-ct tumor segmentation,"BACKGROUND AND OBJECTIVE: [18f]-fluorodeoxyglucose (fdg) positron emission tomography - computed tomography (pet-ct) is now the preferred imaging modality for staging many cancers. Pet images characterize tumoral glucose metabolism while ct depicts the complementary anatomical localization of the tumor. Automatic tumor segmentation is an important step in image analysis in computer aided diagnosis systems. Recently, fully convolutional networks (fcns), with their ability to leverage annotated datasets and extract image feature representations, have become the state-of-the-art in tumor segmentation. There are limited fcn based methods that support multi-modality images and current methods have primarily focused on the fusion of multi-modality image features at various stages, i.e., early-fusion where the multi-modality image features are fused prior to fcn, late-fusion with the resultant features fused and hyper-fusion where multi-modality image features are fused across multiple image feature scales. Early- and late-fusion methods, however, have inherent, limited freedom to fuse complementary multi-modality image features. The hyper-fusion methods learn different image features across different image feature scales that can result in inaccurate segmentations, in particular, in situations where the tumors have heterogeneous textures. METHODS: we propose a recurrent fusion network (rfn), which consists of multiple recurrent fusion phases to progressively fuse the complementary multi-modality image features with intermediary segmentation results derived at individual recurrent fusion phases: (1) the recurrent fusion phases iteratively learn the image features and then refine the subsequent segmentation results; and, (2) the intermediary segmentation results allows our method to focus on learning the multi-modality image features around these intermediary segmentation results, which minimize the risk of inconsistent feature learning. RESULTS: we evaluated our method on two pathologically proven non-small cell lung cancer pet-ct datasets. We compared our method to the commonly used fusion methods (early-fusion, late-fusion and hyper-fusion) and the state-of-the-art pet-ct tumor segmentation methods on various network backbones (resnet, densenet and 3d-unet). Our results show that the rfn provides more accurate segmentation compared to the existing methods and is generalizable to different datasets. CONCLUSIONS: we show that learning through multiple recurrent fusion phases allows the iterative re-use of multi-modality image features that refines tumor segmentation results. We also identify that our rfn produces consistent segmentation results across different network architectures.",2021,10.1016/j.cmpb.2021.106043,cross-sectional,diagnosis,PET - CT,Lungs
Reducing False-Positives in Lung Nodules Detection Using Balanced Datasets,"Malignant pulmonary nodules are one of the main manifestations of lung cancer in early CT image screening. Since lung cancer may have no early obvious symptoms, it is important to develop a computer-aided detection (CAD) system to assist doctors to detect the malignant pulmonary nodules in the early stage of lung cancer CT diagnosis. Due to the recent successful applications of deep learning in image processing, more and more researchers have been trying to apply it to the diagnosis of pulmonary nodules. However, due to the ratio of nodules and non-nodules samples used in the training and testing datasets usually being different from the practical ratio of lung cancer, the CAD classification systems may easily produce higher false-positives while using this imbalanced dataset. This work introduces a filtering step to remove the irrelevant images from the dataset, and the results show that the false-positives can be reduced and the accuracy can be above 98%. There are two steps in nodule detection. Firstly, the images with pulmonary nodules are screened from the whole lung CT images of the patients. Secondly, the exact locations of pulmonary nodules will be detected using Faster R-CNN. Final results show that this method can effectively detect the pulmonary nodules in the CT images and hence potentially assist doctors in the early diagnosis of lung cancer.",2021,10.3389/fpubh.2021.671070,cross-sectional,diagnosis,CT,Lungs
Registration-based lung mechanical analysis of chronic obstructive pulmonary disease (COPD) using a supervised machine learning framework,"RATIONALE AND OBJECTIVES: This study evaluated the performance of computed tomography (CT)-derived biomechanical based features of lung function and the presence and severity of chronic obstructive pulmonary disease (COPD). It performed well when compared to CT-derived density and textural features of lung function and the presence and severity of COPD. MATERIALS AND METHODS: A total of 162 subjects (Global Initiative for Chronic Obstructive Lung Disease [GOLD] stages 0-4 and nonsmokers) subjects with CT scan performed at total lung capacity or expiration to functional residual capacity were evaluated. CT-derived biomechanical, density, and textural feature sets were compared to forced expiratory volume in 1 second (FEV1)%, FEV1/forced vital capacity, and total St. George's respiratory questionnaire scores. The ability of these feature sets to assess the presence and severity of COPD was also evaluated. Optimal features are selected by linear forward feature selection and the classification is done using k nearest neighbor learning algorithm. RESULTS: The proposed biomechanical features showed good correlations with the pulmonary function tests and health status metrics. In COPD versus non-COPD classification, biomechanical feature set achieved an area under the curve (AUC) of 0.85 performing well in comparison to density (AUC = 0.83) and texture (AUC = 0.89) feature sets. Classifying the subjects into the severity of GOLD stage using biomechanical features (AUC = 0.81) performed better than the density- and texture-based feature sets, AUC = 0.76 and 0.73, respectively. The biomechanical features performed better alone than in combination with the other two feature sets. CONCLUSION: This study shows the effectiveness of CT-derived biomechanical measures in the assessment of airflow obstruction and quality of life in subjects with COPD. CT-derived biomechanical features performed well in assessing the presence and severity of COPD.",2013,10.1016/j.acra.2013.01.019,cross-sectional,diagnosis,CT,Lungs
Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation in CT Scans,"Pulmonary lobe segmentation in computed tomography scans is essential for regional assessment of pulmonary diseases. Recent works based on convolution neural networks have achieved good performance for this task. However, they are still limited in capturing structured relationships due to the nature of convolution. The shape of the pulmonary lobes affect each other and their borders relate to the appearance of other structures, such as vessels, airways, and the pleural wall. We argue that such structural relationships play a critical role in the accurate delineation of pulmonary lobes when the lungs are affected by diseases such as COVID-19 or COPD. In this paper, we propose a relational approach (RTSU-Net) that leverages structured relationships by introducing a novel non-local neural network module. The proposed module learns both visual and geometric relationships among all convolution features to produce self-attention weights. With a limited amount of training data available from COVID-19 subjects, we initially train and validate RTSU-Net on a cohort of 5000 subjects from the COPDGene study (4000 for training and 1000 for evaluation). Using models pre-trained on COPDGene, we apply transfer learning to retrain and evaluate RTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation). Experimental results show that RTSU-Net outperforms three baselines and performs robustly on cases with severe lung infection due to COVID-19.",2020,10.1109/tmi.2020.2995108,cross-sectional,diagnosis,CT,Lungs
Repeatability and reproducibility study of radiomic features on a phantom and human cohort,"The repeatability and reproducibility of radiomic features extracted from CT scans need to be investigated to evaluate the temporal stability of imaging features with respect to a controlled scenario (test-retest), as well as their dependence on acquisition parameters such as slice thickness, or tube current. Only robust and stable features should be used in prognostication/prediction models to improve generalizability across multiple institutions. In this study, we investigated the repeatability and reproducibility of radiomic features with respect to three different scanners, variable slice thickness, tube current, and use of intravenous (IV) contrast medium, combining phantom studies and human subjects with non-small cell lung cancer. In all, half of the radiomic features showed good repeatability (ICC > 0.9) independent of scanner model. Within acquisition protocols, changes in slice thickness was associated with poorer reproducibility compared to the use of IV contrast. Broad feature classes exhibit different behaviors, with only few features appearing to be the most stable. 108 features presented both good repeatability and reproducibility in all the experiments, most of them being wavelet and Laplacian of Gaussian features.",2021,10.1038/s41598-021-81526-8,,,,
Reproducibility of radiomic features in CT images of NSCLC patients: an integrative analysis on the impact of acquisition and reconstruction parameters,"BACKGROUND: We investigated to what extent tube voltage, scanner model, and reconstruction algorithm affect radiomic feature reproducibility in a single-institution retrospective database of computed tomography images of non-small-cell lung cancer patients. METHODS: This study was approved by the Institutional Review Board (UID 2412). Images of 103 patients were considered, being acquired on either among two scanners, at 100 or 120 kVp. For each patient, images were reconstructed with six iterative blending levels, and 1414 features were extracted from each reconstruction. At univariate analysis, Wilcoxon-Mann-Whitney test was applied to evaluate feature differences within scanners and voltages, whereas the impact of the reconstruction was established with the overall concordance correlation coefficient (OCCC). A multivariable mixed model was also applied to investigate the independent contribution of each acquisition/reconstruction parameter. Univariate and multivariable analyses were combined to analyse feature behaviour. RESULTS: Scanner model and voltage did not affect features significantly. The reconstruction blending level showed a significant impact at both univariate analysis (154/1414 features yielding an OCCC < 0.85) and multivariable analysis, with most features (1042/1414) revealing a systematic trend with the blending level (multiple comparisons adjusted p < 0.05). Reproducibility increased in association to image processing with smooth filters, nonetheless specific investigation in relation to clinical endpoints should be performed to ensure that textural information is not removed. CONCLUSIONS: Combining univariate and multivariable models is allowed to identify features for which corrections may be applied to reduce the trend with the algorithm and increase reproducibility. Subsequent clustering may be applied to eliminate residual redundancy.",2022,10.1186/s41747-021-00258-6,,,,
Reproducible Machine Learning Methods for Lung Cancer Detection Using Computed Tomography Images: Algorithm Development and Validation,"BACKGROUND: Chest computed tomography (CT) is crucial for the detection of lung cancer, and many automated CT evaluation methods have been proposed. Due to the divergent software dependencies of the reported approaches, the developed methods are rarely compared or reproduced. OBJECTIVE: The goal of the research was to generate reproducible machine learning modules for lung cancer detection and compare the approaches and performances of the award-winning algorithms developed in the Kaggle Data Science Bowl. METHODS: We obtained the source codes of all award-winning solutions of the Kaggle Data Science Bowl Challenge, where participants developed automated CT evaluation methods to detect lung cancer (training set n=1397, public test set n=198, final test set n=506). The performance of the algorithms was evaluated by the log-loss function, and the Spearman correlation coefficient of the performance in the public and final test sets was computed. RESULTS: Most solutions implemented distinct image preprocessing, segmentation, and classification modules. Variants of U-Net, VGGNet, and residual net were commonly used in nodule segmentation, and transfer learning was used in most of the classification algorithms. Substantial performance variations in the public and final test sets were observed (Spearman correlation coefficient = .39 among the top 10 teams). To ensure the reproducibility of results, we generated a Docker container for each of the top solutions. CONCLUSIONS: We compared the award-winning algorithms for lung cancer detection and generated reproducible Docker images for the top solutions. Although convolutional neural networks achieved decent accuracy, there is plenty of room for improvement regarding model generalizability.",2020,10.2196/16709,cross-sectional,diagnosis,CT,Lungs
Res-trans networks for lung nodule classification,"PURPOSE: Lung cancer usually presents as pulmonary nodules on early diagnostic images, and accurately estimating the malignancy of pulmonary nodules is crucial to the prevention and diagnosis of lung cancer. Recently, deep learning algorithms based on convolutional neural networks have shown potential for pulmonary nodules classification. However, the size of the nodules is very diverse, ranging from 3 to 30 mm, which makes classifying them to be a challenging task. In this study, we propose a novel architecture called Res-trans networks to classify nodules in computed tomography (CT) scans. METHODS: We designed local and global blocks to extract features that capture the long-range dependencies between pixels to adapt to the correct classification of lung nodules of different sizes. Specifically, we designed residual blocks with convolutional operations to extract local features and transformer blocks with self-attention to capture global features. Moreover, the Res-trans network has a sequence fusion block that aggregates and extracts the sequence feature information output by the transformer block that improves classification accuracy. RESULTS: Our proposed method is extensively evaluated on the public LIDC-IDRI dataset, which contains 1,018 CT scans. A tenfold cross-validation result shows that our method obtains better performance with AUC = 0.9628 and Accuracy = 0.9292 compared with recently leading methods. CONCLUSION: In this paper, a network that can capture local and global features is proposed to classify nodules in chest CT. Experimental results show that our proposed method has better classification performance and can help radiologists to accurately analyze lung nodules.",2022,10.1007/s11548-022-02576-5,case control,diagnosis,CT,Lungs
Research on Lung Ultrasound Image Classification Based on Compressed Sensing,"Pneumothorax is a common injury in disaster rescue, traffic accidents, and war trauma environments and requires early diagnosis and treatment. The commonly used X-ray, CT, and other diagnostic instruments are not suitable for rescue sites due to their large size, heavy weight, and difficulty in transportation. Ultrasound equipment is easy to carry and suitable for rescue environments. However, ultrasound images are noisy, have low resolution, and are difficult to get started, which affects the efficiency of diagnosis. This paper studies the effect of lung ultrasound image recognition and classification based on compressed sensing and BP neural network. We use ultrasound equipment to build a lung simulation model, collect five typical features of lung ultrasound images in M-mode, and build a dataset. Using compressed sensing theory, we design sparse matrix and observation matrix and perform data compression on the image data in the dataset to obtain observation values. We design a BP neural network, input the observations into the network for training, and compare it with the commonly used VGG16 network. The method proposed in this paper has higher recognition accuracy and significantly fewer parameters than VGG16, so it is suitable for use in embedded devices.",2022,10.1155/2022/1414723,cross-sectional,diagnosis,Ultrasound,Lungs
Resolution-based distillation for efficient histology image classification,"Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classification. The proposed approach is robust when used with images that have reduced input resolution, and it can be trained effectively with limited labeled data. Moreover, our approach operates at either the tissue- or slide-level, removing the need for laborious patch-level labeling. Our method uses knowledge distillation to transfer knowledge from a teacher model pre-trained at high resolution to a student model trained on the same images at a considerably lower resolution. Also, to address the lack of large-scale labeled histology image datasets, we perform the knowledge distillation in a self-supervised fashion. We evaluate our approach on three distinct histology image datasets associated with celiac disease, lung adenocarcinoma, and renal cell carcinoma. Our results on these datasets demonstrate that a combination of knowledge distillation and self-supervision allows the student model to approach and, in some cases, surpass the teacher model's classification accuracy while being much more computationally efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential for this method to scale further with additional unlabeled data. Our model outperforms the high-resolution teacher model for celiac disease in accuracy, F1-score, precision, and recall while requiring 4 times fewer computations. For lung adenocarcinoma, our results at 1.25× magnification are within 1.5% of the results for the teacher model at 10× magnification, with a reduction in computational cost by a factor of 64. Our model on renal cell carcinoma at 1.25× magnification performs within 1% of the teacher model at 5× magnification while requiring 16 times fewer computations. Furthermore, our celiac disease outcomes benefit from additional performance scaling with the use of more unlabeled data. In the case of 0.625× magnification, using unlabeled data improves accuracy by 4% over the tissue-level baseline. Therefore, our approach can improve the feasibility of deep learning solutions for digital pathology on standard computational hardware and infrastructures.",2021,10.1016/j.artmed.2021.102136,,,,
Review of Texture Quantification of CT Images for Classification of Lung Diseases,"Computer-based identification of abnormal regions and classification of diseases using CT images of the lung has been a goal of many investigators. In this paper, we review research that has used texture analysis along with segmentation and fractal analysis. First, a review of texture methods is performed. Recent research on quantitative analysis of the lung using texture methods is categorized into six groups of computational methods: structural, statistical, model based, transform domain, texture-segmentation, and texture-fractal analysis. Finally, the applications of texture-based methods combined with either segmentation algorithms or fractal analysis is evaluated on lung CT images from patients with diseases such as emphysema, COPD, and cancer. We also discuss applications of artificial neural networks, support vector machine, k-nearest, and Bayesian methods to classify normal and diseased segments of CT images of the lung. A combination of these texture methods followed by classifiers could lead to efficient and accurate diagnosis of pulmonary diseases such as pulmonary fibrosis, emphysema, and cancer.",2015,10.1615/CritRevBiomedEng.2015011026,,,,
Revisiting segmentation of lung tumors from CT images,"Lung cancer is a leading cause of death throughout the world. Because the prompt diagnosis of tumors allows oncologists to discern their nature, type, and mode of treatment, tumor detection and segmentation from CT scan images is a crucial field of study. This paper investigates lung tumor segmentation via a two-dimensional Discrete Wavelet Transform (DWT) on the LOTUS dataset (31,247 training, and 4458 testing samples) and a Deeply Supervised MultiResUNet model. Coupling the DWT, which is used to achieve a more meticulous textural analysis while integrating information from neighboring CT slices, with the deep supervision of the model architecture results in an improved dice coefficient of 0.8472. A key characteristic of our approach is its avoidance of 3D kernels (despite being used for a 3D segmentation task), thereby making it quite lightweight.",2022,10.1016/j.compbiomed.2022.105385,case control,diagnosis,CT,Lungs
"Revisiting the levels of Aerosol Optical Depth in south-southeast Asia, Europe and USA amid the COVID-19 pandemic using satellite observations","The countries around the world are dealing with air quality issues for decades due to their mode of production and energy usages. The outbreak of COVID-19 as a pandemic and consequent global economic shutdown, for the first time, provided a base for the real-time experiment of the effect of reduced emissions across the globe in abetting the air pollution issue. The present study dealt with the changes in Aerosol Optical Depth (AOD), a marker of air pollution, because of global economic shutdown due to the coronavirus pandemic. The study considered the countries in south and south-east Asia (SSEA), Europe and the USA for their extended period of lockdown due to coronavirus pandemic. Daily Aerosol Optical Depth (AOD) from Moderate-resolution imaging spectroradiometer (MODIS) and tropospheric column density of NO(2) and SO(2) from Ozone monitoring instrument (OMI) sensors, including meteorological data such as wind speed (WS) and relative humidity (RH) were analyzed during the pre-lockdown (2017-2019) and lockdown periods (2020). The average AOD, NO(2) and SO(2) during the lockdown period were statistically compared with their pre-lockdown average using Wilcoxon-signed-paired-rank test. The accuracy of the MODIS-derived AOD, including the changing pattern of AOD due to lockdown was estimated using AERONET data. The weekly anomaly of AOD, NO(2) and SO(2) was used for analyzing the space-time variation of aerosol load as restrictions were imposed by the concerned countries at the different points of time. Additionally, a random forest-based regression (RF) model was used to examine the effects of meteorological and emission parameters on the spatial variation of AOD. A significant reduction of AOD (-20%) was obtained for majority of the areas in SSEA, Europe and USA during the lockdown period. Yet, the clusters of increased AOD (30-60%) was obtained in the south-east part of SSEA, the western part of Europe and US regions. NO(2) reductions were measured up to 20-40%, while SO(2) emission increased up to 30% for a majority of areas in these regions. A notable space-time variation was observed in weekly anomaly. We found the evidence of the formation of new particles for causing high AOD under high RH and low WS, aided by the downward vertical wind flow. The RF model showed a distinguishable relative importance of emission and meteorological factors among these regions to account for the spatial variability of AOD. Our findings suggest that the continued lockdown might provide a temporary solution to air pollution; however, to combat persistent air quality issues, it needs switching over to the cleaner mode of production and energy. The findings of this study, thus, advocated for alternative energy policy at the global scale.",2021,10.1016/j.envres.2020.110514,,,,
Risk prediction of pleural effusion in lung malignancy patients treated with CT-guided percutaneous microwave ablation: a nomogram and artificial neural network model,"OBJECTIVES: To develop an effective nomogram and artificial neural network (ANN) model for predicting pleural effusion after percutaneous microwave ablation (MWA) in lung malignancy (LM) patients. METHODS: LM patients treated with MWA were randomly allocated to either the training cohort or the validation cohort (7:3). The predictors of pleural effusion identified by univariable and multivariable analyses in the training cohort were used to develop a nomogram and ANN model. The C-statistic was used to evaluate the predictive accuracy in both the training and validation cohorts. RESULTS: A total of 496 patients (training cohort: n = 357; validation cohort: n = 139) were enrolled in this study. The predictors selected into the nomogram for pleural effusion included the maximum power (hazard ratio [HR], 1.060; 95% confidence interval [CI], 1.022-1.100, p = 0.002), the number of pleural punctures (HR, 2.280; 95% CI, 1.103-4.722; p = 0.026) and the minimum distance from needle to pleura (HR, 0.840; 95% CI, 0.775-0.899; p < 0.001). The C-statistic showed good predictive performance in both cohorts, with a C-statistic of 0.866 (95% CI, 0.787-0.945) internally and 0.782 (95% CI, 0.644-0.920) externally (training cohort and validation cohort, respectively). The optimal cutoff value for the risk of pleural effusion was 0.16. CONCLUSIONS: Maximum power, number of pleural punctures and minimum distance from needle to pleura were predictors of pleural effusion after MWA in LM patients. The nomogram and ANN model could effectively predict the risk of pleural effusion after MWA. Patients showing a high risk (>0.16) on the nomogram should be monitored for pleural effusion.",2021,10.1080/02656736.2021.1885755,cross-sectional,diagnosis,CT,Lungs
Robust Cell Detection and Segmentation in Histopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders,"Computer-aided diagnosis (CAD) is a promising tool for accurate and consistent diagnosis and prognosis. Cell detection and segmentation are essential steps for CAD. These tasks are challenging due to variations in cell shapes, touching cells, and cluttered background. In this paper, we present a cell detection and segmentation algorithm using the sparse reconstruction with trivial templates and a stacked denoising autoencoder (sDAE). The sparse reconstruction handles the shape variations by representing a testing patch as a linear combination of shapes in the learned dictionary. Trivial templates are used to model the touching parts. The sDAE, trained with the original data and their structured labels, is used for cell segmentation. To the best of our knowledge, this is the first study to apply sparse reconstruction and sDAE with structured labels for cell detection and segmentation. The proposed method is extensively tested on two data sets containing more than 3000 cells obtained from brain tumor and lung cancer images. Our algorithm achieves the best performance compared with other state of the arts.",2015,10.1007/978-3-319-24574-4_46,,,,
Robust feature selection to predict tumor treatment outcome,"OBJECTIVE: Recurrence of cancer after treatment increases the risk of death. The ability to predict the treatment outcome can help to design the treatment planning and can thus be beneficial to the patient. We aim to select predictive features from clinical and PET (positron emission tomography) based features, in order to provide doctors with informative factors so as to anticipate the outcome of the patient treatment. METHODS: In order to overcome the small sample size problem of datasets usually met in the medical domain, we propose a novel wrapper feature selection algorithm, named HFS (hierarchical forward selection), which searches forward in a hierarchical feature subset space. Feature subsets are iteratively evaluated with the prediction performance using SVM (support vector machine). All feature subsets performing better than those at the preceding iteration are retained. Moreover, as SUV (standardized uptake value) based features have been recognized as significant predictive factors for a patient outcome, we propose to incorporate this prior knowledge into the selection procedure to improve its robustness and reduce its computational cost. RESULTS: Two real-world datasets from cancer patients are included in the evaluation. We extract dozens of clinical and PET-based features to characterize the patient's state, including SUV parameters and texture features. We use leave-one-out cross-validation to evaluate the prediction performance, in terms of prediction accuracy and robustness. Using SVM as the classifier, our HFS method produces accuracy values of 100% and 94% on the two datasets, respectively, and robustness values of 89% and 96%. Without accuracy loss, the prior-based version (pHFS) improves the robustness up to 100% and 98% on the two datasets, respectively. CONCLUSIONS: Compared with other feature selection methods, the proposed HFS and pHFS provide the most promising results. For our HFS method, we have empirically shown that the addition of prior knowledge improves the robustness and accelerates the convergence.",2015,10.1016/j.artmed.2015.07.002,cross-sectional,diagnosis,CT,Lungs
Robust radiogenomics approach to the identification of EGFR mutations among patients with NSCLC from three different countries using topologically invariant Betti numbers,"OBJECTIVES: To propose a novel robust radiogenomics approach to the identification of epidermal growth factor receptor (EGFR) mutations among patients with non-small cell lung cancer (NSCLC) using Betti numbers (BNs). MATERIALS AND METHODS: Contrast enhanced computed tomography (CT) images of 194 multi-racial NSCLC patients (79 EGFR mutants and 115 wildtypes) were collected from three different countries using 5 manufacturers' scanners with a variety of scanning parameters. Ninety-nine cases obtained from the University of Malaya Medical Centre (UMMC) in Malaysia were used for training and validation procedures. Forty-one cases collected from the Kyushu University Hospital (KUH) in Japan and fifty-four cases obtained from The Cancer Imaging Archive (TCIA) in America were used for a test procedure. Radiomic features were obtained from BN maps, which represent topologically invariant heterogeneous characteristics of lung cancer on CT images, by applying histogram- and texture-based feature computations. A BN-based signature was determined using support vector machine (SVM) models with the best combination of features that maximized a robustness index (RI) which defined a higher total area under receiver operating characteristics curves (AUCs) and lower difference of AUCs between the training and the validation. The SVM model was built using the signature and optimized in a five-fold cross validation. The BN-based model was compared to conventional original image (OI)- and wavelet-decomposition (WD)-based models with respect to the RI between the validation and the test. RESULTS: The BN-based model showed a higher RI of 1.51 compared with the models based on the OI (RI: 1.33) and the WD (RI: 1.29). CONCLUSION: The proposed model showed higher robustness than the conventional models in the identification of EGFR mutations among NSCLC patients. The results suggested the robustness of the BN-based approach against variations in image scanner/scanning parameters.",2021,10.1371/journal.pone.0244354,cross-sectional,diagnosis,CT,Lungs
"Role of Hybrid Deep Neural Networks (HDNNs), Computed Tomography, and Chest X-rays for the Detection of COVID-19","COVID-19 syndrome has extensively escalated worldwide with the induction of the year 2020 and has resulted in the illness of millions of people. COVID-19 patients bear an elevated risk once the symptoms deteriorate. Hence, early recognition of diseased patients can facilitate early intervention and avoid disease succession. This article intends to develop a hybrid deep neural networks (HDNNs), using computed tomography (CT) and X-ray imaging, to predict the risk of the onset of disease in patients suffering from COVID-19. To be precise, the subjects were classified into 3 categories namely normal, Pneumonia, and COVID-19. Initially, the CT and chest X-ray images, denoted as 'hybrid images' (with resolution 1080 × 1080) were collected from different sources, including GitHub, COVID-19 radiography database, Kaggle, COVID-19 image data collection, and Actual Med COVID-19 Chest X-ray Dataset, which are open source and publicly available data repositories. The 80% hybrid images were used to train the hybrid deep neural network model and the remaining 20% were used for the testing purpose. The capability and prediction accuracy of the HDNNs were calculated using the confusion matrix. The hybrid deep neural network showed a 99% classification accuracy on the test set data.",2021,10.3390/ijerph18063056,cross-sectional,diagnosis,CXR - CT,Lungs
Saliva-based detection of COVID-19 infection in a real-world setting using reagent-free Raman spectroscopy and machine learning,"SIGNIFICANCE: The primary method of COVID-19 detection is reverse transcription polymerase chain reaction (RT-PCR) testing. PCR test sensitivity may decrease as more variants of concern arise and reagents may become less specific to the virus. AIM: We aimed to develop a reagent-free way to detect COVID-19 in a real-world setting with minimal constraints on sample acquisition. The machine learning (ML) models involved could be frequently updated to include spectral information about variants without needing to develop new reagents. APPROACH: We present a workflow for collecting, preparing, and imaging dried saliva supernatant droplets using a non-invasive, label-free technique-Raman spectroscopy-to detect changes in the molecular profile of saliva associated with COVID-19 infection. RESULTS: We used an innovative multiple instance learning-based ML approach and droplet segmentation to analyze droplets. Amongst all confounding factors, we discriminated between COVID-positive and COVID-negative individuals yielding receiver operating coefficient curves with an area under curve (AUC) of 0.8 in both males (79% sensitivity and 75% specificity) and females (84% sensitivity and 64% specificity). Taking the sex of the saliva donor into account increased the AUC by 5%. CONCLUSION: These findings may pave the way for new rapid Raman spectroscopic screening tools for COVID-19 and other infectious diseases.",2022,10.1117/1.Jbo.27.2.025002,,,,
SC-Dynamic R-CNN: A Self-Calibrated Dynamic R-CNN Model for Lung Cancer Lesion Detection,"Lung cancer has complex biological characteristics and a high degree of malignancy. It has always been the number one ""killer"" in cancer, threatening human life and health. The diagnosis and early treatment of lung cancer still require improvement and further development. With high morbidity and mortality, there is an urgent need for an accurate diagnosis method. However, the existing computer-aided detection system has a complicated process and low detection accuracy. To solve this problem, this paper proposed a two-stage detection method based on the dynamic region-based convolutional neural network (Dynamic R-CNN). We divide lung cancer into squamous cell carcinoma, adenocarcinoma, and small cell carcinoma. By adding the self-calibrated convolution module into the feature network, we extracted more abundant lung cancer features and proposed a new regression loss function to further improve the detection performance of lung cancer. After experimental verification, the mAP (mean average precision) of the model can reach 88.1% on the lung cancer dataset and it performed particularly well with a high IoU (intersection over union) threshold. This method has a good performance in the detection of lung cancer and can improve the efficiency of doctors' diagnoses. It can avoid false detection and miss detection to a certain extent.",2022,10.1155/2022/9452157,cross-sectional,diagnosis,CT,Lungs
Scattering features for lung cancer detection in fibered confocal fluorescence microscopy images,"OBJECTIVE: To assess the feasibility of lung cancer diagnosis using fibered confocal fluorescence microscopy (FCFM) imaging technique and scattering features for pattern recognition. METHODS: FCFM imaging technique is a new medical imaging technique for which interest has yet to be established for diagnosis. This paper addresses the problem of lung cancer detection using FCFM images and, as a first contribution, assesses the feasibility of computer-aided diagnosis through these images. Towards this aim, we have built a pattern recognition scheme which involves a feature extraction stage and a classification stage. The second contribution relies on the features used for discrimination. Indeed, we have employed the so-called scattering transform for extracting discriminative features, which are robust to small deformations in the images. We have also compared and combined these features with classical yet powerful features like local binary patterns (LBP) and their variants denoted as local quinary patterns (LQP). RESULTS: We show that scattering features yielded to better recognition performances than classical features like LBP and their LQP variants for the FCFM image classification problems. Another finding is that LBP-based and scattering-based features provide complementary discriminative information and, in some situations, we empirically establish that performance can be improved when jointly using LBP, LQP and scattering features. CONCLUSIONS: In this work we analyze the joint capability of FCFM images and scattering features for lung cancer diagnosis. The proposed method achieves a good recognition rate for such a diagnosis problem. It also performs well when used in conjunction with other features for other classical medical imaging classification problems.",2014,10.1016/j.artmed.2014.05.003,,,,
SCPM-Net: An anchor-free 3D lung nodule detection network using sphere representation and center points matching,"Automatic and accurate lung nodule detection from 3D Computed Tomography (CT) scans plays a vital role in efficient lung cancer screening. Despite the state-of-the-art performance obtained by recent anchor-based detectors using Convolutional Neural Networks (CNNs) for this task, they require predetermined anchor parameters such as the size, number, and aspect ratio of anchors, and have limited robustness when dealing with lung nodules with a massive variety of sizes. To overcome these problems, we propose a 3D sphere representation-based center-points matching detection network (SCPM-Net) that is anchor-free and automatically predicts the position, radius, and offset of nodules without manual design of nodule/anchor parameters. The SCPM-Net consists of two novel components: sphere representation and center points matching. First, to match the nodule annotation in clinical practice, we replace the commonly used bounding box with our proposed bounding sphere to represent nodules with the centroid, radius, and local offset in 3D space. A compatible sphere-based intersection over-union loss function is introduced to train the lung nodule detection network stably and efficiently. Second, we empower the network anchor-free by designing a positive center-points selection and matching (CPM) process, which naturally discards pre-determined anchor boxes. An online hard example mining and re-focal loss subsequently enable the CPM process to be more robust, resulting in more accurate point assignment and mitigation of class imbalance. In addition, to better capture spatial information and 3D context for the detection, we propose to fuse multi-level spatial coordinate maps with the feature extractor and combine them with 3D squeeze-and-excitation attention modules. Experimental results on the LUNA16 dataset showed that our proposed SCPM-Net framework achieves superior performance compared with existing anchor-based and anchor-free methods for lung nodule detection with the average sensitivity at 7 predefined FPs/scan of 89.2%. Moreover, our sphere representation is verified to achieve higher detection accuracy than the traditional bounding box representation of lung nodules. Code is available at: https://github.com/HiLab-git/SCPM-Net.",2022,10.1016/j.media.2021.102287,cross-sectional,diagnosis,CT,Lungs
Screening of COVID-19 based on the extracted radiomics features from chest CT images,"BACKGROUND AND OBJECTIVE: Radiomics has been widely used in quantitative analysis of medical images for disease diagnosis and prognosis assessment. The objective of this study is to test a machine-learning (ML) method based on radiomics features extracted from chest CT images for screening COVID-19 cases. METHODS: The study is carried out on two groups of patients, including 138 patients with confirmed and 140 patients with suspected COVID-19. We focus on distinguishing pneumonia caused by COVID-19 from the suspected cases by segmentation of whole lung volume and extraction of 86 radiomics features. Followed by feature extraction, nine feature-selection procedures are used to identify valuable features. Then, ten ML classifiers are applied to classify and predict COVID-19 cases. Each ML models is trained and tested using a ten-fold cross-validation method. The predictive performance of each ML model is evaluated using the area under the curve (AUC) and accuracy. RESULTS: The range of accuracy and AUC is from 0.32 (recursive feature elimination [RFE]+Multinomial Naive Bayes [MNB] classifier) to 0.984 (RFE+bagging [BAG], RFE+decision tree [DT] classifiers) and 0.27 (mutual information [MI]+MNB classifier) to 0.997 (RFE+k-nearest neighborhood [KNN] classifier), respectively. There is no direct correlation among the number of the selected features, accuracy, and AUC, however, with changes in the number of the selected features, the accuracy and AUC values will change. Feature selection procedure RFE+BAG classifier and RFE+DT classifier achieve the highest prediction accuracy (accuracy: 0.984), followed by MI+Gaussian Naive Bayes (GNB) and logistic regression (LGR)+DT classifiers (accuracy: 0.976). RFE+KNN classifier as a feature selection procedure achieve the highest AUC (AUC: 0.997), followed by RFE+BAG classifier (AUC: 0.991) and RFE+gradient boosting decision tree (GBDT) classifier (AUC: 0.99). CONCLUSION: This study demonstrates that the ML model based on RFE+KNN classifier achieves the highest performance to differentiate patients with a confirmed infection caused by COVID-19 from the suspected cases.",2021,10.3233/xst-200831,cross-sectional,diagnosis,CT,Lungs
"Season, not lockdown, improved air quality during COVID-19 State of Emergency in Nigeria","Globally, ambient air pollution claims ~9 million lives yearly, prompting researchers to investigate changes in air quality. Of special interest is the impact of COVID-19 lockdown. Many studies reported substantial improvements in air quality during lockdowns compared with pre-lockdown or as compared with baseline values. Since the lockdown period coincided with the onset of the rainy season in some tropical countries such as Nigeria, it is unclear if such improvements can be fully attributed to the lockdown. We investigate whether significant changes in air quality in Nigeria occurred primarily due to statewide COVID-19 lockdown. We applied a neural network approach to derive monthly average ground-level fine aerosol optical depth (AOD(f)) across Nigeria from year 2001-2020, using the Multi-angle Implementation of Atmospheric Correction (MAIAC) AODs from Terra and Aqua Moderate Resolution Imaging Spectroradiometer (MODIS) satellites, AERONET aerosol optical properties, meteorological and spatial parameters. During the year 2020, we found a 21% or 26% decline in average AOD(f) level across Nigeria during lockdown (April) as compared to pre-lockdown (March), or during the easing phase-1 (May) as compared to lockdown, respectively. Throughout the 20-year period, AOD(f) levels were highest in January and lowest in May or June, but not April. Comparison of AOD(f) levels between 2020 and 2019 shows a small decline (1%) in pollution level in April of 2020 compare to 2019. Using a linear time-lag model to compare changes in AOD(f) levels for similar months from 2002 to 2020, we found no significant difference (Levene's test and ANCOVA; α = 0.05) in the pollution levels by year, which indicates that the lockdown did not significantly improve air quality in Nigeria. Impact analysis using multiple linear regression revealed that favorable meteorological conditions due to seasonal change in temperature, relative humidity, planetary boundary layer height, wind speed and rainfall improved air quality during the lockdown.",2021,10.1016/j.scitotenv.2021.145187,,,,
Segmentation and suppression of pulmonary vessels in low-dose chest CT scans,"PURPOSE: The suppression of pulmonary vessels in chest computed tomography (CT) images can enhance the conspicuity of lung nodules, thereby improving the detection rate of early lung cancer. This study aimed to develop two key techniques in vessel suppression, that is, segmentation and removal of pulmonary vessels while preserving the nodules. METHODS: Pulmonary vessel segmentation and removal methods in CT images were developed. The vessel segmentation method used a framework of two cascaded convolutional neural networks (CNNs). A bi-class segmentation network was utilized in the first step to extract high-intensity structures, including both vessels and nonvascular tissues such as nodules. A tri-class segmentation network was employed in the second step to distinguish the vessels from nonvascular tissues (mainly nodules) and the lung parenchyma. In the vessel removal method, the voxels in the segmented vessels were replaced with randomly selected voxels from the surrounding lung parenchyma. The dataset in this study comprised 50 three-dimensional (3D) low-dose chest CT images. The labels for vessel and nodule segmentation were annotated with a semi automatic approach. The two cascaded networks for pulmonary vessel segmentation were trained with CT images of 40 cases and tested with CT images of ten cases. Pulmonary vessels were removed from the ten testing scans based on the predicted segmentation results. In addition to qualitative evaluation to the effects of segmentation and removal, the segmentation results were quantitatively evaluated using Dice coefficient (DICE), Jaccard index (JAC), and volumetric similarity (VS) and the removal results were evaluated using contrast-to-noise ratio (CNR). RESULTS: In the first step of vessel segmentation, the mean DICE, JAC, and VS for high-intensity tissues, including both vessels and nodules, were 0.943, 0.893, and 0.991, respectively. In the second step, all the nodules were separated from the vessels, and the mean DICE, JAC, and VS for the vessels were 0.941, 0.890, and 0.991, respectively. After vessel removal, the mean CNR for nodules was improved from 4.23 (6.26 dB) to 6.95 (8.42 dB). CONCLUSIONS: Quantitative and qualitative evaluations demonstrated that the proposed method achieved a high accuracy for pulmonary vessel segmentation and a good effect on pulmonary vessel suppression.",2019,10.1002/mp.13648,cross-sectional,diagnosis,CT,Lungs
Segmentation of CT Lung Images Using FCM with Active Contour and CNN Classifier,"OBJECTIVE: Lung cancer is one of the unsafe diseases for human which reduces the patient life time. Generally, most of the lung cancers are identified after it has been spread into the lung parts and moreover it is difficult to find the lung cancer at the early stage. It requires radiologist and special doctors to find the tumoral tissue of the lung cancer. For this reason, the recommended work helps to segment the tumoral tissue of CT lung image in an effective way. METHODS: The research work uses hybrid segmentation technique to separate the lung cancer cells to diagnose the lung tumour. It is a technique which combines active contour along with Fuzzy c means to diagnose the tumoral tissue. Further the segmented portion was trained by Convolutional Neural Network (CNN) in order to classify the segmented region as normal or abnormal. RESULTS: The evaluation of the proposed method was done by analyzing the results of test image with the ground truth image. Finally, the results of the implemented technique provided good accuracy, Peak signal to noise ratio (PSNR), Mean Square Error (MSE) value. In future the other techniques can be utilized to improve the details before segmentation. The proposed work provides 96.67 % accuracy. CONCLUSION: Hybrid segmentation technique involves several steps like preprocessing, binarization, thresholding, segmentation and feature extraction using GLCM.",2022,10.31557/apjcp.2022.23.3.905,cross-sectional,diagnosis,CT,Lungs
Segmentation of infected region in CT images of COVID-19 patients based on QC-HC U-net,"Since the outbreak of COVID-19 in 2019, the rapid spread of the epidemic has brought huge challenges to medical institutions. If the pathological region in the COVID-19 CT image can be automatically segmented, it will help doctors quickly determine the patient's infection, thereby speeding up the diagnosis process. To be able to automatically segment the infected area, we proposed a new network structure and named QC-HC U-Net. First, we combine residual connection and dense connection to form a new connection method and apply it to the encoder and the decoder. Second, we choose to add Hypercolumns in the decoder section. Compared with the benchmark 3D U-Net, the improved network can effectively avoid vanishing gradient while extracting more features. To improve the situation of insufficient data, resampling and data enhancement methods are selected in this paper to expand the datasets. We used 63 cases of MSD lung tumor data for training and testing, continuously verified to ensure the training effect of this model, and then selected 20 cases of public COVID-19 data for training and testing. Experimental results showed that in the segmentation of COVID-19, the specificity and sensitivity were 85.3% and 83.6%, respectively, and in the segmentation of MSD lung tumors, the specificity and sensitivity were 81.45% and 80.93%, respectively, without any fitting.",2021,10.1038/s41598-021-01502-0,cross-sectional,diagnosis,CT,Lungs
Segmentation of pulmonary nodules in computed tomography using a regression neural network approach and its application to the Lung Image Database Consortium and Image Database Resource Initiative dataset,"We present new pulmonary nodule segmentation algorithms for computed tomography (CT). These include a fully-automated (FA) system, a semi-automated (SA) system, and a hybrid system. Like most traditional systems, the new FA system requires only a single user-supplied cue point. On the other hand, the SA system represents a new algorithm class requiring 8 user-supplied control points. This does increase the burden on the user, but we show that the resulting system is highly robust and can handle a variety of challenging cases. The proposed hybrid system starts with the FA system. If improved segmentation results are needed, the SA system is then deployed. The FA segmentation engine has 2 free parameters, and the SA system has 3. These parameters are adaptively determined for each nodule in a search process guided by a regression neural network (RNN). The RNN uses a number of features computed for each candidate segmentation. We train and test our systems using the new Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) data. To the best of our knowledge, this is one of the first nodule-specific performance benchmarks using the new LIDC-IDRI dataset. We also compare the performance of the proposed methods with several previously reported results on the same data used by those other methods. Our results suggest that the proposed FA system improves upon the state-of-the-art, and the SA system offers a considerable boost over the FA system.",2015,10.1016/j.media.2015.02.002,cross-sectional,diagnosis,CT,Lungs
Segmenting lung tumors on longitudinal imaging studies via a patient-specific adaptive convolutional neural network,"PURPOSE: To design a deep learning algorithm that automatically delineates lung tumors seen on weekly magnetic resonance imaging (MRI) scans acquired during radiotherapy and facilitates the analysis of geometric tumor changes. METHODS: This longitudinal imaging study comprised 9 lung cancer patients who had 6-7 weekly T2-weighted MRI scans during radiotherapy. Tumors on all scans were manually contoured as the ground truth. Meanwhile, a patient-specific adaptive convolutional neural network (A-net) was developed to simulate the workflow of adaptive radiotherapy and to utilize past weekly MRI and tumor contours to segment tumors on the current weekly MRI. To augment the training data, each voxel inside the volume of interest was expanded to a 3 × 3 cm patch as the input, whereas the classification of the corresponding patch, background or tumor, was the output. Training was updated weekly to incorporate the latest MRI scan. For comparison, a population-based neural network was implemented, trained, and validated on the leave-one-out scheme. Both algorithms were evaluated by their precision, DICE coefficient, and root mean square surface distance between the manual and computerized segmentations. RESULTS: Training of A-net converged well within 2 h of computations on a computer cluster. A-net segmented the weekly MR with a precision, DICE, and root mean square surface distance of 0.81 ± 0.10, 0.82 ± 0.10, and 2.4 ± 1.4 mm, and outperformed the population-based algorithm with 0.63 ± 0.21, 0.64 ± 0.19, and 4.1 ± 3.0 mm, respectively. CONCLUSION: A-net can be feasibly integrated into the clinical workflow of a longitudinal imaging study and become a valuable tool to facilitate decision- making in adaptive radiotherapy.",2019,10.1016/j.radonc.2018.10.037,cross-sectional,diagnosis,CT,Lungs
"Selection, Visualization, and Interpretation of Deep Features in Lung Adenocarcinoma and Squamous Cell Carcinoma","Although deep learning networks applied to digital images have shown impressive results for many pathology-related tasks, their black-box approach and limitation in terms of interpretability are significant obstacles for their widespread clinical utility. This study investigates the visualization of deep features (DFs) to characterize two lung cancer subtypes, adenocarcinoma and squamous cell carcinoma. It demonstrates that a subset of DFs, called prominent DFs, can accurately distinguish these two cancer subtypes. Visualization of such individual DFs allows for a better understanding of histopathologic patterns at both the whole-slide and patch levels, and discrimination of these cancer types. These DFs were visualized at the whole slide image level through DF-specific heatmaps and at tissue patch level through the generation of activation maps. In addition, these prominent DFs can distinguish carcinomas of organs other than the lung. This framework may serve as a platform for evaluating the interpretability of any deep network for diagnostic decision making.",2021,10.1016/j.ajpath.2021.08.013,,,,
Self-Ensembling Co-Training Framework for Semi-Supervised COVID-19 CT Segmentation,"The coronavirus disease 2019 (COVID-19) has become a severe worldwide health emergency and is spreading at a rapid rate. Segmentation of COVID lesions from computed tomography (CT) scans is of great importance for supervising disease progression and further clinical treatment. As labeling COVID-19 CT scans is labor-intensive and time-consuming, it is essential to develop a segmentation method based on limited labeled data to conduct this task. In this paper, we propose a self-ensembled co-training framework, which is trained by limited labeled data and large-scale unlabeled data, to automatically extract COVID lesions from CT scans. Specifically, to enrich the diversity of unsupervised information, we build a co-training framework consisting of two collaborative models, in which the two models teach each other during training by using their respective predicted pseudo-labels of unlabeled data. Moreover, to alleviate the adverse impacts of noisy pseudo-labels for each model, we propose a self-ensembling strategy to perform consistency regularization for the up-to-date predictions of unlabeled data, in which the predictions of unlabeled data are gradually ensembled via moving average at the end of every training epoch. We evaluate our framework on a COVID-19 dataset containing 103 CT scans. Experimental results show that our proposed method achieves better performance in the case of only 4 labeled CT scans compared to the state-of-the-art semi-supervised segmentation networks.",2021,10.1109/jbhi.2021.3103646,cross-sectional,diagnosis,CT,Lungs
"Self-supervised deep learning model for COVID-19 lung CT image segmentation highlighting putative causal relationship among age, underlying disease and COVID-19","BACKGROUND: Coronavirus disease 2019 (COVID-19) is very contagious. Cases appear faster than the available Polymerase Chain Reaction test kits in many countries. Recently, lung computerized tomography (CT) has been used as an auxiliary COVID-19 testing approach. Automatic analysis of the lung CT images is needed to increase the diagnostic efficiency and release the human participant. Deep learning is successful in automatically solving computer vision problems. Thus, it can be introduced to the automatic and rapid COVID-19 CT diagnosis. Many advanced deep learning-based computer vison techniques were developed to increase the model performance but have not been introduced to medical image analysis. METHODS: In this study, we propose a self-supervised two-stage deep learning model to segment COVID-19 lesions (ground-glass opacity and consolidation) from chest CT images to support rapid COVID-19 diagnosis. The proposed deep learning model integrates several advanced computer vision techniques such as generative adversarial image inpainting, focal loss, and lookahead optimizer. Two real-life datasets were used to evaluate the model's performance compared to the previous related works. To explore the clinical and biological mechanism of the predicted lesion segments, we extract some engineered features from the predicted lung lesions. We evaluate their mediation effects on the relationship of age with COVID-19 severity, as well as the relationship of underlying diseases with COVID-19 severity using statistic mediation analysis. RESULTS: The best overall F1 score is observed in the proposed self-supervised two-stage segmentation model (0.63) compared to the two related baseline models (0.55, 0.49). We also identified several CT image phenotypes that mediate the potential causal relationship between underlying diseases with COVID-19 severity as well as the potential causal relationship between age with COVID-19 severity. CONCLUSIONS: This work contributes a promising COVID-19 lung CT image segmentation model and provides predicted lesion segments with potential clinical interpretability. The model could automatically segment the COVID-19 lesions from the raw CT images with higher accuracy than related works. The features of these lesions are associated with COVID-19 severity through mediating the known causal of the COVID-19 severity (age and underlying diseases).",2021,10.1186/s12967-021-02992-2,cross-sectional,diagnosis,CT,Lungs
Semantic segmentation and detection of mediastinal lymph nodes and anatomical structures in CT data for lung cancer staging,"PURPOSE: Accurate lung cancer diagnosis is crucial to select the best course of action for treating the patient. From a simple chest CT volume, it is necessary to identify whether the cancer has spread to nearby lymph nodes or not. It is equally important to know precisely where each malignant lymph node is with respect to the surrounding anatomical structures and the airways. In this paper, we introduce a new data-set containing annotations of fifteen different anatomical structures in the mediastinal area, including lymph nodes of varying sizes. We present a 2D pipeline for semantic segmentation and instance detection of anatomical structures and potentially malignant lymph nodes in the mediastinal area. METHODS: We propose a 2D pipeline combining the strengths of U-Net for pixel-wise segmentation using a loss function dealing with data imbalance and Mask R-CNN providing instance detection and improved pixel-wise segmentation within bounding boxes. A final stage performs pixel-wise labels refinement and 3D instance detection using a tracking approach along the slicing dimension. Detected instances are represented by a 3D pixel-wise mask, bounding volume, and centroid position. RESULTS: We validated our approach following a fivefold cross-validation over our new data-set of fifteen lung cancer patients. For the semantic segmentation task, we reach an average Dice score of 76% over all fifteen anatomical structures. For the lymph node instance detection task, we reach 75% recall for 9 false positives per patient, with an average centroid position estimation error of 3 mm in each dimension. CONCLUSION: Fusing 2D networks' results increases pixel-wise segmentation results while enabling good instance detection. Better leveraging of the 3D information and station mapping for the detected lymph nodes are the next steps.",2019,10.1007/s11548-019-01948-8,cross-sectional,diagnosis,CT,Lungs
Semi-supervised adversarial model for benign-malignant lung nodule classification on chest CT,"Classification of benign-malignant lung nodules on chest CT is the most critical step in the early detection of lung cancer and prolongation of patient survival. Despite their success in image classification, deep convolutional neural networks (DCNNs) always require a large number of labeled training data, which are not available for most medical image analysis applications due to the work required in image acquisition and particularly image annotation. In this paper, we propose a semi-supervised adversarial classification (SSAC) model that can be trained by using both labeled and unlabeled data for benign-malignant lung nodule classification. This model consists of an adversarial autoencoder-based unsupervised reconstruction network R, a supervised classification network C, and learnable transition layers that enable the adaption of the image representation ability learned by R to C. The SSAC model has been extended to the multi-view knowledge-based collaborative learning, aiming to employ three SSACs to characterize each nodule's overall appearance, heterogeneity in shape and texture, respectively, and to perform such characterization on nine planar views. The MK-SSAC model has been evaluated on the benchmark LIDC-IDRI dataset and achieves an accuracy of 92.53% and an AUC of 95.81%, which are superior to the performance of other lung nodule classification and semi-supervised learning approaches.",2019,10.1016/j.media.2019.07.004,cross-sectional,diagnosis,CT,Lungs
Semi-Supervised Deep Transfer Learning for Benign-Malignant Diagnosis of Pulmonary Nodules in Chest CT Images,"Lung cancer is the leading cause of cancer deaths worldwide. Accurately diagnosing the malignancy of suspected lung nodules is of paramount clinical importance. However, to date, the pathologically-proven lung nodule dataset is largely limited and is highly imbalanced in benign and malignant distributions. In this study, we proposed a Semi-supervised Deep Transfer Learning (SDTL) framework for benign-malignant pulmonary nodule diagnosis. First, we utilize a transfer learning strategy by adopting a pre-trained classification network that is used to differentiate pulmonary nodules from nodule-like tissues. Second, since the size of samples with pathological-proven is small, an iterated feature-matching-based semi-supervised method is proposed to take advantage of a large available dataset with no pathological results. Specifically, a similarity metric function is adopted in the network semantic representation space for gradually including a small subset of samples with no pathological results to iteratively optimize the classification network. In this study, a total of 3,038 pulmonary nodules (from 2,853 subjects) with pathologically-proven benign or malignant labels and 14,735 unlabeled nodules (from 4,391 subjects) were retrospectively collected. Experimental results demonstrate that our proposed SDTL framework achieves superior diagnosis performance, with accuracy = 88.3%, AUC = 91.0% in the main dataset, and accuracy = 74.5%, AUC = 79.5% in the independent testing dataset. Furthermore, ablation study shows that the use of transfer learning provides 2% accuracy improvement, and the use of semi-supervised learning further contributes 2.9% accuracy improvement. Results implicate that our proposed classification network could provide an effective diagnostic tool for suspected lung nodules, and might have a promising application in clinical practice.",2022,10.1109/tmi.2021.3123572,cross-sectional,diagnosis,CT,Lungs
Semi-supervised learning for an improved diagnosis of COVID-19 in CT images,"Coronavirus disease 2019 (COVID-19) has been spread out all over the world. Although a real-time reverse-transcription polymerase chain reaction (RT-PCR) test has been used as a primary diagnostic tool for COVID-19, the utility of CT based diagnostic tools have been suggested to improve the diagnostic accuracy and reliability. Herein we propose a semi-supervised deep neural network for an improved detection of COVID-19. The proposed method utilizes CT images in a supervised and unsupervised manner to improve the accuracy and robustness of COVID-19 diagnosis. Both labeled and unlabeled CT images are employed. Labeled CT images are used for supervised leaning. Unlabeled CT images are utilized for unsupervised learning in a way that the feature representations are invariant to perturbations in CT images. To systematically evaluate the proposed method, two COVID-19 CT datasets and three public CT datasets with no COVID-19 CT images are employed. In distinguishing COVID-19 from non-COVID-19 CT images, the proposed method achieves an overall accuracy of 99.83%, sensitivity of 0.9286, specificity of 0.9832, and positive predictive value (PPV) of 0.9192. The results are consistent between the COVID-19 challenge dataset and the public CT datasets. For discriminating between COVID-19 and common pneumonia CT images, the proposed method obtains 97.32% accuracy, 0.9971 sensitivity, 0.9598 specificity, and 0.9326 PPV. Moreover, the comparative experiments with respect to supervised learning and training strategies demonstrate that the proposed method is able to improve the diagnostic accuracy and robustness without exhaustive labeling. The proposed semi-supervised method, exploiting both supervised and unsupervised learning, facilitates an accurate and reliable diagnosis for COVID-19, leading to an improved patient care and management.",2021,10.1371/journal.pone.0249450,cross-sectional,diagnosis,CT,Lungs
Semi-Supervised Medical Image Classification With Relation-Driven Self-Ensembling Model,"Training deep neural networks usually requires a large amount of labeled data to obtain good performance. However, in medical image analysis, obtaining high-quality labels for the data is laborious and expensive, as accurately annotating medical images demands expertise knowledge of the clinicians. In this paper, we present a novel relation-driven semi-supervised framework for medical image classification. It is a consistency-based method which exploits the unlabeled data by encouraging the prediction consistency of given input under perturbations, and leverages a self-ensembling model to produce high-quality consistency targets for the unlabeled data. Considering that human diagnosis often refers to previous analogous cases to make reliable decisions, we introduce a novel sample relation consistency (SRC) paradigm to effectively exploit unlabeled data by modeling the relationship information among different samples. Superior to existing consistency-based methods which simply enforce consistency of individual predictions, our framework explicitly enforces the consistency of semantic relation among different samples under perturbations, encouraging the model to explore extra semantic information from unlabeled data. We have conducted extensive experiments to evaluate our method on two public benchmark medical image classification datasets, i.e., skin lesion diagnosis with ISIC 2018 challenge and thorax disease classification with ChestX-ray14. Our method outperforms many state-of-the-art semi-supervised learning methods on both single-label and multi-label image classification scenarios.",2020,10.1109/tmi.2020.2995518,cross-sectional,diagnosis,CXR,Lungs
Sensitivity analysis of FDG PET tumor voxel cluster radiomics and dosimetry for predicting mid-chemoradiation regional response of locally advanced lung cancer,"We investigated the sensitivity of regional tumor response prediction to variability in voxel clustering techniques, imaging features, and machine learning algorithms in 25 patients with locally advanced non-small cell lung cancer (LA-NSCLC) enrolled on the FLARE-RT clinical trial. Metabolic tumor volumes (MTV) from pre-chemoradiation (PETpre) and mid-chemoradiation fluorodeoxyglucose-positron emission tomography (FDG PET) images (PETmid) were subdivided into K-means or hierarchical voxel clusters by standardized uptake values (SUV) and 3D-positions. MTV cluster separability was evaluated by CH index, and morphologic changes were captured by Dice similarity and centroid Euclidean distance. PETpre conventional features included SUVmean, MTV/MTV cluster size, and mean radiation dose. PETpre radiomics consisted of 41 intensity histogram and 3D texture features (PET Oncology Radiomics Test Suite) extracted from MTV or MTV clusters. Machine learning models (multiple linear regression, support vector regression, logistic regression, support vector machines) of conventional features or radiomic features were constructed to predict PETmid response. Leave-one-out-cross-validated root-mean-squared-error (RMSE) for continuous response regression (ΔSUVmean) and area-under-receiver-operating-characteristic-curve (AUC) for binary response classification were calculated. K-means MTV 2-clusters (MTVhi, MTVlo) achieved maximum CH index separability (Friedman p < 0.001). Between PETpre and PETmid, MTV cluster pairs overlapped (Dice 0.70-0.87) and migrated 0.6-1.1 cm. PETmid ΔSUVmean response prediction was superior in MTV and MTVlo (RMSE = 0.17-0.21) compared to MTVhi (RMSE = 0.42-0.52, Friedman p < 0.001). PETmid ΔSUVmean response class prediction performance trended higher in MTVlo (AUC = 0.83-0.88) compared to MTVhi (AUC = 0.44-0.58, Friedman p = 0.052). Models were more sensitive to MTV/MTV cluster regions (Friedman p = 0.026) than feature sets/algorithms (Wilcoxon signed-rank p = 0.36). Top-ranked radiomic features included GLZSM-LZHGE (large-zone-high-SUV), GTSDM-CP (cluster-prominence), GTSDM-CS (cluster-shade) and NGTDM-CNT (contrast). Top-ranked features were consistent between MTVhi and MTVlo cluster pairs but varied between MTVhi-MTVlo clusters, reflecting distinct regional radiomic phenotypes. Variability in tumor voxel cluster response prediction can inform robust radiomic target definition for risk-adaptive chemoradiation in patients with LA-NSCLC. FLARE-RT trial: NCT02773238.",2020,10.1088/1361-6560/abb0c7,cross-sectional,diagnosis,PET,Lungs
"Serum Biomarkers May Prognosticate Recurrence in Node-Negative, Non-Small Cell Lung Cancers Less Than 4 Centimeters","BACKGROUND: A significant proportion of patients who undergo lung resection for less than 4 cm non-small cell lung cancer (NSCLC) will die of disease recurrence within 5 years. The ability to identify patients at greatest risk for recurrence may help individualize treatment and surveillance regimens and improve outcomes. We hypothesized that a serum-based biomarker panel could help risk stratify patients with node-negative NSCLC less than 4 cm for recurrence after lung resection. METHODS: An institutional biorepository of more than 1,800 cases was used to identify patients with resected, node-negative NSCLC less than 4 cm in size. Clinical and radiographic data were collected. Preoperative serum specimens were evaluated in a blinded manner for 47 biomarkers that sampled biological processes associated with metastatic progression, including angiogenesis, energy metabolism, apoptosis, and inflammation. Receiver-operating characteristics curves and log rank tests were used to evaluate individual biomarkers with respect to recurrence, followed by random forest analysis to generate and cross validate a multiple-analyte panel to risk stratify patients for recurrence. RESULTS: The cohort included 123 patients with a median follow-up of 58.2 months; 23 patients had recurrences. A seven-analyte panel consisting of human epididymis protein 4, insulinlike growth factor-binding protein 1, beta-human chorionic gonadotropin, follistatin, prolactin, angiopoietin-2, and hepatocyte growth factor optimally identified patients with disease recurrence with a cross-validated specificity of 91%, sensitivity of 22%, negative predictive value of 83%, positive predictive value of 36%, and accuracy of 78%, providing an area under the receiver-operating characteristics curve of 0.70. CONCLUSIONS: Serum-based biomarkers may be useful for risk stratifying patients with node-negative NSCLC less than 4 cm for recurrence after lung resection.",2017,10.1016/j.athoracsur.2017.06.036,,,,
Setting up an Easy-to-Use Machine Learning Pipeline for Medical Decision Support: A Case Study for COVID-19 Diagnosis Based on Deep Learning with CT Scans,"Coronavirus disease (COVID-19) constitutes an ongoing global health problem with significant morbidity and mortality. It usually presents characteristic findings on a chest CT scan, which may lead to early detection of the disease. A timely and accurate diagnosis of COVID-19 is the cornerstone for the prompt management of the patients. The aim of the present study was to evaluate the performance of an automated machine learning algorithm in the diagnosis of Covid-19 pneumonia using chest CT scans. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity, and positive predictive value. The method's average precision was 0.932. We suggest that auto-ML platforms help users with limited ML expertise train image recognition models by only uploading the examined dataset and performing some basic settings. Such methods could deliver significant potential benefits for patients in the future by allowing for earlier disease detection and care.",2020,10.3233/shti200481,cross-sectional,diagnosis,CT,Lungs
Severity and Consolidation Quantification of COVID-19 From CT Images Using Deep Learning Based on Hybrid Weak Labels,"Early and accurate diagnosis of Coronavirus disease (COVID-19) is essential for patient isolation and contact tracing so that the spread of infection can be limited. Computed tomography (CT) can provide important information in COVID-19, especially for patients with moderate to severe disease as well as those with worsening cardiopulmonary status. As an automatic tool, deep learning methods can be utilized to perform semantic segmentation of affected lung regions, which is important to establish disease severity and prognosis prediction. Both the extent and type of pulmonary opacities help assess disease severity. However, manually pixel-level multi-class labelling is time-consuming, subjective, and non-quantitative. In this article, we proposed a hybrid weak label-based deep learning method that utilize both the manually annotated pulmonary opacities from COVID-19 pneumonia and the patient-level disease-type information available from the clinical report. A UNet was firstly trained with semantic labels to segment the total infected region. It was used to initialize another UNet, which was trained to segment the consolidations with patient-level information using the Expectation-Maximization (EM) algorithm. To demonstrate the performance of the proposed method, multi-institutional CT datasets from Iran, Italy, South Korea, and the United States were utilized. Results show that our proposed method can predict the infected regions as well as the consolidation regions with good correlation to human annotation.",2020,10.1109/jbhi.2020.3030224,cross-sectional,diagnosis,CT,Lungs
Severity of Chest Imaging is Correlated with Risk of Acute Neuroimaging Findings among Patients with COVID-19,"BACKGROUND AND PURPOSE: Severe respiratory distress in patients with COVID-19 has been associated with higher rate of neurologic manifestations. Our aim was to investigate whether the severity of chest imaging findings among patients with coronavirus disease 2019 (COVID-19) correlates with the risk of acute neuroimaging findings. MATERIALS AND METHODS: This retrospective study included all patients with COVID-19 who received care at our hospital between March 3, 2020, and May 6, 2020, and underwent chest imaging within 10 days of neuroimaging. Chest radiographs were assessed using a previously validated automated neural network algorithm for COVID-19 (Pulmonary X-ray Severity score). Chest CTs were graded using a Chest CT Severity scoring system based on involvement of each lobe. Associations between chest imaging severity scores and acute neuroimaging findings were assessed using multivariable logistic regression. RESULTS: Twenty-four of 93 patients (26%) included in the study had positive acute neuroimaging findings, including intracranial hemorrhage (n = 7), infarction (n = 7), leukoencephalopathy (n = 6), or a combination of findings (n = 4). The average length of hospitalization, prevalence of intensive care unit admission, and proportion of patients requiring intubation were significantly greater in patients with acute neuroimaging findings than in patients without them (P < .05 for all). Compared with patients without acute neuroimaging findings, patients with acute neuroimaging findings had significantly higher mean Pulmonary X-ray Severity scores (5.0 [SD, 2.9] versus 9.2 [SD, 3.4], P < .001) and mean Chest CT Severity scores (9.0 [SD, 5.1] versus 12.1 [SD, 5.0], P = .041). The pulmonary x-ray severity score was a significant predictor of acute neuroimaging findings in patients with COVID-19. CONCLUSIONS: Patients with COVID-19 and acute neuroimaging findings had more severe findings on chest imaging on both radiographs and CT compared with patients with COVID-19 without acute neuroimaging findings. The severity of findings on chest radiography was a strong predictor of acute neuroimaging findings in patients with COVID-19.",2021,10.3174/ajnr.A7032,cross-sectional,diagnosis,CT,Lungs
Shape and margin-aware lung nodule classification in low-dose CT images via soft activation mapping,"A number of studies on lung nodule classification lack clinical/biological interpretations of the features extracted by convolutional neural network (CNN). The methods like class activation mapping (CAM) and gradient-based CAM (Grad-CAM) are tailored for interpreting localization and classification tasks while they ignored fine-grained features. Therefore, CAM and Grad-CAM cannot provide optimal interpretation for lung nodule categorization task in low-dose CT images, in that fine-grained pathological clues like discrete and irregular shape and margins of nodules are capable of enhancing sensitivity and specificity of nodule classification with regards to CNN. In this paper, we first develop a soft activation mapping (SAM) to enable fine-grained lung nodule shape & margin (LNSM) feature analysis with a CNN so that it can access rich discrete features. Secondly, by combining high-level convolutional features with SAM, we further propose a high-level feature enhancement scheme (HESAM) to localize LNSM features. Experiments on the LIDC-IDRI dataset indicate that 1) SAM captures more fine-grained and discrete attention regions than existing methods, 2) HESAM localizes more accurately on LNSM features and obtains the state-of-the-art predictive performance, reducing the false positive rate, and 3) we design and conduct a visually matching experiment which incorporates radiologists study to increase the confidence level of applying our method to clinical diagnosis.",2020,10.1016/j.media.2019.101628,cross-sectional,diagnosis,CT,Lungs
Short Keynote Paper: Mainstreaming Personalized Healthcare-Transforming Healthcare Through New Era of Artificial Intelligence,"Medicine has entered the digital era, driven by data from new modalities, especially genomics and imaging, as well as new sources such as wearables and Internet of Things. As we gain a deeper understanding of the disease biology and how diseases affect an individual, we are developing targeted therapies to personalize treatments. There is a need for technologies like Artificial Intelligence (AI) to be able to support predictions for personalized treatments. In order to mainstream AI in healthcare we will need to address issues such as explainability, liability and privacy. Developing explainable algorithms and including AI training in medical education are many of the solutions that can help alleviate these concerns.",2020,10.1109/jbhi.2020.2970807,,,,
Short-term Reproducibility of Pulmonary Nodule and Mass Detection in Chest Radiographs: Comparison among Radiologists and Four Different Computer-Aided Detections with Convolutional Neural Net,"To investigate the reproducibility of computer-aided detection (CAD) for detection of pulmonary nodules and masses for consecutive chest radiographies (CXRs) of the same patient within a short-term period. A total of 944 CXRs (Chest PA) with nodules and masses, recorded between January 2010 and November 2016 at the Asan Medical Center, were obtained. In all, 1092 regions of interest for the nodules and mass were delineated using an in-house software. All CXRs were randomly split into 6:2:2 sets for training, development, and validation. Furthermore, paired follow-up CXRs (n = 121) acquired within one week in the validation set, in which expert thoracic radiologists confirmed no changes, were used to evaluate the reproducibility of CAD by two radiologists (R1 and R2). The reproducibility comparison of four different convolutional neural net algorithms and two chest radiologists (with 13- and 14-years' experience) was conducted. Model performances were evaluated by figure-of-merit (FOM) analysis of the jackknife free-response receiver operating curve and reproducibility rates were evaluated in terms of percent positive agreement (PPA) and Chamberlain's percent positive agreement (CPPA). Reproducibility analysis of the four CADs and R1 and R2 showed variations in the PPA and CPPA. Model performance of YOLO (You Only Look Once) v2 based eDenseYOLO showed a higher FOM (0.89; 0.85-0.93) than RetinaNet (0.89; 0.85-0.93) and atrous spatial pyramid pooling U-Net (0.85; 0.80-0.89). eDenseYOLO showed higher PPAs (97.87%) and CPPAs (95.80%) than Mask R-CNN, RetinaNet, ASSP U-Net, R1, and R2 (PPA: 96.52%, 94.23%, 95.04%, 96.55%, and 94.98%; CPPA: 93.18%, 89.09%, 90.57%, 93.33%, and 90.43%). There were moderate variations in the reproducibility of CAD with different algorithms, which likely indicates that measurement of reproducibility is necessary for evaluating CAD performance in actual clinical environments.",2019,10.1038/s41598-019-55373-7,cross-sectional,diagnosis,CXR,Lungs
Similarity measurement of lung masses for medical image retrieval using kernel based semisupervised distance metric,"PURPOSE: To develop a new algorithm to measure the similarity between the query lung mass and reference lung mass data set for content-based medical image retrieval (CBMIR). METHODS: A lung mass data set including 746 mass regions of interest (ROIs) was assembled. Among them, 375 ROIs depicted malignant lesions and 371 depicted benign lesions. Each mass ROI is represented by a vector of 26 texture features. A kernel function was employed to map the original data in input space to a feature space. In this space, a semisupervised distance metric was learned, which used differential scatter discriminant criterion to represent the semantic relevance, and the regularization term to represent the visual similarity. The learned distance metric can measure the similarity of the query mass and reference mass data set. The clustering accuracy is used to configure the parameters. The retrieval accuracy and classification accuracy are used as the performance assessment index. RESULTS: After configuring the parameters, a mean clustering accuracy of 0.87 can be achieved. For retrieval accuracy, our algorithm achieves better performance than other state-of-the-art retrieval algorithms when applying a leave-one-out validation method to the testing data set. For classification accuracy, the area under the ROC curve of our algorithm can be achieved as 0.941 ± 0.006. The running times of 346 query images with the proposed algorithm are 5.399 and 6.0 s, respectively. CONCLUSIONS: The study results demonstrated the proposed algorithm outperforms the compared algorithms, when taking the semantic relevant and visual similarity into account in kernel space. The algorithm can be used in a CBMIR system for a query mass to retrieve similarity masses, which can help doctors make better decisions.",2016,10.1118/1.4966030,cross-sectional,diagnosis,CT,Lungs
Simulated four-dimensional CT for markerless tumor tracking using a deep learning network with multi-task learning,"INTRODUCTION: Our markerless tumor tracking algorithm requires 4DCT data to train models. 4DCT cannot be used for markerless tracking for respiratory-gated treatment due to inaccuracies and a high radiation dose. We developed a deep neural network (DNN) to generate 4DCT from 3DCT data. METHODS: We used 2420 thoracic 4DCT datasets from 436 patients to train a DNN, designed to export 9 deformation vector fields (each field representing one-ninth of the respiratory cycle) from each CT dataset based on a 3D convolutional autoencoder with shortcut connections using deformable image registration. Then 3DCT data at exhale were transformed using the predicted deformation vector fields to obtain simulated 4DCT data. We compared markerless tracking accuracy between original and simulated 4DCT datasets for 20 patients. Our tracking algorithm used a machine learning approach with patient-specific model parameters. For the training stage, a pair of digitally reconstructed radiography images was generated using 4DCT for each patient. For the prediction stage, the tracking algorithm calculated tumor position using incoming fluoroscopic image data. RESULTS: Diaphragmatic displacement averaged over 40 cases for the original 4DCT were slightly higher (<1.3 mm) than those for the simulated 4DCT. Tracking positional errors (95th percentile of the absolute value of displacement, ""simulated 4DCT"" minus ""original 4DCT"") averaged over the 20 cases were 0.56 mm, 0.65 mm, and 0.96 mm in the X, Y and Z directions, respectively. CONCLUSIONS: We developed a DNN to generate simulated 4DCT data that are useful for markerless tumor tracking when original 4DCT is not available. Using this DNN would accelerate markerless tumor tracking and increase treatment accuracy in thoracoabdominal treatment.",2020,10.1016/j.ejmp.2020.10.023,cross-sectional,diagnosis,CT,Lungs
Simultaneous cosegmentation of tumors in PET-CT images using deep fully convolutional networks,"PURPOSE: To investigate the use and efficiency of 3-D deep learning, fully convolutional networks (DFCN) for simultaneous tumor cosegmentation on dual-modality nonsmall cell lung cancer (NSCLC) and positron emission tomography (PET)-computed tomography (CT) images. METHODS: We used DFCN cosegmentation for NSCLC tumors in PET-CT images, considering both the CT and PET information. The proposed DFCN-based cosegmentation method consists of two coupled three-dimensional (3D)-UNets with an encoder-decoder architecture, which can communicate with the other in order to share complementary information between PET and CT. The weighted average sensitivity and positive predictive values denoted as Scores, dice similarity coefficients (DSCs), and the average symmetric surface distances were used to assess the performance of the proposed approach on 60 pairs of PET/CTs. A Simultaneous Truth and Performance Level Estimation Algorithm (STAPLE) of 3 expert physicians' delineations were used as a reference. The proposed DFCN framework was compared to 3 graph-based cosegmentation methods. RESULTS: Strong agreement was observed when using the STAPLE references for the proposed DFCN cosegmentation on the PET-CT images. The average DSCs on CT and PET are 0.861 ± 0.037 and 0.828 ± 0.087, respectively, using DFCN, compared to 0.638 ± 0.165 and 0.643 ± 0.141, respectively, when using the graph-based cosegmentation method. The proposed DFCN cosegmentation using both PET and CT also outperforms the deep learning method using either PET or CT alone. CONCLUSIONS: The proposed DFCN cosegmentation is able to outperform existing graph-based segmentation methods. The proposed DFCN cosegmentation shows promise for further integration with quantitative multimodality imaging tools in clinical trials.",2019,10.1002/mp.13331,cross-sectional,diagnosis,CT,Lungs
Single patient convolutional neural networks for real-time MR reconstruction: a proof of concept application in lung tumor segmentation for adaptive radiotherapy,"Investigate 3D (spatial and temporal) convolutional neural networks (CNNs) for real-time on-the-fly magnetic resonance imaging (MRI) reconstruction. In particular, we investigated the applicability of training CNNs on a patient-by-patient basis for the purpose of lung tumor segmentation. Data were acquired with our 3 T Philips Achieva system. A retrospective analysis was performed on six non-small cell lung cancer patients who received fully sampled dynamic acquisitions consisting of 650 free breathing images using a bSSFP sequence. We retrospectively undersampled the six patient's data by 5× and 10× acceleration. The retrospective data was used to quantitatively compare the CNN reconstruction to gold truth data via the Dice coefficient (DC) and centroid displacement to compare the tumor segmentations. Reconstruction noise was investigated using the normalized mean square error (NMSE). We further validated the technique using prospectively undersampled data from a volunteer and motion phantom. The retrospectively undersampled data at 5× and 10× acceleration was reconstructed using patient specific trained CNNs. The patient average DCs for the tumor segmentation at 5× and 10× acceleration were 0.94 and 0.92, respectively. These DC values are greater than the inter- and intra-observer segmentations acquired by radiation oncologist experts as reported in a previous study of ours. Furthermore, the patient specific CNN can be trained in under 6 h and the reconstruction time was 65 ms per image. The prospectively undersampled CNN reconstruction data yielded qualitatively acceptable images. We have shown that 3D CNNs can be used for real-time on-the-fly dynamic image reconstruction utilizing both spatial and temporal data in this proof of concept study. We evaluated the technique using six retrospectively undersampled lung cancer patient data sets, as well as prospectively undersampled data acquired from a volunteer and motion phantom. The reconstruction speed achieved for our current implementation was 65 ms per image.",2019,10.1088/1361-6560/ab408e,cross-sectional,informatics,CT,Lungs
Single patient convolutional neural networks for real-time MR reconstruction: coherent low-resolution versus incoherent undersampling,"Accelerated MRI involves undersampling k-space, creating unwanted artifacts when reconstructing the data. While the strategy of incoherent k-space acquisition is proven for techniques such as compressed sensing, it may not be optimal for all techniques. This study compares the use of coherent low-resolution (coherent-LR) and incoherent undersampling phase-encoding for real-time 3D CNN image reconstruction. Data were acquired with our 3 T Philips Achieva system. A retrospective analysis was performed on six non-small cell lung cancer patients who received dynamic acquisitions consisting of 650 free breathing images using a bSSFP sequence. We retrospectively undersampled the data by 5x and 10x acceleration using the two phase-encoding schemes. A quantitative analysis was conducted evaluating the tumor segmentations from the CNN reconstructed data using the Dice coefficient (DC) and centroid displacement. The reconstruction noise was evaluated using the structural similarity index (SSIM). Furthermore, we qualitatively investigated the CNN reconstruction using prospectively undersampled data, where the fully sampled training data set is acquired separately from the accelerated undersampled data. The patient averaged DC, centroid displacement, and SSIM for the tumor segmentation at 5x and 10x was superior using coherent low-resolution undersampling. Furthermore, the patient-specific CNN can be trained in under 6 h and the reconstruction time was 54 ms per image. Both the incoherent and coherent-LR prospective CNN reconstructions yielded qualitatively acceptable images; however, the coherent-LR reconstruction appeared superior to the incoherent reconstruction. We have demonstrated that coherent-LR undersampling for real-time CNN image reconstruction performs quantitatively better for the retrospective case of lung tumor segmentation, and qualitatively better for the prospective case. The tumor segmentation mean DC increased for all six patients at 5x acceleration and the temporal (dynamic) variance of the segmentation was reduced. The reconstruction speed achieved for our current implementation was 54 ms, providing an acceptable frame rate for real-time on-the-fly MR imaging.",2020,10.1088/1361-6560/ab7d13,cross-sectional,informatics,CT,Lungs
Single-view 2D CNNs with fully automatic non-nodule categorization for false positive reduction in pulmonary nodule detection,"BACKGROUND AND OBJECTIVE: In pulmonary nodule detection, the first stage, candidate detection, aims to detect suspicious pulmonary nodules. However, detected candidates include many false positives and thus in the following stage, false positive reduction, such false positives are reliably reduced. Note that this task is challenging due to 1) the imbalance between the numbers of nodules and non-nodules and 2) the intra-class diversity of non-nodules. Although techniques using 3D convolutional neural networks (CNNs) have shown promising performance, they suffer from high computational complexity which hinders constructing deep networks. To efficiently address these problems, we propose a novel framework using the ensemble of 2D CNNs using single views, which outperforms existing 3D CNN-based methods. METHODS: Our ensemble of 2D CNNs utilizes single-view 2D patches to improve both computational and memory efficiency compared to previous techniques exploiting 3D CNNs. We first categorize non-nodules on the basis of features encoded by an autoencoder. Then, all 2D CNNs are trained by using the same nodule samples, but with different types of non-nodules. By extending the learning capability, this training scheme resolves difficulties of extracting representative features from non-nodules with large appearance variations. Note that, instead of manual categorization requiring the heavy workload of radiologists, we propose to automatically categorize non-nodules based on the autoencoder and k-means clustering. RESULTS: We performed extensive experiments to validate the effectiveness of our framework based on the database of the lung nodule analysis 2016 challenge. The superiority of our framework is demonstrated through comparing the performance of five frameworks trained with differently constructed training sets. Our proposed framework achieved state-of-the-art performance (0.922 of the competition performance metric score) with low computational demands (789K of parameters and 1024M of floating point operations per second). CONCLUSION: We presented a novel false positive reduction framework, the ensemble of single-view 2D CNNs with fully automatic non-nodule categorization, for pulmonary nodule detection. Unlike previous 3D CNN-based frameworks, we utilized 2D CNNs using 2D single views to improve computational efficiency. Also, our training scheme using categorized non-nodules, extends the learning capability of representative features of different non-nodules. Our framework achieved state-of-the-art performance with low computational complexity.",2018,10.1016/j.cmpb.2018.08.012,cross-sectional,diagnosis,CT,Lungs
Six artificial intelligence paradigms for tissue characterisation and classification of non-COVID-19 pneumonia against COVID-19 pneumonia in computed tomography lungs,"BACKGROUND: COVID-19 pandemic has currently no vaccines. Thus, the only feasible solution for prevention relies on the detection of COVID-19-positive cases through quick and accurate testing. Since artificial intelligence (AI) offers the powerful mechanism to automatically extract the tissue features and characterise the disease, we therefore hypothesise that AI-based strategies can provide quick detection and classification, especially for radiological computed tomography (CT) lung scans. METHODOLOGY: Six models, two traditional machine learning (ML)-based (k-NN and RF), two transfer learning (TL)-based (VGG19 and InceptionV3), and the last two were our custom-designed deep learning (DL) models (CNN and iCNN), were developed for classification between COVID pneumonia (CoP) and non-COVID pneumonia (NCoP). K10 cross-validation (90% training: 10% testing) protocol on an Italian cohort of 100 CoP and 30 NCoP patients was used for performance evaluation and bispectrum analysis for CT lung characterisation. RESULTS: Using K10 protocol, our results showed the accuracy in the order of DL > TL > ML, ranging the six accuracies for k-NN, RF, VGG19, IV3, CNN, iCNN as 74.58 ± 2.44%, 96.84 ± 2.6, 94.84 ± 2.85%, 99.53 ± 0.75%, 99.53 ± 1.05%, and 99.69 ± 0.66%, respectively. The corresponding AUCs were 0.74, 0.94, 0.96, 0.99, 0.99, and 0.99 (p-values < 0.0001), respectively. Our Bispectrum-based characterisation system suggested CoP can be separated against NCoP using AI models. COVID risk severity stratification also showed a high correlation of 0.7270 (p < 0.0001) with clinical scores such as ground-glass opacities (GGO), further validating our AI models. CONCLUSIONS: We prove our hypothesis by demonstrating that all the six AI models successfully classified CoP against NCoP due to the strong presence of contrasting features such as ground-glass opacities (GGO), consolidations, and pleural effusion in CoP patients. Further, our online system takes < 2 s for inference.",2021,10.1007/s11548-021-02317-0,cross-sectional,diagnosis,CT,Lungs
Small lung nodules detection based on local variance analysis and probabilistic neural network,"BACKGROUND AND OBJECTIVE: In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist's difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis. METHODS: In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier. RESULTS: The performance of our approach is 92% of correct classifications, while the sensitivity is 95% and the specificity is 89.7%. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6%) and true nodules with the false ones (2%). CONCLUSIONS: Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances.",2018,10.1016/j.cmpb.2018.04.025,cross-sectional,diagnosis,CT,Lungs
Soft computing approach to 3D lung nodule segmentation in CT,"This paper presents a novel, multilevel approach to the segmentation of various types of pulmonary nodules in computed tomography studies. It is based on two branches of computational intelligence: the fuzzy connectedness (FC) and the evolutionary computation. First, the image and auxiliary data are prepared for the 3D FC analysis during the first stage of an algorithm - the masks generation. Its main goal is to process some specific types of nodules connected to the pleura or vessels. It consists of some basic image processing operations as well as dedicated routines for the specific cases of nodules. The evolutionary computation is performed on the image and seed points in order to shorten the FC analysis and improve its accuracy. After the FC application, the remaining vessels are removed during the postprocessing stage. The method has been validated using the first dataset of studies acquired and described by the Lung Image Database Consortium (LIDC) and by its latest release - the LIDC-IDRI (Image Database Resource Initiative) database.",2014,10.1016/j.compbiomed.2014.08.005,,,,
Solitary solid pulmonary nodules: a CT-based deep learning nomogram helps differentiate tuberculosis granulomas from lung adenocarcinomas,"OBJECTIVES: To evaluate the differential diagnostic performance of a computed tomography (CT)-based deep learning nomogram (DLN) in identifying tuberculous granuloma (TBG) and lung adenocarcinoma (LAC) presenting as solitary solid pulmonary nodules (SSPNs). METHODS: Routine CT images of 550 patients with SSPNs were retrospectively obtained from two centers. A convolutional neural network was used to extract deep learning features from all lesions. The training set consisted of data for 218 patients. The least absolute shrinkage and selection operator logistic regression was used to create a deep learning signature (DLS). Clinical factors and CT-based subjective findings were combined in a clinical model. An individualized DLN incorporating DLS, clinical factors, and CT-based subjective findings was constructed to validate the diagnostic ability. The performance of the DLN was assessed by discrimination and calibration using internal (n = 140) and external validation cohorts (n = 192). RESULTS: DLS, gender, age, and lobulated shape were found to be independent predictors and were used to build the DLN. The combination showed better diagnostic accuracy than any single model evaluated using the net reclassification improvement method (p < 0.05). The areas under the curve in the training, internal validation, and external validation cohorts were 0.889 (95% confidence interval [CI], 0.839-0.927), 0.879 (95% CI, 0.813-0.928), and 0.809 (95% CI, 0.746-0.862), respectively. Decision curve analysis and stratification analysis showed that the DLN has potential generalization ability. CONCLUSIONS: The CT-based DLN can preoperatively distinguish between LAC and TBG in patients presenting with SSPNs. KEY POINTS: • The deep learning nomogram was developed to preoperatively differentiate TBG from LAC in patients with SSPNs. • The performance of the deep learning feature was superior to that of the radiomics feature. • The deep learning nomogram achieved superior performance compared to the deep learning signature, the radiomics signature, or the clinical model alone.",2020,10.1007/s00330-020-07024-z,cross-sectional,diagnosis,CT,Lungs
SOM-LWL method for identification of COVID-19 on chest X-rays,"The outbreak of coronavirus disease 2019 (COVID-19) has had an immense impact on world health and daily life in many countries. Sturdy observing of the initial site of infection in patients is crucial to gain control in the struggle with COVID-19. The early automated detection of the recent coronavirus disease (COVID-19) will help to limit its dissemination worldwide. Many initial studies have focused on the identification of the genetic material of coronavirus and have a poor detection rate for long-term surgery. The first imaging procedure that played an important role in COVID-19 treatment was the chest X-ray. Radiological imaging is often used as a method that emphasizes the performance of chest X-rays. Recent findings indicate the presence of COVID-19 in patients with irregular findings on chest X-rays. There are many reports on this topic that include machine learning strategies for the identification of COVID-19 using chest X-rays. Other current studies have used non-public datasets and complex artificial intelligence (AI) systems. In our research, we suggested a new COVID-19 identification technique based on the locality-weighted learning and self-organization map (LWL-SOM) strategy for detecting and capturing COVID-19 cases. We first grouped images from chest X-ray datasets based on their similar features in different clusters using the SOM strategy in order to discriminate between the COVID-19 and non-COVID-19 cases. Then, we built our intelligent learning model based on the LWL algorithm to diagnose and detect COVID-19 cases. The proposed SOM-LWL model improved the correlation coefficient performance results between the Covid19, no-finding, and pneumonia cases; pneumonia and no-finding cases; Covid19 and pneumonia cases; and Covid19 and no-finding cases from 0.9613 to 0.9788, 0.6113 to 1 0.8783 to 0.9999, and 0.8894 to 1, respectively. The proposed LWL-SOM had better results for discriminating COVID-19 and non-COVID-19 patients than the current machine learning-based solutions using AI evaluation measures.",2021,10.1371/journal.pone.0247176,cross-sectional,diagnosis,CXR,Lungs
"SOMA: Subject-, object-, and modality-adapted precision atlas approach for automatic anatomy recognition and delineation in medical images","PURPOSE: In the multi-atlas segmentation (MAS) method, a large enough atlas set, which can cover the complete spectrum of the whole population pattern of the target object will benefit the segmentation quality. However, the difficulty in obtaining and generating such a large set of atlases and the computational burden required in the segmentation procedure make this approach impractical. In this paper, we propose a method called SOMA to select subject-, object-, and modality-adapted precision atlases for automatic anatomy recognition in medical images with pathology, following the idea that different regions of the target object in a novel image can be recognized by different atlases with regionally best similarity, so that effective atlases have no need to be globally similar to the target subject and also have no need to be overall similar to the target object. METHODS: The SOMA method consists of three main components: atlas building, object recognition, and object delineation. Considering the computational complexity, we utilize an all-to-template strategy to align all images to the same image space belonging to the root image determined by the minimum spanning tree (MST) strategy among a subset of radiologically near-normal images. The object recognition process is composed of two stages: rough recognition and refined recognition. In rough recognition, subimage matching is conducted between the test image and each image of the whole atlas set, and only the atlas corresponding to the best-matched subimage contributes to the recognition map regionally. The frequency of best match for each atlas is recorded by a counter, and the atlases with the highest frequencies are selected as the precision atlases. In refined recognition, only the precision atlases are examined, and the subimage matching is conducted in a nonlocal manner of searching to further increase the accuracy of boundary matching. Delineation is based on a U-net-based deep learning network, where the original gray scale image together with the fuzzy map from refined recognition compose a two-channel input to the network, and the output is a segmentation map of the target object. RESULTS: Experiments are conducted on computed tomography (CT) images with different qualities in two body regions - head and neck (H&N) and thorax, from 298 subjects with nine objects and 241 subjects with six objects, respectively. Most objects achieve a localization error within two voxels after refined recognition, with marked improvement in localization accuracy from rough to refined recognition of 0.6-3 mm in H&N and 0.8-4.9 mm in thorax, and also in delineation accuracy (Dice coefficient) from refined recognition to delineation of 0.01-0.11 in H&N and 0.01-0.18 in thorax. CONCLUSIONS: The SOMA method shows high accuracy and robustness in anatomy recognition and delineation. The improvements from rough to refined recognition and further to delineation, as well as immunity of recognition accuracy to varying image and object qualities, demonstrate the core principles of SOMA where segmentation accuracy increases with precision atlases and gradually refined object matching.",2021,10.1002/mp.15308,cross-sectional,informatics,CT,Thorax
Somatic Mutations Drive Distinct Imaging Phenotypes in Lung Cancer,"Tumors are characterized by somatic mutations that drive biological processes ultimately reflected in tumor phenotype. With regard to radiographic phenotypes, generally unconnected through present understanding to the presence of specific mutations, artificial intelligence methods can automatically quantify phenotypic characters by using predefined, engineered algorithms or automatic deep-learning methods, a process also known as radiomics. Here we demonstrate how imaging phenotypes can be connected to somatic mutations through an integrated analysis of independent datasets of 763 lung adenocarcinoma patients with somatic mutation testing and engineered CT image analytics. We developed radiomic signatures capable of distinguishing between tumor genotypes in a discovery cohort (n = 353) and verified them in an independent validation cohort (n = 352). All radiomic signatures significantly outperformed conventional radiographic predictors (tumor volume and maximum diameter). We found a radiomic signature related to radiographic heterogeneity that successfully discriminated between EGFR(+) and EGFR(-) cases (AUC = 0.69). Combining this signature with a clinical model of EGFR status (AUC = 0.70) significantly improved prediction accuracy (AUC = 0.75). The highest performing signature was capable of distinguishing between EGFR(+) and KRAS(+) tumors (AUC = 0.80) and, when combined with a clinical model (AUC = 0.81), substantially improved its performance (AUC = 0.86). A KRAS(+)/KRAS(-) radiomic signature also showed significant albeit lower performance (AUC = 0.63) and did not improve the accuracy of a clinical predictor of KRAS status. Our results argue that somatic mutations drive distinct radiographic phenotypes that can be predicted by radiomics. This work has implications for the use of imaging-based biomarkers in the clinic, as applied noninvasively, repeatedly, and at low cost. Cancer Res; 77(14); 3922-30. ©2017 AACR.",2017,10.1158/0008-5472.Can-17-0122,cross-sectional,diagnosis,CT,Lungs
Spatial metabolomics for evaluating response to neoadjuvant therapy in non-small cell lung cancer patients,"BACKGROUND: The response to neoadjuvant chemotherapy (NAC) differs substantially among individual patients with non-small cell lung cancer (NSCLC). Major pathological response (MPR) is a histomorphological read-out used to assess treatment response and prognosis in patients NSCLC after NAC. Although spatial metabolomics is a promising tool for evaluating metabolic phenotypes, it has not yet been utilized to assess therapy responses in patients with NSCLC. We evaluated the potential application of spatial metabolomics in cancer tissues to assess the response to NAC, using a metabolic classifier that utilizes mass spectrometry imaging combined with machine learning. METHODS: Resected NSCLC tissue specimens obtained after NAC (n = 88) were subjected to high-resolution mass spectrometry, and these data were used to develop an approach for assessing the response to NAC in patients with NSCLC. The specificities of the generated tumor cell and stroma classifiers were validated by applying this approach to a cohort of biologically matched chemotherapy-naïve patients with NSCLC (n = 85). RESULTS: The developed tumor cell metabolic classifier stratified patients into different prognostic groups with 81.6% accuracy, whereas the stroma metabolic classifier displayed 78.4% accuracy. By contrast, the accuracies of MPR and TNM staging for stratification were 62.5% and 54.1%, respectively. The combination of metabolic and MPR classifiers showed slightly lower accuracy than either individual metabolic classifier. In multivariate analysis, metabolic classifiers were the only independent prognostic factors identified (tumor: P = 0.001, hazards ratio [HR] = 3.823, 95% confidence interval [CI] = 1.716-8.514; stroma: P = 0.049, HR = 2.180, 95% CI = 1.004-4.737), whereas MPR (P = 0.804; HR = 0.913; 95% CI = 0.445-1.874) and TNM staging (P = 0.078; HR = 1.223; 95% CI = 0.977-1.550) were not independent prognostic factors. Using Kaplan-Meier survival analyses, both tumor and stroma metabolic classifiers were able to further stratify patients as NAC responders (P < 0.001) and non-responders (P < 0.001). CONCLUSIONS: Our findings indicate that the metabolic constitutions of both tumor cells and the stroma are valuable additions to the classical histomorphology-based assessment of tumor response.",2022,10.1002/cac2.12310,,,,
Spatial Pyramid Pooling With 3D Convolution Improves Lung Cancer Detection,"Lung cancer is the leading cause of cancer deaths. Low-dose computed tomography (CT)screening has been shown to significantly reduce lung cancer mortality but suffers from a high false positive rate that leads to unnecessary diagnostic procedures. The development of deep learning techniques has the potential to help improve lung cancer screening technology. Here we present the algorithm, DeepScreener, which can predict a patient's cancer status from a volumetric lung CT scan. DeepScreener is based on our model of Spatial Pyramid Pooling, which ranked 16th of 1972 teams (top 1 percent)in the Data Science Bowl 2017 competition (DSB2017), evaluated with the challenge datasets. Here we test the algorithm with an independent set of 1449 low-dose CT scans of the National Lung Screening Trial (NLST)cohort, and we find that DeepScreener has consistent performance of high accuracy. Furthermore, by combining Spatial Pyramid Pooling and 3D Convolution, it achieves an AUC of 0.892, surpassing the previous state-of-the-art algorithms using only 3D convolution. The advancement of deep learning algorithms can potentially help improve lung cancer detection with low-dose CT scans.",2022,10.1109/tcbb.2020.3027744,cross-sectional,diagnosis,CT,Lungs
Spatial transcriptomics inferred from pathology whole-slide images links tumor heterogeneity to survival in breast and lung cancer,"Digital analysis of pathology whole-slide images is fast becoming a game changer in cancer diagnosis and treatment. Specifically, deep learning methods have shown great potential to support pathology analysis, with recent studies identifying molecular traits that were not previously recognized in pathology H&E whole-slide images. Simultaneous to these developments, it is becoming increasingly evident that tumor heterogeneity is an important determinant of cancer prognosis and susceptibility to treatment, and should therefore play a role in the evolving practices of matching treatment protocols to patients. State of the art diagnostic procedures, however, do not provide automated methods for characterizing and/or quantifying tumor heterogeneity, certainly not in a spatial context. Further, existing methods for analyzing pathology whole-slide images from bulk measurements require many training samples and complex pipelines. Our work addresses these two challenges. First, we train deep learning models to spatially resolve bulk mRNA and miRNA expression levels on pathology whole-slide images (WSIs). Our models reach up to 0.95 AUC on held-out test sets from two cancer cohorts using a simple training pipeline and a small number of training samples. Using the inferred gene expression levels, we further develop a method to spatially characterize tumor heterogeneity. Specifically, we produce tumor molecular cartographies and heterogeneity maps of WSIs and formulate a heterogeneity index (HTI) that quantifies the level of heterogeneity within these maps. Applying our methods to breast and lung cancer slides, we show a significant statistical link between heterogeneity and survival. Our methods potentially open a new and accessible approach to investigating tumor heterogeneity and other spatial molecular properties and their link to clinical characteristics, including treatment susceptibility and survival.",2020,10.1038/s41598-020-75708-z,,,,
Spectral augmentation for heart chambers segmentation on conventional contrasted and unenhanced CT scans: an in-depth study,"PURPOSE: Recently, machine learning has outperformed established tools for automated segmentation in medical imaging. However, segmentation of cardiac chambers still proves challenging due to the variety of contrast agent injection protocols used in clinical practice, inducing disparities of contrast between cavities. Hence, training a generalist network requires large training datasets representative of these protocols. Furthermore, segmentation on unenhanced CT scans is further hindered by the challenge of obtaining ground truths from these images. Newly available spectral CT scanners allow innovative image reconstructions such as virtual non-contrast (VNC) imaging, mimicking non-contrasted conventional CT studies from a contrasted scan. Recent publications have demonstrated that networks can be trained using VNC to segment contrasted and unenhanced conventional CT scans to reduce annotated data requirements and the need for annotations on unenhanced scans. We propose an extensive evaluation of this statement. METHOD: We undertake multiple trainings of a 3D multi-label heart segmentation network with (HU-VNC) and without (HUonly) VNC as augmentation, using decreasing training dataset sizes (114, 76, 57, 38, 29, 19 patients). At each step, both networks are tested on a multi-vendor, multi-centric dataset of 122 patients, including different protocols: pulmonary embolism (PE), chest-abdomen-pelvis (CAP), heart CT angiography (CTA) and true non-contrast scans (TNC). An in-depth comparison of resulting Dice coefficients and distance metrics is performed for the networks trained on the largest dataset. RESULTS: HU-VNC-trained on 57 patients significantly outperforms HUonly trained on 114 regarding CAP and TNC scans (mean Dice coefficients of 0.881/0.835 and 0.882/0.416, respectively). When trained on the largest dataset, significant improvements in all labels are noted for TNC and CAP scans (mean Dice coefficient of 0.882/0.416 and 0.891/0.835, respectively). CONCLUSION: Adding VNC images as training augmentation allows the network to perform on unenhanced scans and improves segmentations on other imaging protocols, while using a reduced training dataset.",2021,10.1007/s11548-021-02468-0,cross-sectional,diagnosis,CT,Heart
Spiculation Sign Recognition in a Pulmonary Nodule Based on Spiking Neural P Systems,"The spiculation sign is one of the main signs to distinguish benign and malignant pulmonary nodules. In order to effectively extract the image feature of a pulmonary nodule for the spiculation sign distinguishment, a new spiculation sign recognition model is proposed based on the doctors' diagnosis process of pulmonary nodules. A maximum density projection model is established to fuse the local three-dimensional information into the two-dimensional image. The complete boundary of a pulmonary nodule is extracted by the improved Snake model, which can take full advantage of the parallel calculation of the Spike Neural P Systems to build a new neural network structure. In this paper, our experiments show that the proposed algorithm can accurately extract the boundary of a pulmonary nodule and effectively improve the recognition rate of the spiculation sign.",2020,10.1155/2020/6619076,cross-sectional,diagnosis,CT,Lungs
SSA-Net: Spatial self-attention network for COVID-19 pneumonia infection segmentation with semi-supervised few-shot learning,"Coronavirus disease (COVID-19) broke out at the end of 2019, and has resulted in an ongoing global pandemic. Segmentation of pneumonia infections from chest computed tomography (CT) scans of COVID-19 patients is significant for accurate diagnosis and quantitative analysis. Deep learning-based methods can be developed for automatic segmentation and offer a great potential to strengthen timely quarantine and medical treatment. Unfortunately, due to the urgent nature of the COVID-19 pandemic, a systematic collection of CT data sets for deep neural network training is quite difficult, especially high-quality annotations of multi-category infections are limited. In addition, it is still a challenge to segment the infected areas from CT slices because of the irregular shapes and fuzzy boundaries. To solve these issues, we propose a novel COVID-19 pneumonia lesion segmentation network, called Spatial Self-Attention network (SSA-Net), to identify infected regions from chest CT images automatically. In our SSA-Net, a self-attention mechanism is utilized to expand the receptive field and enhance the representation learning by distilling useful contextual information from deeper layers without extra training time, and spatial convolution is introduced to strengthen the network and accelerate the training convergence. Furthermore, to alleviate the insufficiency of labeled multi-class data and the long-tailed distribution of training data, we present a semi-supervised few-shot iterative segmentation framework based on re-weighting the loss and selecting prediction values with high confidence, which can accurately classify different kinds of infections with a small number of labeled image data. Experimental results show that SSA-Net outperforms state-of-the-art medical image segmentation networks and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage. Meanwhile, our semi-supervised iterative segmentation model can improve the learning ability in small and unbalanced training set and can achieve higher performance.",2022,10.1016/j.media.2022.102459,cross-sectional,diagnosis,CT,Lungs
StackNet-DenVIS: a multi-layer perceptron stacked ensembling approach for COVID-19 detection using X-ray images,"The highly contagious nature of Coronavirus disease 2019 (Covid-19) resulted in a global pandemic. Due to the relatively slow and taxing nature of conventional testing for Covid-19, a faster method needs to be in place. The current researches have suggested that visible irregularities found in the chest X-ray of Covid-19 positive patients are indicative of the presence of the disease. Hence, Deep Learning and Image Classification techniques can be employed to learn from these irregularities, and classify accordingly with high accuracy. This research presents an approach to create a classifier model named StackNet-DenVIS which is designed to act as a screening process before conducting the existing swab tests. Using a novel approach, which incorporates Transfer Learning and Stacked Generalization, the model aims to lower the False Negative rate of classification compensating for the 30% False Negative rate of the swab tests. A dataset gathered from multiple reliable sources consisting of 9953 Chest X-rays (868 Covid and 9085 Non-Covid) was used. Also, this research demonstrates handling data imbalance using various techniques involving Generative Adversarial Networks and sampling techniques. The accuracy, sensitivity, and specificity obtained on our proposed model were 95.07%, 99.40% and 94.61% respectively. To the best of our knowledge, the combination of accuracy and false negative rate obtained by this paper outperforms the current implementations. We must also highlight that our proposed architecture also considers other types of viral pneumonia. Given the unprecedented sensitivity of our model we are optimistic it contributes to a better Covid-19 detection.",2020,10.1007/s13246-020-00952-6,cross-sectional,diagnosis,CXR,Lungs
Standardization of imaging features for radiomics analysis,"Radiomics has the potential to provide tumor characteristics with noninvasive and repeatable way. The purpose of this paper is to evaluate the standardization effect of imaging features for radiomics analysis. For this purpose, we prepared two CT databases ; one includes 40 non-small cell lung cancer (NSCLC) patients for whom tumor biopsies was performed before stereotactic body radiation therapy in The University of Tokyo Hospital, and the other includes 29 early-stage NSCLC datasets from the Cancer Imaging Archive. The former was used as the training data, whereas the later was used as the test data in the evaluation of the prediction model. In total, 476 imaging features were extracted from each data. Then, both training and test data were standardized as the min-max normalization, the z-score normalization, and the whitening from the principle component analysis. All of standardization strategies improved the accuracy for the histology prediction. The area under the receiver observed characteristics curve was 0.725, 0.789, and 0.785 in above standardizations, respectively. Radiomics analysis has shown that robust features have a high prognostic power in predicting early-stage NSCLC histology subtypes. The performance was able to be improved by standardizing the data in the feature space. J. Med. Invest. 66 : 35-37, February, 2019.",2019,10.2152/jmi.66.35,cross-sectional,diagnosis,CT,Lungs
Statistical modeling can determine what factors are predictive of appropriate follow-up in patients presenting with incidental pulmonary nodules on CT,"PURPOSE: To assess the performance of statistical modeling in predicting follow-up adherence of incidentally detected pulmonary nodules (IPN) on CT, based on patient variables (PV), radiology report related variables (RRRV) and physician-patient communication variables (PPCV). METHODS: 200 patients with IPN on CT were retrospectively identified and randomly selected. PV (age, gender, smoking status, ethnicity), RRRV (nodule size, patient context, whether follow-up recommendations were provided) and PPCV (whether referring physician documented IPN and ordered follow-up on the electronic medical record) were recorded. Primary outcome was whether patients received appropriate follow-up within +/- 1 month of the recommended time frame. Statistical methods included logistic regression and machine learning (K-nearest neighbors and support vector machine). RESULTS: Adherence was low, with or without recommendations provided in the radiology report (23.4 %-27.4 %). Whether the referring physician ordered follow-up was the dominant predictor of adherence in all models. The following variables were statistically significant predictors of whether referring physician ordered follow-up: recommendations provided in the radiology report, smoking status, patient context and nodule size (FDR logworth of respectively 21.18, 11.66, 2.35, 1.63, p < 0.05). Prediction accuracy varied from 72 % (PV) to 93 % (PPCV, all variables). CONCLUSION: PPCV are the most important predictors of adherence. Amongst all variables, patient context, smoking status, nodule size, and whether the radiologist provided follow-up recommendations in the report were all statistically significant predictors of patient follow-up adherence, supporting the utility of statistical modeling for analytics, quality assurance and optimization of outcomes related to IPN.",2020,10.1016/j.ejrad.2020.109062,cross-sectional,diagnosis,CT,Lungs
Strategies to develop radiomics and machine learning models for lung cancer stage and histology prediction using small data samples,"Predictive models based on radiomics and machine-learning (ML) need large and annotated datasets for training, often difficult to collect. We designed an operative pipeline for model training to exploit data already available to the scientific community. The aim of this work was to explore the capability of radiomic features in predicting tumor histology and stage in patients with non-small cell lung cancer (NSCLC). We analyzed the radiotherapy planning thoracic CT scans of a proprietary sample of 47 subjects (L-RT) and integrated this dataset with a publicly available set of 130 patients from the MAASTRO NSCLC collection (Lung1). We implemented intra- and inter-sample cross-validation strategies (CV) for evaluating the ML predictive model performances with not so large datasets. We carried out two classification tasks: histology classification (3 classes) and overall stage classification (two classes: stage I and II). In the first task, the best performance was obtained by a Random Forest classifier, once the analysis has been restricted to stage I and II tumors of the Lung1 and L-RT merged dataset (AUC = 0.72 ± 0.11). For the overall stage classification, the best results were obtained when training on Lung1 and testing of L-RT dataset (AUC = 0.72 ± 0.04 for Random Forest and AUC = 0.84 ± 0.03 for linear-kernel Support Vector Machine). According to the classification task to be accomplished and to the heterogeneity of the available dataset(s), different CV strategies have to be explored and compared to make a robust assessment of the potential of a predictive model based on radiomics and ML.",2021,10.1016/j.ejmp.2021.08.015,cross-sectional,diagnosis,CT,Lungs
Stratifying the early radiologic trajectory in dyspneic patients with COVID-19 pneumonia,"OBJECTIVE: This study aimed to stratify the early pneumonia trajectory on chest radiographs and compare patient characteristics in dyspneic patients with coronavirus disease 2019 (COVID-19). MATERIALS AND METHODS: We retrospectively included 139 COVID-19 patients with dyspnea (87 men, 62.7±16.3 years) and serial chest radiographs from January to September 2020. Radiographic pneumonia extent was quantified as a percentage using a previously-developed deep learning algorithm. A group-based trajectory model was used to categorize the pneumonia trajectory after symptom onset during hospitalization. Clinical findings, and outcomes were compared, and Cox regression was performed for survival analysis. RESULTS: Radiographic pneumonia trajectories were categorized into four groups. Group 1 (n = 83, 59.7%) had negligible pneumonia, and group 2 (n = 29, 20.9%) had mild pneumonia. Group 3 (n = 13, 9.4%) and group 4 (n = 14, 10.1%) showed similar considerable pneumonia extents at baseline, but group 3 had decreasing pneumonia extent at 1-2 weeks, while group 4 had increasing pneumonia extent. Intensive care unit admission and mortality were significantly more frequent in groups 3 and 4 than in groups 1 and 2 (P < .05). Groups 3 and 4 shared similar clinical and laboratory findings, but thrombocytopenia (<150×103/μL) was exclusively observed in group 4 (P = .016). When compared to groups 1 and 2, group 4 (hazard ratio, 63.3; 95% confidence interval, 7.9-504.9) had a two-fold higher risk for mortality than group 3 (hazard ratio, 31.2; 95% confidence interval, 3.5-280.2), and this elevated risk was maintained after adjusting confounders. CONCLUSION: Monitoring the early radiologic trajectory beyond baseline further prognosticated at-risk COVID-19 patients, who potentially had thrombo-inflammatory responses.",2021,10.1371/journal.pone.0259010,cross-sectional,diagnosis,CT,Lungs
Streamlining follicular monitoring during controlled ovarian stimulation: a data-driven approach to efficient IVF care in the new era of social distancing,"STUDY QUESTION: What is the optimal follicular tracking strategy for controlled ovarian stimulation (COS) in order to minimise face-to-face interactions? SUMMARY ANSWER: As data from follicular tracking scans on Days 5, 6 or 7 of stimulation are the most useful to accurately predict trigger timing and risk of over-response, scans on these days should be prioritised if streamlined monitoring is necessary. WHAT IS KNOWN ALREADY: British Fertility Society guidance for centres restarting ART following coronavirus disease 2019 (COVID-19) pandemic-related shutdowns recommends reducing the number of patient visits for monitoring during COS. Current evidence on optimal monitoring during ovarian stimulation is sparse, and protocols vary significantly. Small studies of simplifying IVF therapy by minimising monitoring have reported no adverse effects on outcomes, including live birth rate. There are opportunities to learn from the adaptations necessary during these extraordinary times to improve the efficiency of IVF care in the longer term. STUDY DESIGN, SIZE, DURATION: A retrospective database analysis of 9294 ultrasound scans performed during monitoring of 2322 IVF cycles undertaken by 1875 women in a single centre was performed. The primary objective was to identify when in the IVF cycle the data obtained from ultrasound are most predictive of both oocyte maturation trigger timing and an over-response to stimulation. If a reduced frequency of clinic visits is needed due to COVID-19 precautions, prioritising attendance for monitoring scans on the most predictive cycle days may be prudent. PARTICIPANTS/MATERIALS, SETTING, METHODS: The study comprised anonymised retrospective database analysis of IVF/ICSI cycles at a tertiary referral IVF centre. Machine learning models are used in combining demographic and follicular tracking data to predict cycle oocyte maturation trigger timing and over-response. The primary outcome was the day or days in cycle from which scan data yield optimal model prediction performance statistics. The model for predicting trigger day uses patient age, number of follicles at baseline scan and follicle count by size for the current scan. The model to predict over-response uses age and number of follicles of a given size. MAIN RESULTS AND THE ROLE OF CHANCE: The earliest cycle day for which our model has high accuracy to predict both trigger day and risk of over-response is stimulation Day 5. The Day 5 model to predict trigger date has a mean squared error 2.16 ± 0.12 and to predict over-response an area under the receiver operating characteristic curve 0.91 ± 0.01. LIMITATIONS, REASONS FOR CAUTION: This is a retrospective single-centre study and the results may not be generalisable to centres using different treatment protocols. The results are derived from modelling, and further clinical validation studies will verify the accuracy of the model. WIDER IMPLICATIONS OF THE FINDINGS: Follicular tracking starting at Day 5 of stimulation may help to streamline the amount of monitoring required in COS. Previous small studies have shown that minimal monitoring protocols did not adversely impact outcomes. If IVF can safely be made less onerous on the clinic's resources and patient's time, without compromising success, this could help to reduce burden-related treatment drop-out. STUDY FUNDING/COMPETING INTEREST(S): F.P.C. acknowledges funding from the NIHR Applied Research Collaboration Wessex. The authors declare they have no competing interests in relation to this work. TRIAL REGISTRATION NUMBER: N/A.",2021,10.1093/humrep/deaa251,,,,
Structure Correction for Robust Volume Segmentation in Presence of Tumors,"CNN based lung segmentation models in absence of diverse training dataset fail to segment lung volumes in presence of severe pathologies such as large masses, scars, and tumors. To rectify this problem, we propose a multi-stage algorithm for lung volume segmentation from CT scans. The algorithm uses a 3D CNN in the first stage to obtain a coarse segmentation of the left and right lungs. In the second stage, shape correction is performed on the segmentation mask using a 3D structure correction CNN. A novel data augmentation strategy is adopted to train a 3D CNN which helps in incorporating global shape prior. Finally, the shape corrected segmentation mask is up-sampled and refined using a parallel flood-fill operation. The proposed multi-stage algorithm is robust in the presence of large nodules/tumors and does not require labeled segmentation masks for entire pathological lung volume for training. Through extensive experiments conducted on publicly available datasets such as NSCLC, LUNA, and LOLA11 we demonstrate that the proposed approach improves the recall of large juxtapleural tumor voxels by at least 15% over state-of-the-art models without sacrificing segmentation accuracy in case of normal lungs. The proposed method also meets the requirement of CAD software by performing segmentation within 5 seconds which is significantly faster than present methods.",2021,10.1109/jbhi.2020.3004296,cross-sectional,diagnosis,CT,Lungs
Subtyping non-small cell lung cancer by histology-guided spatial metabolomics,"PURPOSE: Most cancer-related deaths worldwide are associated with lung cancer. Subtyping of non-small cell lung cancer (NSCLC) into adenocarcinoma (AC) and squamous cell carcinoma (SqCC) is of importance, as therapy regimes differ. However, conventional staining and immunohistochemistry have their limitations. Therefore, a spatial metabolomics approach was aimed to detect differences between subtypes and to discriminate tumor and stroma regions in tissues. METHODS: Fresh-frozen NSCLC tissues (n = 35) were analyzed by matrix-assisted laser desorption/ionization-mass spectrometry imaging (MALDI-MSI) of small molecules (< m/z 1000). Measured samples were subsequently stained and histopathologically examined. A differentiation of subtypes and a discrimination of tumor and stroma regions was performed by receiver operating characteristic analysis and machine learning algorithms. RESULTS: Histology-guided spatial metabolomics revealed differences between AC and SqCC and between NSCLC tumor and tumor microenvironment. A diagnostic ability of 0.95 was achieved for the discrimination of AC and SqCC. Metabolomic contrast to the tumor microenvironment was revealed with an area under the curve of 0.96 due to differences in phospholipid profile. Furthermore, the detection of NSCLC with rarely arising mutations of the isocitrate dehydrogenase (IDH) gene was demonstrated through 45 times enhanced oncometabolite levels. CONCLUSION: MALDI-MSI of small molecules can contribute to NSCLC subtyping. Measurements can be performed intraoperatively on a single tissue section to support currently available approaches. Moreover, the technique can be beneficial in screening of IDH-mutants for the characterization of these seldom cases promoting the development of treatment strategies.",2022,10.1007/s00432-021-03834-w,,,,
Synthetic CT image generation of shape-controlled lung cancer using semi-conditional InfoGAN and its applicability for type classification,"PURPOSE: In recent years, convolutional neural network (CNN), an artificial intelligence technology with superior image recognition, has become increasingly popular and frequently used for classification tasks in medical imaging. However, the amount of labelled data available for classifying medical images is often significantly less than that of natural images, and the handling of rare diseases is often challenging. To overcome these problems, data augmentation has been performed using generative adversarial networks (GANs). However, conventional GAN cannot effectively handle the various shapes of tumours because it randomly generates images. In this study, we introduced semi-conditional InfoGAN, which enables some labels to be added to InfoGAN, for the generation of shape-controlled tumour images. InfoGAN is a derived model of GAN, and it can represent object features in images without any label. METHODS: Chest computed tomography images of 66 patients diagnosed with three histological types of lung cancer (adenocarcinoma, squamous cell carcinoma, and small cell lung cancer) were used for analysis. To investigate the applicability of the generated images, we classified the histological types of lung cancer using a CNN that was pre-trained with the generated images. RESULTS: As a result of the training, InfoGAN was possible to generate images that controlled the diameters of each lesion and the presence or absence of the chest wall. The classification accuracy of the pre-trained CNN was 57.7%, which was higher than that of the CNN trained only with real images (34.2%), thereby suggesting the potential of image generation. CONCLUSION: The applicability of semi-conditional InfoGAN for feature learning and representation in medical images was demonstrated in this study. InfoGAN can perform constant feature learning and generate images with a variety of shapes using a small dataset.",2021,10.1007/s11548-021-02308-1,cross-sectional,diagnosis,CT,Lungs
Synthetic pulmonary perfusion images from 4DCT for functional avoidance using deep learning,"Purpose.To develop and evaluate the performance of a deep learning model to generate synthetic pulmonary perfusion images from clinical 4DCT images for patients undergoing radiotherapy for lung cancer.Methods. A clinical data set of 58 pre- and post-radiotherapy(99m)Tc-labeled MAA-SPECT perfusion studies (32 patients) each with contemporaneous 4DCT studies was collected. Using the inhale and exhale phases of the 4DCT, a 3D-residual network was trained to create synthetic perfusion images utilizing the MAA-SPECT as ground truth. The training process was repeated for a 50-imaging study, five-fold validation with twenty model instances trained per fold. The highest performing model instance from each fold was selected for inference upon the eight-study test set. A manual lung segmentation was used to compute correlation metrics constrained to the voxels within the lungs. From the pre-treatment test cases (N = 5), 50th percentile contours of well-perfused lung were generated from both the clinical and synthetic perfusion images and the agreement was quantified.Results. Across the hold-out test set, our deep learning model predicted perfusion with a Spearman correlation coefficient of 0.70 (IQR: 0.61-0.76) and a Pearson correlation coefficient of 0.66 (IQR: 0.49-0.73). The agreement of the functional avoidance contour pairs was Dice of 0.803 (IQR: 0.750-0.810) and average surface distance of 5.92 mm (IQR: 5.68-7.55).Conclusion. We demonstrate that from 4DCT alone, a deep learning model can generate synthetic perfusion images with potential application in functional avoidance treatment planning.",2021,10.1088/1361-6560/ac16ec,cross-sectional,diagnosis,CT - Pulmunary perfusion imaging,Lungs
Systematic Review of Artificial Intelligence in Acute Respiratory Distress Syndrome for COVID-19 Lung Patients: A Biomedical Imaging Perspective,"SARS-CoV-2 has infected over ∼165 million people worldwide causing Acute Respiratory Distress Syndrome (ARDS) and has killed ∼3.4 million people. Artificial Intelligence (AI) has shown to benefit in the biomedical image such as X-ray/Computed Tomography in diagnosis of ARDS, but there are limited AI-based systematic reviews (aiSR). The purpose of this study is to understand the Risk-of-Bias (RoB) in a non-randomized AI trial for handling ARDS using novel AtheroPoint-AI-Bias (AP(ai)Bias). Our hypothesis for acceptance of a study to be in low RoB must have a mean score of 80% in a study. Using the PRISMA model, 42 best AI studies were analyzed to understand the RoB. Using the AP(ai)Bias paradigm, the top 19 studies were then chosen using the raw-cutoff of 1.9. This was obtained using the intersection of the cumulative plot of ""mean score vs. study"" and score distribution. Finally, these studies were benchmarked against ROBINS-I and PROBAST paradigm. Our observation showed that AP(ai)Bias, ROBINS-I, and PROBAST had only 32%, 16%, and 26% studies, respectively in low-moderate RoB (cutoff>2.5), however none of them met the RoB hypothesis. Further, the aiSR analysis recommends six primary and six secondary recommendations for the non-randomized AI for ARDS. The primary recommendations for improvement in AI-based ARDS design inclusive of (i) comorbidity, (ii) inter-and intra-observer variability studies, (iii) large data size, (iv) clinical validation, (v) granularity of COVID-19 risk, and (vi) cross-modality scientific validation. The AI is an important component for diagnosis of ARDS and the recommendations must be followed to lower the RoB.",2021,10.1109/jbhi.2021.3103839,,,,
Tailoring steroids in the treatment of COVID-19 pneumonia assisted by CT scans: three case reports,"In this article, we analyze and report cases of three patients who were admitted to Renmin Hospital, Wuhan University, China, for treating COVID-19 pneumonia in February 2020 and were unresponsive to initial treatment of steroids. They were then received titrated steroids treatment based on the assessment of computed tomography (CT) images augmented and analyzed with the artificial intelligence (AI) tool and output. Three patients were finally recovered and discharged. The result indicated that sufficient steroids may be effective in treating the COVID-19 patients after frequent evaluation and timely adjustment according to the disease severity assessed based on the quantitative analysis of the images of serial CT scans.",2020,10.3233/xst-200710,,,,
Target dose conversion modeling from pencil beam (PB) to Monte Carlo (MC) for lung SBRT,"BACKGROUND: A challenge preventing routine clinical implementation of Monte Carlo (MC)-based lung SBRT is the difficulty of reinterpreting historical outcome data calculated with inaccurate dose algorithms, because the target dose was found to decrease to varying degrees when recalculated with MC. The large variability was previously found to be affected by factors such as tumour size, location, and lung density, usually through sub-group comparisons. We hereby conducted a pilot study to systematically and quantitatively analyze these patient factors and explore accurate target dose conversion models, so that large-scale historical outcome data can be correlated with more accurate MC dose without recalculation. METHODS: Twenty-one patients that underwent SBRT for early-stage lung cancer were replanned with 6MV 360° dynamic conformal arcs using pencil-beam (PB) and recalculated with MC. The percent D95 difference (PB-MC) was calculated for the PTV and GTV. Using single linear regression, this difference was correlated with the following quantitative patient indices: maximum tumour diameter (MaxD); PTV and GTV volumes; minimum distance from tumour to soft tissue (dmin); and mean density and standard deviation of the PTV, GTV, PTV margin, lung, and 2 mm, 15 mm, 50 mm shells outside the PTV. Multiple linear regression and artificial neural network (ANN) were employed to model multiple factors and improve dose conversion accuracy. RESULTS: Single linear regression with PTV D95 deficiency identified the strongest correlation on mean-density (location) indices, weaker on lung density, and the weakest on size indices, with the following R(2) values in decreasing orders: shell2mm (0.71), PTV (0.68), PTV margin (0.65), shell15mm (0.62), shell50mm (0.49), lung (0.40), dmin (0.22), GTV (0.19), MaxD (0.17), PTV volume (0.15), and GTV volume (0.08). A multiple linear regression model yielded the significance factor of 3.0E-7 using two independent features: mean density of shell2mm (P = 1.6E-7) and PTV volume (P = 0.006). A 4-feature ANN model slightly improved the modeling accuracy. CONCLUSION: Quantifiable density features were proposed, replacing simple central/peripheral location designation, which showed strong correlations with PB-to-MC target dose conversion magnitude, followed by lung density and target size. Density in the immediate outer and inner areas of the PTV showed the strongest correlations. A multiple linear regression model with one such feature and PTV volume established a high significance factor, improving dose conversion accuracy.",2016,10.1186/s13014-016-0661-3,cross-sectional,treatment,CT,Lungs
TBNet: a context-aware graph network for tuberculosis diagnosis,"Tuberculosis (TB) is an infectious bacterial disease. It can affect the human lungs, brain, bones, and kidneys. Pulmonary tuberculosis is the most common. This airborne bacterium can be transmitted with the droplets by coughing and sneezing. So far, the most convenient and effective method for diagnosing TB is through medical imaging. Computed tomography (CT) is the first choice for lung imaging in clinics because the conditions of the lungs can be interpreted from CT images. However, manual screening poses an enormous burden for radiologists, resulting in high inter-observer variances. Hence, developing computer-aided diagnosis systems to implement automatic TB diagnosis is an emergent and significant task for researchers and practitioners. This paper proposed a novel context-aware graph neural network called TBNet to detect TB from chest CT images METHODS: Traditional convolutional neural networks can extract high-level image features to achieve good classification performance on the ImageNet dataset. However, we observed that the spatial relationships between the feature vectors are beneficial for the classification because the feature vector may share some common characteristics with its neighboring feature vectors. To utilize this context information for the classification of chest CT images, we proposed to use a feature graph to generate context-aware features. Finally, a context-aware random vector functional-link net served as the classifier of the TBNet to identify these context-aware features as TB or normal RESULTS: The proposed TBNet produced state-of-the-art classification performance for detecting TB from healthy samples in the experiments CONCLUSIONS: Our TBNet can be an accurate and effective verification tool for manual screening in clinical diagnosis.",2022,10.1016/j.cmpb.2021.106587,case control,diagnosis,CT,Lungs
Teacher-student approach for lung tumor segmentation from mixed-supervised datasets,"PURPOSE: Cancer is among the leading causes of death in the developed world, and lung cancer is the most lethal type. Early detection is crucial for better prognosis, but can be resource intensive to achieve. Automating tasks such as lung tumor localization and segmentation in radiological images can free valuable time for radiologists and other clinical personnel. Convolutional neural networks may be suited for such tasks, but require substantial amounts of labeled data to train. Obtaining labeled data is a challenge, especially in the medical domain. METHODS: This paper investigates the use of a teacher-student design to utilize datasets with different types of supervision to train an automatic model performing pulmonary tumor segmentation on computed tomography images. The framework consists of two models: the student that performs end-to-end automatic tumor segmentation and the teacher that supplies the student additional pseudo-annotated data during training. RESULTS: Using only a small proportion of semantically labeled data and a large number of bounding box annotated data, we achieved competitive performance using a teacher-student design. Models trained on larger amounts of semantic annotations did not perform better than those trained on teacher-annotated data. Our model trained on a small number of semantically labeled data achieved a mean dice similarity coefficient of 71.0 on the MSD Lung dataset. CONCLUSIONS: Our results demonstrate the potential of utilizing teacher-student designs to reduce the annotation load, as less supervised annotation schemes may be performed, without any real degradation in segmentation accuracy.",2022,10.1371/journal.pone.0266147,cross-sectional,diagnosis,CT,Lungs
Technical Note: 3D localization of lung tumors on cone beam CT projections via a convolutional recurrent neural network,"PURPOSE: To design a convolutional recurrent neural network (CRNN) that calculates three-dimensional (3D) positions of lung tumors from continuously acquired cone beam computed tomography (CBCT) projections, and facilitates the sorting and reconstruction of 4D-CBCT images. METHOD: Under an IRB-approved clinical lung protocol, kilovoltage (kV) projections of the setup CBCT were collected in free-breathing. Concurrently, an electromagnetic signal-guided system recorded motion traces of three transponders implanted in or near the tumor. Convolutional recurrent neural network was designed to utilize a convolutional neural network (CNN) for extracting relevant features of the kV projections around the tumor, followed by a recurrent neural network for analyzing the temporal patterns of the moving features. Convolutional recurrent neural network was trained on the simultaneously collected kV projections and motion traces, subsequently utilized to calculate motion traces solely based on the continuous feed of kV projections. To enhance performance, CRNN was also facilitated by frequent calibrations (e.g., at 10° gantry rotation intervals) derived from cross-correlation-based registrations between kV projections and templates created from the planning 4DCT. Convolutional recurrent neural network was validated on a leave-one-out strategy using data from 11 lung patients, including 5500 kV images. The root-mean-square error between the CRNN and motion traces was calculated to evaluate the localization accuracy. RESULT: Three-dimensional displacement around the simulation position shown in the Calypso traces was 3.4 ± 1.7 mm. Using motion traces as ground truth, the 3D localization error of CRNN with calibrations was 1.3 ± 1.4 mm. CRNN had a success rate of 86 ± 8% in determining whether the motion was within a 3D displacement window of 2 mm. The latency was 20 ms when CRNN ran on a high-performance computer cluster. CONCLUSIONS: CRNN is able to provide accurate localization of lung tumors with aid from frequent recalibrations using the conventional cross-correlation-based registration approach, and has the potential to remove reliance on the implanted fiducials.",2020,10.1002/mp.14007,cross-sectional,treatment,CT,Lungs
"Technical note: Evaluation of a V-Net autosegmentation algorithm for pediatric CT scans: Performance, generalizability, and application to patient-specific CT dosimetry","PURPOSE: This study developed and evaluated a fully convolutional network (FCN) for pediatric CT organ segmentation and investigated the generalizability of the FCN across image heterogeneities such as CT scanner model protocols and patient age. We also evaluated the autosegmentation models as part of a software tool for patient-specific CT dose estimation. METHODS: A collection of 359 pediatric CT datasets with expert organ contours were used for model development and evaluation. Autosegmentation models were trained for each organ using a modified FCN 3D V-Net. An independent test set of 60 patients was withheld for testing. To evaluate the impact of CT scanner model protocol and patient age heterogeneities, separate models were trained using a subset of scanner model protocols and pediatric age groups. Train and test sets were split to answer questions about the generalizability of pediatric FCN autosegmentation models to unseen age groups and scanner model protocols, as well as the merit of scanner model protocol or age-group-specific models. Finally, the organ contours resulting from the autosegmentation models were applied to patient-specific dose maps to evaluate the impact of segmentation errors on organ dose estimation. RESULTS: Results demonstrate that the autosegmentation models generalize to CT scanner acquisition and reconstruction methods which were not present in the training dataset. While models are not equally generalizable across age groups, age-group-specific models do not hold any advantage over combining heterogeneous age groups into a single training set. Dice similarity coefficient (DSC) and mean surface distance results are presented for 19 organ structures, for example, median DSC of 0.52 (duodenum), 0.74 (pancreas), 0.92 (stomach), and 0.96 (heart). The FCN models achieve a mean dose error within 5% of expert segmentations for all 19 organs except for the spinal canal, where the mean error was 6.31%. CONCLUSIONS: Overall, these results are promising for the adoption of FCN autosegmentation models for pediatric CT, including applications for patient-specific CT dose estimation.",2022,10.1002/mp.15521,cross-sectional,informatics,CT,Lungs
Technical Note: Synthesizing of lung tumors in computed tomography images,"PURPOSE: When investigating new radiation therapy techniques in the treatment planning stage, it can be extremely time consuming to locate multiple patient scans that match the desired characteristics for the treatment. With the help of machine learning, we propose to bypass the difficulty in finding patient computed tomography (CT) scans that match the treatment requirements. Furthermore, we aim to provide the developed method as a tool that is easily accessible to interested researchers. METHODS: We propose a generative adversarial network (GAN) to edit individual volumes of interest (VOIs) in pre-existing CT scans, translating features of the healthy VOIs into features of cancerous volumes. Training and testing was done using VOIs from a dataset of 460 diagnostic and lung cancer screening CT scans. Agreement between real tumors and those produced by the editor was tested by comparing the distributions of several histogram parameters and second-order statistics as well as using qualitative analysis. RESULTS: After training, the network was successfully able to map healthy CT segments to realistic looking cancerous volumes. Based on visual inspection, tumors produced by the editor were found to be both realistic and visually consistent with the surrounding anatomy when placed back into the original CT scan. Furthermore, the network was found to be able to extrapolate well beyond the upper size limit of the training set. Lastly, a graphical user interface (GUI) was developed to easily interact with the resulting network. CONCLUSION: The trained network and associated GUI can serve as a tool to develop an abundance of lung cancer patient data to be used in treatment planning. In addition, this method can be extended to a variety of cancer types if given an appropriate baseline dataset. The GUI and instructions on how to utilize the tool have been made publicly available at https://github.com/teaghan/CT_Editor.",2020,10.1002/mp.14437,cross-sectional,informatics,CT,Lungs
"Temporal changes of quantitative CT findings from 102 patients with COVID-19 in Wuhan, China: A longitudinal study","BACKGROUND: Computed tomography (CT) imaging combined with artificial intelligence is important in the diagnosis and prognosis of lung diseases. OBJECTIVE: This study aimed to investigate temporal changes of quantitative CT findings in patients with COVID-19 in three clinic types, including moderate, severe, and non-survivors, and to predict severe cases in the early stage from the results. METHODS: One hundred and two patients with confirmed COVID-19 were included in this study. Based on the time interval between onset of symptoms and the CT scan, four stages were defined in this study: Stage-1 (0 ∼7 days); Stage-2 (8 ∼ 14 days); Stage-3 (15 ∼ 21days); Stage-4 (> 21 days). Eight parameters, the infection volume and percentage of the whole lung in four different Hounsfield (HU) ranges, ((-, -750), [-750, -300), [-300, 50) and [50, +)), were calculated and compared between different groups. RESULTS: The infection volume and percentage of four HU ranges peaked in Stage-2. The highest proportion of HU [-750, 50) was found in the infected regions in non-survivors among three groups. CONCLUSIONS: The findings indicate rapid deterioration in the first week since the onset of symptoms in non-survivors. Higher proportion of HU [-750, 50) in the lesion area might be a potential bio-marker for poor prognosis in patients with COVID-19.",2021,10.3233/thc-218027,,,,
Tertiary lymphoid structures (TLS) identification and density assessment on H&E-stained digital slides of lung cancer,"Tertiary lymphoid structures (TLS) are ectopic aggregates of lymphoid cells in inflamed, infected, or tumoral tissues that are easily recognized on an H&E histology slide as discrete entities, distinct from lymphocytes. TLS are associated with improved cancer prognosis but there is no standardised method available to quantify their presence. Previous studies have used immunohistochemistry to determine the presence of specific cells as a marker of the TLS. This has now been proven to be an underestimate of the true number of TLS. Thus, we propose a methodology for the automated identification and quantification of TLS, based on H&E slides. We subsequently determined the mathematical criteria defining a TLS. TLS regions were identified through a deep convolutional neural network and segmentation of lymphocytes was performed through an ellipsoidal model. This methodology had a 92.87% specificity at 95% sensitivity, 88.79% specificity at 98% sensitivity and 84.32% specificity at 99% sensitivity level based on 144 TLS annotated H&E slides implying that the automated approach was able to reproduce the histopathologists' assessment with great accuracy. We showed that the minimum number of lymphocytes within TLS is 45 and the minimum TLS area is 6,245μm2. Furthermore, we have shown that the density of the lymphocytes is more than 3 times those outside of the TLS. The mean density and standard deviation of lymphocytes within a TLS area are 0.0128/μm2 and 0.0026/μm2 respectively compared to 0.004/μm2 and 0.001/μm2 in non-TLS regions. The proposed methodology shows great potential for automated identification and quantification of the TLS density on digital H&E slides.",2021,10.1371/journal.pone.0256907,,,,
Test-retest reproducibility of a deep learning-based automatic detection algorithm for the chest radiograph,"OBJECTIVES: To perform test-retest reproducibility analyses for deep learning-based automatic detection algorithm (DLAD) using two stationary chest radiographs (CRs) with short-term intervals, to analyze influential factors on test-retest variations, and to investigate the robustness of DLAD to simulated post-processing and positional changes. METHODS: This retrospective study included patients with pulmonary nodules resected in 2017. Preoperative CRs without interval changes were used. Test-retest reproducibility was analyzed in terms of median differences of abnormality scores, intraclass correlation coefficients (ICC), and 95% limits of agreement (LoA). Factors associated with test-retest variation were investigated using univariable and multivariable analyses. Shifts in classification between the two CRs were analyzed using pre-determined cutoffs. Radiograph post-processing (blurring and sharpening) and positional changes (translations in x- and y-axes, rotation, and shearing) were simulated and agreement of abnormality scores between the original and simulated CRs was investigated. RESULTS: Our study analyzed 169 patients (median age, 65 years; 91 men). The median difference of abnormality scores was 1-2% and ICC ranged from 0.83 to 0.90. The 95% LoA was approximately ± 30%. Test-retest variation was negatively associated with solid portion size (β, - 0.50; p = 0.008) and good nodule conspicuity (β, - 0.94; p < 0.001). A small fraction (15/169) showed discordant classifications when the high-specificity cutoff (46%) was applied to the model outputs (p = 0.04). DLAD was robust to the simulated positional change (ICC, 0.984, 0.996), but relatively less robust to post-processing (ICC, 0.872, 0.968). CONCLUSIONS: DLAD was robust to the test-retest variation. However, inconspicuous nodules may cause fluctuations of the model output and subsequent misclassifications. KEY POINTS: • The deep learning-based automatic detection algorithm was robust to the test-retest variation of the chest radiographs in general. • The test-retest variation was negatively associated with solid portion size and good nodule conspicuity. • High-specificity cutoff (46%) resulted in discordant classifications of 8.9% (15/169; p = 0.04) between the test-retest radiographs.",2020,10.1007/s00330-019-06589-8,cross-sectional,diagnosis,CT,Lungs
Texture Analysis in the Evaluation of COVID-19 Pneumonia in Chest X-Ray Images: A Proof of Concept Study,"BACKGROUND: One of the most challenging aspects related to Covid-19 is to establish the presence of infection in an early phase of the disease. Texture analysis might be an additional tool for the evaluation of Chest X-ray in patients with clinical suspicion of Covid-19 related pneumonia. OBJECTIVE: To evaluate the diagnostic performance of texture analysis and machine learning models for the diagnosis of Covid-19 interstitial pneumonia in Chest X-ray images. METHODS: Chest X-ray images were accessed from a publicly available repository(https://www.kaggle. com/tawsifurrahman/covid19-radiography-database). Lung areas were manually segmented using a polygonal region of interest covering both lung areas, using MaZda, a freely available software for texture analysis. A total of 308 features per ROI was extracted. One hundred-ten Covid-19 Chest X-ray images were selected for the final analysis. RESULTS: Six models, namely NB, GLM, DL, GBT, ANN, and PLS-DA were selected and ensembled. According to Youden's index, the Covid-19 Ensemble Machine Learning Score showing the highest area under the curve (0.971±0.015) was 132.57. Assuming this cut-off the Ensemble model performance was estimated by evaluating both true and false positive/negative, resulting in 91.8% accuracy with 93% sensitivity and 90% specificity. Moving the cut-off value to -100, although the accuracy resulted lower (90.6%), the Ensemble Machine Learning showed 100% sensitivity, with 80% specificity. CONCLUSION: Texture analysis of Chest X-ray images and machine learning algorithms may help in differentiating patients with Covid-19 pneumonia. Despite several limitations, this study can lay the ground for future research works in this field and help to develop more rapid and accurate screening tools for these patients.",2021,10.2174/1573405617999210112195450,cross-sectional,diagnosis,CXT,Lungs
Texture feature-based machine learning classifier could assist in the diagnosis of COVID-19,"PURPOSE: Differentiating COVID-19 from other acute infectious pneumonias rapidly is challenging at present. This study aims to improve the diagnosis of COVID-19 using computed tomography (CT). METHOD: COVID-19 was confirmed mainly by virus nucleic acid testing and epidemiological history according to WHO interim guidance, while other infectious pneumonias were diagnosed by antigen testing. The texture features were extracted from CT images by two radiologists with 5 years of work experience using modified wavelet transform and matrix computation analyses. The random forest (RF) classifier was applied to identify COVID-19 patients and images. RESULTS: We retrospectively analysed the data of 95 individuals (291 images) with COVID-19 and 96 individuals (279 images) with other acute infectious pneumonias, including 50 individuals (160 images) with influenza A/B. In total, 6 texture features showed a positive association with COVID-19, while 4 features were negatively associated. The mean AUROC, accuracy, sensitivity, and specificity values of the 5-fold test sets were 0.800, 0.722, 0.770, and 0.680 for image classification and 0.858, 0.826, 0.809, and 0.842 for individual classification, respectively. The feature 'Correlation' contributed most both at the image level and individual level, even compared with the clinical factors. In addition, the texture features could discriminate COVID-19 from influenza A/B, with an AUROC of 0.883 for images and 0.957 for individuals. CONCLUSIONS: The developed texture feature-based RF classifier could assist in the diagnosis of COVID-19, which could be a rapid screening tool in the era of pandemic.",2021,10.1016/j.ejrad.2021.109602,case control,diagnosis,CT,Lungs
The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT,"In 2020, the new type of coronal pneumonitis became a pandemic in the world, and has firstly been reported in Wuhan, China. Chest CT is a vital component in the diagnostic algorithm for patients with suspected or confirmed COVID-19 infection. Therefore, it is necessary to conduct automatic and accurate detection of COVID-19 by chest CT.The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT.From the COVID-19 cases in our institution, 136 moderate patients and 83 severe patients were screened, and their clinical and laboratory data on admission were collected for statistical analysis. Initial CT Radiomics were modeled by automatic machine learning, and diagnostic performance was evaluated according to AUC, TPR, TNR, PPV and NPV of the subjects. At the same time, the initial CT main features of the two groups were analyzed semi-quantitatively, and the results were statistically analyzed.There was a statistical difference in age between the moderate group and the severe group. The model cohort showed TPR 96.9%, TNR 99.1%, PPV98.4%, NPV98.2%, and AUC 0.98. The test cohort showed TPR 94.4%, TNR100%, PPV100%, NPV96.2%, and AUC 0.97. There was statistical difference between the two groups with grade 1 score (P = .001), the AUC of grade 1 score, grade 2 score, grade 3 score and CT score were 0.619, 0.519, 0.478 and 0.548, respectively.Radiomics' Auto ML model was built by CT image of initial COVID -19 pneumonia, and it proved to be effectively used to predict the clinical classification of COVID-19 pneumonia. CT features have limited ability to predict the clinical typing of Covid-19 pneumonia.",2021,10.1097/md.0000000000025307,cross-sectional,diagnosis,CT,Lungs
The deep learning model combining CT image and clinicopathological information for predicting ALK fusion status and response to ALK-TKI therapy in non-small cell lung cancer patients,"PURPOSE: This study aimed to investigate the deep learning model (DLM) combining computed tomography (CT) images and clinicopathological information for predicting anaplastic lymphoma kinase (ALK) fusion status in non-small cell lung cancer (NSCLC) patients. MATERIALS AND METHODS: Preoperative CT images, clinicopathological information as well as the ALK fusion status from 937 patients in three hospitals were retrospectively collected to train and validate the DLM for the prediction of ALK fusion status in tumors. Another cohort of patients (n = 91) received ALK tyrosine kinase inhibitor (TKI) treatment was also included to evaluate the value of the DLM in predicting the clinical outcomes of the patients. RESULTS: The performances of the DLM trained only by CT images in the primary and validation cohorts were AUC = 0.8046 (95% CI 0.7715-0.8378) and AUC = 0.7754 (95% CI 0.7199-0.8310), respectively, while the DLM trained by both CT images and clinicopathological information exhibited better performance for the prediction of ALK fusion status (AUC = 0.8540, 95% CI 0.8257-0.8823 in the primary cohort, p < 0.001; AUC = 0.8481, 95% CI 0.8036-0.8926 in the validation cohort, p < 0.001). In addition, the deep learning scores of the DLMs showed significant differences between the wild-type and ALK infusion tumors. In the ALK-target therapy cohort (n = 91), the patients predicted as ALK-positive by the DLM showed better performance of progression-free survival than the patients predicted as ALK-negative (16.8 vs. 7.5 months, p = 0.010). CONCLUSION: Our findings showed that the DLM trained by both CT images and clinicopathological information could effectively predict the ALK fusion status and treatment responses of patients. For the small size of the ALK-target therapy cohort, larger data sets would be collected to further validate the performance of the model for predicting the response to ALK-TKI treatment.",2021,10.1007/s00259-020-04986-6,cross-sectional,treatment,CT,Lungs
The detection of lung cancer using massive artificial neural network based on soft tissue technique,"BACKGROUND: A proposed computer aided detection (CAD) scheme faces major issues during subtle nodule recognition. However, radiologists have not noticed subtle nodules in beginning stage of lung cancer while a proposed CAD scheme recognizes non subtle nodules using x-ray images. METHOD: Such an issue has been resolved by creating MANN (Massive Artificial Neural Network) based soft tissue technique from the lung segmented x-ray image. A soft tissue image recognizes nodule candidate for feature extortion and classification. X-ray images are downloaded using Japanese society of radiological technology (JSRT) image set. This image set includes 233 images (140 nodule x-ray images and 93 normal x-ray images). A mean size for a nodule is 17.8 mm and it is validated with computed tomography (CT) image. Thirty percent (42/140) abnormal represents subtle nodules and it is split into five stages (tremendously subtle, very subtle, subtle, observable, relatively observable) by radiologists. RESULT: A proposed CAD scheme without soft tissue technique attained 66.42% (93/140) sensitivity and 66.76% accuracy having 2.5 false positives per image. Utilizing soft tissue technique, many nodules superimposed by ribs as well as clavicles have identified (sensitivity is 72.85% (102/140) and accuracy is 72.96% at one false positive rate). CONCLUSION: In particular, a proposed CAD system determine sensitivity and accuracy in support of subtle nodules (sensitivity is 14/42 = 33.33% and accuracy is 33.66%) is statistically higher than CAD (sensitivity is 13/42 = 30.95% and accuracy is 30.97%) scheme without soft tissue technique. A proposed CAD scheme attained tremendously minimum false positive rate and it is a promising technique in support of cancerous recognition due to improved sensitivity and specificity.",2020,10.1186/s12911-020-01220-z,cross-sectional,diagnosis,CXR,Lungs
The Effect of CT Scan Parameters on the Measurement of CT Radiomic Features: A Lung Nodule Phantom Study,"The purpose of this study was to explore the effects of CT slice thickness, reconstruction algorithm, and radiation dose on quantification of CT features to characterize lung nodules using a chest phantom. Spherical lung nodule phantoms of known densities (-630 and + 100 HU) were inserted into an anthropomorphic thorax phantom. CT scan was performed ten times with relocations. CT data were reconstructed using 12 different imaging settings; three different slice thicknesses of 1.25, 2.5, and 5.0 mm, two reconstruction kernels of sharp and standard, and two radiation dose of 30 mAs and 12 mAs. Lesions were segmented using a semiautomated method. Twenty representative CT quantitative features representing CT density and texture were compared using multiple regression analysis. In 100 HU nodule phantoms, 18 and 19 among 20 computer features showed significant difference between different mAs and reconstruction algorithms, respectively (p ≤ 0.05). 20, 19, and 19 computer features showed difference between slice thickness of 5.0 vs 1.25, 5.0 vs 2.5, and 2.5 vs 1.25 mm, respectively (p ≤ 0.05). In -630 HU nodule phantoms, 18 and 19 showed significant difference between different mAs and reconstruction algorithms, respectively (p ≤ 0.05). 18, 11, and 17 computer features showed difference between slice thickness of 5.0 vs 1.25, 5.0 vs 2.5, and 2.5 vs 1.25 mm, respectively (p ≤ 0.05). When comparing the absolute value of regression coefficient, the effect of slice thickness in 100 HU nodule and reconstruction algorithm in -630 HU nodule was greater than the effect of remaining scan parameters. The slice thickness, mAs, and reconstruction algorithm had a significant impact on the quantitative image features. In clinical studies involving deep learning or radiomics, it should be noted that differences in values can occur when using computer features obtained from different CT scan parameters in combination. Therefore, when interpreting the statistical analysis results, it is necessary to reflect the difference in the computer features depending on the scan parameters.",2019,10.1155/2019/8790694,,,,
The effect of pulmonary vessel suppression on computerized detection of nodules in chest CT scans,"PURPOSE: In chest computed tomography (CT) scans, pulmonary vessel suppression can make pulmonary nodules more evident, and therefore may increase the detectability of early lung cancer. The purpose of this study was to develop a computer-aided detection (CAD) system with a vessel suppression function and to verify the effectiveness of the vessel suppression on the performance of the pulmonary nodule CAD system. METHODS: A CAD system with a vessel suppression function capable of suppressing vessels and detecting nodules was developed. First, a convolutional neural network (CNN)-based pulmonary vessel suppression technique was employed to remove the vessels from lungs while preserving the nodules. Then, a CNN-based pulmonary nodule detector was utilized to sequentially generate nodule candidates and reduce false positives (FPs). The performance levels of CAD systems with and without the vessel suppression function were compared using 888 three-dimensional chest CT scans from the Lung Nodule Analysis 2016 (LUNA16) dataset. The pulmonary nodule detection results were quantitatively evaluated using the average sensitivity at seven predefined FP rates: 0.125, 0.25, 0.5, 1, 2, 4, and 8 FPs per scan. RESULTS: The developed pulmonary nodule CAD system improved the average sensitivity to 0.977 from 0.950 owing to the addition of the vessel suppression function. CONCLUSIONS: The vessel suppression function considerably improved the performance of the CAD system for pulmonary nodule detection. In practice, it would be embedded in CAD systems to assist radiologists in detecting pulmonary nodules in chest CT scans.",2020,10.1002/mp.14401,cross-sectional,diagnosis,CT,Lungs
The Effects of Perinodular Features on Solid Lung Nodule Classification,"Lung cancer is the most lethal malignant neoplasm worldwide, with an annual estimated rate of 1.8 million deaths. Computed tomography has been widely used to diagnose and detect lung cancer, but its diagnosis remains an intricate and challenging work, even for experienced radiologists. Computer-aided diagnosis tools and radiomics tools have provided support to the radiologist's decision, acting as a second opinion. The main focus of these tools has been to analyze the intranodular zone; nevertheless, recent works indicate that the interaction between the nodule and its surroundings (perinodular zone) could be relevant to the diagnosis process. However, only a few works have investigated the importance of specific attributes of the perinodular zone and have shown how important they are in the classification of lung nodules. In this context, the purpose of this work is to evaluate the impact of using the perinodular zone on the characterization of lung lesions. Motivated by reproducible research, we used a large public dataset of solid lung nodule images and extracted fine-tuned radiomic attributes from the perinodular and intranodular zones. Our best-evaluated model obtained an average AUC of 0.916, an accuracy of 84.26%, a sensitivity of 84.45%, and specificity of 83.84%. The combination of attributes from the perinodular and intranodular zones in the image characterization resulted in an improvement in all the metrics analyzed when compared to intranodular-only characterization. Therefore, our results highlighted the importance of using the perinodular zone in the solid pulmonary nodules classification process.",2021,10.1007/s10278-021-00453-2,,,,
The effects of physics-based data augmentation on the generalizability of deep neural networks: Demonstration on nodule false-positive reduction,"PURPOSE: An important challenge for deep learning models is generalizing to new datasets that may be acquired with acquisition protocols different from the training set. It is not always feasible to expand training data to the range encountered in clinical practice. We introduce a new technique, physics-based data augmentation (PBDA), that can emulate new computed tomography (CT) data acquisition protocols. We demonstrate two forms of PBDA, emulating increases in slice thickness and reductions of dose, on the specific problem of false-positive reduction in the automatic detection of lung nodules. METHODS: We worked with CT images from the lung image database consortium (LIDC) collection. We employed a hybrid ensemble convolutional neural network (CNN), which consists of multiple CNN modules (VGG, DenseNet, ResNet), for a classification task of determining whether an image patch was a suspicious nodule or a false positive. To emulate a reduction in tube current, we injected noise by simulating forward projection, noise addition, and backprojection corresponding to 1.5 mAs (a ""chest x-ray"" dose). To simulate thick slice CT scans from thin slice CT scans, we grouped and averaged spatially contiguous CT within thin slice data. The neural network was trained with 10% of the LIDC dataset that was selected to have either the highest tube current or the thinnest slices. The network was tested on the remaining data. We compared PBDA to a baseline with standard geometric augmentations (such as shifts and rotations) and Gaussian noise addition. RESULTS: PBDA improved the performance of the networks when generalizing to the test dataset in a limited number of cases. We found that the best performance was obtained by applying augmentation at very low doses (1.5 mAs), about an order of magnitude less than most screening protocols. In the baseline augmentation, a comparable level of Gaussian noise was injected. For dose reduction PBDA, the average sensitivity of 0.931 for the hybrid ensemble network was not statistically different from the average sensitivity of 0.935 without PBDA. Similarly for slice thickness PBDA, the average sensitivity of 0.900 when augmenting with doubled simulated slice thicknesses was not statistically different from the average sensitivity of 0.895 without PBDA. While there were cases detailed in this paper in which we observed improvements, the overall picture was one that suggests PBDA may not be an effective data enrichment tool. CONCLUSIONS: PBDA is a newly proposed strategy for mitigating the performance loss of neural networks related to the variation of acquisition protocol between the training dataset and the data that is encountered in deployment or testing. We found that PBDA does not provide robust improvements with the four neural networks (three modules and the ensemble) tested and for the specific task of false-positive reduction in nodule detection.",2019,10.1002/mp.13755,cross-sectional,diagnosis,CT,Lungs
The evolution of computer-based analysis of high-resolution CT of the chest in patients with IPF,"In patients with idiopathic pulmonary fibrosis (IPF), there is an urgent need of biomarkers which can predict disease behaviour or response to treatment. Most published studies report results based on continuous data which can be difficult to apply to individual patients in clinical practice. Having antifibrotic therapies makes it even more important that we can accurately diagnose and prognosticate in IPF patients. Advances in computer technology over the past decade have provided computer-based methods for objectively quantifying fibrotic lung disease on high-resolution CT of the chest with greater strength than visual CT analysis scores. These computer-based methods and, more recently, the arrival of deep learning-based image analysis might provide a response to these unsolved problems. The purpose of this commentary is to provide insights into the problems associated with visual interpretation of HRCT, describe of the current technologies used to provide quantification of disease on HRCT and prognostication in IPF patients, discuss challenges to the implementation of this technology and future directions.",2022,10.1259/bjr.20200944,,,,
The General Explanation Method with NMR Spectroscopy Enables the Identification of Metabolite Profiles Specific for Normal and Tumor Cell Lines,"Machine learning models in metabolomics, despite their great prediction accuracy, are still not widely adopted owing to the lack of an efficient explanation for their predictions. In this study, we propose the use of the general explanation method to explain the predictions of a machine learning model to gain detailed insight into metabolic differences between biological systems. The method was tested on a dataset of (1) H NMR spectra acquired on normal lung and mesothelial cell lines and their tumor counterparts. Initially, the random forests and artificial neural network models were applied to the dataset, and excellent prediction accuracy was achieved. The predictions of the models were explained with the general explanation method, which enabled identification of discriminating metabolic concentration differences between individual cell lines and enabled the construction of their specific metabolic concentration profiles. This intuitive and robust method holds great promise for in-depth understanding of the mechanisms that underline phenotypes as well as for biomarker discovery in complex diseases.",2018,10.1002/cbic.201800392,,,,
The human-AI scoring system: A new method for CT-based assessment of COVID-19 severity,"BACKGROUND: Chest computed tomography (CT) plays an important role in the diagnosis and assessment of coronavirus disease 2019 (COVID-19). OBJECTIVE: To evaluate the value of an artificial intelligence (AI) scoring system for radiologically assessing the severity of COVID-19. MATERIALS AND METHODS: Chest CT images of 81 patients (61 of normal type and 20 of severe type) with confirmed COVID-19 were used. The test data were anonymized. The scores achieved by four methods (junior radiologists; AI scoring system; human-AI segmentation system; human-AI scoring system) were compared with that by two experienced radiologists (reference score). The mean absolute errors (MAEs) between the four methods and experienced radiologists were calculated separately. The Wilcoxon test is used to predict the significance of the severity of COVID-19. Then use Spearman correlation analysis ROC analysis was used to evaluate the performance of different scores. RESULTS: The AI score had a relatively low MAE (1.67-2.21). Score of human-AI scoring system had the lowest MAE (1.67), a diagnostic value almost equal to reference score (r= 0.97), and a strongest correlation with clinical severity (r= 0.59, p< 0.001). The AUCs of reference score, score of junior radiologists, AI score, score of human-AI segmentation system, and score of human-AI scoring system were 0.874, 0.841, 0.852, 0.857 and 0.865, respectively. CONCLUSION: The human-AI scoring system can help radiologists to improve the accuracy of COVID-19 severity assessment.",2022,10.3233/thc-213199,cross-sectional,diagnosis,CT,Lungs
The image quality of deep-learning image reconstruction of chest CT images on a mediastinal window setting,"AIM: To assess the image quality of deep-learning image reconstruction (DLIR) of chest computed tomography (CT) images on a mediastinal window setting in comparison to an adaptive statistical iterative reconstruction (ASiR-V). MATERIALS AND METHODS: Thirty-six patients were evaluated retrospectively. All patients underwent contrast-enhanced chest CT and thin-section images were reconstructed using filtered back projection (FBP); ASiR-V (60% and 100% blending setting); and DLIR (low, medium, and high settings). Image noise, signal-to-noise ratio (SNR), and contrast-to-noise ratio (CNR) were evaluated objectively. Two independent radiologists evaluated ASiR-V 60% and DLIR subjectively, in comparison with FBP, on a five-point scale in terms of noise, streak artefact, lymph nodes, small vessels, and overall image quality on a mediastinal window setting (width 400 HU, level 60 HU). In addition, image texture of ASiR-Vs (60% and 100%) and DLIR-high was analysed subjectively. RESULTS: Compared with ASiR-V 60%, DLIR-med and DLIR-high showed significantly less noise, higher SNR, and higher CNR (p<0.0001). DLIR-high and ASiR-V 100% were not significantly different regarding noise (p=0.2918) and CNR (p=0.0642). At a higher DLIR setting, noise was lower and SNR and CNR were higher (p<0.0001). DLIR-high showed the best subjective scores for noise, streak artefact, and overall image quality (p<0.0001). Compared with ASiR-V 60%, DLIR-med and DLIR-high scored worse in the assessment of small vessels (p<0.0001). The image texture of DLIR-high was significantly finer than that of ASIR-Vs (p<0.0001). CONCLUSIONS: DLIR-high improved the objective parameters and subjective image quality by reducing noise and streak artefacts and providing finer image texture.",2021,10.1016/j.crad.2020.10.011,cross-sectional,informatics,CT,Lungs
The importance of standardisation - COVID-19 CT & Radiograph Image Data Stock for deep learning purpose,"With the number of affected individuals still growing world-wide, the research on COVID-19 is continuously expanding. The deep learning community concentrates their efforts on exploring if neural networks can potentially support the diagnosis using CT and radiograph images of patients' lungs. The two most popular publicly available datasets for COVID-19 classification are COVID-CT and COVID-19 Image Data Collection. In this work, we propose a new dataset which we call COVID-19 CT & Radiograph Image Data Stock. It contains both CT and radiograph samples of COVID-19 lung findings and combines them with additional data to ensure a sufficient number of diverse COVID-19-negative samples. Moreover, it is supplemented with a carefully defined split. The aim of COVID-19 CT & Radiograph Image Data Stock is to create a public pool of CT and radiograph images of lungs to increase the efficiency of distinguishing COVID-19 disease from other types of pneumonia and from healthy chest. We hope that the creation of this dataset would allow standardisation of the approach taken for training deep neural networks for COVID-19 classification and eventually for building more reliable models.",2020,10.1016/j.compbiomed.2020.104092,cross-sectional,diagnosis,CT,Lungs
The incremental value of computed tomography of COVID-19 pneumonia in predicting ICU admission,"Triage is crucial for patient's management and estimation of the required intensive care unit (ICU) beds is fundamental for health systems during the COVID-19 pandemic. We assessed whether chest computed tomography (CT) of COVID-19 pneumonia has an incremental role in predicting patient's admission to ICU. We performed volumetric and texture analysis of the areas of the affected lung in CT of 115 outpatients with COVID-19 infection presenting to the emergency room with dyspnea and unresponsive hypoxyemia. Admission blood laboratory including lymphocyte count, serum lactate dehydrogenase, D-dimer and C-reactive protein and the ratio between the arterial partial pressure of oxygen and inspired oxygen were collected. By calculating the areas under the receiver-operating characteristic curves (AUC), we compared the performance of blood laboratory-arterial gas analyses features alone and combined with the CT features in two hybrid models (Hybrid radiological and Hybrid radiomics)for predicting ICU admission. Following a machine learning approach, 63 patients were allocated to the training and 52 to the validation set. Twenty-nine (25%) of patients were admitted to ICU. The Hybrid radiological model comprising the lung %consolidation performed significantly (p = 0.04) better in predicting ICU admission in the validation (AUC = 0.82; 95% confidence interval 0.73-0.97) set than the blood laboratory-arterial gas analyses features alone (AUC = 0.71; 95% confidence interval 0.56-0.86). A risk calculator for ICU admission was derived and is available at: https://github.com/cgplab/covidapp . The volume of the consolidated lung in CT of patients with COVID-19 pneumonia has a mild but significant incremental value in predicting ICU admission.",2021,10.1038/s41598-021-95114-3,cross-sectional,diagnosis,CT,Lungs
The Influence of a Coherent Annotation and Synthetic Addition of Lung Nodules for Lung Segmentation in CT Scans,"Lung cancer is a highly prevalent pathology and a leading cause of cancer-related deaths. Most patients are diagnosed when the disease has manifested itself, which usually is a sign of lung cancer in an advanced stage and, as a consequence, the 5-year survival rates are low. To increase the chances of survival, improving the cancer early detection capacity is crucial, for which computed tomography (CT) scans represent a key role. The manual evaluation of the CTs is a time-consuming task and computer-aided diagnosis (CAD) systems can help relieve that burden. The segmentation of the lung is one of the first steps in these systems, yet it is very challenging given the heterogeneity of lung diseases usually present and associated with cancer development. In our previous work, a segmentation model based on a ResNet34 and U-Net combination was developed on a cross-cohort dataset that yielded good segmentation masks for multiple pathological conditions but misclassified some of the lung nodules. The multiple datasets used for the model development were originated from different annotation protocols, which generated inconsistencies for the learning process, and the annotations are usually not adequate for lung cancer studies since they did not comprise lung nodules. In addition, the initial datasets used for training presented a reduced number of nodules, which was showed not to be enough to allow the segmentation model to learn to include them as a lung part. In this work, an objective protocol for the lung mask's segmentation was defined and the previous annotations were carefully reviewed and corrected to create consistent and adequate ground-truth masks for the development of the segmentation model. Data augmentation with domain knowledge was used to create lung nodules in the cases used to train the model. The model developed achieved a Dice similarity coefficient (DSC) above 0.9350 for all test datasets and it showed an ability to cope, not only with a variety of lung patterns, but also with the presence of lung nodules as well. This study shows the importance of using consistent annotations for the supervised learning process, which is a very time-consuming task, but that has great importance to healthcare applications. Due to the lack of massive datasets in the medical field, which consequently brings a lack of wide representativity, data augmentation with domain knowledge could represent a promising help to overcome this limitation for learning models development.",2022,10.3390/s22093443,cross-sectional,diagnosis,CT,Lungs
The Invasiveness Classification of Ground-Glass Nodules Using 3D Attention Network and HRCT,"The early stage lung cancer often appears as ground-glass nodules (GGNs). The diagnosis of GGN as preinvasive lesion (PIL) or invasive adenocarcinoma (IA) is very important for further treatment planning. This paper proposes an automatic GGNs' invasiveness classification algorithm for the adenocarcinoma. 1431 clinical cases and a total of 1624 GGNs (3-30 mm) were collected from Shanghai Cancer Center for the study. The data is in high-resolution computed tomography (HRCT) format. Firstly, the automatic GGN detector which is composed by a 3D U-Net and a 3D multi-receptive field (multi-RF) network detects the location of GGNs. Then, a deep 3D convolutional neural network (3D-CNN) called Attention-v1 is used to identify the GGNs' invasiveness. The attention mechanism was introduced to the 3D-CNN. This paper conducted a contract experiment to compare the performance of Attention-v1, ResNet, and random forest algorithm. ResNet is one of the most advanced convolutional neural network structures. The competition performance metrics (CPM) of automatic GGN detector reached 0.896. The accuracy, sensitivity, specificity, and area under curve (AUC) value of Attention-v1 structure are 85.2%, 83.7%, 86.3%, and 92.6%. The algorithm proposed in this paper outperforms ResNet and random forest in sensitivity, accuracy, and AUC value. The deep 3D-CNN's classification result is better than traditional machine learning method. Attention mechanism improves 3D-CNN's performance compared with the residual block. The automatic GGN detector with the addition of Attention-v1 can be used to construct the GGN invasiveness classification algorithm to help the patients and doctors in treatment.",2020,10.1007/s10278-020-00355-9,cross-sectional,diagnosis,CT,Lungs
The method and efficacy of support vector machine classifiers based on texture features and multi-resolution histogram from (18)F-FDG PET-CT images for the evaluation of mediastinal lymph nodes in patients with lung cancer,"OBJECTIVES: In clinical practice, image analysis is dependent on simply visual perception and the diagnostic efficacy of this analysis pattern is limited for mediastinal lymph nodes in patients with lung cancer. In order to improve diagnostic efficacy, we developed a new computer-based algorithm and tested its diagnostic efficacy. METHODS: 132 consecutive patients with lung cancer underwent (18)F-FDG PET/CT examination before treatment. After all data were imported into the database of an on-line medical image analysis platform, the diagnostic efficacy of visual analysis was first evaluated without knowing pathological results, and the maximum short diameter and maximum standardized uptake value (SUVmax) were measured. Then lymph nodes were segmented manually. Three classifiers based on support vector machine (SVM) were constructed from CT, PET, and combined PET-CT images, respectively. The diagnostic efficacy of SVM classifiers was obtained and evaluated. RESULTS: According to ROC curves, the areas under curves for maximum short diameter and SUVmax were 0.684 and 0.652, respectively. The areas under the ROC curve for SVM1, SVM2, and SVM3 were 0.689, 0.579, and 0.685, respectively. CONCLUSION: The algorithm based on SVM was potential in the diagnosis of mediastinal lymph nodes.",2015,10.1016/j.ejrad.2014.11.006,cross-sectional,diagnosis,PET-CT,Lungs
The Potential Role of Grid-Like Software in Bedside Chest Radiography in Improving Image Quality and Dose Reduction: An Observer Preference Study,"OBJECTIVE: To compare the observer preference of image quality and radiation dose between non-grid, grid-like, and grid images. MATERIALS AND METHODS: Each of the 38 patients underwent bedside chest radiography with and without a grid. A grid-like image was generated from a non-grid image using SimGrid software (Samsung Electronics Co. Ltd.) employing deep-learning-based scatter correction technology. Two readers recorded the preference for 10 anatomic landmarks and the overall appearance on a five-point scale for a pair of non-grid and grid-like images, and a pair of grid-like and grid images, respectively, which were randomly presented. The dose area product (DAP) was also recorded. Wilcoxon's rank sum test was used to assess the significance of preference. RESULTS: Both readers preferred grid-like images to non-grid images significantly (p < 0.001); with a significant difference in terms of the preference for grid images to grid-like images (p = 0.317, 0.034, respectively). In terms of anatomic landmarks, both readers preferred grid-like images to non-grid images (p < 0.05). No significant differences existed between grid-like and grid images except for the preference for grid images in proximal airways by two readers, and in retrocardiac lung and thoracic spine by one reader. The median DAP were 1.48 (range, 1.37-2.17) dGy(*)cm(2) in grid images and 1.22 (range, 1.11-1.78) dGy(*)cm(2) in grid-like images with a significant difference (p < 0.001). CONCLUSION: The SimGrid software significantly improved the image quality of non-grid images to a level comparable to that of grid images with a relatively lower level of radiation exposure.",2018,10.3348/kjr.2018.19.3.526,cross-sectional,informatics,CXR,Lungs
The predictive power of artificial intelligence on mediastinal lymphnode metastasis,"OBJECTIVE: The aim of this study was to create the preoperative predictive model on mediastinal lymph-node metastasis based on artificial intelligence in surgically resected lung adenocarcinoma. METHODS: We enrolled 301 surgical resections of patients with clinical stage N0-1 lung adenocarcinoma, who received positron emission tomography preoperatively between 2015 and 2019. We randomly assigned the patients into two groups: the training (n = 201) and validation groups (n = 100). The training group was used to obtain basic data for learning by artificial intelligence, whereas the validation group was used to verify the constructed algorithm. We used an automatic machine learning platform, to create artificial intelligence model. For comparison, multivariate analysis was performed in the training group, whereas for calculating and verifying the prediction accuracy rate, significant predicting factors were applied to the validation group. RESULTS: Of the 301 patients, 41 patients were diagnosed as mediastinal lymph node metastasis. In multivariate analysis, the maximum standardized uptake value was an individual predictive factor. The accuracy rate of artificial intelligence model was 84%, and the specificity was 98% which were higher than those of the maximum standardized uptake value (61% and 57%). However, in terms of sensitivity, artificial intelligence model remarked low at 12%. CONCLUSIONS: An artificial intelligence-based diagnostic algorithm showed remarkable specificity compared with the maximum standardized uptake value. Although this model is not ready to practical use and the result was preliminary because of poor sensitivity, artificial intelligence could be able to complement the shortcomings of existing diagnostic modalities.",2021,10.1007/s11748-021-01671-9,cross-sectional,diagnosis,CT,Lungs
The Predictive Role of Artificial Intelligence-Based Chest CT Quantification in Patients with COVID-19 Pneumonia,"We sought to analyze the prognostic value of laboratory and clinical data, and an artificial intelligence (AI)-based algorithm for Coronavirus disease 2019 (COVID-19) severity scoring, on CT-scans of patients hospitalized with COVID-19. Moreover, we aimed to determine personalized probabilities of clinical deterioration. Data of symptomatic patients with COVID-19 who underwent chest-CT-examination at the time of hospital admission between April and November 2020 were analyzed. COVID-19 severity score was automatically quantified for each pulmonary lobe as the percentage of affected lung parenchyma with the AI-based algorithm. Clinical deterioration was defined as a composite of admission to the intensive care unit, need for invasive mechanical ventilation, use of vasopressors or in-hospital mortality. In total 326 consecutive patients were included in the analysis (mean age 66.7 ± 15.3 years, 52.1% male) of whom 85 (26.1%) experienced clinical deterioration. In the multivariable regression analysis prior myocardial infarction (OR = 2.81, 95% CI = 1.12-7.04, p = 0.027), immunodeficiency (OR = 2.08, 95% CI = 1.02-4.25, p = 0.043), C-reactive protein (OR = 1.73, 95% CI = 1.32-2.33, p < 0.001) and AI-based COVID-19 severity score (OR = 1.08; 95% CI = 1.02-1.15, p = 0.013) appeared to be independent predictors of clinical deterioration. Personalized probability values were determined. AI-based COVID-19 severity score assessed at hospital admission can provide additional information about the prognosis of COVID-19, possibly serving as a useful tool for individualized risk-stratification.",2021,10.3390/tomography7040058,cross-sectional,prognosis,CT,Lungs
The Regimen of Computed Tomography Screening for Lung Cancer: Lessons Learned Over 25 Years From the International Early Lung Cancer Action Program,"We learned many unanticipated and valuable lessons since we started planning our study of low-dose computed tomography (CT) screening for lung cancer in 1991. The publication of the baseline results of the Early Lung Cancer Action Project (ELCAP) in Lancet 1999 showed that CT screening could identify a high proportion of early, curable lung cancers. This stimulated large national screening studies to be quickly started. The ELCAP design, which provided evidence about screening in the context of a clinical program, was able to rapidly expand to a 12-institution study in New York State (NY-ELCAP) and to many international institutions (International-ELCAP), ultimately working with 82 institutions, all using the common I-ELCAP protocol. This expansion was possible because the investigators had developed the ELCAP Management System for screening, capturing data and CT images, and providing for quality assurance. This advanced registry and its rapid accumulation of data and images allowed continual assessment and updating of the regimen of screening as advances in knowledge and new technology emerged. For example, in the initial ELCAP study, introduction of helical CT scanners had allowed imaging of the entire lungs in a single breath, but the images were obtained in 10 mm increments resulting in about 30 images per person. Today, images are obtained in submillimeter slice thickness, resulting in around 700 images per person, which are viewed on high-resolution monitors. The regimen provides the imaging acquisition parameters, imaging interpretation, definition of positive result, and the recommendations for further workup, which now include identification of emphysema and coronary artery calcifications. Continual updating is critical to maximize the benefit of screening and to minimize potential harms. Insights were gained about the natural history of lung cancers, identification and management of nodule subtypes, increased understanding of nodule imaging and pathologic features, and measurement variability inherent in CT scanners. The registry also provides the foundation for assessment of new statistical techniques, including artificial intelligence, and integration of effective genomic and blood-based biomarkers, as they are developed.",2021,10.1097/rti.0000000000000538,,,,
The Relative Importance of Clinical and Socio-demographic Variables in Prognostic Prediction in Non-Small Cell Lung Cancer: A Variable Importance Approach,"BACKGROUND: Prognostic modeling in health care has been predominantly statistical, despite a rapid growth of literature on machine-learning approaches in biological data analysis. We aim to assess the relative importance of variables in predicting overall survival among patients with non-small cell lung cancer using a Variable Importance (VIMP) approach in a machine-learning Random Survival Forest (RSF) model for posttreatment planning and follow-up. METHODS: A total of 935 non-small cell lung cancer patients were randomly and equally divided into 2 training and testing cohorts in an RFS model. The prognostic variables included age, sex, race, the TNM Classification of Malignant Tumors (TNM) stage, smoking history, Eastern Cooperative Oncology Group performance status, histologic type, treatment category, maximum standard uptake value of whole-body tumor (SUVmaxWB), whole-body metabolic tumor volume (MTVwb), and Charlson Comorbidity Index. The VIMP was calculated using a permutation method in the RSF model. We further compared the VIMP of the RSF model to that of the standard Cox survival model. We examined the order of VIMP with the differential functional forms of the variables. RESULTS: In both the RSF and the standard Cox models, the most important variables are treatment category, TNM stage, and MTVwb. The order of VIMP is more robust in RSF model than in Cox model regarding the differential functional forms of the variables. CONCLUSIONS: The RSF VIMP approach can be applied alongside with the Cox model to further advance the understanding of the roles of prognostic factors, and improve prognostic precision and care efficiency.",2020,10.1097/mlr.0000000000001288,,,,
The RSNA International COVID-19 Open Radiology Database (RICORD),"The coronavirus disease 2019 (COVID-19) pandemic is a global health care emergency. Although reverse-transcription polymerase chain reaction testing is the reference standard method to identify patients with COVID-19 infection, chest radiography and CT play a vital role in the detection and management of these patients. Prediction models for COVID-19 imaging are rapidly being developed to support medical decision making. However, inadequate availability of a diverse annotated data set has limited the performance and generalizability of existing models. To address this unmet need, the RSNA and Society of Thoracic Radiology collaborated to develop the RSNA International COVID-19 Open Radiology Database (RICORD). This database is the first multi-institutional, multinational, expert-annotated COVID-19 imaging data set. It is made freely available to the machine learning community as a research and educational resource for COVID-19 chest imaging. Pixel-level volumetric segmentation with clinical annotations was performed by thoracic radiology subspecialists for all COVID-19-positive thoracic CT scans. The labeling schema was coordinated with other international consensus panels and COVID-19 data annotation efforts, the European Society of Medical Imaging Informatics, the American College of Radiology, and the American Association of Physicists in Medicine. Study-level COVID-19 classification labels for chest radiographs were annotated by three radiologists, with majority vote adjudication by board-certified radiologists. RICORD consists of 240 thoracic CT scans and 1000 chest radiographs contributed from four international sites. It is anticipated that RICORD will ideally lead to prediction models that can demonstrate sustained performance across populations and health care systems.",2021,10.1148/radiol.2021203957,,,,
The usage of deep neural network improves distinguishing COVID-19 from other suspected viral pneumonia by clinicians on chest CT: a real-world study,"OBJECTIVES: Based on the current clinical routine, we aimed to develop a novel deep learning model to distinguish coronavirus disease 2019 (COVID-19) pneumonia from other types of pneumonia and validate it with a real-world dataset (RWD). METHODS: A total of 563 chest CT scans of 380 patients (227/380 were diagnosed with COVID-19 pneumonia) from 5 hospitals were collected to train our deep learning (DL) model. Lung regions were extracted by U-net, then transformed and fed to pre-trained ResNet-50-based IDANNet (Identification and Analysis of New covid-19 Net) to produce a diagnostic probability. Fivefold cross-validation was employed to validate the application of our model. Another 318 scans of 316 patients (243/316 were diagnosed with COVID-19 pneumonia) from 2 other hospitals were enrolled prospectively as the RWDs to testify our DL model's performance and compared it with that from 3 experienced radiologists. RESULTS: A three-dimensional DL model was successfully established. The diagnostic threshold to differentiate COVID-19 and non-COVID-19 pneumonia was 0.685 with an AUC of 0.906 (95% CI: 0.886-0.913) in the internal validation group. In the RWD cohort, our model achieved an AUC of 0.868 (95% CI: 0.851-0.876) with the sensitivity of 0.811 and the specificity of 0.822, non-inferior to the performance of 3 experienced radiologists, suggesting promising clinical practical usage. CONCLUSIONS: The established DL model was able to achieve accurate identification of COVID-19 pneumonia from other suspected ones in the real-world situation, which could become a reliable tool in clinical routine. KEY POINTS: • In an internal validation set, our DL model achieved the best performance to differentiate COVID-19 from non-COVID-19 pneumonia with a sensitivity of 0.836, a specificity of 0.800, and an AUC of 0.906 (95% CI: 0.886-0.913) when the threshold was set at 0.685. • In the prospective RWD cohort, our DL diagnostic model achieved a sensitivity of 0.811, a specificity of 0.822, and AUC of 0.868 (95% CI: 0.851-0.876), non-inferior to the performance of 3 experienced radiologists. • The attention heatmaps were fully generated by the model without additional manual annotation and the attention regions were highly aligned with the ROIs acquired by human radiologists for diagnosis.",2021,10.1007/s00330-020-07553-7,cross-sectional,diagnosis,CT,Lungs
"The utility of a convolutional neural network (CNN) model score for cancer risk in indeterminate small solid pulmonary nodules, compared to clinical practice according to British Thoracic Society guidelines","PURPOSE: To determine how implementation of an artificial intelligence nodule algorithm, the Lung Cancer Prediction Convolutional Neural Network (LCP-CNN), at the point of incidental nodule detection would have influenced further investigation and management using a series of threshold scores at both the benign and malignant end of the spectrum. METHOD: An observational retrospective study was performed in the assessment of nodules between 5-15 mm (158 benign, 32 malignant) detected on CT scans, which were performed as part of routine practice. The LCP-CNN was applied to the baseline CT scan producing a percentage score, and subsequent imaging and management determined for each threshold group. We hypothesized that the 5% low risk threshold group requires only one follow-up, the 0.56% very low risk threshold group requires no follow-up and the 80% high risk threshold group warrants expedited intervention. RESULTS: The 158 benign nodules had an LCP-CNN score between 0.1 and 70.8%, median 5.5% (IQR 1.4-18.0), whilst the 32 cancer nodules had an LCP-CNN score between 10.1 and 98.7%, median 59.0% (IQR 37.1-83.9). 24/61 CT scans in the 0.56-5% group (n = 37) and 21/21 CT scans <0.56% group (n = 13) could be obviated resulting in an overall reduction of 18.6% (45/242) CT scans in the benign cohort. In the 80% group (n = 10), expedited intervention of malignant nodules could result in a 3.6-month reduction in time delay in 5 cancer patients. CONCLUSION: We show the potential of artificial intelligence to reduce the need for follow-up scans and intervention in low-scoring benign nodules, whilst potentially accelerating the investigation and treatment of high-scoring cancer nodules.",2021,10.1016/j.ejrad.2021.109553,cross-sectional,prognosis,CT,Lungs
"The Value of Artificial Intelligence Film Reading System Based on Deep Learning in the Diagnosis of Non-Small-Cell Lung Cancer and the Significance of Efficacy Monitoring: A Retrospective, Clinical, Nonrandomized, Controlled Study","OBJECTIVE: To explore the value of artificial intelligence (AI) film reading system based on deep learning in the diagnosis of non-small-cell lung cancer (NSCLC) and the significance of curative effect monitoring. METHODS: We retrospectively selected 104 suspected NSCLC cases from the self-built chest CT pulmonary nodule database in our hospital, and all of them were confirmed by pathological examination. The lung CT images of the selected patients were introduced into the AI reading system of pulmonary nodules, and the recording software automatically identified the nodules, and the results were compared with the results of the original image report. The nodules detected by the AI software and film readers were evaluated by two chest experts and recorded their size and characteristics. Comparison of calculation sensitivity, false positive rate evaluation of the NSCLC software, and physician's efficiency of nodule detection whether there was a significant difference between the two groups. RESULTS: The sensitivity, specificity, accuracy, positive predictive rate, and false positive rate of NSCLC diagnosed by radiologists were 72.94% (62/85), 92.06% (58/63), 81.08% (62+58/148), 92.53% (62/67), and 7.93% (5/63), respectively. The sensitivity, specificity, accuracy, positive prediction rate, and false positive rate of AI film reading system in the diagnosis of NSCLC were 94.12% (80/85), 77.77% (49/63), 87.161% (80 + 49/148), 85.11% (80/94), and 22.22% (14/63), respectively. Compared with radiologists, the sensitivity and false positive rate of artificial intelligence film reading system in the diagnosis of NSCLC were higher (P < 0.05). The sensitivity, specificity, accuracy, positive prediction rate, and negative prediction rate of artificial intelligence film reading system in evaluating the efficacy of patients with NSCLC were 87.50% (63/72), 69.23% (9/13), 84.70% (63 + 9)/85, 94.02% (63/67), and 50% (9/18), respectively. CONCLUSION: The AI film reading system based on deep learning has higher sensitivity for the diagnosis of NSCLC than radiologists and can be used as an auxiliary detection tool for doctors to screen for NSCLC, but its false positive rate is relatively high. Attention should be paid to identification. Meanwhile, the AI film reading system based on deep learning also has a certain guiding significance for the diagnosis and treatment monitoring of NSCLC.",2022,10.1155/2022/2864170,cross-sectional,diagnosis,CT,Lungs
The value of nodal information in predicting lung cancer relapse using 4DPET/4DCT,"PURPOSE: There is evidence that computed tomography (CT) and positron emission tomography (PET) imaging metrics are prognostic and predictive in nonsmall cell lung cancer (NSCLC) treatment outcomes. However, few studies have explored the use of standardized uptake value (SUV)-based image features of nodal regions as predictive features. The authors investigated and compared the use of tumor and node image features extracted from the radiotherapy target volumes to predict relapse in a cohort of NSCLC patients undergoing chemoradiation treatment. METHODS: A prospective cohort of 25 patients with locally advanced NSCLC underwent 4DPET/4DCT imaging for radiation planning. Thirty-seven image features were derived from the CT-defined volumes and SUVs of the PET image from both the tumor and nodal target regions. The machine learning methods of logistic regression and repeated stratified five-fold cross-validation (CV) were used to predict local and overall relapses in 2 yr. The authors used well-known feature selection methods (Spearman's rank correlation, recursive feature elimination) within each fold of CV. Classifiers were ranked on their Matthew's correlation coefficient (MCC) after CV. Area under the curve, sensitivity, and specificity values are also presented. RESULTS: For predicting local relapse, the best classifier found had a mean MCC of 0.07 and was composed of eight tumor features. For predicting overall relapse, the best classifier found had a mean MCC of 0.29 and was composed of a single feature: the volume greater than 0.5 times the maximum SUV (N). CONCLUSIONS: The best classifier for predicting local relapse had only tumor features. In contrast, the best classifier for predicting overall relapse included a node feature. Overall, the methods showed that nodes add value in predicting overall relapse but not local relapse.",2015,10.1118/1.4926755,cross-sectional,prognosis,CT,Lungs
The year in cardiovascular medicine 2021: heart failure and cardiomyopathies,"In the year 2021, the universal definition and classification of heart failure (HF) was published that defines HF as a clinical syndrome with symptoms and/or signs caused by a cardiac abnormality and corroborated by elevated natriuretic peptide levels or objective evidence of cardiogenic congestion. This definition and the classification of HF with reduced ejection fraction (HFrEF), mildly reduced, and HF with preserved ejection fraction (HFpEF) is consistent with the 2021 ESC Guidelines on HF. Among several other new recommendations, these guidelines give a Class I indication for the use of the sodium-glucose co-transporter 2 (SGLT2) inhibitors dapagliflozin and empagliflozin in HFrEF patients. As the first evidence-based treatment for HFpEF, in the EMPEROR-Preserved trial, empagliflozin reduced the composite endpoint of cardiovascular death and HF hospitalizations. Several reports in 2021 have provided novel and detailed analyses of device and medical therapy in HF, especially regarding sacubitril/valsartan, SGLT2 inhibitors, mineralocorticoid receptor antagonists, ferric carboxymaltose, soluble guanylate cyclase activators, and cardiac myosin activators. In patients hospitalized with COVID-19, acute HF and myocardial injury is quite frequent, whereas myocarditis and long-term damage to the heart are rather uncommon.",2022,10.1093/eurheartj/ehab887,,,,
Think positive: An interpretable neural network for image recognition,"The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X-rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48%, 0.99, 0.99 and 0.99, respectively.",2022,10.1016/j.neunet.2022.03.034,cross-sectional,diagnosis,CT,Lungs
Thoracic Point-of-Care Ultrasound: A SARS-CoV-2 Data Repository for Future Artificial Intelligence and Machine Learning,"Current experience suggests that artificial intelligence (AI) and machine learning (ML) may be useful in the management of hospitalized patients, including those with COVID-19. In light of the challenges faced with diagnostic and prognostic indicators in SARS-CoV-2 infection, our center has developed an international clinical protocol to collect standardized thoracic point of care ultrasound data in these patients for later AI/ML modeling. We surmise that in the future AI/ML may assist in the management of SARS-CoV-2 patients potentially leading to improved outcomes, and to that end, a corpus of curated ultrasound images and linked patient clinical metadata is an invaluable research resource.",2021,10.1177/15533506211018671,,,,
Three cuts method for identification of COPD,"Two main forms of COPD (Chronic Obstructive Pulmonary Disease) refer to a group of lung diseases that block airflow and cause a huge degree of human suffering. A new method for identifying and estimating the severity of COPD from three-dimensional (3-D) pulmonary X-ray CT images would be helpful for evaluation of treatment effects and early diagnosing is presented in this paper. This method has five main steps. Firstly, corresponding positions of lungs in inspiration and expiration are found based on anatomical structures. Secondly, lung regions are segmented from the CT images by active contours. Next, the left and right lungs are separated using a sequence of morphological operations. Then, parenchyma variations of three main cuts which selected by a feed-forward neural network are found based on the inspiratory and expiratory states. Finally, a pattern classifier is used to decide about the disease and its severity. Twenty patients with air-trapping problems and twelve normal adults were enrolled in this study. Based on the results, a mathematical model was developed to relate variations of lung volumes to severity of disease. The sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) and the accuracy of our method for right regions were %81.6, %80.5, %87.5, %72.5 and %81.3 respectively. And these parameters for left regions were %90, %83.3, %90, %83.3 and %87.5 respectively. The proposed method may assist radiologists in detection of Asthma and COPD as a computer aided diagnosis (CAD) system.",2013,,cross-sectional,diagnosis,CT,Lungs
Three-Dimensional Analysis of Particle Distribution on Filter Layers inside N95 Respirators by Deep Learning,"The global COVID-19 pandemic has changed many aspects of daily lives. Wearing personal protective equipment, especially respirators (face masks), has become common for both the public and medical professionals, proving to be effective in preventing spread of the virus. Nevertheless, a detailed understanding of respirator filtration-layer internal structures and their physical configurations is lacking. Here, we report three-dimensional (3D) internal analysis of N95 filtration layers via X-ray tomography. Using deep learning methods, we uncover how the distribution and diameters of fibers within these layers directly affect contaminant particle filtration. The average porosity of the filter layers is found to be 89.1%. Contaminants are more efficiently captured by denser fiber regions, with fibers <1.8 μm in diameter being particularly effective, presumably because of the stronger electric field gradient on smaller diameter fibers. This study provides critical information for further development of N95-type respirators that combine high efficiency with good breathability.",2021,10.1021/acs.nanolett.0c04230,,,,
Three-dimensional dose prediction for lung IMRT patients with deep neural networks: robust learning from heterogeneous beam configurations,"PURPOSE: The use of neural networks to directly predict three-dimensional dose distributions for automatic planning is becoming popular. However, the existing methods use only patient anatomy as input and assume consistent beam configuration for all patients in the training database. The purpose of this work was to develop a more general model that considers variable beam configurations in addition to patient anatomy to achieve more comprehensive automatic planning with a potentially easier clinical implementation, without the need to train specific models for different beam settings. METHODS: The proposed anatomy and beam (AB) model is based on our newly developed deep learning architecture, and hierarchically densely connected U-Net (HD U-Net), which combines U-Net and DenseNet. The AB model contains 10 input channels: one for beam setup and the other 9 for anatomical information (PTV and organs). The beam setup information is represented by a 3D matrix of the non-modulated beam's eye view ray-tracing dose distribution. We used a set of images from 129 patients with lung cancer treated with IMRT with heterogeneous beam configurations (4-9 beams of various orientations) for training/validation (100 patients) and testing (29 patients). Mean squared error was used as the loss function. We evaluated the model's accuracy by comparing the mean dose, maximum dose, and other relevant dose-volume metrics for the predicted dose distribution against those of the clinically delivered dose distribution. Dice similarity coefficients were computed to address the spatial correspondence of the isodose volumes between the predicted and clinically delivered doses. The model was also compared with our previous work, the anatomy only (AO) model, which does not consider beam setup information and uses only 9 channels for anatomical information. RESULTS: The AB model outperformed the AO model, especially in the low and medium dose regions. In terms of dose-volume metrics, AB outperformed AO by about 1-2%. The largest improvement was found to be about 5% in lung volume receiving a dose of 5Gy or more (V(5) ). The improvement for spinal cord maximum dose was also important, that is, 3.6% for cross-validation and 2.6% for testing. The AB model achieved Dice scores for isodose volumes as much as 10% higher than the AO model in low and medium dose regions and about 2-5% higher in high dose regions. CONCLUSIONS: The AO model, which does not use beam configuration as input, can still predict dose distributions with reasonable accuracy in high dose regions but introduces large errors in low and medium dose regions for IMRT cases with variable beam numbers and orientations. The proposed AB model outperforms the AO model substantially in low and medium dose regions, and slightly in high dose regions, by considering beam setup information through a cumulative non-modulated beam's eye view ray-tracing dose distribution. This new model represents a major step forward towards predicting 3D dose distributions in real clinical practices, where beam configuration could vary from patient to patient, from planner to planner, and from institution to institution.",2019,10.1002/mp.13597,cross-sectional,treatment,CT,Lungs
Three-dimensional SVM with latent variable: application for detection of lung lesions in CT images,"The study aims to improve the performance of current computer-aided schemes for the detection of lung lesions, especially the low-contrast in gray density or irregular in shape. The relative position between suspected lesion and whole lung is, for the first time, added as a latent feature to enrich current Three-dimensional (3D) features such as shape, texture. Subsequently, 3D matrix patterns-based Support Vector Machine (SVM) with the latent variable, referred to as L-SVM3Dmatrix, was constructed accordingly. A CT image database containing 750 abnormal cases with 1050 lesions was used to train and evaluate several similar computer-aided detection (CAD) schemes: traditional features-based SVM (SVMfeature), 3D matrix patterns-based SVM (SVM3Dmatrix) and L-SVM3Dmatrix. The classifier performances were evaluated by computing the area under the ROC curve (AUC), using a 5-fold cross-validation. The L-SVM3Dmatrix sensitivity was 93.0 with 1.23% percentage of False Positive (FP), the SVM3Dmatrix sensitivity was 88.4 with 1.49% percentage of FP, and the SVMfeature sensitivity was 87.2 with 1.78% percentage of FP. The L-SVM3Dmatrix outperformed other current lung CAD schemes, especially regarding the difficult lesions.",2015,10.1007/s10916-014-0171-5,cross-sectional,diagnosis,CT,Lungs
Three-Dimensional Texture Feature Analysis of Pulmonary Nodules in CT Images: Lung Cancer Predictive Models Based on Support Vector Machine Classifier,"To extract texture features of pulmonary nodules from three-dimensional views and to assess if predictive models of lung CT images from a three-dimensional texture feature could improve assessments conducted by radiologists. Clinical and CT imaging data for three dimensions (axial, coronal, and sagittal) in pulmonary nodules in 285 patients were collected from multiple centers and the Cancer Imaging Archive after ethics committee approval. Three-dimensional texture feature values (contourlets), and clinical and computed tomography (CT) imaging data were built into support vector machine (SVM) models to predict lung cancer, using four evaluation methods (disjunctive, conjunctive, voting, and synthetic); sensitivity, specificity, the Youden index, discriminant power (DP), and F value were calculated to assess model effectiveness. Additionally, diagnostic accuracy (three-dimensional model, axial model, and radiologist assessment) was assessed using the area under the curves for receiver operating characteristic (ROC) curves. Cross-sectional data from 285 patients (median age, 62 [range, 45-83] years; 115 males [40.4%]) were evaluated. Integrating three-dimensional assessments, the voting method had relatively high effectiveness based on both sensitivity (0.98) and specificity (0.79), which could improve radiologist diagnosis (maximum sensitivity, 0.75; maximum specificity, 0.51) for 23% and 28% respectively. Furthermore, the three-dimensional texture feature model of the voting method has the best diagnosis of precision rate (95.4%). Of all three-dimensional texture feature methods, the result of the voting method was the best, maintaining both high sensitivity and specificity scores. Additionally, the three-dimensional texture feature models were superior to two-dimensional models and radiologist-based assessments.",2020,10.1007/s10278-019-00238-8,cross-sectional,diagnosis,CT,Lungs
Three-Terminal Ovonic Threshold Switch (3T-OTS) with Tunable Threshold Voltage for Versatile Artificial Sensory Neurons,"Inspired by information processing in biological systems, sensor-combined edge-computing systems attract attention requesting artificial sensory neurons as essential ingredients. Here, we introduce a simple and versatile structure of artificial sensory neurons based on a novel three-terminal Ovonic threshold switch (3T-OTS), which features an electrically controllable threshold voltage (V(th)). Combined with a sensor driving an output voltage, this 3T-OTS generates spikes with a frequency depending on an external stimulus. As a proof of concept, we have built an artificial retinal ganglion cell (RGC) by combining a 3T-OTS and a photodiode. Furthermore, this artificial RGC is combined with the reservoir-computing technique to perform a classification of chest X-ray images for normal, viral pneumonia, and COVID-19 infections, releasing the recognition accuracy of about 86.5%. These results indicate that the 3T-OTS is highly promising for applications in neuromorphic sensory systems, providing a building block for energy-efficient in-sensor computing devices.",2022,10.1021/acs.nanolett.1c04125,cross-sectional,diagnosis,CXR,Lungs
Tiled Sparse Coding in Eigenspaces for Image Classification,"The automation in the diagnosis of medical images is currently a challenging task. The use of Computer Aided Diagnosis (CAD) systems can be a powerful tool for clinicians, especially in situations when hospitals are overflowed. These tools are usually based on artificial intelligence (AI), a field that has been recently revolutionized by deep learning approaches. These alternatives usually obtain a large performance based on complex solutions, leading to a high computational cost and the need of having large databases. In this work, we propose a classification framework based on sparse coding. Images are first partitioned into different tiles, and a dictionary is built after applying PCA to these tiles. The original signals are then transformed as a linear combination of the elements of the dictionary. Then, they are reconstructed by iteratively deactivating the elements associated with each component. Classification is finally performed employing as features the subsequent reconstruction errors. Performance is evaluated in a real context where distinguishing between four different pathologies: control versus bacterial pneumonia versus viral pneumonia versus COVID-19. Our system differentiates between pneumonia patients and controls with an accuracy of 97.74%, whereas in the 4-class context the accuracy is 86.73%. The excellent results and the pioneering use of sparse coding in this scenario evidence that our proposal can assist clinicians when their workload is high.",2022,10.1142/s0129065722500071,,,,
Toward an Expert Level of Lung Cancer Detection and Classification Using a Deep Convolutional Neural Network,"BACKGROUND: Computed tomography (CT) is essential for pulmonary nodule detection in diagnosing lung cancer. As deep learning algorithms have recently been regarded as a promising technique in medical fields, we attempt to integrate a well-trained deep learning algorithm to detect and classify pulmonary nodules derived from clinical CT images. MATERIALS AND METHODS: Open-source data sets and multicenter data sets have been used in this study. A three-dimensional convolutional neural network (CNN) was designed to detect pulmonary nodules and classify them into malignant or benign diseases based on pathologically and laboratory proven results. RESULTS: The sensitivity and specificity of this well-trained model were found to be 84.4% (95% confidence interval [CI], 80.5%-88.3%) and 83.0% (95% CI, 79.5%-86.5%), respectively. Subgroup analysis of smaller nodules (<10 mm) have demonstrated remarkable sensitivity and specificity, similar to that of larger nodules (10-30 mm). Additional model validation was implemented by comparing manual assessments done by different ranks of doctors with those performed by three-dimensional CNN. The results show that the performance of the CNN model was superior to manual assessment. CONCLUSION: Under the companion diagnostics, the three-dimensional CNN with a deep learning algorithm may assist radiologists in the future by providing accurate and timely information for diagnosing pulmonary nodules in regular clinical practices. IMPLICATIONS FOR PRACTICE: The three-dimensional convolutional neural network described in this article demonstrated both high sensitivity and high specificity in classifying pulmonary nodules regardless of diameters as well as superiority compared with manual assessment. Although it still warrants further improvement and validation in larger screening cohorts, its clinical application could definitely facilitate and assist doctors in clinical practice.",2019,10.1634/theoncologist.2018-0908,cross-sectional,diagnosis,CT,Lungs
Toward automatic prediction of EGFR mutation status in pulmonary adenocarcinoma with 3D deep learning,"To develop a deep learning system based on 3D convolutional neural networks (CNNs), and to automatically predict EGFR-mutant pulmonary adenocarcinoma in CT images. A dataset of 579 nodules with EGFR mutation status labels of mutant (Mut) or wild-type (WT) was retrospectively analyzed. A deep learning system, namely 3D DenseNets, was developed to process 3D patches of nodules from CT data, and learn strong representations with supervised end-to-end training. The 3D DenseNets were trained with a training subset of 348 nodules and tuned with a development subset of 116 nodules. A strong data augmentation technique, mixup, was used for better generalization. We evaluated our model on a holdout subset of 115 nodules. An independent public dataset of 37 nodules from the cancer imaging archive (TCIA) was also used to test the generalization of our method. Conventional radiomics analysis was also performed for comparison. Our method achieved promising performance on predicting EGFR mutation status, with AUCs of 75.8% and 75.0% for our holdout test set and public test set, respectively. Moreover, strong relations were found between deep learning feature and conventional radiomics, while deep learning worked through an enhanced radiomics manner, that is, deep learned radiomics (DLR), in terms of robustness, compactness and expressiveness. The proposed deep learning system predicts EGFR-mutant of lung adenocarcinomas in CT images noninvasively and automatically, indicating its potential to help clinical decision-making by identifying eligible patients of pulmonary adenocarcinoma for EGFR-targeted therapy.",2019,10.1002/cam4.2233,cross-sectional,diagnosis,CT,Lungs
Toward optical spectroscopy-guided lung biopsy: Demonstration of tissue-type classification,"The diagnostic yield of standard tissue-sampling modalities of suspected lung cancers, whether by bronchoscopy or interventional radiology, can be nonoptimal, varying with the size and location of lesions. What is needed is an insitu sensor, integrated in the biopsy tool, to objectively distinguish among tissue types in real time, not to replace biopsy with an optical diagnostic, but to verify that the sampling tool is properly located within the target lesion. We investigated the feasibility of elastic scattering spectroscopy (ESS), coupled with machine learning, to distinguish lung lesions from the various nearby tissue types, in a study with freshly-excised lung tissues from surgical resections. Optical spectra were recorded with an ESS fiberoptic probe in different areas of the resected pulmonary tissues, including benign-margin tissue sites as well as the periphery and core of the lesion. An artificial-intelligence model was used to analyze, retrospectively, 2032 measurements from excised tissues of 35 patients. With high accuracy, ESS was able to distinguish alveolar tissue from bronchi, alveolar tissue from lesions, and bronchi from lesions. This ex vivo study indicates promise for ESS fiberoptic probes to be integrated with surgical intervention tools, to improve reliability of pulmonary lesion targeting.",2021,10.1002/jbio.202100132,,,,
Toward predicting the evolution of lung tumors during radiotherapy observed on a longitudinal MR imaging study via a deep learning algorithm,"PURPOSE: To predict the spatial and temporal trajectories of lung tumor during radiotherapy monitored under a longitudinal magnetic resonance imaging (MRI) study via a deep learning algorithm for facilitating adaptive radiotherapy (ART). METHODS: We monitored 10 lung cancer patients by acquiring weekly MRI-T2w scans over a course of radiotherapy. Under an ART workflow, we developed a predictive neural network (P-net) to predict the spatial distributions of tumors in the coming weeks utilizing images acquired earlier in the course. The three-step P-net consisted of a convolutional neural network to extract relevant features of the tumor and its environment, followed by a recurrence neural network constructed with gated recurrent units to analyze trajectories of tumor evolution in response to radiotherapy, and finally an attention model to weight the importance of weekly observations and produce the predictions. The performance of P-net was measured with Dice and root mean square surface distance (RMSSD) between the algorithm-predicted and experts-contoured tumors under a leave-one-out scheme. RESULTS: Tumor shrinkage was 60% ± 27% (mean ± standard deviation) by the end of radiotherapy across nine patients. Using images from the first three weeks, P-net predicted tumors on future weeks (4, 5, 6) with a Dice and RMSSD of (0.78 ± 0.22, 0.69 ± 0.24, 0.69 ± 0.26), and (2.1 ± 1.1 mm, 2.3 ± 0.8 mm, 2.6 ± 1.4 mm), respectively. CONCLUSION: The proposed deep learning algorithm can capture and predict spatial and temporal patterns of tumor regression in a longitudinal imaging study. It closely follows the clinical workflow, and could facilitate the decision-making of ART. A prospective study including more patients is warranted.",2019,10.1002/mp.13765,cross-sectional,treatment,MRI,Lungs
Towards automatic pulmonary nodule management in lung cancer screening with deep learning,"The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type that surpasses the one of classical machine learning approaches and is within the inter-observer variability among four experienced human observers.",2017,10.1038/srep46479,cross-sectional,diagnosis,CT,Lungs
Towards large-scale case-finding: training and validation of residual networks for detection of chronic obstructive pulmonary disease using low-dose CT,"BACKGROUND: Chronic obstructive pulmonary disease (COPD) is underdiagnosed in the community. Thoracic CT scans are widely used for diagnostic and screening purposes for lung cancer. In this proof-of-concept study, we aimed to evaluate a software pipeline for the automated detection of COPD, based on deep learning and a dataset of low-dose CTs that were performed for early detection of lung cancer. METHODS: We examined the use of deep residual networks, a type of artificial residual network, for the automated detection of COPD. Three versions of the residual networks were independently trained to perform COPD diagnosis using random subsets of CT scans collected from the PanCan study, which enrolled ex-smokers and current smokers at high risk of lung cancer, and evaluated the networks using three-fold cross-validation experiments. External validation was performed using 2153 CT scans acquired from a separate cohort of individuals with COPD in the ECLIPSE study. Spirometric data were used to define COPD, with stages defined according to the GOLD criteria. FINDINGS: The best performing networks achieved an area under the receiver operating characteristic curve (AUC) of 0·889 (SD 0·017) in three-fold cross-validation experiments. When the same set of networks was applied to the ECLIPSE cohort without any modifications to the trained models, they achieved an AUC of 0·886 (0·017), a positive predictive value of 0·847 (0·056), and a negative predictive value of 0·755 (0·097), which is a greater performance than the best quantitative CT measure, the percentage of lung volumes of less than or equal to -950 Hounsfield units (AUC 0·742). INTERPRETATION: Our proposed approach could identify patients with COPD among ex-smokers and current smokers without a previous diagnosis of COPD, with clinically acceptable performance. The use of deep residual networks on chest CT scans could be an effective case-finding tool for COPD detection and diagnosis, particularly in ex-smokers and current smokers who are being screened for lung cancer. FUNDING: Data Science Institute, University of British Columbia; Canadian Institutes of Health Research.",2020,10.1016/s2589-7500(20)30064-9,cross-sectional,diagnosis,CT,Lungs
Towards radiologist-level cancer risk assessment in CT lung screening using deep learning,"PURPOSE: Lung cancer is the leading cause of cancer mortality in the US, responsible for more deaths than breast, prostate, colon and pancreas cancer combined and large population studies have indicated that low-dose computed tomography (CT) screening of the chest can significantly reduce this death rate. Recently, the usefulness of Deep Learning (DL) models for lung cancer risk assessment has been demonstrated. However, in many cases model performances are evaluated on small/medium size test sets, thus not providing strong model generalization and stability guarantees which are necessary for clinical adoption. In this work, our goal is to contribute towards clinical adoption by investigating a deep learning framework on larger and heterogeneous datasets while also comparing to state-of-the-art models. METHODS: Three low-dose CT lung cancer screening datasets were used: National Lung Screening Trial (NLST, n = 3410), Lahey Hospital and Medical Center (LHMC, n = 3154) data, Kaggle competition data (from both stages, n = 1397 + 505) and the University of Chicago data (UCM, a subset of NLST, annotated by radiologists, n = 132). At the first stage, our framework employs a nodule detector; while in the second stage, we use both the image context around the nodules and nodule features as inputs to a neural network that estimates the malignancy risk for the entire CT scan. We trained our algorithm on a part of the NLST dataset, and validated it on the other datasets. Special care was taken to ensure there was no patient overlap between the train and validation sets. RESULTS AND CONCLUSIONS: The proposed deep learning model is shown to: (a) generalize well across all three data sets, achieving AUC between 86% to 94%, with our external test-set (LHMC) being at least twice as large compared to other works; (b) have better performance than the widely accepted PanCan Risk Model, achieving 6 and 9% better AUC score in our two test sets; (c) have improved performance compared to the state-of-the-art represented by the winners of the Kaggle Data Science Bowl 2017 competition on lung cancer screening; (d) have comparable performance to radiologists in estimating cancer risk at a patient level.",2021,10.1016/j.compmedimag.2021.101883,cross-sectional,diagnosis,CT,Lungs
Towards robust diagnosis of COVID-19 using vision self-attention transformer,"The outbreak of COVID-19, since its appearance, has affected about 200 countries and endangered millions of lives. COVID-19 is extremely contagious disease, and it can quickly incapacitate the healthcare systems if infected cases are not handled timely. Several Conventional Neural Networks (CNN) based techniques have been developed to diagnose the COVID-19. These techniques require a large, labelled dataset to train the algorithm fully, but there are not too many labelled datasets. To mitigate this problem and facilitate the diagnosis of COVID-19, we developed a self-attention transformer-based approach having self-attention mechanism using CT slices. The architecture of transformer can exploit the ample unlabelled datasets using pre-training. The paper aims to compare the performances of self-attention transformer-based approach with CNN and Ensemble classifiers for diagnosis of COVID-19 using binary Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection and multi-class Hybrid-learning for UnbiaSed predicTion of COVID-19 (HUST-19) CT scan dataset. To perform this comparison, we have tested Deep learning-based classifiers and ensemble classifiers with proposed approach using CT scan images. Proposed approach is more effective in detection of COVID-19 with an accuracy of 99.7% on multi-class HUST-19, whereas 98% on binary class SARS-CoV-2 dataset. Cross corpus evaluation achieves accuracy of 93% by training the model with Hust19 dataset and testing using Brazilian COVID dataset.",2022,10.1038/s41598-022-13039-x,cross-sectional,diagnosis,CT,Lungs
Transcriptional Profiling of Endobronchial Ultrasound-Guided Lymph Node Samples Aids Diagnosis of Mediastinal Lymphadenopathy,"BACKGROUND: Endobronchial ultrasound (EBUS)-guided biopsy is the mainstay for investigation of mediastinal lymphadenopathy for laboratory diagnosis of malignancy, sarcoidosis, or TB. However, improved methods for discriminating between TB and sarcoidosis and excluding malignancy are still needed. We sought to evaluate the role of genomewide transcriptional profiling to aid diagnostic processes in this setting. METHODS: Mediastinal lymph node samples from 88 individuals were obtained by EBUS-guided aspiration for investigation of mediastinal lymphadenopathy and subjected to transcriptional profiling in addition to conventional laboratory assessments. Computational strategies were used to evaluate the potential for using the transcriptome to distinguish between diagnostic categories. RESULTS: Molecular signatures associated with granulomas or neoplastic and metastatic processes were clearly discernible in granulomatous and malignant lymph node samples, respectively. Support vector machine (SVM) learning using differentially expressed genes showed excellent sensitivity and specificity profiles in receiver operating characteristic curve analysis with area under curve values > 0.9 for discriminating between granulomatous and nongranulomatous disease, TB and sarcoidosis, and between cancer and reactive lymphadenopathy. A two-step decision tree using SVM to distinguish granulomatous and nongranulomatous disease, then between TB and sarcoidosis in granulomatous cases, and between cancer and reactive lymphadenopathy in nongranulomatous cases, achieved > 90% specificity for each diagnosis and afforded greater sensitivity than existing tests to detect TB and cancer. In some diagnostically ambiguous cases, computational classification predicted granulomatous disease or cancer before pathologic abnormalities were evident. CONCLUSIONS: Machine learning analysis of transcriptional profiling in mediastinal lymphadenopathy may significantly improve the clinical utility of EBUS-guided biopsies.",2016,10.1378/chest.15-0647,,,,
Transfer learning based novel ensemble classifier for COVID-19 detection from chest CT-scans,"Coronavirus Disease 2019 (COVID-19) is a deadly infection that affects the respiratory organs in humans as well as animals. By 2020, this disease turned out to be a pandemic affecting millions of individuals across the globe. Conducting rapid tests for a large number of suspects preventing the spread of the virus has become a challenge. In the recent past, several deep learning based approaches have been developed for automating the process of detecting COVID-19 infection from Lung Computerized Tomography (CT) scan images. However, most of them rely on a single model prediction for the final decision which may or may not be accurate. In this paper, we propose a novel ensemble approach that aggregates the strength of multiple deep neural network architectures before arriving at the final decision. We use various pre-trained models such as VGG16, VGG19, InceptionV3, ResNet50, ResNet50V2, InceptionResNetV2, Xception, and MobileNet and fine-tune them using Lung CT Scan images. All these trained models are further used to create a strong ensemble classifier that makes the final prediction. Our experiments exhibit that the proposed ensemble approach is superior to existing ensemble approaches and set state-of-the-art results for detecting COVID-19 infection from lung CT scan images.",2022,10.1016/j.compbiomed.2021.105127,cross-sectional,diagnosis,CT,Lungs
Transfer Learning for Multicenter Classification of Chronic Obstructive Pulmonary Disease,"Chronic obstructive pulmonary disease (COPD) is a lung disease that can be quantified using chest computed tomography scans. Recent studies have shown that COPD can be automatically diagnosed using weakly supervised learning of intensity and texture distributions. However, up till now such classifiers have only been evaluated on scans from a single domain, and it is unclear whether they would generalize across domains, such as different scanners or scanning protocols. To address this problem, we investigate classification of COPD in a multicenter dataset with a total of 803 scans from three different centers, four different scanners, with heterogenous subject distributions. Our method is based on Gaussian texture features, and a weighted logistic classifier, which increases the weights of samples similar to the test data. We show that Gaussian texture features outperform intensity features previously used in multicenter classification tasks. We also show that a weighting strategy based on a classifier that is trained to discriminate between scans from different domains can further improve the results. To encourage further research into transfer learning methods for the classification of COPD, upon acceptance of this paper we will release two feature datasets used in this study on http://bigr.nl/research/projects/copd.",2018,10.1109/jbhi.2017.2769800,cross-sectional,diagnosis,CT,Lungs
Transfer learning-based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data,"The novel discovered disease coronavirus popularly known as COVID-19 is caused due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and declared a pandemic by the World Health Organization (WHO). An early-stage detection of COVID-19 is crucial for the containment of the pandemic it has caused. In this study, a transfer learning-based COVID-19 screening technique is proposed. The motivation of this study is to design an automated system that can assist medical staff especially in areas where trained staff are outnumbered. The study investigates the potential of transfer learning-based models for automatically diagnosing diseases like COVID-19 to assist the medical force, especially in times of an outbreak. In the proposed work, a deep learning model, i.e., truncated VGG16 (Visual Geometry Group from Oxford) is implemented to screen COVID-19 CT scans. The VGG16 architecture is fine-tuned and used to extract features from CT scan images. Further principal component analysis (PCA) is used for feature selection. For the final classification, four different classifiers, namely deep convolutional neural network (DCNN), extreme learning machine (ELM), online sequential ELM, and bagging ensemble with support vector machine (SVM) are compared. The best performing classifier bagging ensemble with SVM within 385 ms achieved an accuracy of 95.7%, the precision of 95.8%, area under curve (AUC) of 0.958, and an F1 score of 95.3% on 208 test images. The results obtained on diverse datasets prove the superiority and robustness of the proposed work. A pre-processing technique has also been proposed for radiological data. The study further compares pre-trained CNN architectures and classification models against the proposed technique.",2021,10.1007/s11517-020-02299-2,cross-sectional,diagnosis,CT,Lungs
Transfer Shape Modeling Towards High-throughput Microscopy Image Segmentation,"In order to deal with ambiguous image appearances in cell segmentation, high-level shape modeling has been introduced to delineate cell boundaries. However, shape modeling usually requires sufficient annotated training shapes, which are often labor intensive or unavailable. Meanwhile, when applying the model to different datasets, it is necessary to repeat the tedious annotation process to generate enough training data, and this will significantly limit the applicability of the model. In this paper, we propose to transfer shape modeling learned from an existing but different dataset (e.g. lung cancer) to assist cell segmentation in a new target dataset (e.g. skeletal muscle) without expensive manual annotations. Considering the intrinsic geometry structure of cell shapes, we incorporate the shape transfer model into a sparse representation framework with a manifold embedding constraint, and provide an efficient algorithm to solve the optimization problem. The proposed algorithm is tested on multiple microscopy image datasets with different tissue and staining preparations, and the experiments demonstrate its effectiveness.",2016,10.1007/978-3-319-46726-9_22,,,,
Trends in the application of deep learning networks in medical image analysis: Evolution between 2012 and 2020,"PURPOSE: To evaluate the general rules and future trajectories of deep learning (DL) networks in medical image analysis through bibliometric and hot spot analysis of original articles published between 2012 and 2020. METHODS: Original articles related to DL and medical imaging were retrieved from the PubMed database. For the analysis, data regarding radiological subspecialties; imaging techniques; DL networks; sample size; study purposes, setting, origins and design; statistical analysis; funding sources; authors; and first authors' affiliation was manually extracted from each article. The Bibliographic Item Co-Occurrence Matrix Builder and VOSviewer were used to identify the research topics of the included articles and illustrate the future trajectories of studies. RESULTS: The study included 2685 original articles. The number of publications on DL and medical imaging has increased substantially since 2017, accounting for 97.2% of all included articles. We evaluated the rules of the application of 47 DL networks to eight radiological tasks on 11 human organ sites. Neuroradiology, thorax, and abdomen were frequent research subjects, while thyroid was under-represented. Segmentation and classification tasks were the primary purposes. U-Net, ResNet, and VGG were the most frequently used Convolutional neural network-derived networks. GAN-derived networks were widely developed and applied in 2020, and transfer learning was highlighted in the COVID-19 studies. Brain, prostate, and diabetic retinopathy-related studies were mature research topics in the field. Breast- and lung-related studies were in a stage of rapid development. CONCLUSIONS: This study evaluates the general rules and future trajectories of DL network application in medical image analyses and provides guidance for future studies.",2022,10.1016/j.ejrad.2021.110069,,,,
Truncated inception net: COVID-19 outbreak screening using chest X-rays,"Since December 2019, the Coronavirus Disease (COVID-19) pandemic has caused world-wide turmoil in a short period of time, and the infection, caused by SARS-CoV-2, is spreading rapidly. AI-driven tools are used to identify Coronavirus outbreaks as well as forecast their nature of spread, where imaging techniques are widely used, such as CT scans and chest X-rays (CXRs). In this paper, motivated by the fact that X-ray imaging systems are more prevalent and cheaper than CT scan systems, a deep learning-based Convolutional Neural Network (CNN) model, which we call Truncated Inception Net, is proposed to screen COVID-19 positive CXRs from other non-COVID and/or healthy cases. To validate our proposal, six different types of datasets were employed by taking the following CXRs: COVID-19 positive, Pneumonia positive, Tuberculosis positive, and healthy cases into account. The proposed model achieved an accuracy of 99.96% (AUC of 1.0) in classifying COVID-19 positive cases from combined Pneumonia and healthy cases. Similarly, it achieved an accuracy of 99.92% (AUC of 0.99) in classifying COVID-19 positive cases from combined Pneumonia, Tuberculosis, and healthy CXRs. To the best of our knowledge, as of now, the achieved results outperform the existing AI-driven tools for screening COVID-19 using the acquired CXRs, and proves the viability of using the proposed Truncated Inception Net as a screening tool.",2020,10.1007/s13246-020-00888-x,cross-sectional,diagnosis,CXR,Lungs
Tumor heterogeneity estimation for radiomics in cancer,"Radiomics is an emerging field of medical image analysis research where quantitative measurements are obtained from radiological images that can be utilized to predict patient outcomes and inform treatment decisions. Cancer patients routinely undergo radiological evaluations when images of various modalities including computed tomography, positron emission tomography, and magnetic resonance images are collected for diagnosis and for evaluation of disease progression. Tumor characteristics, often referred to as measures of tumor heterogeneity, can be computed using these clinical images and used as predictors of disease progression and patient survival. Several approaches for quantifying tumor heterogeneity have been proposed, including intensity histogram-based measures, shape and volume-based features, and texture analysis. Taking into account the topology of the tumors we propose a statistical framework for estimating tumor heterogeneity using clustering based on Markov random field theory. We model the voxel intensities using a Gaussian mixture model using a Gibbs prior to incorporate voxel neighborhood information. We propose a novel approach to choosing the number of mixture components. Subsequently, we show that the proposed procedure outperforms the existing approaches when predicting lung cancer survival.",2020,10.1002/sim.8749,,,,
Tumor immune profiles noninvasively estimated by FDG PET with deep learning correlate with immunotherapy response in lung adenocarcinoma,"Rationale: The clinical application of biomarkers reflecting tumor immune microenvironment is hurdled by the invasiveness of obtaining tissues despite its importance in immunotherapy. We developed a deep learning-based biomarker which noninvasively estimates a tumor immune profile with fluorodeoxyglucose positron emission tomography (FDG-PET) in lung adenocarcinoma (LUAD). Methods: A deep learning model to predict cytolytic activity score (CytAct) using semi-automatically segmented tumors on FDG-PET trained by a publicly available dataset paired with tissue RNA sequencing (n = 93). This model was validated in two independent cohorts of LUAD: SNUH (n = 43) and The Cancer Genome Atlas (TCGA) cohort (n = 16). The model was applied to the immune checkpoint blockade (ICB) cohort, which consists of patients with metastatic LUAD who underwent ICB treatment (n = 29). Results: The predicted CytAct showed a positive correlation with CytAct of RNA sequencing in validation cohorts (Spearman rho = 0.32, p = 0.04 in SNUH cohort; spearman rho = 0.47, p = 0.07 in TCGA cohort). In ICB cohort, the higher predicted CytAct of individual lesion was associated with more decrement in tumor size after ICB treatment (Spearman rho = -0.54, p < 0.001). Higher minimum predicted CytAct in each patient associated with significantly prolonged progression free survival and overall survival (Hazard ratio 0.25, p = 0.001 and 0.18, p = 0.004, respectively). In patients with multiple lesions, ICB responders had significantly lower variance of predicted CytActs (p = 0.005). Conclusion: The deep learning model that predicts CytAct using FDG-PET of LUAD was validated in independent cohorts. Our approach may be used to noninvasively assess an immune profile and predict outcomes of LUAD patients treated with ICB.",2020,10.7150/thno.50283,cross-sectional,prognosis,CT,Lungs
Tumor margin identification and prediction of the primary tumor from brain metastases using FTIR imaging and support vector machines,"Infrared spectroscopy enables the identification of tissue types based on their inherent vibrational fingerprint without staining in a nondestructive way. Here, Fourier transform infrared microscopic images were collected from 22 brain metastasis tissue sections of bladder carcinoma, lung carcinoma, mamma carcinoma, colon carcinoma, prostate carcinoma and renal cell carcinoma. The scope of this study was to distinguish the infrared spectra of carcinoma from normal tissue and necrosis and to use the infrared spectra of carcinoma to determine the primary tumor of brain metastasis. Data processing follows procedures that have previously been developed for the analysis of Raman images of these samples and includes the unmixing algorithm N-FINDR, segmentation by k-means clustering, and classification by support vector machines (SVMs). Upon comparison with the subsequent hematoxylin and eosin stained tissue sections of training specimens, correct classification rates of the first level SVM were 98.8% for brain tissue, 98.4% for necrosis and 94.4% for carcinoma. The primary tumors were correctly predicted with an overall rate of 98.7% for FTIR images of the training dataset by a second level SVM. Finally, the two level discrimination models were applied to four independent specimens for validation. Although the classification rates are slightly reduced compared to the training specimens, the majority of the infrared spectra of the independent specimens were assigned to the correct primary tumor. The results demonstrate the capability of FTIR imaging to complement histopathological tools for brain tissue diagnosis.",2013,10.1039/c3an00326d,,,,
Tumor Segmentation and Feature Extraction from Whole-Body FDG-PET/CT Using Cascaded 2D and 3D Convolutional Neural Networks,"(18)F-Fluorodeoxyglucose-positron emission tomography (FDG-PET) is commonly used in clinical practice and clinical drug development to identify and quantify metabolically active tumors. Manual or computer-assisted tumor segmentation in FDG-PET images is a common way to assess tumor burden, such approaches are both labor intensive and may suffer from high inter-reader variability. We propose an end-to-end method leveraging 2D and 3D convolutional neural networks to rapidly identify and segment tumors and to extract metabolic information in eyes to thighs (whole body) FDG-PET/CT scans. The developed architecture is computationally efficient and devised to accommodate the size of whole-body scans, the extreme imbalance between tumor burden and the volume of healthy tissue, and the heterogeneous nature of the input images. Our dataset consists of a total of 3664 eyes to thighs FDG-PET/CT scans, from multi-site clinical trials in patients with non-Hodgkin's lymphoma (NHL) and advanced non-small cell lung cancer (NSCLC). Tumors were segmented and reviewed by board-certified radiologists. We report a mean 3D Dice score of 88.6% on an NHL hold-out set of 1124 scans and a 93% sensitivity on 274 NSCLC hold-out scans. The method is a potential tool for radiologists to rapidly assess eyes to thighs FDG-avid tumor burden.",2020,10.1007/s10278-020-00341-1,cross-sectional,diagnosis,PET-CT,Lungs
Tumour auto-contouring on 2d cine MRI for locally advanced lung cancer: A comparative study,"BACKGROUND AND PURPOSE: Radiotherapy guidance based on magnetic resonance imaging (MRI) is currently becoming a clinical reality. Fast 2d cine MRI sequences are expected to increase the precision of radiation delivery by facilitating tumour delineation during treatment. This study compares four auto-contouring algorithms for the task of delineating the primary tumour in six locally advanced (LA) lung cancer patients. MATERIAL AND METHODS: Twenty-two cine MRI sequences were acquired using either a balanced steady-state free precession or a spoiled gradient echo imaging technique. Contours derived by the auto-contouring algorithms were compared against manual reference contours. A selection of eight image data sets was also used to assess the inter-observer delineation uncertainty. RESULTS: Algorithmically derived contours agreed well with the manual reference contours (median Dice similarity index: ⩾0.91). Multi-template matching and deformable image registration performed significantly better than feature-driven registration and the pulse-coupled neural network (PCNN). Neither MRI sequence nor image orientation was a conclusive predictor for algorithmic performance. Motion significantly degraded the performance of the PCNN. The inter-observer variability was of the same order of magnitude as the algorithmic performance. CONCLUSION: Auto-contouring of tumours on cine MRI is feasible in LA lung cancer patients. Despite large variations in implementation complexity, the different algorithms all have relatively similar performance.",2017,10.1016/j.radonc.2017.09.013,cross-sectional,diagnosis,MRI,Lungs
U-net-based deformation vector field estimation for motion-compensated 4D-CBCT reconstruction,"PURPOSE: For four-dimensional cone-beam computed tomography (4D-CBCT), its image quality is usually degraded by insufficient projections at each respiratory phase after phase-sorting. Recently, we developed a simultaneous motion estimation and image reconstruction (SMEIR) technique, which can improve lung 4D-CBCT reconstruction quality by incorporating an interphase motion model generated as deformation vector fields (DVFs). Simultaneous motion estimation and image reconstruction uses an intensity-driven two-dimensional (2D)-three-dimensional (3D) deformation technique to estimate these DVFs by intensity-matching 2D projections. However, 2D-3D deformation may fail to generate accurate intra-lung DVFs, since the motion of intricate, small lung structures only leads to subtle intensity variations on 2D projections that are insufficient to drive accurate DVF optimization. This study is to develop convolutional neural network (CNN)-based methods to fine-tune the 2D-3D deformation DVFs to improve the efficiency and accuracy of 4D-CBCT reconstruction. METHODS: We built two U-net-based architectures for this study. The first architecture (U-net-3C) uses 2D-3D deformation-estimated DVFs (in three cardinal directions) as the input with three channels (3C), and outputs are fine-tuned DVFs. For the second architecture (U-net-4C), the reference phase CBCT image reconstructed by SMEIR was added as an additional input channel (4C) to represent patient-specific heterogeneous properties of the lung. The output fine-tuned high-quality DVFs of both models were input again into the SMEIR workflow, as an optimized motion model, to generate the final 4D-CBCT. Both methods were evaluated on 11 lung patient cases, using fivefold cross-validation. We also reconstructed 4D-CBCTs by the original SMEIR and the SMEIR-Bio (SMEIR with biomechanical modeling) algorithms for comparison. The 4D-CBCT accuracy was quantitatively assessed through metrics including root-mean-square-error (RMSE), universal quality index (UQI), and normalized cross-correlation (NCC). The DVF accuracy was evaluated by manually tracked lung landmarks. We also evaluated our proposed methods on the SPARE challenge dataset based on reconstructed 4D-CBCT quality using the above metrics. RESULTS: The average (±standard deviation) residual DVF errors of SMEIR-U-net-3C, SMEIR-U-net-4C, SMEIR-Bio, and SMEIR were 3.88 ± 3.12 mm, 3.71 ± 2.90 mm, 3.75 ± 3.40 mm, and 5.73 ± 4.61 mm, respectively. The SMEIR-U-net-3C and SMEIR-U-net-4C generated images of generally improved RMSE, UQI, and NCC as compared to the other methods. Compared with SMERI-U-net-3C, SMEIR-U-net-4C has slightly higher 4D-CBCT reconstruction and DVF estimation accuracy. For the SPARE dataset, the UQI for SMEIR-U-net-3C, SMEIR-U-net-4C, SMEIR-Bio, and SMEIR were 0.96, 0.97, 0.96, and 0.94. CONCLUSION: The CNN-based models can achieve fast (~10 s) and accurate DVF fine-tuning to improve the efficiency and accuracy of 4D-CBCT reconstruction.",2020,10.1002/mp.14150,cross-sectional,informatics,CT,Lungs
U-survival for prognostic prediction of disease progression and mortality of patients with COVID-19,"The rapid increase of patients with coronavirus disease 2019 (COVID-19) has introduced major challenges to healthcare services worldwide. Therefore, fast and accurate clinical assessment of COVID-19 progression and mortality is vital for the management of COVID-19 patients. We developed an automated image-based survival prediction model, called U-survival, which combines deep learning of chest CT images with the established survival analysis methodology of an elastic-net Cox survival model. In an evaluation of 383 COVID-19 positive patients from two hospitals, the prognostic bootstrap prediction performance of U-survival was significantly higher (P < 0.0001) than those of existing laboratory and image-based reference predictors both for COVID-19 progression (maximum concordance index: 91.6% [95% confidence interval 91.5, 91.7]) and for mortality (88.7% [88.6, 88.9]), and the separation between the Kaplan-Meier survival curves of patients stratified into low- and high-risk groups was largest for U-survival (P < 3 × 10(-14)). The results indicate that U-survival can be used to provide automated and objective prognostic predictions for the management of COVID-19 patients.",2021,10.1038/s41598-021-88591-z,cross-sectional,prognosis,CT,Lungs
UBNet: Deep learning-based approach for automatic X-ray image detection of pneumonia and COVID-19 patients,"BACKGROUND: Analysis of chest X-ray images is one of the primary standards in diagnosing patients with COVID-19 and pneumonia, which is faster than using PCR Swab method. However, accuracy of using X-ray images needs to be improved. OBJECTIVE: To develop a new deep learning system of chest X-ray images and evaluate whether it can quickly and accurately detect pneumonia and COVID-19 patients. METHODS: The developed deep learning system (UBNet v3) uses three architectural hierarchies, namely first, to build an architecture containing 7 convolution layers and 3 ANN layers (UBNet v1) to classify between normal images and pneumonia images. Second, using 4 layers of convolution and 3 layers of ANN (UBNet v2) to classify between bacterial and viral pneumonia images. Third, using UBNet v1 to classify between pneumonia virus images and COVID-19 virus infected images. An open-source database with 9,250 chest X-ray images including 3,592 COVID-19 images were used in this study to train and test the developed deep learning models. RESULTS: CNN architecture with a hierarchical scheme developed in UBNet v3 using a simple architecture yielded following performance indices to detect chest X-ray images of COVID-19 patients namely, 99.6%accuracy, 99.7%precision, 99.7%sensitivity, 99.1%specificity, and F1 score of 99.74%. A desktop GUI-based monitoring and classification system supported by a simple CNN architecture can process each chest X-ray image to detect and classify COVID-19 image with an average time of 1.21 seconds. CONCLUSION: Using three hierarchical architectures in UBNet v3 improves system performance in classifying chest X-ray images of pneumonia and COVID-19 patients. A simple architecture also speeds up image processing time.",2022,10.3233/xst-211005,cross-sectional,diagnosis,CXR,Lungs
ULNet for the detection of coronavirus (COVID-19) from chest X-ray images,"Novel coronavirus disease 2019 (COVID-19) is an infectious disease that spreads very rapidly and threatens the health of billions of people worldwide. With the number of cases increasing rapidly, most countries are facing the problem of a shortage of testing kits and resources, and it is necessary to use other diagnostic methods as an alternative to these test kits. In this paper, we propose a convolutional neural network (CNN) model (ULNet) to detect COVID-19 using chest X-ray images. The proposed architecture is constructed by adding a new downsampling side, skip connections and fully connected layers on the basis of U-net. Because the shape of the network is similar to UL, it is named ULNet. This model is trained and tested on a publicly available Kaggle dataset (consisting of a combination of 219 COVID-19, 1314 normal and 1345 viral pneumonia chest X-ray images), including binary classification (COVID-19 vs. Normal) and multiclass classification (COVID-19 vs. Normal vs. Viral Pneumonia). The accuracy of the proposed model in the detection of COVID-19 in the binary-class and multiclass tasks is 99.53% and 95.35%, respectively. Based on these promising results, this method is expected to help doctors diagnose and detect COVID-19. Overall, our ULNet provides a quick method for identifying patients with COVID-19, which is conducive to the control of the COVID-19 pandemic.",2021,10.1016/j.compbiomed.2021.104834,cross-sectional,diagnosis,CXR,Lungs
Ultra-low-dose chest CT imaging of COVID-19 patients using a deep residual neural network,"OBJECTIVES: The current study aimed to design an ultra-low-dose CT examination protocol using a deep learning approach suitable for clinical diagnosis of COVID-19 patients. METHODS: In this study, 800, 170, and 171 pairs of ultra-low-dose and full-dose CT images were used as input/output as training, test, and external validation set, respectively, to implement the full-dose prediction technique. A residual convolutional neural network was applied to generate full-dose from ultra-low-dose CT images. The quality of predicted CT images was assessed using root mean square error (RMSE), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR). Scores ranging from 1 to 5 were assigned reflecting subjective assessment of image quality and related COVID-19 features, including ground glass opacities (GGO), crazy paving (CP), consolidation (CS), nodular infiltrates (NI), bronchovascular thickening (BVT), and pleural effusion (PE). RESULTS: The radiation dose in terms of CT dose index (CTDI(vol)) was reduced by up to 89%. The RMSE decreased from 0.16 ± 0.05 to 0.09 ± 0.02 and from 0.16 ± 0.06 to 0.08 ± 0.02 for the predicted compared with ultra-low-dose CT images in the test and external validation set, respectively. The overall scoring assigned by radiologists showed an acceptance rate of 4.72 ± 0.57 out of 5 for reference full-dose CT images, while ultra-low-dose CT images rated 2.78 ± 0.9. The predicted CT images using the deep learning algorithm achieved a score of 4.42 ± 0.8. CONCLUSIONS: The results demonstrated that the deep learning algorithm is capable of predicting standard full-dose CT images with acceptable quality for the clinical diagnosis of COVID-19 positive patients with substantial radiation dose reduction. KEY POINTS: • Ultra-low-dose CT imaging of COVID-19 patients would result in the loss of critical information about lesion types, which could potentially affect clinical diagnosis. • Deep learning-based prediction of full-dose from ultra-low-dose CT images for the diagnosis of COVID-19 could reduce the radiation dose by up to 89%. • Deep learning algorithms failed to recover the correct lesion structure/density for a number of patients considered outliers, and as such, further research and development is warranted to address these limitations.",2021,10.1007/s00330-020-07225-6,cross-sectional,informatics,CT,Lungs
Ultra-low-dose CT combined with noise reduction techniques for quantification of emphysema in COPD patients: An intra-individual comparison study with standard-dose CT,"PURPOSE: Phantom studies in CT emphysema quantification show that iterative reconstruction and deep learning-based noise reduction (DLNR) allow lower radiation dose. We compared emphysema quantification on ultra-low-dose CT (ULDCT) with and without noise reduction, to standard-dose CT (SDCT) in chronic obstructive pulmonary disease (COPD). METHOD: Forty-nine COPD patients underwent ULDCT (third generation dual-source CT; 70ref-mAs, Sn-filter 100kVp; median CTDIvol 0.38 mGy) and SDCT (64-multidetector CT; 40mAs, 120kVp; CTDIvol 3.04 mGy). Scans were reconstructed with filtered backprojection (FBP) and soft kernel. For ULDCT, we also applied advanced modelled iterative reconstruction (ADMIRE), levels 1/3/5, and DLNR, levels 1/3/5/9. Emphysema was quantified as Low Attenuation Value percentage (LAV%, ≤-950HU). ULDCT measures were compared to SDCT as reference standard. RESULTS: For ULDCT, the median radiation dose was 84 % lower than for SDCT. Median extent of emphysema was 18.6 % for ULD-FBP and 15.4 % for SDCT (inter-quartile range: 11.8-28.4 % and 9.2 %-28.7 %, p = 0.002). Compared to SDCT, the range in limits of agreement of emphysema quantification as measure of variability was 14.4 for ULD-FBP, 11.0-13.1 for ULD-ADMIRE levels and 10.1-13.9 for ULD-DLNR levels. Optimal settings were ADMIRE 3 and DLNR 3, reducing variability of emphysema quantification by 24 % and 27 %, at slight underestimation of emphysema extent (-1.5 % and -2.9 %, respectively). CONCLUSIONS: Ultra-low-dose CT in COPD patients allows dose reduction by 84 %. State-of-the-art noise reduction methods in ULDCT resulted in slight underestimation of emphysema compared to SDCT. Noise reduction methods (especially ADMIRE 3 and DLNR 3) reduced variability of emphysema quantification in ULDCT by up to 27 % compared to FBP.",2021,10.1016/j.ejrad.2021.109646,cross-sectional,informatics,CT,Lungs
UMLF-COVID: an unsupervised meta-learning model specifically designed to identify X-ray images of COVID-19 patients,"BACKGROUND: With the rapid spread of COVID-19 worldwide, quick screening for possible COVID-19 patients has become the focus of international researchers. Recently, many deep learning-based Computed Tomography (CT) image/X-ray image fast screening models for potential COVID-19 patients have been proposed. However, the existing models still have two main problems. First, most of the existing supervised models are based on pre-trained model parameters. The pre-training model needs to be constructed on a dataset with features similar to those in COVID-19 X-ray images, which limits the construction and use of the model. Second, the number of categories based on the X-ray dataset of COVID-19 and other pneumonia patients is usually imbalanced. In addition, the quality is difficult to distinguish, leading to non-ideal results with the existing model in the multi-class classification COVID-19 recognition task. Moreover, no researchers have proposed a COVID-19 X-ray image learning model based on unsupervised meta-learning. METHODS: This paper first constructed an unsupervised meta-learning model for fast screening of COVID-19 patients (UMLF-COVID). This model does not require a pre-trained model, which solves the limitation problem of model construction, and the proposed unsupervised meta-learning framework solves the problem of sample imbalance and sample quality. RESULTS: The UMLF-COVID model is tested on two real datasets, each of which builds a three-category and four-category model. And the experimental results show that the accuracy of the UMLF-COVID model is 3-10% higher than that of the existing models. CONCLUSION: In summary, we believe that the UMLF-COVID model is a good complement to COVID-19 X-ray fast screening models.",2021,10.1186/s12880-021-00704-2,cross-sectional,diagnosis,CXR,Lungs
Unboxing AI - Radiological Insights Into a Deep Neural Network for Lung Nodule Characterization,"RATIONALE AND OBJECTIVES: To explain predictions of a deep residual convolutional network for characterization of lung nodule by analyzing heat maps. MATERIALS AND METHODS: A 20-layer deep residual CNN was trained on 1245 Chest CTs from National Lung Screening Trial (NLST) trial to predict the malignancy risk of a nodule. We used occlusion to systematically block regions of a nodule and map drops in malignancy risk score to generate clinical attribution heatmaps on 103 nodules from Lung Image Database Consortium image collection and Image Database Resource Initiative (LIDC-IDRI) dataset, which were analyzed by a thoracic radiologist. The features were described as heat inside nodule -bright areas inside nodule, peripheral heat continuous/interrupted bright areas along nodule contours, heat in adjacent plane -brightness in scan planes juxtaposed with the nodule, satellite heat - a smaller bright spot in proximity to nodule in the same scan plane, heat map larger than nodule bright areas corresponding to the shape of the nodule seen outside the nodule margins and heat in calcification. RESULTS: These six features were assigned binary values. This feature vector was fedinto a standard J48 decision tree with 10-fold cross-validation, which gave an 85 % weighted classification accuracy with a 77.8% True Positive (TP) rate, 8% False Positive (FP) rate for benign cases and 91.8% TP and 22.2% FP rates for malignant cases. Heat Inside nodule was more frequently observed in nodules classified as malignant whereas peripheral heat, heat in adjacent plane, and satellite heat were more commonly seen in nodules classified as benign. CONCLUSION: We discuss the potential ability of a radiologist to visually parse the deep learning algorithm generated ""heat map"" to identify features aiding classification.",2020,10.1016/j.acra.2019.09.015,cross-sectional,diagnosis,CT,Lungs
"Unraveling the interplay of image formation, data representation and learning in CT-based COPD phenotyping automation: The need for a meta-strategy","PURPOSE: In the literature on automated phenotyping of chronic obstructive pulmonary disease (COPD), there is a multitude of isolated classical machine learning and deep learning techniques, mostly investigating individual phenotypes, with small study cohorts and heterogeneous meta-parameters, e.g., different scan protocols or segmented regions. The objective is to compare the impact of different experimental setups, i.e., varying meta-parameters related to image formation and data representation, with the impact of the learning technique for subtyping automation for a variety of phenotypes. The identified associations of these parameters with automation performance and their interactions might be a first step towards a determination of optimal meta-parameters, i.e., a meta-strategy. METHODS: A clinical cohort of 981 patients (53.8 ± 15.1 years, 554 male) was examined. The inspiratory CT images were analyzed to automate the diagnosis of 13 COPD phenotypes given by two radiologists. A benchmark feature set that integrates many quantitative criteria was extracted from the lung and trained a variety of learning algorithms on the first 654 patients (two thirds) and the respective algorithm retrospectively assessed the remaining 327 patients (one third). The automation performance was evaluated by the area under the receiver operating characteristic curve (AUC). 1717 experiments were conducted with varying meta-parameters such as reconstruction kernel, segmented regions and input dimensionality, i.e., number of extracted features. The association of the meta-parameters with the automation performance was analyzed by multivariable general linear model decomposition of the automation performance in the contributions of meta-parameters and the learning technique. RESULTS: The automation performance varied strongly for varying meta-parameters. For emphysema-predominant phenotypes, an AUC of 93%-95% could be achieved for the best meta-configuration. The airways-predominant phenotypes led to a lower performance of 65%-85%, while smooth kernel configurations on average were unexpectedly superior to those with sharp kernels. The performance impact of meta-parameters, even that of often neglected ones like the missing-data imputation, was in general larger than that of the learning technique. Advanced learning techniques like 3D deep learning or automated machine learning yielded inferior automation performance for non-optimal meta-configurations in comparison to simple techniques with suitable meta-configurations. The best automation performance was achieved by a combination of modern learning techniques and a suitable meta-configuration. CONCLUSIONS: Our results indicate that for COPD phenotype automation, study design parameters such as reconstruction kernel and the model input dimensionality should be adapted to the learning technique and may be more important than the technique itself. To achieve optimal automation and prediction results, the interaction between input those meta-parameters and the learning technique should be considered. This might be particularly relevant for the development of specific scan protocols for novel learning algorithms, and towards an understanding of good study design for automated phenotyping.",2021,10.1002/mp.15049,cross-sectional,diagnosis,CT,Lungs
Unsupervised machine learning of radiomic features for predicting treatment response and overall survival of early stage non-small cell lung cancer patients treated with stereotactic body radiation therapy,"BACKGROUND AND PURPOSE: To predict treatment response and survival of NSCLC patients receiving stereotactic body radiation therapy (SBRT), we develop an unsupervised machine learning method for stratifying patients and extracting meta-features simultaneously based on imaging data. MATERIAL AND METHODS: This study was performed based on an (18)F-FDG-PET dataset of 100 consecutive patients who were treated with SBRT for early stage NSCLC. Each patient's tumor was characterized by 722 radiomic features. An unsupervised two-way clustering method was used to identify groups of patients and radiomic features simultaneously. The groups of patients were compared in terms of survival and freedom from nodal failure. Meta-features were computed for building survival models to predict survival and free of nodal failure. RESULTS: Differences were found between 2 groups of patients when the patients were clustered into 3 groups in terms of both survival (p = 0.003) and freedom from nodal failure (p = 0.038). Average concordance measures for predicting survival and nodal failure were 0.640±0.029 and 0.664±0.063 respectively, better than those obtained by prediction models built upon clinical variables (p < 0.04). CONCLUSIONS: The evaluation results demonstrate that our method allows us to stratify patients and predict survival and freedom from nodal failure with better performance than current alternative methods.",2018,10.1016/j.radonc.2018.06.025,cross-sectional,prognosis,PET,Lungs
Urinary detection of lung cancer in mice via noninvasive pulmonary protease profiling,"Lung cancer is the leading cause of cancer-related death, and patients most commonly present with incurable advanced-stage disease. U.S. national guidelines recommend screening for high-risk patients with low-dose computed tomography, but this approach has limitations including high false-positive rates. Activity-based nanosensors can detect dysregulated proteases in vivo and release a reporter to provide a urinary readout of disease activity. Here, we demonstrate the translational potential of activity-based nanosensors for lung cancer by coupling nanosensor multiplexing with intrapulmonary delivery and machine learning to detect localized disease in two immunocompetent genetically engineered mouse models. The design of our multiplexed panel of sensors was informed by comparative transcriptomic analysis of human and mouse lung adenocarcinoma datasets and in vitro cleavage assays with recombinant candidate proteases. Intrapulmonary administration of the nanosensors to a Kras- and Trp53-mutant lung adenocarcinoma mouse model confirmed the role of metalloproteases in lung cancer and enabled accurate detection of localized disease, with 100% specificity and 81% sensitivity. Furthermore, this approach generalized to an alternative autochthonous model of lung adenocarcinoma, where it detected cancer with 100% specificity and 95% sensitivity and was not confounded by lipopolysaccharide-driven lung inflammation. These results encourage the clinical development of activity-based nanosensors for the detection of lung cancer.",2020,10.1126/scitranslmed.aaw0262,,,,
Urine Proteome Profiling Predicts Lung Cancer from Control Cases and Other Tumors,"Development of noninvasive, reliable biomarkers for lung cancer diagnosis has many clinical benefits knowing that most of lung cancer patients are diagnosed at the late stage. For this purpose, we conducted proteomic analyses of 231 human urine samples in healthy individuals (n=33), benign pulmonary diseases (n=40), lung cancer (n=33), bladder cancer (n=17), cervical cancer (n=25), colorectal cancer (n=22), esophageal cancer (n=14), and gastric cancer (n=47) patients collected from multiple medical centers. By random forest modeling, we nominated a list of urine proteins that could separate lung cancers from other cases. With a feature selection algorithm, we selected a panel of five urinary biomarkers (FTL: Ferritin light chain; MAPK1IP1L: Mitogen-Activated Protein Kinase 1 Interacting Protein 1 Like; FGB: Fibrinogen Beta Chain; RAB33B: RAB33B, Member RAS Oncogene Family; RAB15: RAB15, Member RAS Oncogene Family) and established a combinatorial model that can correctly classify the majority of lung cancer cases both in the training set (n=46) and the test sets (n=14-47 per set) with an AUC ranging from 0.8747 to 0.9853. A combination of five urinary biomarkers not only discriminates lung cancer patients from control groups but also differentiates lung cancer from other common tumors. The biomarker panel and the predictive model, when validated by more samples in a multi-center setting, may be used as an auxiliary diagnostic tool along with imaging technology for lung cancer detection.",2018,10.1016/j.ebiom.2018.03.009,,,,
Use of a Commercially Available Deep Learning Algorithm to Measure the Solid Portions of Lung Cancer Manifesting as Subsolid Lesions at CT: Comparisons with Radiologists and Invasive Component Size at Pathologic Examination,"Background The solid portion size of lung cancer lesions manifesting as subsolid lesions is key in their management, but the automatic measurement of such lesions by means of a deep learning (DL) algorithm needs evaluation. Purpose To evaluate the performance of a commercially available DL algorithm for automatic measurement of the solid portion of surgically proven lung adenocarcinomas manifesting as subsolid lesions. Materials and Methods Surgically proven lung adenocarcinomas manifesting as subsolid lesions on CT images between January 2018 and December 2018 were retrospectively included. Five radiologists independently measured the maximal axial diameter of the solid portion of lesions. The DL algorithm automatically segmented and measured the maximal axial diameter of the solid portion. Reader measurements, software measurements, and invasive component size at pathologic examination were compared by using intraclass correlation coefficient (ICC) and Bland-Altman plots. Results A total of 448 patients (mean age, 63 years ± 10 [standard deviation]; 264 women) with 448 lesions were evaluated (invasive component size, 3-65 mm). The measurement agreements between each radiologist and the DL algorithm were very good (ICC range, 0.82-0.89). When a radiologist was replaced with the DL algorithm, the ICCs ranged from 0.87 to 0.90, with an ICC of 0.90 among five radiologists. The mean difference between the DL algorithm and each radiologist ranged from -3.7 to 1.5 mm. The widest 95% limit of agreement between the DL algorithm and each radiologist (-15.7 to 8.3 mm) was wider than pairwise comparisons of radiologists (-7.7 to 13.0 mm). The agreement between the DL algorithm and invasive component size at pathologic evaluation was good, with an ICC of 0.67. Measurements by the DL algorithm (mean difference, -6.0 mm) and radiologists (mean difference, -7.5 to -2.3 mm) both underestimated invasive component size. Conclusion Automatic measurements of solid portions of lung cancer manifesting as subsolid lesions by the deep learning algorithm were comparable with manual measurements and showed good agreement with invasive component size at pathologic evaluation. © RSNA, 2021 Online supplemental material is available for this article.",2021,10.1148/radiol.2021202803,cross-sectional,diagnosis,CT,Lungs
Use of a Dual Artificial Intelligence Platform to Detect Unreported Lung Nodules,"OBJECTIVE: To investigate the performance of Dual-AI Deep Learning Platform in detecting unreported pulmonary nodules that are 6 mm or greater, comprising computer-vision (CV) algorithm to detect pulmonary nodules, with positive results filtered by natural language processing (NLP) analysis of the dictated report. METHODS: Retrospective analysis of 5047 chest CT scans and corresponding reports. Cases which were both CV algorithm positive (nodule ≥ 6 mm) and NLP negative (nodule not reported), were outputted for review by 2 chest radiologists. RESULTS: The CV algorithm detected nodules that are 6 mm or greater in 1830 (36.3%) of 5047 cases. Three hundred fifty-five (19.4%) were unreported by the radiologist, as per NLP algorithm. Expert review determined that 139 (39.2%) of 355 cases were true positives (2.8% of all cases). One hundred thirty (36.7%) of 355 cases were unnecessary alerts-vague language in the report confounded the NLP algorithm. Eighty-six (24.2%) of 355 cases were false positives. CONCLUSIONS: Dual-AI platform detected actionable unreported nodules in 2.8% of chest CT scans, yet minimized intrusion to radiologist's workflow by avoiding alerts for most already-reported nodules.",2021,10.1097/rct.0000000000001118,cross-sectional,diagnosis,CT,Lungs
Use of Crowd Innovation to Develop an Artificial Intelligence-Based Solution for Radiation Therapy Targeting,"IMPORTANCE: Radiation therapy (RT) is a critical cancer treatment, but the existing radiation oncologist work force does not meet growing global demand. One key physician task in RT planning involves tumor segmentation for targeting, which requires substantial training and is subject to significant interobserver variation. OBJECTIVE: To determine whether crowd innovation could be used to rapidly produce artificial intelligence (AI) solutions that replicate the accuracy of an expert radiation oncologist in segmenting lung tumors for RT targeting. DESIGN, SETTING, AND PARTICIPANTS: We conducted a 10-week, prize-based, online, 3-phase challenge (prizes totaled $55 000). A well-curated data set, including computed tomographic (CT) scans and lung tumor segmentations generated by an expert for clinical care, was used for the contest (CT scans from 461 patients; median 157 images per scan; 77 942 images in total; 8144 images with tumor present). Contestants were provided a training set of 229 CT scans with accompanying expert contours to develop their algorithms and given feedback on their performance throughout the contest, including from the expert clinician. MAIN OUTCOMES AND MEASURES: The AI algorithms generated by contestants were automatically scored on an independent data set that was withheld from contestants, and performance ranked using quantitative metrics that evaluated overlap of each algorithm's automated segmentations with the expert's segmentations. Performance was further benchmarked against human expert interobserver and intraobserver variation. RESULTS: A total of 564 contestants from 62 countries registered for this challenge, and 34 (6%) submitted algorithms. The automated segmentations produced by the top 5 AI algorithms, when combined using an ensemble model, had an accuracy (Dice coefficient = 0.79) that was within the benchmark of mean interobserver variation measured between 6 human experts. For phase 1, the top 7 algorithms had average custom segmentation scores (S scores) on the holdout data set ranging from 0.15 to 0.38, and suboptimal performance using relative measures of error. The average S scores for phase 2 increased to 0.53 to 0.57, with a similar improvement in other performance metrics. In phase 3, performance of the top algorithm increased by an additional 9%. Combining the top 5 algorithms from phase 2 and phase 3 using an ensemble model, yielded an additional 9% to 12% improvement in performance with a final S score reaching 0.68. CONCLUSIONS AND RELEVANCE: A combined crowd innovation and AI approach rapidly produced automated algorithms that replicated the skills of a highly trained physician for a critical task in radiation therapy. These AI algorithms could improve cancer care globally by transferring the skills of expert clinicians to under-resourced health care settings.",2019,10.1001/jamaoncol.2019.0159,,,,
Use of CT and artificial intelligence in suspected or COVID-19 positive patients: statement of the Italian Society of Medical and Interventional Radiology,"The COVID-19 pandemic started in Italy in February 2020 with an exponential growth that has exceeded the number of cases reported in China. Italian radiology departments found themselves at the forefront in the management of suspected and positive COVID cases, both in diagnosis, in estimating the severity of the disease and in follow-up. In this context SIRM recommends chest X-ray as first-line imaging tool, CT as additional tool that shows typical features of COVID pneumonia, and ultrasound of the lungs as monitoring tool. SIRM recommends, as high priority, to ensure appropriate sanitation procedures on the scan equipment after detecting any suspected or positive COVID-19 patients. In this emergency situation, several expectations have been raised by the scientific community about the role that artificial intelligence can have in improving the diagnosis and treatment of coronavirus infection, and SIRM wishes to deliver clear statements to the radiological community, on the usefulness of artificial intelligence as a radiological decision support system in COVID-19 positive patients. (1) SIRM supports the research on the use of artificial intelligence as a predictive and prognostic decision support system, especially in hospitalized patients and those admitted to intensive care, and welcomes single center of multicenter studies for a clinical validation of the test. (2) SIRM does not support the use of CT with artificial intelligence for screening or as first-line test to diagnose COVID-19. (3) Chest CT with artificial intelligence cannot replace molecular diagnosis tests with nose-pharyngeal swab (rRT-PCR) in suspected for COVID-19 patients.",2020,10.1007/s11547-020-01197-9,,,,
Use of CT radiomics to differentiate minimally invasive adenocarcinomas and invasive adenocarcinomas presenting as pure ground-glass nodules larger than 10 mm,"PURPOSE: This study aimed to develop a model based on radiomics features extracted from computed tomography (CT) images to effectively differentiate between minimally invasive adenocarcinomas (MIAs) and invasive adenocarcinomas (IAs) manifesting as pure ground-glass nodules (pGGNs) larger than 10 mm. METHOD: This retrospective study included patients who underwent surgical resection for persistent pGGN between November 2012 and June 2018 and diagnosed with MIAs or IAs. The patients were randomly assigned to the training and test cohorts. The correlation coefficient method and the least absolute shrinkage and selection operator (LASSO) method were applied to select radiomics features useful for constructing a model whose performance was assessed by the area under the receiver operating characteristic curve (AUC-ROC). The radiomics model was compared to a standard CT model (shape, volume and mean CT value of the largest cross-section) and the combined radiomics-standard CT model using univariate and multivariate logistic regression analysis. RESULTS: The radiomics model showed better discriminative ability (training AUC, 0.879; test AUC, 0.877) than the standard CT model (training AUC, 0.820; test AUC, 0.828). The combined model (training AUC, 0.879; test AUC, 0.870) did not demonstrate improved performance compared with the radiomics model. Radiomics_score was an independent predictor of invasiveness following multivariate logistic analysis. CONCLUSIONS: For pGGNs larger than 10 mm, the radiomics model demonstrated superior diagnostic performance in differentiating between IAs and MIAs, which may be useful to clinicians for diagnosis and treatment selection.",2021,10.1016/j.ejrad.2021.109772,cross-sectional,diagnosis,CT,Lungs
Usefulness of gradient tree boosting for predicting histological subtype and EGFR mutation status of non-small cell lung cancer on (18)F FDG-PET/CT,"OBJECTIVE: To develop and evaluate a radiomics approach for classifying histological subtypes and epidermal growth factor receptor (EGFR) mutation status in lung cancer on PET/CT images. METHODS: PET/CT images of lung cancer patients were obtained from public databases and used to establish two datasets, respectively to classify histological subtypes (156 adenocarcinomas and 32 squamous cell carcinomas) and EGFR mutation status (38 mutant and 100 wild-type samples). Seven types of imaging features were obtained from PET/CT images of lung cancer. Two types of machine learning algorithms were used to predict histological subtypes and EGFR mutation status: random forest (RF) and gradient tree boosting (XGB). The classifiers used either a single type or multiple types of imaging features. In the latter case, the optimal combination of the seven types of imaging features was selected by Bayesian optimization. Receiver operating characteristic analysis, area under the curve (AUC), and tenfold cross validation were used to assess the performance of the approach. RESULTS: In the classification of histological subtypes, the AUC values of the various classifiers were as follows: RF, single type: 0.759; XGB, single type: 0.760; RF, multiple types: 0.720; XGB, multiple types: 0.843. In the classification of EGFR mutation status, the AUC values were: RF, single type: 0.625; XGB, single type: 0.617; RF, multiple types: 0.577; XGB, multiple types: 0.659. CONCLUSIONS: The radiomics approach to PET/CT images, together with XGB and Bayesian optimization, is useful for classifying histological subtypes and EGFR mutation status in lung cancer.",2020,10.1007/s12149-019-01414-0,cross-sectional,diagnosis,PET-CT,Lungs
Using 164 Million Google Street View Images to Derive Built Environment Predictors of COVID-19 Cases,"The spread of COVID-19 is not evenly distributed. Neighborhood environments may structure risks and resources that produce COVID-19 disparities. Neighborhood built environments that allow greater flow of people into an area or impede social distancing practices may increase residents' risk for contracting the virus. We leveraged Google Street View (GSV) images and computer vision to detect built environment features (presence of a crosswalk, non-single family home, single-lane roads, dilapidated building and visible wires). We utilized Poisson regression models to determine associations of built environment characteristics with COVID-19 cases. Indicators of mixed land use (non-single family home), walkability (sidewalks), and physical disorder (dilapidated buildings and visible wires) were connected with higher COVID-19 cases. Indicators of lower urban development (single lane roads and green streets) were connected with fewer COVID-19 cases. Percent black and percent with less than a high school education were associated with more COVID-19 cases. Our findings suggest that built environment characteristics can help characterize community-level COVID-19 risk. Sociodemographic disparities also highlight differential COVID-19 risk across groups of people. Computer vision and big data image sources make national studies of built environment effects on COVID-19 risk possible, to inform local area decision-making.",2020,10.3390/ijerph17176359,,,,
Using a convolutional neural network for classification of squamous and non-squamous non-small cell lung cancer based on diagnostic histopathology HES images,"Histological stratification in metastatic non-small cell lung cancer (NSCLC) is essential to properly guide therapy. Morphological evaluation remains the basis for subtyping and is completed by additional immunohistochemistry labelling to confirm the diagnosis, which delays molecular analysis and utilises precious sample. Therefore, we tested the capacity of convolutional neural networks (CNNs) to classify NSCLC based on pathologic HES diagnostic biopsies. The model was estimated with a learning cohort of 132 NSCLC patients and validated on an external validation cohort of 65 NSCLC patients. Based on image patches, a CNN using InceptionV3 architecture was trained and optimized to classify NSCLC between squamous and non-squamous subtypes. Accuracies of 0.99, 0.87, 0.85, 0.85 was reached in the training, validation and test sets and in the external validation cohort. At the patient level, the CNN model showed a capacity to predict the tumour histology with accuracy of 0.73 and 0.78 in the learning and external validation cohorts respectively. Selecting tumour area using virtual tissue micro-array improved prediction, with accuracy of 0.82 in the external validation cohort. This study underlines the capacity of CNN to predict NSCLC subtype with good accuracy and to be applied to small pathologic samples without annotation.",2021,10.1038/s41598-021-03206-x,,,,
Using a Deep Learning Model to Explore the Impact of Clinical Data on COVID-19 Diagnosis Using Chest X-ray,"The coronavirus pandemic (COVID-19) is disrupting the entire world; its rapid global spread threatens to affect millions of people. Accurate and timely diagnosis of COVID-19 is essential to control the spread and alleviate risk. Due to the promising results achieved by integrating machine learning (ML), particularly deep learning (DL), in automating the multiple disease diagnosis process. In the current study, a model based on deep learning was proposed for the automated diagnosis of COVID-19 using chest X-ray images (CXR) and clinical data of the patient. The aim of this study is to investigate the effects of integrating clinical patient data with the CXR for automated COVID-19 diagnosis. The proposed model used data collected from King Fahad University Hospital, Dammam, KSA, which consists of 270 patient records. The experiments were carried out first with clinical data, second with the CXR, and finally with clinical data and CXR. The fusion technique was used to combine the clinical features and features extracted from images. The study found that integrating clinical data with the CXR improves diagnostic accuracy. Using the clinical data and the CXR, the model achieved an accuracy of 0.970, a recall of 0.986, a precision of 0.978, and an F-score of 0.982. Further validation was performed by comparing the performance of the proposed system with the diagnosis of an expert. Additionally, the results have shown that the proposed system can be used as a tool that can help the doctors in COVID-19 diagnosis.",2022,10.3390/s22020669,cross-sectional,diagnosis,CXR,Lungs
Using a risk model for probability of cancer in pulmonary nodules,"BACKGROUND: Considering the high morbidity and mortality of lung cancer and the high incidence of pulmonary nodules, clearly distinguishing benign from malignant lung nodules at an early stage is of great significance. However, determining the kind of lung nodule which is more prone to lung cancer remains a problem worldwide. METHODS: A total of 480 patients with pulmonary nodule data were collected from Shandong, China. We assessed the clinical characteristics and computed tomography (CT) imaging features among pulmonary nodules in patients who had undergone video-assisted thoracoscopic surgery (VATS) lobectomy from 2013 to 2018. Preliminary selection of features was based on a statistical analysis using SPSS. We used WEKA to assess the machine learning models using its multiple algorithms and selected the best decision tree model using its optimization algorithm. RESULTS: The combination of decision tree and logistics regression optimized the decision tree without affecting its AUC. The decision tree structure showed that lobulation was the most important feature, followed by spiculation, vessel convergence sign, nodule type, satellite nodule, nodule size and age of patient. CONCLUSIONS: Our study shows that decision tree analyses can be applied to screen individuals for early lung cancer with CT. Our decision tree provides a new way to help clinicians establish a logical diagnosis by a stepwise progression method, but still needs to be validated for prospective trials in a larger patient population.",2021,10.1111/1759-7714.13991,cross-sectional,diagnosis,CT,Lungs
Using artificial intelligence to assist radiologists in distinguishing COVID-19 from other pulmonary infections,"BACKGROUND: Accurate and rapid diagnosis of coronavirus disease (COVID-19) is crucial for timely quarantine and treatment. PURPOSE: In this study, a deep learning algorithm-based AI model using ResUNet network was developed to evaluate the performance of radiologists with and without AI assistance in distinguishing COVID-19 infected pneumonia patients from other pulmonary infections on CT scans. METHODS: For model development and validation, a total number of 694 cases with 111,066 CT slides were retrospectively collected as training data and independent test data in the study. Among them, 118 are confirmed COVID-19 infected pneumonia cases and 576 are other pulmonary infection cases (e.g. tuberculosis cases, common pneumonia cases and non-COVID-19 viral pneumonia cases). The cases were divided into training and testing datasets. The independent test was performed by evaluating and comparing the performance of three radiologists with different years of practice experience in distinguishing COVID-19 infected pneumonia cases with and without the AI assistance. RESULTS: Our final model achieved an overall test accuracy of 0.914 with an area of the receiver operating characteristic (ROC) curve (AUC) of 0.903 in which the sensitivity and specificity are 0.918 and 0.909, respectively. The deep learning-based model then achieved a comparable performance by improving the radiologists' performance in distinguish COVOD-19 from other pulmonary infections, yielding better average accuracy and sensitivity, from 0.941 to 0.951 and from 0.895 to 0.942, respectively, when compared to radiologists without using AI assistance. CONCLUSION: A deep learning algorithm-based AI model developed in this study successfully improved radiologists' performance in distinguishing COVID-19 from other pulmonary infections using chest CT images.",2021,10.3233/xst-200735,cross-sectional,diagnosis,CT,Lungs
Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020200905,cross-sectional,diagnosis,CT,Lungs
Using artificial intelligence to improve the diagnostic efficiency of pulmonologists in differentiating COVID-19 pneumonia from community-acquired pneumonia,"Coronavirus disease 2019 (COVID-19) has quickly turned into a global health problem. Computed tomography (CT) findings of COVID-19 pneumonia and community-acquired pneumonia (CAP) may be similar. Artificial intelligence (AI) is a popular topic among medical imaging techniques and has caused significant developments in diagnostic techniques. This retrospective study aims to analyze the contribution of AI to the diagnostic performance of pulmonologists in distinguishing COVID-19 pneumonia from CAP using CT scans. A deep learning-based AI model was created to be utilized in the detection of COVID-19, which extracted visual data from volumetric CT scans. The final data set covered a total of 2496 scans (887 patients), which included 1428 (57.2%) from the COVID-19 group and 1068 (42.8%) from the CAP group. CT slices were classified into training, validation, and test datasets in an 8:1:1. The independent test data set was analyzed by comparing the performance of four pulmonologists in differentiating COVID-19 pneumonia both with and without the help of the AI. The accuracy, sensitivity, and specificity values of the proposed AI model for determining COVID-19 in the independent test data set were 93.2%, 85.8%, and 99.3%, respectively, with the area under the receiver operating characteristic curve of 0.984. With the assistance of the AI, the pulmonologists accomplished a higher mean accuracy (88.9% vs. 79.9%, p < 0.001), sensitivity (79.1% vs. 70%, p < 0.001), and specificity (96.5% vs. 87.5%, p < 0.001). AI support significantly increases the diagnostic efficiency of pulmonologists in the diagnosis of COVID-19 via CT. Studies in the future should focus on real-time applications of AI to fight the COVID-19 infection.",2022,10.1002/jmv.27777,cross-sectional,diagnosis,CT,Lungs
Using Auto-Segmentation to Reduce Contouring and Dose Inconsistency in Clinical Trials: The Simulated Impact on RTOG 0617,"PURPOSE: Contouring inconsistencies are known but understudied in clinical radiation therapy trials. We applied auto-contouring to the Radiation Therapy Oncology Group (RTOG) 0617 dose escalation trial data. We hypothesized that the trial heart doses were higher than reported due to inconsistent and insufficient heart segmentation. We tested our hypothesis by comparing doses between deep-learning (DL) segmented hearts and trial hearts. METHODS AND MATERIALS: The RTOG 0617 data were downloaded from The Cancer Imaging Archive; the 442 patients with trial hearts and dose distributions were included. All hearts were resegmented using our DL pipeline and quality assured to meet the requirements for clinical implementation. Dose (V5%, V30%, and mean heart dose) was compared between the 2 sets of hearts (Wilcoxon signed-rank test). Each dose metric was associated with overall survival (Cox proportional hazards). Lastly, 18 volume similarity metrics were assessed for the hearts and correlated with |Dose(DL) - Dose(RTOG0617)| (linear regression; significance: P ≤ .0028; corrected for 18 tests). RESULTS: Dose metrics were significantly higher for DL hearts compared with trial hearts (eg, mean heart dose: 15 Gy vs 12 Gy; P = 5.8E-16). All 3 DL heart dose metrics were stronger overall survival predictors than those of the trial hearts (median, P = 2.8E-5 vs 2.0E-4). Thirteen similarity metrics explained |Dose(DL) - Dose(RTOG0617)|; the axial distance between the 2 centers of mass was the strongest predictor (CENT(Axial); median, R(2) = 0.47; P = 6.1E-62). CENT(Axial) agreed with the qualitatively identified inconsistencies in the superior direction. The trial's qualitative heart contouring score was not correlated with |Dose(DL) - Dose(RTOG0617)| (median, R(2) = 0.01; P = .02) or with any of the similarity metrics (median, Rs = 0.13 [range, -0.22 to 0.31]). CONCLUSIONS: Using a coherent heart definition, as enabled through our open-source DL algorithm, the trial heart doses in RTOG 0617 were found to be significantly higher than previously reported, which may have led to an even more rapid mortality accumulation. Auto-segmentation is likely to reduce contouring and dose inconsistencies and increase the quality of clinical RT trials.",2021,10.1016/j.ijrobp.2020.11.011,cross-sectional,treatment,CT,Heart
Using contrast-enhanced CT and non-contrast-enhanced CT to predict EGFR mutation status in NSCLC patients-a radiomics nomogram analysis,"OBJECTIVES: To develop and validate a general radiomics nomogram capable of identifying EGFR mutation status in non-small cell lung cancer (NSCLC) patients, regardless of patient with either contrast-enhanced CT (CE-CT) or non-contrast-enhanced CT (NE-CT). METHODS: A total of 412 NSCLC patients were retrospectively enrolled in this study. Patients' radiomics features not significantly different between NE-CT and CE-CT were defined as general features, and were further used to construct the general radiomics signature. Fivefold cross-validation was used to select the best machine learning algorithm. Finally, a general radiomics nomogram was developed using general radiomics signature, and clinical and radiological characteristics. Two groups of data collected at different time periods were used as two test sets to access the discrimination and clinical usefulness. Area under the receiver operating characteristic curve (ROC-AUC) was applied to performance evaluation. RESULT: The general radiomics signature yielded the highest AUC of 0.756 and 0.739 in the two test sets, respectively. When applying to same type of CT, the performance of general radiomics signature was always similar to or higher than that of models built using only NE-CT or CE-CT features. The general radiomics nomogram combining general radiomics signature, smoking history, emphysema, and ILD achieved higher performance whether applying to NE-CT or CE-CT (test set 1, AUC = 0.833 and 0.842; test set 2, AUC = 0.839 and 0.850). CONCLUSIONS: Our work demonstrated that using general features to construct radiomics signature and nomogram could help identify EGFR mutation status of NSCLC patients and expand its scope of clinical application. KEY POINTS: • General features were proposed to construct general radiomics signature using different types of CT of different patients at the same time to identify EGFR mutation status of NSCLC patients. • The general radiomics nomogram based on general radiomics signature, and clinical and radiological characteristics could identify EGFR mutation status of patients with NSCLC and outperformed the general radiomics signature. • The general radiomics nomogram had a wider scope of clinical application; no matter which of NE-CT and CE-CT the patient has, its EGFR mutation status could be predicted.",2022,10.1007/s00330-021-08366-y,cross-sectional,diagnosis,CT,Lungs
Using Deep Learning for Classification of Lung Nodules on Computed Tomography Images,"Lung cancer is the most common cancer that cannot be ignored and cause death with late health care. Currently, CT can be used to help doctors detect the lung cancer in the early stages. In many cases, the diagnosis of identifying the lung cancer depends on the experience of doctors, which may ignore some patients and cause some problems. Deep learning has been proved as a popular and powerful method in many medical imaging diagnosis areas. In this paper, three types of deep neural networks (e.g., CNN, DNN, and SAE) are designed for lung cancer calcification. Those networks are applied to the CT image classification task with some modification for the benign and malignant lung nodules. Those networks were evaluated on the LIDC-IDRI database. The experimental results show that the CNN network archived the best performance with an accuracy of 84.15%, sensitivity of 83.96%, and specificity of 84.32%, which has the best result among the three networks.",2017,10.1155/2017/8314740,cross-sectional,diagnosis,CT,Lungs
Using deep learning to model the biological dose prediction on bulky lung cancer patients of partial stereotactic ablation radiotherapy,"PURPOSE: To develop a biological dose prediction model considering tissue bio-reactions in addition to patient anatomy for achieving a more comprehensive evaluation of tumor control and promoting the automatic planning of bulky lung cancer. METHODS: A database containing images and partial stereotactic ablation boost radiotherapy (P-SABR) plans of 94 bulky lung cancer patients was studied. Patient-specific parameters of gross tumor boost volume (GTVb), planning gross target volume (PGTV), and identified organs at risk (OARs) were extracted via Numpy and simple ITK. The original dose and structure maps for P-SABR patients were resampled to have a voxel resolution of 3.9 × 3.9 × 3 mm(3) . Biological equivalent dose (BED) distributions were reprogrammed based on physical dose volumes. A developed deep learning architecture, Nestnet, was adopted as the training framework. We utilized two approaches for data organization to correlate the structures and BED: (a) BED programming before training model (B-Nestnet); (b) BED programming after the training process (D-B Nestnet). The early-stop mechanism was adopted on the validation set to avoid overfitting. The evaluation criteria of predictive accuracy contain the minimum BED of GTVb and PGTV, the maximum and the mean BED of all targets, BED-volume metrics. For comparison, we also used the original Unet for BED prediction. The absolute differences were statistically analyzed with the paired-samples t test. RESULTS: The statistical outcomes demonstrate that D-B Nestnet model predicts biological dose distributions accurately. The average absolute biases of [max, mean] BED for GTVb, PGTV are [2.1%, 3.3%] and [2.1%, 4.7%], respectively. Averaging across most of OARs, the D-B Nestnet model is capable of predicting the errors of the max and mean BED within 6.3% and 6.1%, respectively. While the compared models performed worse with averaged max and mean BED prediction errors surpassing 10% on some specific OARs. CONCLUSIONS: The study developed a D-B Nestnet model capable of predicting BED distribution accurately for bulky lung cancer patients in P-SABR. The predicted BED map enables a quick intuitive evaluation of tumor ablation, modification of the ablation range to improve BED of tumor targets, and quality assessment. It represents a major step forward toward automated P-SABR planning on bulky lung cancer in real clinical practice.",2020,10.1002/mp.14518,cross-sectional,treatment,CT,Lungs
Using deep-learning techniques for pulmonary-thoracic segmentations and improvement of pneumonia diagnosis in pediatric chest radiographs,"PURPOSE: To evaluate the efficacy of a deep-learning model to segment the lung and thorax regions in pediatric chest X-rays (CXRs). Validating the diagnosis of bacterial or viral pneumonia could be improved after lung segmentation. MATERIALS AND METHODS: A clinical-pediatric CXR set including 1351 patients was proposed to develop a deep-learning model for the pulmonary-thoracic segmentations. Model performance was evaluated by Jaccard's similarity coefficient (JSC) and Dice's coefficient (DC). Two adult CXR sets were used to assess the model's generalizability. According to the pulmonary-thoracic ratio, Pearson's correlation coefficient and the Bland-Altman plot were generated to demonstrate the correlation and agreement between manual and automatic segmentations. The receiver operating characteristic curves and areas under the curve (AUCs) were used to compare the pneumonia classification performance based on the lung-extracted images with that based on the original images. RESULTS: The model achieved JSCs of 0.910 and 0.950, DCs of 0.948 and 0.974 for lung and thorax segmentations, respectively. Pearson's r = 0.96, P < .0001. In the Bland-Altman plot, the mean difference was 0.0025 with a 95% confidence interval of (-0.0451, 0.0501). For testing with two adult CXR sets, the JSCs were 0.903 and 0.888, respectively, while the DCs were 0.948 and 0.937, respectively. After lung segmentation, the AUC of a classifier to identify bacterial or viral pneumonia increased from 0.815 to 0.879. CONCLUSION: We built a pediatric CXR dataset and exploited a deep-learning model for accurate pulmonary-thoracic segmentations. Lung segmentation can notably improve the diagnosis of bacterial or viral pneumonia.",2019,10.1002/ppul.24431,cross-sectional,diagnosis,CT,Lungs
Using machine learning algorithms to review computed tomography scans and assess risk for cardiovascular disease: Retrospective analysis from the National Lung Screening Trial (NLST),"BACKGROUND: The National Lung Screening Trial (NLST) demonstrated that annual screening with low dose CT in high-risk population was associated with reduction in lung cancer mortality. Nonetheless, the leading cause of mortality in the study was from cardiovascular diseases. PURPOSE: To determine whether the used machine learning automatic algorithms assessing coronary calcium score (CCS), level of liver steatosis and emphysema percentage in the lungs are good predictors of cardiovascular disease (CVD) mortality and incidence when applied on low dose CT scans. MATERIALS AND METHODS: Three fully automated machine learning algorithms were used to assess CCS, level of liver steatosis and emphysema percentage in the lung. The algorithms were used on low-dose computed tomography scans acquired from 12,332 participants in NLST. RESULTS: In a multivariate analysis, association between the three algorithm scores and CVD mortality have shown an OR of 1.72 (p = 0.003), 2.62 (p < 0.0001) for CCS scores of 101-400 and above 400 respectively, and an OR of 1.12 (p = 0.044) for level of liver steatosis. Similar results were shown for the incidence of CVD, OR of 1.96 (p < 0.0001), 4.94 (p < 0.0001) for CCS scores of 101-400 and above 400 respectively. Also, emphysema percentage demonstrated an OR of 0.89 (p < 0.0001). Similar results are shown for univariate analyses of the algorithms. CONCLUSION: The three automated machine learning algorithms could help physicians to assess the incidence and risk of CVD mortality in this specific population. Application of these algorithms to existing LDCT scans can provide valuable health care information and assist in future research.",2020,10.1371/journal.pone.0236021,cross-sectional,diagnosis,CT,Heart - Lungs
Using needle orientation sensing as surrogate signal for respiratory motion estimation in percutaneous interventions,"PURPOSE: To develop and evaluate an approach to estimate the respiratory-induced motion of lesions in the chest and abdomen. MATERIALS AND METHODS: The proposed approach uses the motion of an initial reference needle inserted into a moving organ to estimate the lesion (target) displacement that is caused by respiration. The needles position is measured using an inertial measurement unit (IMU) sensor externally attached to the hub of an initially placed reference needle. Data obtained from the IMU sensor and the target motion are used to train a learning-based approach to estimate the position of the moving target. An experimental platform was designed to mimic respiratory motion of the liver. Liver motion profiles of human subjects provided inputs to the experimental platform. Variables including the insertion angle, target depth, target motion velocity and target proximity to the reference needle were evaluated by measuring the error of the estimated target position and processing time. RESULTS: The mean error of estimation of the target position ranged between 0.86 and 1.29 mm. The processing maximum training and testing time was 5 ms which is suitable for real-time target motion estimation using the needle position sensor. CONCLUSION: The external motion of an initially placed reference needle inserted into a moving organ can be used as a surrogate, measurable and accessible signal to estimate in real-time the position of a moving target caused by respiration; this technique could then be used to guide the placement of subsequently inserted needles directly into the target.",2018,10.1007/s11548-017-1644-z,,,,
Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays,"Chest X-ray becomes one of the most common medical diagnoses due to its noninvasiveness. The number of chest X-ray images has skyrocketed, but reading chest X-rays still have been manually performed by radiologists, which creates huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology that can extract a large number of quantitative features from medical images, demonstrates its potential to facilitate medical imaging diagnosis before the deep learning era. In this paper, we develop an end-to-end framework, ChexRadiNet, that can utilize the radiomics features to improve the abnormality classification performance. Specifically, ChexRadiNet first applies a light-weight but efficient triplet-attention mechanism to classify the chest X-rays and highlight the abnormal regions. Then it uses the generated class activation map to extract radiomic features, which further guides our model to learn more robust image features. After a number of iterations and with the help of radiomic features, our framework can converge to more accurate image regions. We evaluate the ChexRadiNet framework using three public datasets: NIH ChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the state-of-the-art on both disease detection (0.843 in AUC) and localization (0.679 in T(IoU) = 0.1). We make the code publicly available at https://github. com/bionlplab/lung_disease_detection_amia2021, with the hope that this method can facilitate the development of automatic systems with a higher-level understanding of the radiological world.",2021,,cross-sectional,diagnosis,CXR,Lungs
Validating deep learning inference during chest X-ray classification for COVID-19 screening,"The new coronavirus unleashed a worldwide pandemic in early 2020, and a fatality rate several times that of the flu. As the number of infections soared, and capabilities for testing lagged behind, chest X-ray (CXR) imaging became more relevant in the early diagnosis and treatment planning for patients with suspected or confirmed COVID-19 infection. In a few weeks, proposed new methods for lung screening using deep learning rapidly appeared, while quality assurance discussions lagged behind. This paper proposes a set of protocols to validate deep learning algorithms, including our ROI Hide-and-Seek protocol, which emphasizes or hides key regions of interest from CXR data. Our protocol allows assessing the classification performance for anomaly detection and its correlation to radiological signatures, an important issue overlooked in several deep learning approaches proposed so far. By running a set of systematic tests over CXR representations using public image datasets, we demonstrate the weaknesses of current techniques and offer perspectives on the advantages and limitations of automated radiography analysis when using heterogeneous data sources.",2021,10.1038/s41598-021-95561-y,,,,
Validation of a Deep Learning Algorithm for the Detection of Malignant Pulmonary Nodules in Chest Radiographs,"IMPORTANCE: The improvement of pulmonary nodule detection, which is a challenging task when using chest radiographs, may help to elevate the role of chest radiographs for the diagnosis of lung cancer. OBJECTIVE: To assess the performance of a deep learning-based nodule detection algorithm for the detection of lung cancer on chest radiographs from participants in the National Lung Screening Trial (NLST). DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study used data from participants in the NLST ro assess the performance of a deep learning-based artificial intelligence (AI) algorithm for the detection of pulmonary nodules and lung cancer on chest radiographs using separate training (in-house) and validation (NLST) data sets. Baseline (T0) posteroanterior chest radiographs from 5485 participants (full T0 data set) were used to assess lung cancer detection performance, and a subset of 577 of these images (nodule data set) were used to assess nodule detection performance. Participants aged 55 to 74 years who currently or formerly (ie, quit within the past 15 years) smoked cigarettes for 30 pack-years or more were enrolled in the NLST at 23 US centers between August 2002 and April 2004. Information on lung cancer diagnoses was collected through December 31, 2009. Analyses were performed between August 20, 2019, and February 14, 2020. EXPOSURES: Abnormality scores produced by the AI algorithm. MAIN OUTCOMES AND MEASURES: The performance of an AI algorithm for the detection of lung nodules and lung cancer on radiographs, with lung cancer incidence and mortality as primary end points. RESULTS: A total of 5485 participants (mean [SD] age, 61.7 [5.0] years; 3030 men [55.2%]) were included, with a median follow-up duration of 6.5 years (interquartile range, 6.1-6.9 years). For the nodule data set, the sensitivity and specificity of the AI algorithm for the detection of pulmonary nodules were 86.2% (95% CI, 77.8%-94.6%) and 85.0% (95% CI, 81.9%-88.1%), respectively. For the detection of all cancers, the sensitivity was 75.0% (95% CI, 62.8%-87.2%), the specificity was 83.3% (95% CI, 82.3%-84.3%), the positive predictive value was 3.8% (95% CI, 2.6%-5.0%), and the negative predictive value was 99.8% (95% CI, 99.6%-99.9%). For the detection of malignant pulmonary nodules in all images of the full T0 data set, the sensitivity was 94.1% (95% CI, 86.2%-100.0%), the specificity was 83.3% (95% CI, 82.3%-84.3%), the positive predictive value was 3.4% (95% CI, 2.2%-4.5%), and the negative predictive value was 100.0% (95% CI, 99.9%-100.0%). In digital radiographs of the nodule data set, the AI algorithm had higher sensitivity (96.0% [95% CI, 88.3%-100.0%] vs 88.0% [95% CI, 75.3%-100.0%]; P = .32) and higher specificity (93.2% [95% CI, 89.9%-96.5%] vs 82.8% [95% CI, 77.8%-87.8%]; P = .001) for nodule detection compared with the NLST radiologists. For malignant pulmonary nodule detection on digital radiographs of the full T0 data set, the sensitivity of the AI algorithm was higher (100.0% [95% CI, 100.0%-100.0%] vs 94.1% [95% CI, 82.9%-100.0%]; P = .32) compared with the NLST radiologists, and the specificity (90.9% [95% CI, 89.6%-92.1%] vs 91.0% [95% CI, 89.7%-92.2%]; P = .91), positive predictive value (8.2% [95% CI, 4.4%-11.9%] vs 7.8% [95% CI, 4.1%-11.5%]; P = .65), and negative predictive value (100.0% [95% CI, 100.0%-100.0%] vs 99.9% [95% CI, 99.8%-100.0%]; P = .32) were similar to those of NLST radiologists. CONCLUSIONS AND RELEVANCE: In this study, the AI algorithm performed better than NLST radiologists for the detection of pulmonary nodules on digital radiographs. When used as a second reader, the AI algorithm may help to detect lung cancer.",2020,10.1001/jamanetworkopen.2020.17135,cross-sectional,diagnosis,CXR,Lungs
"Validation of a deep learning computer aided system for CT based lung nodule detection, classification, and growth rate estimation in a routine clinical population","OBJECTIVE: In this study, we evaluated a commercially available computer assisted diagnosis system (CAD). The deep learning algorithm of the CAD was trained with a lung cancer screening cohort and developed for detection, classification, quantification, and growth of actionable pulmonary nodules on chest CT scans. Here, we evaluated the CAD in a retrospective cohort of a routine clinical population. MATERIALS AND METHODS: In total, a number of 337 scans of 314 different subjects with reported nodules of 3-30 mm in size were included into the evaluation. Two independent thoracic radiologists alternately reviewed scans with or without CAD assistance to detect, classify, segment, and register pulmonary nodules. A third, more experienced, radiologist served as an adjudicator. In addition, the cohort was analyzed by the CAD alone. The study cohort was divided into five different groups: 1) 178 CT studies without reported pulmonary nodules, 2) 95 studies with 1-10 pulmonary nodules, 23 studies from the same patients with 3) baseline and 4) follow-up studies, and 5) 18 CT studies with subsolid nodules. A reference standard for nodules was based on majority consensus with the third thoracic radiologist as required. Sensitivity, false positive (FP) rate and Dice inter-reader coefficient were calculated. RESULTS: After analysis of 470 pulmonary nodules, the sensitivity readings for radiologists without CAD and radiologist with CAD, were 71.9% (95% CI: 66.0%, 77.0%) and 80.3% (95% CI: 75.2%, 85.0%) (p < 0.01), with average FP rate of 0.11 and 0.16 per CT scan, respectively. Accuracy and kappa of CAD for classifying solid vs sub-solid nodules was 94.2% and 0.77, respectively. Average inter-reader Dice coefficient for nodule segmentation was 0.83 (95% CI: 0.39, 0.96) and 0.86 (95% CI: 0.51, 0.95) for CAD versus readers. Mean growth percentage discrepancy of readers and CAD alone was 1.30 (95% CI: 1.02, 2.21) and 1.35 (95% CI: 1.01, 4.99), respectively. CONCLUSION: The applied CAD significantly increased radiologist's detection of actionable nodules yet also minimally increasing the false positive rate. The CAD can automatically classify and quantify nodules and calculate nodule growth rate in a cohort of a routine clinical population. Results suggest this Deep Learning software has the potential to assist chest radiologists in the tasks of pulmonary nodule detection and management within their routine clinical practice.",2022,10.1371/journal.pone.0266799,cross-sectional,diagnosis,CT,Lungs
Validation of Deep-Learning Image Reconstruction for Low-Dose Chest Computed Tomography Scan: Emphasis on Image Quality and Noise,"OBJECTIVE: Iterative reconstruction degrades image quality. Thus, further advances in image reconstruction are necessary to overcome some limitations of this technique in low-dose computed tomography (LDCT) scan of the chest. Deep-learning image reconstruction (DLIR) is a new method used to reduce dose while maintaining image quality. The purposes of this study was to evaluate image quality and noise of LDCT scan images reconstructed with DLIR and compare with those of images reconstructed with the adaptive statistical iterative reconstruction-Veo at a level of 30% (ASiR-V 30%). MATERIALS AND METHODS: This retrospective study included 58 patients who underwent LDCT scan for lung cancer screening. Datasets were reconstructed with ASiR-V 30% and DLIR at medium and high levels (DLIR-M and DLIR-H, respectively). The objective image signal and noise, which represented mean attenuation value and standard deviation in Hounsfield units for the lungs, mediastinum, liver, and background air, and subjective image contrast, image noise, and conspicuity of structures were evaluated. The differences between CT scan images subjected to ASiR-V 30%, DLIR-M, and DLIR-H were evaluated. RESULTS: Based on the objective analysis, the image signals did not significantly differ among ASiR-V 30%, DLIR-M, and DLIR-H (p = 0.949, 0.737, 0.366, and 0.358 in the lungs, mediastinum, liver, and background air, respectively). However, the noise was significantly lower in DLIR-M and DLIR-H than in ASiR-V 30% (all p < 0.001). DLIR had higher signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) than ASiR-V 30% (p = 0.027, < 0.001, and < 0.001 in the SNR of the lungs, mediastinum, and liver, respectively; all p < 0.001 in the CNR). According to the subjective analysis, DLIR had higher image contrast and lower image noise than ASiR-V 30% (all p < 0.001). DLIR was superior to ASiR-V 30% in identifying the pulmonary arteries and veins, trachea and bronchi, lymph nodes, and pleura and pericardium (all p < 0.001). CONCLUSION: DLIR significantly reduced the image noise in chest LDCT scan images compared with ASiR-V 30% while maintaining superior image quality.",2021,10.3348/kjr.2020.0116,cross-sectional,informatics,CT,-
Validation of expert system enhanced deep learning algorithm for automated screening for COVID-Pneumonia on chest X-rays,"SARS-CoV2 pandemic exposed the limitations of artificial intelligence based medical imaging systems. Earlier in the pandemic, the absence of sufficient training data prevented effective deep learning (DL) solutions for the diagnosis of COVID-19 based on X-Ray data. Here, addressing the lacunae in existing literature and algorithms with the paucity of initial training data; we describe CovBaseAI, an explainable tool using an ensemble of three DL models and an expert decision system (EDS) for COVID-Pneumonia diagnosis, trained entirely on pre-COVID-19 datasets. The performance and explainability of CovBaseAI was primarily validated on two independent datasets. Firstly, 1401 randomly selected CxR from an Indian quarantine center to assess effectiveness in excluding radiological COVID-Pneumonia requiring higher care. Second, curated dataset; 434 RT-PCR positive cases and 471 non-COVID/Normal historical scans, to assess performance in advanced medical settings. CovBaseAI had an accuracy of 87% with a negative predictive value of 98% in the quarantine-center data. However, sensitivity was 0.66-0.90 taking RT-PCR/radiologist opinion as ground truth. This work provides new insights on the usage of EDS with DL methods and the ability of algorithms to confidently predict COVID-Pneumonia while reinforcing the established learning; that benchmarking based on RT-PCR may not serve as reliable ground truth in radiological diagnosis. Such tools can pave the path for multi-modal high throughput detection of COVID-Pneumonia in screening and referral.",2021,10.1038/s41598-021-02003-w,cross-sectional,diagnosis,CXR,Lungs
Validation of lesion simulations in clinical CT data for anonymized chest and abdominal CT databases,"PURPOSE: To make available to the medical imaging community a computed tomography (CT) image database composed of hybrid datasets (patient CT images with digitally inserted anthropomorphic lesions) where lesion ground truth is known a priori. It is envisioned that such a dataset could be a resource for the assessment of CT image quality, machine learning, and imaging technologies [e.g., computer aided detection (CAD) and segmentation algorithms]. ACQUISITION AND VALIDATION METHODS: This HIPPA compliant, IRB waiver of approval study consisted of utilizing 120 chest and 100 abdominal clinically acquired adult CT exams. One image series per patient exam was utilized based on coverage of the anatomical region of interest (either the thorax or abdomen). All image series were de-identified. Simulated lesions were derived from a library of anatomically informed digital lesions (93 lung and 50 liver lesions) where six and four digital lesions with nominal diameters ranging from 4 to 20 mm were inserted into lung and liver image series, respectively. Locations for lesion insertion were randomly chosen. A previously validated lesion simulation and virtual insertion technique were utilized. The resulting hybrid images were reviewed by three experienced radiologists to assure similarity with routine clinical imaging in a diverse adult population. DATA FORMAT AND USAGE NOTES: The database is composed of four datasets that contain 100 patient cases each, for a total of 400 image series accompanied by Matlab.mat tables that provide descriptive information about the virtually inserted lesions (i.e., size, shape, opacity, and insertion location in physical (world) coordinates and voxel indices). All image and metadata are stored in DICOM format on the Quantitative Imaging Data Warehouse (https://qidw.rsna.org/#collection/57d463471cac0a4ec8ff8f46/folder/5b23dceb1cac0a4ec800a770?dialog=login), in two sets: (a) QIBA CT Hybrid Dataset I which contains Lung I and Liver I datasets, and (b) QIBA CT Hybrid Dataset II which contains Lung II and Liver II datasets. The QIDW is supported by the Radiological Society of North America (RSNA). Registration is required upon initial log in. POTENTIAL APPLICATIONS: By simulating lesion opacity (full solid, part solid and ground glass), size, and texture, the relationship between lesion morphology and segmentation or CAD algorithm performance can be investigated without the need for repetitive patient exams. This database can also serve as a reference standard for device and reader performance studies.",2019,10.1002/mp.13412,,,,
Validation of low-dose lung cancer PET-CT protocol and PET image improvement using machine learning,"PURPOSE: To conduct a simplified lesion-detection task of a low-dose (LD) PET-CT protocol for frequent lung screening using 30% of the effective PETCT dose and to investigate the feasibility of increasing clinical value of low-statistics scans using machine learning. METHODS: We acquired 33 SD PET images, of which 13 had actual LD (ALD) PET, and simulated LD (SLD) PET images at seven different count levels from the SD PET scans. We employed image quality transfer (IQT), a machine learning algorithm that performs patch-regression to map parameters from low-quality to high-quality images. At each count level, patches extracted from 23 pairs of SD/SLD PET images were used to train three IQT models - global linear, single tree, and random forest regressions with cubic patch sizes of 3 and 5 voxels. The models were then used to estimate SD images from LD images at each count level for 10 unseen subjects. Lesion-detection task was carried out on matched lesion-present and lesion-absent images. RESULTS: LD PET-CT protocol yielded lesion detectability with sensitivity of 0.98 and specificity of 1. Random forest algorithm with cubic patch size of 5 allowed further 11.7% reduction in the effective PETCT dose without compromising lesion detectability, but underestimated SUV by 30%. CONCLUSION: LD PET-CT protocol was validated for lesion detection using ALD PET scans. Substantial image quality improvement or additional dose reduction while preserving clinical values can be achieved using machine learning methods though SUV quantification may be biased and adjustment of our research protocol is required for clinical use.",2021,10.1016/j.ejmp.2020.11.027,cross-sectional,treatment,CT,Lungs
Validation pipeline for machine learning algorithm assessment for multiple vendors,"A standardized objective evaluation method is needed to compare machine learning (ML) algorithms as these tools become available for clinical use. Therefore, we designed, built, and tested an evaluation pipeline with the goal of normalizing performance measurement of independently developed algorithms, using a common test dataset of our clinical imaging. Three vendor applications for detecting solid, part-solid, and groundglass lung nodules in chest CT examinations were assessed in this retrospective study using our data-preprocessing and algorithm assessment chain. The pipeline included tools for image cohort creation and de-identification; report and image annotation for ground-truth labeling; server partitioning to receive vendor ""black box"" algorithms and to enable model testing on our internal clinical data (100 chest CTs with 243 nodules) from within our security firewall; model validation and result visualization; and performance assessment calculating algorithm recall, precision, and receiver operating characteristic curves (ROC). Algorithm true positives, false positives, false negatives, recall, and precision for detecting lung nodules were as follows: Vendor-1 (194, 23, 49, 0.80, 0.89); Vendor-2 (182, 270, 61, 0.75, 0.40); Vendor-3 (75, 120, 168, 0.32, 0.39). The AUCs for detection of solid (0.61-0.74), groundglass (0.66-0.86) and part-solid (0.52-0.86) nodules varied between the three vendors. Our ML model validation pipeline enabled testing of multi-vendor algorithms within the institutional firewall. Wide variations in algorithm performance for detection as well as classification of lung nodules justifies the premise for a standardized objective ML algorithm evaluation process.",2022,10.1371/journal.pone.0267213,,,,
"Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge","Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.",2017,10.1016/j.media.2017.06.015,,,,
Value of a deep learning-based algorithm for detecting Lung-RADS category 4 nodules on chest radiographs in a health checkup population: estimation of the sample size for a randomized controlled trial,"OBJECTIVE: To explore the value of a deep learning-based algorithm in detecting Lung CT Screening Reporting and Data System category 4 nodules on chest radiographs from an asymptomatic health checkup population. METHODS: Data from an annual retrospective cohort of individuals who underwent chest radiographs for health checkup purposes and chest CT scanning within 3 months were collected. Among 3073 individuals, 118 with category 4 nodules on CT were selected. A reader performance test was performed using those 118 radiographs and randomly selected 51 individuals without any nodules. Four radiologists independently evaluated the radiographs without and with the results of the algorithm; and sensitivities/specificities were compared. The sample size needed to confirm the difference in detection rates was calculated, i.e., the number of true-positive radiographs divided by the total number of radiographs. RESULTS: The sensitivity of the radiologists substantially increased aided by the algorithm (38.8% [183/472] to 45.1% [213/472]; p < .001) without significant change in specificity (94.1% [192/204] vs. 92.2% [188/204]; p = .22). Pooled radiologists detected more nodules with the algorithm (32.0% [156/488] vs. 38.9% [190/488]; p < .001), without alteration of false-positive rates (0.09 [62/676], both). Pooled detection rates for the annual cohort were 1.49% (183/12,292) and 1.73% (213/12,292) without and with the algorithm, respectively. A sample size of 41,776 in each arm would be required to demonstrate significant detection rate difference with < 5% type I error and > 80% power. CONCLUSION: Although readers substantially increased sensitivity in detecting nodules on chest radiographs from a health checkup population aided by the algorithm, detection rate difference was only 0.24%, requiring a sample size >80,000 for a randomized controlled trial. KEY POINTS: • Aided by a deep learning algorithm, pooled radiologists improved their sensitivity in detecting Lung-RADS category 4 nodules on chest radiographs from a health checkup population (38.8% [183/472] to 45.1% [213/472]; p < .001), without increasing false-positive rate. • The prevalence of the Lung-RADS category 4 nodules was 3.8% (118/3073) on the population, resulting in only 0.24% increase of the detection rate for the radiologists with assistance of the algorithm. • To confirm the significant detection rate increase by a randomized controlled trial, a sample size of 84,000 would be required.",2022,10.1007/s00330-021-08162-8,cross-sectional,diagnosis,CT,Lung
"Value of CT application in the screening,diagnosis,and treatment of COVID-19","The coronavirus disease 2019 (COVID-19) has attracted extensive attention all around the world recently. Early screening, early diagnosis, early isolation, and early treatment remain the most effective prevention and control measures. Computed tomography (CT) plays a vital role in the screening, diagnosis, treatment, and follow-up of COVID-19, especially in the early screening, with a higher sensitivity than that of real-time fluorescence RT-PCR. The combination of CT and artificial intelligence has the potential to help clinicians in improving the diagnostic accuracy and working efficiency.",2020,10.11817/j.issn.1672-7347.2020.200132,,,,
Verification of the machine delivery parameters of a treatment plan via deep learning,"We developed a generative adversarial network (GAN)-based deep learning approach to estimate the multileaf collimator (MLC) aperture and corresponding monitor units (MUs) from a given 3D dose distribution. The proposed design of the adversarial network, which integrates a residual block into pix2pix framework, jointly trains a 'U-Net'-like architecture as the generator and a convolutional 'PatchGAN' classifier as the discriminator. 199 patients, including nasopharyngeal, lung and rectum, treated with intensity-modulated radiotherapy and volumetric-modulated arc therapy techniques were utilized to train the network. An additional 47 patients were used to test the prediction accuracy of the proposed deep learning model. The Dice similarity coefficient (DSC) was calculated to evaluate the similarity between the MLC aperture shapes obtained from the treatment planning system (TPS) and the deep learning prediction. The average and standard deviation of the bias between the TPS-generated MUs and predicted MUs was calculated to evaluate the MU prediction accuracy. In addition, the differences between TPS and deep learning-predicted MLC leaf positions were compared. The average and standard deviation of DSC was 0.94 ± 0.043 for 47 testing patients. The average deviation of predicted MUs from the planned MUs normalized to each beam or arc was within 2% for all the testing patients. The average deviation of the predicted MLC leaf positions was around one pixel for all the testing patients. Our results demonstrated the feasibility and reliability of the proposed approach. The proposed technique has strong potential to improve the efficiency and accuracy of the patient plan quality assurance process.",2020,10.1088/1361-6560/aba165,cross-sectional,treatment,,"Nasopharyngeal,lung,rectum"
Viral epitope profiling of COVID-19 patients reveals cross-reactivity and correlates of severity,"Understanding humoral responses to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is critical for improving diagnostics, therapeutics, and vaccines. Deep serological profiling of 232 coronavirus disease 2019 (COVID-19) patients and 190 pre-COVID-19 era controls using VirScan revealed more than 800 epitopes in the SARS-CoV-2 proteome, including 10 epitopes likely recognized by neutralizing antibodies. Preexisting antibodies in controls recognized SARS-CoV-2 ORF1, whereas only COVID-19 patient antibodies primarily recognized spike protein and nucleoprotein. A machine learning model trained on VirScan data predicted SARS-CoV-2 exposure history with 99% sensitivity and 98% specificity; a rapid Luminex-based diagnostic was developed from the most discriminatory SARS-CoV-2 peptides. Individuals with more severe COVID-19 exhibited stronger and broader SARS-CoV-2 responses, weaker antibody responses to prior infections, and higher incidence of cytomegalovirus and herpes simplex virus 1, possibly influenced by demographic covariates. Among hospitalized patients, males produce stronger SARS-CoV-2 antibody responses than females.",2020,10.1126/science.abd4250,,,,
Viral Pneumonia Screening on Chest X-Rays Using Confidence-Aware Anomaly Detection,"Clusters of viral pneumonia occurrences over a short period may be a harbinger of an outbreak or pandemic. Rapid and accurate detection of viral pneumonia using chest X-rays can be of significant value for large-scale screening and epidemic prevention, particularly when other more sophisticated imaging modalities are not readily accessible. However, the emergence of novel mutated viruses causes a substantial dataset shift, which can greatly limit the performance of classification-based approaches. In this paper, we formulate the task of differentiating viral pneumonia from non-viral pneumonia and healthy controls into a one-class classification-based anomaly detection problem. We therefore propose the confidence-aware anomaly detection (CAAD) model, which consists of a shared feature extractor, an anomaly detection module, and a confidence prediction module. If the anomaly score produced by the anomaly detection module is large enough, or the confidence score estimated by the confidence prediction module is small enough, the input will be accepted as an anomaly case (i.e., viral pneumonia). The major advantage of our approach over binary classification is that we avoid modeling individual viral pneumonia classes explicitly and treat all known viral pneumonia cases as anomalies to improve the one-class model. The proposed model outperforms binary classification models on the clinical X-VIRAL dataset that contains 5,977 viral pneumonia (no COVID-19) cases, 37,393 non-viral pneumonia or healthy cases. Moreover, when directly testing on the X-COVID dataset that contains 106 COVID-19 cases and 107 normal controls without any fine-tuning, our model achieves an AUC of 83.61% and sensitivity of 71.70%, which is comparable to the performance of radiologists reported in the literature.",2021,10.1109/tmi.2020.3040950,cross-sectional,diagnosis,X-Ray,Lung
Visible near infrared reflectance molecular chemical imaging of human ex vivo carcinomas and murine in vivo carcinomas,"SIGNIFICANCE: A key risk faced by oncological surgeons continues to be complete removal of tumor. Currently, there is no intraoperative imaging device to detect kidney tumors during excision. AIM: We are evaluating molecular chemical imaging (MCI) as a technology for real-time tumor detection and margin assessment during tumor removal surgeries. APPROACH: In exploratory studies, we evaluate visible near infrared (Vis-NIR) MCI for differentiating tumor from adjacent tissue in ex vivo human kidney specimens, and in anaesthetized mice with breast or lung tumor xenografts. Differentiation of tumor from nontumor tissues is made possible with diffuse reflectance spectroscopic signatures and hyperspectral imaging technology. Tumor detection is achieved by score image generation to localize the tumor, followed by application of computer vision algorithms to define tumor border. RESULTS: Performance of a partial least squares discriminant analysis (PLS-DA) model for kidney tumor in a 22-patient study is 0.96 for area under the receiver operating characteristic curve. A PLS-DA model for in vivo breast and lung tumor xenografts performs with 100% sensitivity, 83% specificity, and 89% accuracy. CONCLUSION: Detection of cancer in surgically resected human kidney tissues is demonstrated ex vivo with Vis-NIR MCI, and in vivo on mice with breast or lung xenografts.",2020,10.1117/1.Jbo.25.2.026003,,,,
Visual and software-based quantitative chest CT assessment of COVID-19: correlation with clinical findings,"PURPOSE: The aim of this study was to evaluate visual and software-based quantitative assessment of parenchymal changes and normal lung parenchyma in patients with coronavirus disease 2019 (COVID-19) pneumonia. The secondary aim of the study was to compare the radiologic findings with clinical and laboratory data. METHODS: Patients with COVID-19 who underwent chest computed tomography (CT) between March 11, 2020 and April 15, 2020 were retrospectively evaluated. Clinical and laboratory findings of patients with abnormal findings on chest CT and PCR-evidence of COVID-19 infection were recorded. Visual quantitative assessment score (VQAS) was performed according to the extent of lung opacities. Software-based quantitative assessment of the normal lung parenchyma percentage (SQNLP) was automatically quantified by a deep learning software. The presence of consolidation and crazy paving pattern (CPP) was also recorded. Statistical analyses were performed to evaluate the correlation between quantitative radiologic assessments, and clinical and laboratory findings, as well as to determine the predictive utility of radiologic findings for estimating severe pneumonia and admission to intensive care unit (ICU). RESULTS: A total of 90 patients were enrolled. Both VQAS and SQNLP were significantly correlated with multiple clinical parameters. While VQAS >8.5 (sensitivity, 84.2%; specificity, 80.3%) and SQNLP <82.45% (sensitivity, 83.1%; specificity, 84.2%) were related to severe pneumonia, VQAS >9.5 (sensitivity, 93.3%; specificity, 86.5%) and SQNLP <81.1% (sensitivity, 86.5%; specificity, 86.7%) were predictive of ICU admission. Both consolidation and CPP were more commonly seen in patients with severe pneumonia than patients with nonsevere pneumonia (P = 0.197 for consolidation; P < 0.001 for CPP). Moreover, the presence of CPP showed high specificity (97.2%) for severe pneumonia. CONCLUSION: Both SQNLP and VQAS were significantly related to the clinical findings, highlighting their clinical utility in predicting severe pneumonia, ICU admission, length of hospital stay, and management of the disease. On the other hand, presence of CPP has high specificity for severe COVID-19 pneumonia.",2020,10.5152/dir.2020.20407,cross-sectional,diagnosis,CT,Lung
Volume-of-Interest Aware Deep Neural Networks for Rapid Chest CT-Based COVID-19 Patient Risk Assessment,"Since December 2019, the world has been devastated by the Coronavirus Disease 2019 (COVID-19) pandemic. Emergency Departments have been experiencing situations of urgency where clinical experts, without long experience and mature means in the fight against COVID-19, have to rapidly decide the most proper patient treatment. In this context, we introduce an artificially intelligent tool for effective and efficient Computed Tomography (CT)-based risk assessment to improve treatment and patient care. In this paper, we introduce a data-driven approach built on top of volume-of-interest aware deep neural networks for automatic COVID-19 patient risk assessment (discharged, hospitalized, intensive care unit) based on lung infection quantization through segmentation and, subsequently, CT classification. We tackle the high and varying dimensionality of the CT input by detecting and analyzing only a sub-volume of the CT, the Volume-of-Interest (VoI). Differently from recent strategies that consider infected CT slices without requiring any spatial coherency between them, or use the whole lung volume by applying abrupt and lossy volume down-sampling, we assess only the ""most infected volume"" composed of slices at its original spatial resolution. To achieve the above, we create, present and publish a new labeled and annotated CT dataset with 626 CT samples from COVID-19 patients. The comparison against such strategies proves the effectiveness of our VoI-based approach. We achieve remarkable performance on patient risk assessment evaluated on balanced data by reaching 88.88%, 89.77%, 94.73% and 88.88% accuracy, sensitivity, specificity and F1-score, respectively.",2021,10.3390/ijerph18062842,retrospective cohort,prognosis,CT,Lung
Volumetric segmentation of ground glass nodule based on 3D attentional cascaded residual U-Net and conditional random field,"BACKGROUND: Ground glass nodule (GGN) segmentation is one of the important and challenging tasks in diagnosing early-stage lung adenocarcinomas. Manually delineating of 3D GGN in a computed tomography (CT) image is a subjective, laborious, and tedious task, which presents poor repeatability. PURPOSE: To reduce the annotation burden and improve the segmentation performance, this study proposes a 3D deep learning-based volumetric segmentation model to segment the GGN in CT images. METHODS: A total of 379 GGNs were retrospectively collected from the public database, Shanghai Pulmonary Hospital (SHPH), and Fudan University Shanghai Cancer Center (FUSCC). First, a series of image preprocessing techniques involving image resampling, intensity normalization, 3D nodule patch cropping, and data augmentation, were adopted to generate the input images for the deep learning model by using CT scans. Then, a 3D attentional cascaded residual network (ACRU-Net) was proposed to develop the deep learning-based segmentation model by using the residual network and the atrous spatial pyramid pooling module. To improve the model performance, a voxel-based conditional random field (CRF) method was used to optimize the segmentation results. Finally, a balanced cross-entropy and Dice combined loss function was applied to train and build the segmentation model. RESULTS: Testing on SHPH and FUSCC datasets, the proposed method generates the Dice coefficients of 0.721 ± 0.167 and 0.733 ± 0.100, respectively, which are higher than those of 3D residual U-Net and ACRU-Net without CRF optimization. CONCLUSIONS: The results demonstrated that combining 3D ACRU-Net and CRF effectively improved the GGN segmentation performance. The proposed segmentation model may provide a potential tool to help the radiologist in the segmentation and diagnosis of 3D GGN.",2022,10.1002/mp.15423,cross-sectional,diagnosis,CT,Lung
Voxel-wise supervised analysis of tumors with multimodal engineered features to highlight interpretable biological patterns,"BACKGROUND: Translation of predictive and prognostic image-based learning models to clinical applications is challenging due in part to their lack of interpretability. Some deep-learning-based methods provide information about the regions driving the model output. Yet, due to the high-level abstraction of deep features, these methods do not completely solve the interpretation challenge. In addition, low sample size cohorts can lead to instabilities and suboptimal convergence for models involving a large number of parameters such as convolutional neural networks. PURPOSE: Here, we propose a method for designing radiomic models that combines the interpretability of handcrafted radiomics with a sub-regional analysis. MATERIALS AND METHODS: Our approach relies on voxel-wise engineered radiomic features with average global aggregation and logistic regression. The method is illustrated using a small dataset of 51 soft tissue sarcoma (STS) patients where the task is to predict the risk of lung metastasis occurrence during the follow-up period. RESULTS: Using positron emission tomography/computed tomography and two magnetic resonance imaging sequences separately to build two radiomic models, we show that our approach produces quantitative maps that highlight the signal that contributes to the decision within the tumor region of interest. In our STS example, the analysis of these maps identified two biological patterns that are consistent with STS grading systems and knowledge: necrosis development and glucose metabolism of the tumor. CONCLUSIONS: We demonstrate how that method makes it possible to spatially and quantitatively interpret radiomic models amenable to sub-regions identification and biological interpretation for patient stratification.",2022,10.1002/mp.15603,,,,
Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks,"Owing the epidemic of the novel coronavirus disease 2019 (COVID-19), chest X-ray computed tomography imaging is being used for effectively screening COVID-19 patients. The development of computer-aided systems based on deep neural networks (DNNs) has become an advanced open source to rapidly and accurately detect COVID-19 cases because the need for expert radiologists, who are limited in number, forms a bottleneck for screening. However, thus far, the vulnerability of DNN-based systems has been poorly evaluated, although realistic and high-risk attacks using universal adversarial perturbation (UAP), a single (input image agnostic) perturbation that can induce DNN failure in most classification tasks, are available. Thus, we focus on representative DNN models for detecting COVID-19 cases from chest X-ray images and evaluate their vulnerability to UAPs. We consider non-targeted UAPs, which cause a task failure, resulting in an input being assigned an incorrect label, and targeted UAPs, which cause the DNN to classify an input into a specific class. The results demonstrate that the models are vulnerable to non-targeted and targeted UAPs, even in the case of small UAPs. In particular, the 2% norm of the UAPs to the average norm of an image in the image dataset achieves >85% and >90% success rates for the non-targeted and targeted attacks, respectively. Owing to the non-targeted UAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The targeted UAPs allow the DNN models to classify most chest X-ray images into a specified target class. The results indicate that careful consideration is required in practical applications of DNNs to COVID-19 diagnosis; in particular, they emphasize the need for strategies to address security concerns. As an example, we show that iterative fine-tuning of DNN models using UAPs improves the robustness of DNN models against UAPs.",2020,10.1371/journal.pone.0243963,cross-sectional,diagnosis,CXR,Lung
Vulture-Based AdaBoost-Feedforward Neural Frame Work for COVID-19 Prediction and Severity Analysis System,"In today's scenario, many scientists and medical researchers have been involved in deep research for discovering the desired medicine to reduce the spread of COVID-19 disease. However, still, it is not the end. Hence, predicting the COVID possibility in an early stage is the most required matter to reduce the death risks. Therefore, many researchers have focused on designing an early prediction mechanism in the basis of deep learning (DL), machine learning (Ml), etc., on detecting the COVID virus and severity in the human body in an earlier stage. However, the complexity of X-ray images has made it difficult to attain the finest prediction accuracy. Hence, the present research work has aimed to develop a novel Vulture Based Adaboost-Feedforward Neural (VbAFN) scheme to forecast the COVID-19 severity early. Here, the chest X-ray images were employed to identify the COVID risk feature in humans. The preprocessing function is done in the initial phase; the error-free data is imported to the classification layer for the feature extraction and segmentation process. This investigation aims to track and segment the affected parts from the trained X-ray images by the vulture fitness and to segment them with a good exactness rate. Subsequently, the designed model has gained a better segmentation accuracy of 99.9% and a lower error rate of 0.0145, which is better than other compared models. Hence, this proposed model in medical applications will offer the finest results.",2022,10.1007/s12539-022-00505-3,cross-sectional,diagnosis,CXR,Lung
"Water-Triggered, Irreversible Conformational Change of SARS-CoV-2 Main Protease on Passing from the Solid State to Aqueous Solution","The main protease from SARS-CoV-2 is a homodimer. Yet, a recent 0.1-ms-long molecular dynamics simulation performed by D. E. Shaw's research group shows that it readily undergoes a symmetry-breaking event on passing from the solid state to aqueous solution. As a result, the subunits present distinct conformations of the binding pocket. By analyzing this long simulation, we uncover a previously unrecognized role of water molecules in triggering the transition. Interestingly, each subunit presents a different collection of long-lived water molecules. Enhanced sampling simulations performed here, along with machine learning approaches, further establish that the transition to the asymmetric state is essentially irreversible.",2021,10.1021/jacs.1c05301,,,,
Wavelet decomposition facilitates training on small datasets for medical image classification by deep learning,"The adoption of low-dose computed tomography (LDCT) as the standard of care for lung cancer screening results in decreased mortality rates in high-risk population while increasing false-positive rate. Convolutional neural networks provide an ideal opportunity to improve malignant nodule detection; however, due to the lack of large adjudicated medical datasets these networks suffer from poor generalizability and overfitting. Using computed tomography images of the thorax from the National Lung Screening Trial (NLST), we compared discrete wavelet transforms (DWTs) against convolutional layers found in a CNN in order to evaluate their ability to classify suspicious lung nodules as either malignant or benign. We explored the use of the DWT as an alternative to the convolutional operations within CNNs in order to decrease the number of parameters to be estimated during training and reduce the risk of overfitting. We found that multi-level DWT performed better than convolutional layers when multiple kernel resolutions were utilized, yielding areas under the receiver-operating curve (AUC) of 94% and 92%, respectively. Furthermore, we found that multi-level DWT reduced the number of network parameters requiring evaluation when compared to a CNN and had a substantially faster convergence rate. We conclude that utilizing multi-level DWT composition in place of early convolutional layers within a DNN may improve for image classification in data-limited domains.",2021,10.1007/s00418-020-01961-y,cross-sectional,diagnosis,CT,Lung
Weakly Supervised Deep Learning for Whole Slide Lung Cancer Image Analysis,"Histopathology image analysis serves as the gold standard for cancer diagnosis. Efficient and precise diagnosis is quite critical for the subsequent therapeutic treatment of patients. So far, computer-aided diagnosis has not been widely applied in pathological field yet as currently well-addressed tasks are only the tip of the iceberg. Whole slide image (WSI) classification is a quite challenging problem. First, the scarcity of annotations heavily impedes the pace of developing effective approaches. Pixelwise delineated annotations on WSIs are time consuming and tedious, which poses difficulties in building a large-scale training dataset. In addition, a variety of heterogeneous patterns of tumor existing in high magnification field are actually the major obstacle. Furthermore, a gigapixel scale WSI cannot be directly analyzed due to the immeasurable computational cost. How to design the weakly supervised learning methods to maximize the use of available WSI-level labels that can be readily obtained in clinical practice is quite appealing. To overcome these challenges, we present a weakly supervised approach in this article for fast and effective classification on the whole slide lung cancer images. Our method first takes advantage of a patch-based fully convolutional network (FCN) to retrieve discriminative blocks and provides representative deep features with high efficiency. Then, different context-aware block selection and feature aggregation strategies are explored to generate globally holistic WSI descriptor which is ultimately fed into a random forest (RF) classifier for the image-level prediction. To the best of our knowledge, this is the first study to exploit the potential of image-level labels along with some coarse annotations for weakly supervised learning. A large-scale lung cancer WSI dataset is constructed in this article for evaluation, which validates the effectiveness and feasibility of the proposed method. Extensive experiments demonstrate the superior performance of our method that surpasses the state-of-the-art approaches by a significant margin with an accuracy of 97.3%. In addition, our method also achieves the best performance on the public lung cancer WSIs dataset from The Cancer Genome Atlas (TCGA). We highlight that a small number of coarse annotations can contribute to further accuracy improvement. We believe that weakly supervised learning methods have great potential to assist pathologists in histology image diagnosis in the near future.",2020,10.1109/tcyb.2019.2935141,,,,
Weakly supervised learning for classification of lung cytological images using attention-based multiple instance learning,"In cytological examination, suspicious cells are evaluated regarding malignancy and cancer type. To assist this, we previously proposed an automated method based on supervised learning that classifies cells in lung cytological images as benign or malignant. However, it is often difficult to label all cells. In this study, we developed a weakly supervised method for the classification of benign and malignant lung cells in cytological images using attention-based deep multiple instance learning (AD MIL). Images of lung cytological specimens were divided into small patch images and stored in bags. Each bag was then labeled as benign or malignant, and classification was conducted using AD MIL. The distribution of attention weights was also calculated as a color map to confirm the presence of malignant cells in the image. AD MIL using the AlexNet-like convolutional neural network model showed the best classification performance, with an accuracy of 0.916, which was better than that of supervised learning. In addition, an attention map of the entire image based on the attention weight allowed AD MIL to focus on most malignant cells. Our weakly supervised method automatically classifies cytological images with acceptable accuracy based on supervised learning without complex annotations.",2021,10.1038/s41598-021-99246-4,,,,
Weakly unsupervised conditional generative adversarial network for image-based prognostic prediction for COVID-19 patients based on chest CT,"Because of the rapid spread and wide range of the clinical manifestations of the coronavirus disease 2019 (COVID-19), fast and accurate estimation of the disease progression and mortality is vital for the management of the patients. Currently available image-based prognostic predictors for patients with COVID-19 are largely limited to semi-automated schemes with manually designed features and supervised learning, and the survival analysis is largely limited to logistic regression. We developed a weakly unsupervised conditional generative adversarial network, called pix2surv, which can be trained to estimate the time-to-event information for survival analysis directly from the chest computed tomography (CT) images of a patient. We show that the performance of pix2surv based on CT images significantly outperforms those of existing laboratory tests and image-based visual and quantitative predictors in estimating the disease progression and mortality of COVID-19 patients. Thus, pix2surv is a promising approach for performing image-based prognostic predictions.",2021,10.1016/j.media.2021.102159,cross-sectional,diagnosis,CT,Lung
Weakly-supervised lesion analysis with a CNN-based framework for COVID-19,"Objective.Lesions of COVID-19 can be clearly visualized using chest CT images, and hence provide valuable evidence for clinicians when making a diagnosis. However, due to the variety of COVID-19 lesions and the complexity of the manual delineation procedure, automatic analysis of lesions with unknown and diverse types from a CT image remains a challenging task. In this paper we propose a weakly-supervised framework for this task requiring only a series of normal and abnormal CT images without the need for annotations of the specific locations and types of lesions.Approach.A deep learning-based diagnosis branch is employed for classification of the CT image and then a lesion identification branch is leveraged to capture multiple types of lesions.Main Results.Our framework is verified on publicly available datasets and CT data collected from 13 patients of the First Affiliated Hospital of Shantou University Medical College, China. The results show that the proposed framework can achieve state-of-the-art diagnosis prediction, and the extracted lesion features are capable of distinguishing between lesions showing ground glass opacity and consolidation.Significance.The proposed approach integrates COVID-19 positive diagnosis and lesion analysis into a unified framework without extra pixel-wise supervision. Further exploration also demonstrates that this framework has the potential to discover lesion types that have not been reported and can potentially be generalized to lesion detection of other chest-based diseases.",2021,10.1088/1361-6560/ac4316,cross-sectional,diagnosis,CT,Lung
Whole-organ analysis of TGF-β-mediated remodelling of the tumour microenvironment by tissue clearing,"Tissue clearing is one of the most powerful strategies for a comprehensive analysis of disease progression. Here, we established an integrated pipeline that combines tissue clearing, 3D imaging, and machine learning and applied to a mouse tumour model of experimental lung metastasis using human lung adenocarcinoma A549 cells. This pipeline provided the spatial information of the tumour microenvironment. We further explored the role of transforming growth factor-β (TGF-β) in cancer metastasis. TGF-β-stimulated cancer cells enhanced metastatic colonization of unstimulated-cancer cells in vivo when both cells were mixed. RNA-sequencing analysis showed that expression of the genes related to coagulation and inflammation were up-regulated in TGF-β-stimulated cancer cells. Further, whole-organ analysis revealed accumulation of platelets or macrophages with TGF-β-stimulated cancer cells, suggesting that TGF-β might promote remodelling of the tumour microenvironment, enhancing the colonization of cancer cells. Hence, our integrated pipeline for 3D profiling will help the understanding of the tumour microenvironment.",2021,10.1038/s42003-021-01786-y,,,,
xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography,"Objective: Since its outbreak, the rapid spread of COrona VIrus Disease 2019 (COVID-19) across the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is imperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain the spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19 screening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several researchers have demonstrated the use of Deep Learning (DL) methods on chest radiography (such as X-ray and CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context due to their inherent image-specific inductive bias. Methods: Motivated by this, in this work, we propose the use of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray and CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity. Furthermore, we show that the features learned by our transformer networks are explainable. Results: We demonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on meaningful regions in the images for detection (as confirmed by Radiologists), aiding not only in accurate diagnosis of COVID-19 but also in localization of the infected area. The code for our implementation can be found here - https://github.com/arnabkmondal/xViTCOS. Conclusion: The proposed method will help in timely identification of COVID-19 and efficient utilization of limited resources.",2022,10.1109/jtehm.2021.3134096,cross-sectional,diagnosis,CXR - CT,Lung
μHEM for identification of differentially expressed miRNAs using hypercuboid equivalence partition matrix,"BACKGROUND: The miRNAs, a class of short approximately 22-nucleotide non-coding RNAs, often act post-transcriptionally to inhibit mRNA expression. In effect, they control gene expression by targeting mRNA. They also help in carrying out normal functioning of a cell as they play an important role in various cellular processes. However, dysregulation of miRNAs is found to be a major cause of a disease. It has been demonstrated that miRNA expression is altered in many human cancers, suggesting that they may play an important role as disease biomarkers. Multiple reports have also noted the utility of miRNAs for the diagnosis of cancer. Among the large number of miRNAs present in a microarray data, a modest number might be sufficient to classify human cancers. Hence, the identification of differentially expressed miRNAs is an important problem particularly for the data sets with large number of miRNAs and small number of samples. RESULTS: In this regard, a new miRNA selection algorithm, called μHEM, is presented based on rough hypercuboid approach. It selects a set of miRNAs from a microarray data by maximizing both relevance and significance of the selected miRNAs. The degree of dependency of sample categories on miRNAs is defined, based on the concept of hypercuboid equivalence partition matrix, to measure both relevance and significance of miRNAs. The effectiveness of the new approach is demonstrated on six publicly available miRNA expression data sets using support vector machine. The.632+ bootstrap error estimate is used to minimize the variability and biasedness of the derived results. CONCLUSIONS: An important finding is that the μHEM algorithm achieves lowest B.632+ error rate of support vector machine with a reduced set of differentially expressed miRNAs on four expression data sets compare to some existing machine learning and statistical methods, while for other two data sets, the error rate of the μHEM algorithm is comparable with the existing techniques. The results on several microarray data sets demonstrate that the proposed method can bring a remarkable improvement on miRNA selection problem. The method is a potentially useful tool for exploration of miRNA expression data and identification of differentially expressed miRNAs worth further investigation.",2013,10.1186/1471-2105-14-266,,,,
Fully automatic segmentation of right and left ventricle on short-axis cardiac MRI images,"Cardiac magnetic resonance imaging (CMR) is a widely used non-invasive imaging modality for evaluating cardiovascular diseases. CMR is the gold standard method for left and right ventricular functional assessment due to its ability to characterize myocardial structure and function and low intra- and inter-observer variability. However the post-processing segmentation during the functional evaluation is time-consuming and challenging. A fully automated segmentation method can assist the experts; therefore, they can do more efficient work. In this paper, a regression-based fully automated method is presented for the right- and left ventricle segmentation. For training and evaluation, our dataset contained MRI short-axis scans of 5570 patients, who underwent CMR examinations at Heart and Vascular Center, Semmelweis University Budapest. Our approach is novel and after training the state-of-the-art algorithm on our dataset, our algorithm proved to be superior on both of the ventricles. The evaluation metrics were the Dice index, Hausdorff distance and volume related parameters. We have achieved average Dice index for the left endocardium: 0.927, left epicardium: 0.940 and right endocardium: 0.873 on our dataset. We have also compared the performance of the algorithm to the human-level segmentation on both ventricles and it is similar to experienced readers for the left, and comparable for the right ventricle. We also evaluated the proposed algorithm on the ACDC dataset, which is publicly available, with and without transfer learning. The results on ACDC were also satisfying and similar to human observers. Our method is lightweight, fast to train and does not require more than 2 GB GPU memory for execution and training.",2020,10.1016/j.compmedimag.2020.101786,cross-sectional,diagnosis,Cardiac MRI,Heart
"Machine learning to predict the long-term risk of myocardial infarction and cardiac death based on clinical risk, coronary calcium, and epicardial adipose tissue: a prospective study","AIMS: Our aim was to evaluate the performance of machine learning (ML), integrating clinical parameters with coronary artery calcium (CAC), and automated epicardial adipose tissue (EAT) quantification, for the prediction of long-term risk of myocardial infarction (MI) and cardiac death in asymptomatic subjects. METHODS AND RESULTS: Our study included 1912 asymptomatic subjects [1117 (58.4%) male, age: 55.8 ± 9.1 years] from the prospective EISNER trial with long-term follow-up after CAC scoring. EAT volume and density were quantified using a fully automated deep learning method. ML extreme gradient boosting was trained using clinical co-variates, plasma lipid panel measurements, risk factors, CAC, aortic calcium, and automated EAT measures, and validated using repeated 10-fold cross validation. During mean follow-up of 14.5 ± 2 years, 76 events of MI and/or cardiac death occurred. ML obtained a significantly higher AUC than atherosclerotic cardiovascular disease (ASCVD) risk and CAC score for predicting events (ML: 0.82; ASCVD: 0.77; CAC: 0.77, P < 0.05 for all). Subjects with a higher ML score (by Youden's index) had high hazard of suffering events (HR: 10.38, P < 0.001); the relationships persisted in multivariable analysis including ASCVD-risk and CAC measures (HR: 2.94, P = 0.005). Age, ASCVD-risk, and CAC were prognostically important for both genders. Systolic blood pressure was more important than cholesterol in women, and the opposite in men. CONCLUSIONS: In this prospective study, machine learning used to integrate clinical and quantitative imaging-based variables significantly improves prediction of MI and cardiac death compared with standard clinical risk assessment. Following further validation, such a personalized paradigm could potentially be used to improve cardiovascular risk assessment.",2020,10.1093/cvr/cvz321,cross-sectional,prognosis,CT Angiography,Heart